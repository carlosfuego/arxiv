[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05347v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Neural Inverse Rendering from Propagating Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Inverse Rendering from Propagating Light"
                },
                "summary": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes."
                },
                "authors": [
                    {
                        "name": "Anagh Malik"
                    },
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Andrew Xie"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "David B. Lindell"
                    }
                ],
                "author_detail": {
                    "name": "David B. Lindell"
                },
                "author": "David B. Lindell",
                "arxiv_comment": "Website: https://anaghmalik.com/InvProp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05071v1",
                "updated": "2025-06-05T14:19:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Hierarchy Design for Caching Middleware in the Age of NVM"
                },
                "summary": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    }
                ],
                "author_detail": {
                    "name": "Jenny Lam"
                },
                "author": "Jenny Lam",
                "arxiv_doi": "10.1109/ICDE.2018.00155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE.2018.00155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version appeared in the IEEE 34th International Conference\n  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:\n  10.1109/ICDE.2018.00155",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v2",
                "updated": "2025-06-05T13:38:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    38,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v2",
                "updated": "2025-06-05T13:20:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04920v1",
                "updated": "2025-06-05T11:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T11:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive."
                },
                "authors": [
                    {
                        "name": "Junior Cedric Tonga"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Preprint, in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04844v1",
                "updated": "2025-06-05T10:11:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments"
                },
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors."
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-García"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04826v1",
                "updated": "2025-06-05T09:49:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T09:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation"
                },
                "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture."
                },
                "authors": [
                    {
                        "name": "Konstantinos Giotis"
                    },
                    {
                        "name": "Dimitrios Stefas"
                    },
                    {
                        "name": "Yanis Agha"
                    },
                    {
                        "name": "Hans Höft"
                    },
                    {
                        "name": "Xavier Duten"
                    },
                    {
                        "name": "Panagiotis Svarnas"
                    },
                    {
                        "name": "Guillaume Lombardi"
                    },
                    {
                        "name": "Kristaq Gazeli"
                    }
                ],
                "author_detail": {
                    "name": "Kristaq Gazeli"
                },
                "arxiv_affiliation": "LSPM",
                "author": "Kristaq Gazeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04642v1",
                "updated": "2025-06-05T05:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T05:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering"
                },
                "summary": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts."
                },
                "authors": [
                    {
                        "name": "Vinay Joshi"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "ACL-2025 industry-track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v3",
                "updated": "2025-06-05T04:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    4,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v1",
                "updated": "2025-06-05T03:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiong Wu"
                },
                "author": "Qiong Wu",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v2",
                "updated": "2025-06-05T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    27,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v2",
                "updated": "2025-06-04T23:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    47,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "arxiv_comment": "This paper was intended to be a v2 version of my previous paper\n  (arXiv:2409.17566), but it was submitted as a new paper by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v3",
                "updated": "2025-06-04T22:37:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    22,
                    37,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v2",
                "updated": "2025-06-04T18:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    10,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03854v1",
                "updated": "2025-06-04T11:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks"
                },
                "summary": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases."
                },
                "authors": [
                    {
                        "name": "Emmanouil Anagnostakis"
                    },
                    {
                        "name": "Polyvios Pratikakis"
                    }
                ],
                "author_detail": {
                    "name": "Polyvios Pratikakis"
                },
                "author": "Polyvios Pratikakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03762v1",
                "updated": "2025-06-04T09:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T09:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Yifeng Gu"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Jianxiu Jin"
                    },
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03700v1",
                "updated": "2025-06-04T08:32:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T08:32:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "arxiv_comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01969v2",
                "updated": "2025-06-04T03:20:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    3,
                    20,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-13T17:45:34Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    45,
                    34,
                    1,
                    133,
                    0
                ],
                "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs"
                },
                "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP."
                },
                "authors": [
                    {
                        "name": "Pengcuo Dege"
                    },
                    {
                        "name": "Qiuming Luo"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Chang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Chang Kong"
                },
                "author": "Chang Kong",
                "arxiv_comment": "15 pages, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v1",
                "updated": "2025-06-03T18:35:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings.APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings.APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03275v1",
                "updated": "2025-06-03T18:03:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:03:32Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas"
                },
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."
                },
                "authors": [
                    {
                        "name": "Austin Silveria"
                    },
                    {
                        "name": "Soham V. Govande"
                    },
                    {
                        "name": "Daniel Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Y. Fu"
                },
                "author": "Daniel Y. Fu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v1",
                "updated": "2025-06-03T13:19:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02671v1",
                "updated": "2025-06-03T09:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet"
                },
                "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v1",
                "updated": "2025-06-03T08:51:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02523v1",
                "updated": "2025-06-03T06:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators."
                },
                "authors": [
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v2",
                "updated": "2025-06-03T03:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    32,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v2",
                "updated": "2025-06-03T01:51:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    51,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3716368.3735166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716368.3735166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 14 figures",
                "arxiv_journal_ref": "Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v2",
                "updated": "2025-06-02T19:27:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    19,
                    27,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01928v1",
                "updated": "2025-06-02T17:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v8",
                "updated": "2025-06-02T17:46:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    46,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01880v1",
                "updated": "2025-06-02T17:09:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:09:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning"
                },
                "summary": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto."
                },
                "authors": [
                    {
                        "name": "Djamel Rassem Lamouri"
                    },
                    {
                        "name": "Iheb Nassim Aouadj"
                    },
                    {
                        "name": "Smail Kourta"
                    },
                    {
                        "name": "Riyadh Baghdadi"
                    }
                ],
                "author_detail": {
                    "name": "Riyadh Baghdadi"
                },
                "author": "Riyadh Baghdadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01827v1",
                "updated": "2025-06-02T16:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T16:12:22Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "title": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts"
                },
                "summary": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations."
                },
                "authors": [
                    {
                        "name": "Spencer Banasik"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Banasik"
                },
                "author": "Spencer Banasik",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01643v1",
                "updated": "2025-06-02T13:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T13:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "title": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker"
                },
                "summary": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips."
                },
                "authors": [
                    {
                        "name": "Dongwei Xuan"
                    },
                    {
                        "name": "Ruiyang Zhang"
                    },
                    {
                        "name": "Jiajun Qin"
                    },
                    {
                        "name": "Hao Han"
                    },
                    {
                        "name": "Xinyu Bin"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Jianbei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Anqing Wang"
                    },
                    {
                        "name": "Aodong Song"
                    },
                    {
                        "name": "Xiangming Sun"
                    },
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Lailin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lailin Xu"
                },
                "author": "Lailin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01215v1",
                "updated": "2025-06-01T23:49:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers"
                },
                "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance."
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01151v1",
                "updated": "2025-06-01T20:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T20:05:30Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron."
                },
                "authors": [
                    {
                        "name": "Xintong Sun"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Minghao Tian"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "arxiv_comment": "ICML2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v3",
                "updated": "2025-06-01T16:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    16,
                    0,
                    34,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v2",
                "updated": "2025-06-01T10:36:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    10,
                    36,
                    7,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v1",
                "updated": "2025-05-31T23:16:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17491v2",
                "updated": "2025-05-31T23:01:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    1,
                    0,
                    5,
                    151,
                    0
                ],
                "published": "2024-07-04T02:35:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    2,
                    35,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting"
                },
                "summary": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness."
                },
                "authors": [
                    {
                        "name": "Changdae Oh"
                    },
                    {
                        "name": "Gyeongdeok Seo"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Hosik Choi"
                    },
                    {
                        "name": "Jiyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "Extended work from the CVPR'23 paper: arxiv:2303.14773; This paper\n  has been submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v2",
                "updated": "2025-05-31T22:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    22,
                    12,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v6",
                "updated": "2025-05-31T17:58:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    17,
                    58,
                    24,
                    5,
                    151,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02038v1",
                "updated": "2025-05-31T06:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment"
                },
                "summary": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme."
                },
                "authors": [
                    {
                        "name": "Anum Nawaz"
                    },
                    {
                        "name": "Hafiz Humza Mahmood Ramzan"
                    },
                    {
                        "name": "Xianjia Yu"
                    },
                    {
                        "name": "Zhuo Zou"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00420v1",
                "updated": "2025-05-31T06:50:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "title": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks"
                },
                "summary": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods."
                },
                "authors": [
                    {
                        "name": "Miao Ye"
                    },
                    {
                        "name": "Suxiao Wang"
                    },
                    {
                        "name": "Jiaguang Han"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Peng Wen"
                    },
                    {
                        "name": "Jing Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jing Cui"
                },
                "author": "Jing Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v1",
                "updated": "2025-05-31T06:10:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v4",
                "updated": "2025-05-31T04:45:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    45,
                    23,
                    5,
                    151,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v1",
                "updated": "2025-05-31T04:27:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Deep-Learning-Driven Prefetching for Far Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-Learning-Driven Prefetching for Far Memory"
                },
                "summary": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v1",
                "updated": "2025-05-31T00:52:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaños"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustín Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v2",
                "updated": "2025-05-30T11:43:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    43,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24221v1",
                "updated": "2025-05-30T05:17:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T05:17:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management"
                },
                "summary": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Wenzhe Zhu"
                    },
                    {
                        "name": "Yongkun Li"
                    },
                    {
                        "name": "Yinlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinlong Xu"
                },
                "author": "Yinlong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v1",
                "updated": "2025-05-30T00:46:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v3",
                "updated": "2025-05-30T00:36:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    36,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23970v1",
                "updated": "2025-05-29T19:52:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T19:52:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving"
                },
                "summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23938v1",
                "updated": "2025-05-29T18:41:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T18:41:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Forensic Investigation of the ChatGPT Windows Application"
                },
                "summary": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics."
                },
                "authors": [
                    {
                        "name": "Malithi Wanniarachchi Kankanamge"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Santiago Carmona"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05346v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets"
                },
                "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers."
                },
                "authors": [
                    {
                        "name": "Lei Hsiung"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yung-Chen Tang"
                    },
                    {
                        "name": "Linyue Song"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Yaoqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaoqing Yang"
                },
                "author": "Yaoqing Yang",
                "arxiv_comment": "Project Page: https://hsiung.cc/llm-similarity-risk/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05341v1",
                "updated": "2025-06-05T17:59:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:42Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    42,
                    3,
                    156,
                    0
                ],
                "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning"
                },
                "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility."
                },
                "authors": [
                    {
                        "name": "Xingjian Ran"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Mulin Yu"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Project Page: https://directlayout.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05336v1",
                "updated": "2025-06-05T17:59:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    29,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:29Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    29,
                    3,
                    156,
                    0
                ],
                "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing"
                },
                "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo."
                },
                "authors": [
                    {
                        "name": "Ghazi Shazan Ahmad"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Hanan Gani"
                    },
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "20 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05334v1",
                "updated": "2025-06-05T17:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    26,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:26Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    26,
                    3,
                    156,
                    0
                ],
                "title": "Search Arena: Analyzing Search-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search Arena: Analyzing Search-Augmented LLMs"
                },
                "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena."
                },
                "authors": [
                    {
                        "name": "Mihran Miroyan"
                    },
                    {
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "name": "Logan King"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Xinyan Hu"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Narges Norouzi"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "arxiv_comment": "Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05333v1",
                "updated": "2025-06-05T17:59:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "Kinetics: Rethinking Test-Time Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinetics: Rethinking Test-Time Scaling Laws"
                },
                "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05331v1",
                "updated": "2025-06-05T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    2,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    2,
                    3,
                    156,
                    0
                ],
                "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT"
                },
                "authors": [
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Code is released at https://github.com/xinyan-cxy/MINT-CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05325v1",
                "updated": "2025-06-05T17:58:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    58,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:58:09Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    58,
                    9,
                    3,
                    156,
                    0
                ],
                "title": "Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via\n  Latent Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via\n  Latent Alignment"
                },
                "summary": "Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels."
                },
                "authors": [
                    {
                        "name": "Yingshuai Ji"
                    },
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Matthew Toole"
                    },
                    {
                        "name": "James McKenzie"
                    },
                    {
                        "name": "Xiaolong Liu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05321v1",
                "updated": "2025-06-05T17:57:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    57,
                    11,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:57:11Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    57,
                    11,
                    3,
                    156,
                    0
                ],
                "title": "LSM-2: Learning from Incomplete Wearable Sensor Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-2: Learning from Incomplete Wearable Sensor Data"
                },
                "summary": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications."
                },
                "authors": [
                    {
                        "name": "Maxwell A. Xu"
                    },
                    {
                        "name": "Girish Narayanswamy"
                    },
                    {
                        "name": "Kumar Ayush"
                    },
                    {
                        "name": "Dimitris Spathis"
                    },
                    {
                        "name": "Shun Liao"
                    },
                    {
                        "name": "Shyam A. Tailor"
                    },
                    {
                        "name": "Ahmed Metwally"
                    },
                    {
                        "name": "A. Ali Heydari"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Jake Garrison"
                    },
                    {
                        "name": "Samy Abdel-Ghaffar"
                    },
                    {
                        "name": "Xuhai Xu"
                    },
                    {
                        "name": "Ken Gu"
                    },
                    {
                        "name": "Jacob Sunshine"
                    },
                    {
                        "name": "Ming-Zher Poh"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    },
                    {
                        "name": "Pushmeet Kohli"
                    },
                    {
                        "name": "Mark Malhotra"
                    },
                    {
                        "name": "Shwetak Patel"
                    },
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "James M. Rehg"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Daniel McDuff"
                    }
                ],
                "author_detail": {
                    "name": "Daniel McDuff"
                },
                "author": "Daniel McDuff",
                "arxiv_comment": "Xu and Narayanswamy are co-first authors. McDuff and Liu are co-last\n  authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05320v1",
                "updated": "2025-06-05T17:57:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    57,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:57:08Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    57,
                    8,
                    3,
                    156,
                    0
                ],
                "title": "Generalizable, real-time neural decoding with hybrid state-space models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable, real-time neural decoding with hybrid state-space models"
                },
                "summary": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications."
                },
                "authors": [
                    {
                        "name": "Avery Hee-Woon Ryoo"
                    },
                    {
                        "name": "Nanda H. Krishna"
                    },
                    {
                        "name": "Ximeng Mao"
                    },
                    {
                        "name": "Mehdi Azabou"
                    },
                    {
                        "name": "Eva L. Dyer"
                    },
                    {
                        "name": "Matthew G. Perich"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Lajoie"
                },
                "author": "Guillaume Lajoie",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05316v1",
                "updated": "2025-06-05T17:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    43,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    43,
                    3,
                    156,
                    0
                ],
                "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay"
                },
                "summary": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05314v1",
                "updated": "2025-06-05T17:55:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    23,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:55:23Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    23,
                    3,
                    156,
                    0
                ],
                "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility."
                },
                "authors": [
                    {
                        "name": "Taha Entesari"
                    },
                    {
                        "name": "Arman Hatami"
                    },
                    {
                        "name": "Rinat Khaziev"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Mahyar Fazlyab"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Fazlyab"
                },
                "author": "Mahyar Fazlyab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v2",
                "updated": "2025-06-05T17:55:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    7,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?"
                },
                "summary": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advances in Large\nLanguage Models (LLMs) and Neural Machine Translation have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates current\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\nlimitations in LRL translation capability. We also explore alternative data\nsources, including news articles and bilingual dictionaries, and demonstrate\nhow knowledge distillation from large pre-trained teacher models can\nsignificantly improve the performance of small LLMs on LRL translation tasks.\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\ndifferent fine-tuning configurations, providing practical insights on optimal\ndata scale, training efficiency, and the preservation of generalization\ncapabilities of models under study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advances in Large\nLanguage Models (LLMs) and Neural Machine Translation have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates current\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\nlimitations in LRL translation capability. We also explore alternative data\nsources, including news articles and bilingual dictionaries, and demonstrate\nhow knowledge distillation from large pre-trained teacher models can\nsignificantly improve the performance of small LLMs on LRL translation tasks.\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\ndifferent fine-tuning configurations, providing practical insights on optimal\ndata scale, training efficiency, and the preservation of generalization\ncapabilities of models under study."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05309v1",
                "updated": "2025-06-05T17:53:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    53,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:53:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    53,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games"
                },
                "summary": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated."
                },
                "authors": [
                    {
                        "name": "Niv Eckhaus"
                    },
                    {
                        "name": "Uri Berger"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05305v1",
                "updated": "2025-06-05T17:52:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    52,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:52:30Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    52,
                    30,
                    3,
                    156,
                    0
                ],
                "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProRefine: Inference-time Prompt Refinement with Textual Feedback"
                },
                "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI."
                },
                "authors": [
                    {
                        "name": "Deepak Pandita"
                    },
                    {
                        "name": "Tharindu Cyril Weerasooriya"
                    },
                    {
                        "name": "Ankit Parag Shah"
                    },
                    {
                        "name": "Christopher M. Homan"
                    },
                    {
                        "name": "Wei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wei"
                },
                "author": "Wei Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05302v1",
                "updated": "2025-06-05T17:51:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    39,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:51:39Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    39,
                    3,
                    156,
                    0
                ],
                "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos"
                },
                "summary": "We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Xinyu Wei"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Tingwei Chen"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "19 pages, 13 figures, Website: https://Perceive-Anything.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05301v1",
                "updated": "2025-06-05T17:51:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:51:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training"
                },
                "summary": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step."
                },
                "authors": [
                    {
                        "name": "Jianyi Wang"
                    },
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Zongsheng Yue"
                    },
                    {
                        "name": "Shangchen Zhou"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Draft Ver. Project page: https://iceclear.github.io/projects/seedvr2/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05300v1",
                "updated": "2025-06-05T17:50:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    50,
                    32,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:50:32Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    50,
                    32,
                    3,
                    156,
                    0
                ],
                "title": "Power Law Guided Dynamic Sifting for Efficient Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power Law Guided Dynamic Sifting for Efficient Attention"
                },
                "summary": "Efficient inference on GPUs using large language models remains challenging\ndue to memory bandwidth limitations, particularly during data transfers between\nHigh Bandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory\noverhead but often rely on expensive top-$k$ operations, which perform poorly\non GPUs. We propose SiftAttention, a novel approximate attention method that\nreplaces the top-$k$ step with a computationally efficient element-wise\nfiltering operation based on a threshold value. Our intuition for doing this is\nbased on our empirical observation that the $\\tau$-th quantile of attention\nscores follows a predictable power-law over sequential generation steps.\nExploiting this insight, our approach dynamically estimates a threshold value\nper prompt at each generation step. Only attention scores above this threshold\nand their corresponding value vectors are loaded/used to compute the attention\noutput, reducing data movement between HBM and SRAM. Our evaluation\ndemonstrates that SiftAttention preserves model quality better than existing\napproximate attention methods while reducing memory bandwidth usage when\nloading value vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference on GPUs using large language models remains challenging\ndue to memory bandwidth limitations, particularly during data transfers between\nHigh Bandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory\noverhead but often rely on expensive top-$k$ operations, which perform poorly\non GPUs. We propose SiftAttention, a novel approximate attention method that\nreplaces the top-$k$ step with a computationally efficient element-wise\nfiltering operation based on a threshold value. Our intuition for doing this is\nbased on our empirical observation that the $\\tau$-th quantile of attention\nscores follows a predictable power-law over sequential generation steps.\nExploiting this insight, our approach dynamically estimates a threshold value\nper prompt at each generation step. Only attention scores above this threshold\nand their corresponding value vectors are loaded/used to compute the attention\noutput, reducing data movement between HBM and SRAM. Our evaluation\ndemonstrates that SiftAttention preserves model quality better than existing\napproximate attention methods while reducing memory bandwidth usage when\nloading value vectors."
                },
                "authors": [
                    {
                        "name": "Nirav Koley"
                    },
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05295v1",
                "updated": "2025-06-05T17:48:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    19,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:48:19Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    19,
                    3,
                    156,
                    0
                ],
                "title": "Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms"
                },
                "summary": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods."
                },
                "authors": [
                    {
                        "name": "Baihe Huang"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Jiantao Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Jiao"
                },
                "author": "Jiantao Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15268v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15268v4",
                "updated": "2025-06-05T17:33:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    33,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-21T17:52:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "title": "Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios,\n  and Stress Testing for Financial Portfolios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios,\n  and Stress Testing for Financial Portfolios"
                },
                "summary": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The\nsolution remains valid under marginal asset distributions characterized by\nnotably different and varying degrees of serial correlation, non-stationarity,\nheavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence\nintervals remain analytically consistent at both the matrix level and the\npairwise cell level. Finally, NAbC maintains validity even when selected cells\nin the matrix are frozen for a given scenario or stress test, thus enabling\nflexible, granular, and realistic scenarios. NAbC stands alone in providing all\nof these capabilities simultaneously, and should prove to be a very useful\nmeans by which we can better understand and manage financial portfolios in our\nmultivariate world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The\nsolution remains valid under marginal asset distributions characterized by\nnotably different and varying degrees of serial correlation, non-stationarity,\nheavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence\nintervals remain analytically consistent at both the matrix level and the\npairwise cell level. Finally, NAbC maintains validity even when selected cells\nin the matrix are frozen for a given scenario or stress test, thus enabling\nflexible, granular, and realistic scenarios. NAbC stands alone in providing all\nof these capabilities simultaneously, and should prove to be a very useful\nmeans by which we can better understand and manage financial portfolios in our\nmultivariate world."
                },
                "authors": [
                    {
                        "name": "JD Opdyke"
                    }
                ],
                "author_detail": {
                    "name": "JD Opdyke"
                },
                "author": "JD Opdyke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15268v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15268v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05278v1",
                "updated": "2025-06-05T17:33:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    33,
                    2,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:33:02Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    33,
                    2,
                    3,
                    156,
                    0
                ],
                "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications."
                },
                "authors": [
                    {
                        "name": "Nan Huo"
                    },
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Ge Qu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Reynold Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Reynold Cheng"
                },
                "author": "Reynold Cheng",
                "arxiv_comment": "Accepted by ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05276v1",
                "updated": "2025-06-05T17:32:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    32,
                    0,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:32:00Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    32,
                    0,
                    3,
                    156,
                    0
                ],
                "title": "How to Unlock Time Series Editing? Diffusion-Driven Approach with\n  Multi-Grained Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Unlock Time Series Editing? Diffusion-Driven Approach with\n  Multi-Grained Control"
                },
                "summary": "Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation."
                },
                "authors": [
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chu Xin Cheng"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Shiwei Tong"
                    },
                    {
                        "name": "Zhaofeng Liu"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05274v1",
                "updated": "2025-06-05T17:31:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    31,
                    17,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:31:17Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    31,
                    17,
                    3,
                    156,
                    0
                ],
                "title": "From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos"
                },
                "summary": "Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82."
                },
                "authors": [
                    {
                        "name": "Animesh Gupta"
                    },
                    {
                        "name": "Jay Parmar"
                    },
                    {
                        "name": "Ishan Rajendrakumar Dave"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18959v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18959v4",
                "updated": "2025-06-05T17:25:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    25,
                    49,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information"
                },
                "summary": "Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/."
                },
                "authors": [
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Jithendaraa Subramanian"
                    },
                    {
                        "name": "Roland Riachi"
                    },
                    {
                        "name": "James Requeima"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "arxiv_comment": "ICML 2025. First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18959v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18959v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05265v1",
                "updated": "2025-06-05T17:24:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    24,
                    37,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:24:37Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    24,
                    37,
                    3,
                    156,
                    0
                ],
                "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams"
                },
                "summary": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination."
                },
                "authors": [
                    {
                        "name": "Mohammed Almutairi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Almutairi"
                },
                "author": "Mohammed Almutairi",
                "arxiv_doi": "10.1145/3699682.3727574",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3699682.3727574",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, UMAP 25, June 16_19, 2025, New York City, NY, USA",
                "arxiv_journal_ref": "ACM International Conference on User Modeling, Adaptation and\n  Personalization 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05260v1",
                "updated": "2025-06-05T17:21:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    21,
                    16,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:21:16Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    21,
                    16,
                    3,
                    156,
                    0
                ],
                "title": "LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs"
                },
                "summary": "Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Peixi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Peixi Peng"
                },
                "author": "Peixi Peng",
                "arxiv_comment": "Code: https://github.com/Wang-Xiaodong1899/LeanPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05256v2",
                "updated": "2025-06-06T02:38:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    38,
                    39,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T17:17:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    17,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties\n  Reinforcement Learning"
                },
                "summary": "Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost."
                },
                "authors": [
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Sang Truong"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Nick Haber"
                    }
                ],
                "author_detail": {
                    "name": "Nick Haber"
                },
                "author": "Nick Haber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00038v2",
                "updated": "2025-06-05T17:10:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    10,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-25T08:41:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "From Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors"
                },
                "summary": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi Yin"
                    },
                    {
                        "name": "Jiangyu Lei"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05250v1",
                "updated": "2025-06-05T17:10:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    10,
                    29,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:10:29Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    10,
                    29,
                    3,
                    156,
                    0
                ],
                "title": "Spatiotemporal Contrastive Learning for Cross-View Video Localization in\n  Unstructured Off-road Terrains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal Contrastive Learning for Cross-View Video Localization in\n  Unstructured Off-road Terrains"
                },
                "summary": "Robust cross-view 3-DoF localization in GPS-denied, off-road environments\nremains challenging due to (1) perceptual ambiguities from repetitive\nvegetation and unstructured terrain, and (2) seasonal shifts that significantly\nalter scene appearance, hindering alignment with outdated satellite imagery. To\naddress this, we introduce MoViX, a self-supervised cross-view video\nlocalization framework that learns viewpoint- and season-invariant\nrepresentations while preserving directional awareness essential for accurate\nlocalization. MoViX employs a pose-dependent positive sampling strategy to\nenhance directional discrimination and temporally aligned hard negative mining\nto discourage shortcut learning from seasonal cues. A motion-informed frame\nsampler selects spatially diverse frames, and a lightweight temporal aggregator\nemphasizes geometrically aligned observations while downweighting ambiguous\nones. At inference, MoViX runs within a Monte Carlo Localization framework,\nusing a learned cross-view matching module in place of handcrafted models.\nEntropy-guided temperature scaling enables robust multi-hypothesis tracking and\nconfident convergence under visual ambiguity. We evaluate MoViX on the\nTartanDrive 2.0 dataset, training on under 30 minutes of data and testing over\n12.29 km. Despite outdated satellite imagery, MoViX localizes within 25 meters\nof ground truth 93% of the time, and within 50 meters 100% of the time in\nunseen regions, outperforming state-of-the-art baselines without\nenvironment-specific tuning. We further demonstrate generalization on a\nreal-world off-road dataset from a geographically distinct site with a\ndifferent robot platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust cross-view 3-DoF localization in GPS-denied, off-road environments\nremains challenging due to (1) perceptual ambiguities from repetitive\nvegetation and unstructured terrain, and (2) seasonal shifts that significantly\nalter scene appearance, hindering alignment with outdated satellite imagery. To\naddress this, we introduce MoViX, a self-supervised cross-view video\nlocalization framework that learns viewpoint- and season-invariant\nrepresentations while preserving directional awareness essential for accurate\nlocalization. MoViX employs a pose-dependent positive sampling strategy to\nenhance directional discrimination and temporally aligned hard negative mining\nto discourage shortcut learning from seasonal cues. A motion-informed frame\nsampler selects spatially diverse frames, and a lightweight temporal aggregator\nemphasizes geometrically aligned observations while downweighting ambiguous\nones. At inference, MoViX runs within a Monte Carlo Localization framework,\nusing a learned cross-view matching module in place of handcrafted models.\nEntropy-guided temperature scaling enables robust multi-hypothesis tracking and\nconfident convergence under visual ambiguity. We evaluate MoViX on the\nTartanDrive 2.0 dataset, training on under 30 minutes of data and testing over\n12.29 km. Despite outdated satellite imagery, MoViX localizes within 25 meters\nof ground truth 93% of the time, and within 50 meters 100% of the time in\nunseen regions, outperforming state-of-the-art baselines without\nenvironment-specific tuning. We further demonstrate generalization on a\nreal-world off-road dataset from a geographically distinct site with a\ndifferent robot platform."
                },
                "authors": [
                    {
                        "name": "Zhiyun Deng"
                    },
                    {
                        "name": "Dongmyeong Lee"
                    },
                    {
                        "name": "Amanda Adkins"
                    },
                    {
                        "name": "Jesse Quattrociocchi"
                    },
                    {
                        "name": "Christian Ellis"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04075v2",
                "updated": "2025-06-05T17:09:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    9,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-07T02:26:17Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    26,
                    17,
                    2,
                    127,
                    0
                ],
                "title": "Rethinking LLM Advancement: Compute-Dependent and Independent Paths to\n  Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Advancement: Compute-Dependent and Independent Paths to\n  Progress"
                },
                "summary": "Regulatory efforts to govern large language model (LLM) development have\npredominantly focused on restricting access to high-performance computational\nresources. This study evaluates the efficacy of such measures by examining\nwhether LLM capabilities can advance through algorithmic innovation in\ncompute-constrained environments. We propose a novel framework distinguishing\ncompute-dependent innovations--which yield disproportionate benefits at high\ncompute--from compute-independent innovations, which improve efficiency across\ncompute scales. The impact is quantified using Compute-Equivalent Gain (CEG).\nExperimental validation with nanoGPT models confirms that compute-independent\nadvancements yield significant performance gains (e.g., with combined CEG up to\n$3.5\\times$) across the tested scales. In contrast, compute-dependent\nadvancements were detrimental to performance at smaller experimental scales,\nbut showed improved CEG (on par with the baseline) as model size increased, a\ntrend consistent with their definition of yielding primary benefits at higher\ncompute. Crucially, these findings indicate that restrictions on computational\nhardware, while potentially slowing LLM progress, are insufficient to prevent\nall capability gains driven by algorithmic advancements. We argue that\neffective AI oversight must therefore incorporate mechanisms for understanding,\nanticipating, and potentially guiding algorithmic research, moving beyond a\nsingular focus on hardware. The proposed framework also serves as an analytical\ntool for forecasting AI progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regulatory efforts to govern large language model (LLM) development have\npredominantly focused on restricting access to high-performance computational\nresources. This study evaluates the efficacy of such measures by examining\nwhether LLM capabilities can advance through algorithmic innovation in\ncompute-constrained environments. We propose a novel framework distinguishing\ncompute-dependent innovations--which yield disproportionate benefits at high\ncompute--from compute-independent innovations, which improve efficiency across\ncompute scales. The impact is quantified using Compute-Equivalent Gain (CEG).\nExperimental validation with nanoGPT models confirms that compute-independent\nadvancements yield significant performance gains (e.g., with combined CEG up to\n$3.5\\times$) across the tested scales. In contrast, compute-dependent\nadvancements were detrimental to performance at smaller experimental scales,\nbut showed improved CEG (on par with the baseline) as model size increased, a\ntrend consistent with their definition of yielding primary benefits at higher\ncompute. Crucially, these findings indicate that restrictions on computational\nhardware, while potentially slowing LLM progress, are insufficient to prevent\nall capability gains driven by algorithmic advancements. We argue that\neffective AI oversight must therefore incorporate mechanisms for understanding,\nanticipating, and potentially guiding algorithmic research, moving beyond a\nsingular focus on hardware. The proposed framework also serves as an analytical\ntool for forecasting AI progress."
                },
                "authors": [
                    {
                        "name": "Jack Sanderson"
                    },
                    {
                        "name": "Teddy Foley"
                    },
                    {
                        "name": "Spencer Guo"
                    },
                    {
                        "name": "Anqi Qu"
                    },
                    {
                        "name": "Henry Josephson"
                    }
                ],
                "author_detail": {
                    "name": "Henry Josephson"
                },
                "author": "Henry Josephson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05243v1",
                "updated": "2025-06-05T17:02:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    2,
                    52,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:02:52Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    2,
                    52,
                    3,
                    156,
                    0
                ],
                "title": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection"
                },
                "summary": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme."
                },
                "authors": [
                    {
                        "name": "Ron Eliav"
                    },
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Eran Hirsch"
                    },
                    {
                        "name": "Shahaf Bassan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05242v1",
                "updated": "2025-06-05T17:01:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    1,
                    28,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:01:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    1,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid\n  Neuron Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid\n  Neuron Encryption"
                },
                "summary": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Haohua Du"
                    },
                    {
                        "name": "Junyang Wang"
                    },
                    {
                        "name": "Haifeng Sun"
                    },
                    {
                        "name": "Kaiwen Guo"
                    },
                    {
                        "name": "Haikuo Yu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00921v2",
                "updated": "2025-06-05T16:55:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    55,
                    6,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-02T21:19:53Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    19,
                    53,
                    6,
                    33,
                    0
                ],
                "title": "Blink of an eye: a simple theory for feature localization in generative\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blink of an eye: a simple theory for feature localization in generative\n  models"
                },
                "summary": "Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Marvin Li"
                    },
                    {
                        "name": "Aayush Karan"
                    },
                    {
                        "name": "Sitan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sitan Chen"
                },
                "author": "Sitan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05233v1",
                "updated": "2025-06-05T16:50:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    50,
                    23,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:50:23Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    50,
                    23,
                    3,
                    156,
                    0
                ],
                "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training"
                },
                "summary": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself."
                },
                "authors": [
                    {
                        "name": "Johannes von Oswald"
                    },
                    {
                        "name": "Nino Scherrer"
                    },
                    {
                        "name": "Seijin Kobayashi"
                    },
                    {
                        "name": "Luca Versari"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Maximilian Schlegel"
                    },
                    {
                        "name": "Kaitlin Maile"
                    },
                    {
                        "name": "Yanick Schimpf"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Alexander Meulemans"
                    },
                    {
                        "name": "Rif A. Saurous"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Charlotte Frenkel"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Blaise Agüera y Arcas"
                    },
                    {
                        "name": "João Sacramento"
                    }
                ],
                "author_detail": {
                    "name": "João Sacramento"
                },
                "author": "João Sacramento",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.06257v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.06257v4",
                "updated": "2025-06-05T16:49:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    49,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2021-10-12T18:12:57Z",
                "published_parsed": [
                    2021,
                    10,
                    12,
                    18,
                    12,
                    57,
                    1,
                    285,
                    0
                ],
                "title": "Causal Discovery from Conditionally Stationary Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Discovery from Conditionally Stationary Time Series"
                },
                "summary": "Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting."
                },
                "authors": [
                    {
                        "name": "Carles Balsells-Rodas"
                    },
                    {
                        "name": "Xavier Sumba"
                    },
                    {
                        "name": "Tanmayee Narendra"
                    },
                    {
                        "name": "Ruibo Tu"
                    },
                    {
                        "name": "Gabriele Schweikert"
                    },
                    {
                        "name": "Hedvig Kjellstrom"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.06257v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.06257v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05229v1",
                "updated": "2025-06-05T16:43:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    43,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:43:48Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    43,
                    48,
                    3,
                    156,
                    0
                ],
                "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts"
                },
                "summary": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications."
                },
                "authors": [
                    {
                        "name": "Danil Sivtsov"
                    },
                    {
                        "name": "Ivan Rodkin"
                    },
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08503v3",
                "updated": "2025-06-06T01:51:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    51,
                    44,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-12T15:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"
                },
                "summary": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs. Code and data are available at\nhttps://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs. Code and data are available at\nhttps://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks"
                },
                "authors": [
                    {
                        "name": "Jiahe Jin"
                    },
                    {
                        "name": "Yanheng He"
                    },
                    {
                        "name": "Mingyan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Yang"
                },
                "author": "Mingyan Yang",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05218v1",
                "updated": "2025-06-05T16:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    57,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    57,
                    3,
                    156,
                    0
                ],
                "title": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm"
                },
                "summary": "We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR."
                },
                "authors": [
                    {
                        "name": "Zhang Li"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Zhiyin Ma"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Zidun Guo"
                    },
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07301v2",
                "updated": "2025-06-05T16:34:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-01-13T13:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models."
                },
                "authors": [
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05213v1",
                "updated": "2025-06-05T16:27:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    27,
                    49,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:27:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    27,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "LLM-First Search: Self-Guided Exploration of the Solution Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-First Search: Self-Guided Exploration of the Solution Space"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}."
                },
                "authors": [
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    },
                    {
                        "name": "Roberta Raileanu"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Raileanu"
                },
                "author": "Roberta Raileanu",
                "arxiv_comment": "9 main pages, 2 figures, 2 tables, 36 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02524v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02524v4",
                "updated": "2025-06-05T16:22:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    22,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-04T17:42:21Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    42,
                    21,
                    1,
                    156,
                    0
                ],
                "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks"
                },
                "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Tanja Srindran"
                    },
                    {
                        "name": "Tomasz Lehmann"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02524v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02524v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02040v2",
                "updated": "2025-06-05T16:22:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    22,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-31T08:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    8,
                    1,
                    11,
                    5,
                    151,
                    0
                ],
                "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context\n  Protocol Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context\n  Protocol Ecosystem"
                },
                "summary": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers."
                },
                "authors": [
                    {
                        "name": "Hao Song"
                    },
                    {
                        "name": "Yiming Shen"
                    },
                    {
                        "name": "Wenxuan Luo"
                    },
                    {
                        "name": "Leixin Guo"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Jiachi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiachi Chen"
                },
                "author": "Jiachi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05209v1",
                "updated": "2025-06-05T16:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:21:30Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text"
                },
                "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models."
                },
                "authors": [
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Colin Raffel"
                    },
                    {
                        "name": "Sebastian Majstorovic"
                    },
                    {
                        "name": "Stella Biderman"
                    },
                    {
                        "name": "Baber Abbasi"
                    },
                    {
                        "name": "Luca Soldaini"
                    },
                    {
                        "name": "Enrico Shippole"
                    },
                    {
                        "name": "A. Feder Cooper"
                    },
                    {
                        "name": "Aviya Skowron"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Lintang Sutawika"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Zhenlin Xu"
                    },
                    {
                        "name": "Guilherme Penedo"
                    },
                    {
                        "name": "Loubna Ben Allal"
                    },
                    {
                        "name": "Elie Bakouch"
                    },
                    {
                        "name": "John David Pressman"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Tyler Murray"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Murray"
                },
                "author": "Tyler Murray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05205v1",
                "updated": "2025-06-05T16:17:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    17,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:17:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    17,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition"
                },
                "summary": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions."
                },
                "authors": [
                    {
                        "name": "Jackson Petty"
                    },
                    {
                        "name": "Michael Y. Hu"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Tal Linzen"
                    }
                ],
                "author_detail": {
                    "name": "Tal Linzen"
                },
                "author": "Tal Linzen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05202v2",
                "updated": "2025-06-06T07:46:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    46,
                    59,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T16:14:35Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    14,
                    35,
                    3,
                    156,
                    0
                ],
                "title": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants"
                },
                "summary": "This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders."
                },
                "authors": [
                    {
                        "name": "Daniele Tramontano"
                    },
                    {
                        "name": "Yaroslav Kivva"
                    },
                    {
                        "name": "Saber Salehkaleybar"
                    },
                    {
                        "name": "Mathias Drton"
                    },
                    {
                        "name": "Negar Kiyavash"
                    }
                ],
                "author_detail": {
                    "name": "Negar Kiyavash"
                },
                "author": "Negar Kiyavash",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18053v2",
                "updated": "2025-06-05T16:13:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    13,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-25T03:54:24Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    54,
                    24,
                    4,
                    115,
                    0
                ],
                "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM."
                },
                "authors": [
                    {
                        "name": "Jianyu Liu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05200v1",
                "updated": "2025-06-05T16:12:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    12,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:12:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    12,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Transformers Meet In-Context Learning: A Universal Approximation Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Meet In-Context Learning: A Universal Approximation Theory"
                },
                "summary": "Modern large language models are capable of in-context learning, the ability\nto perform new tasks at inference time using only a handful of input-output\nexamples in the prompt, without any fine-tuning or parameter updates. We\ndevelop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a\ndistinct task), we demonstrate how to construct a transformer that, without any\nfurther weight updates, can perform reliable prediction given only a few\nin-context examples. In contrast to much of the recent literature that frames\ntransformers as algorithm approximators -- i.e., constructing transformers to\nemulate the iterations of optimization algorithms as a means to approximate\nsolutions of learning problems -- our work adopts a fundamentally different\napproach rooted in universal function approximation. This alternative approach\noffers approximation guarantees that are not constrained by the effectiveness\nof the optimization algorithms being approximated, thereby extending far beyond\nconvex problems and linear function classes. Our construction sheds light on\nhow transformers can simultaneously learn general-purpose representations and\nadapt dynamically to in-context examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models are capable of in-context learning, the ability\nto perform new tasks at inference time using only a handful of input-output\nexamples in the prompt, without any fine-tuning or parameter updates. We\ndevelop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a\ndistinct task), we demonstrate how to construct a transformer that, without any\nfurther weight updates, can perform reliable prediction given only a few\nin-context examples. In contrast to much of the recent literature that frames\ntransformers as algorithm approximators -- i.e., constructing transformers to\nemulate the iterations of optimization algorithms as a means to approximate\nsolutions of learning problems -- our work adopts a fundamentally different\napproach rooted in universal function approximation. This alternative approach\noffers approximation guarantees that are not constrained by the effectiveness\nof the optimization algorithms being approximated, thereby extending far beyond\nconvex problems and linear function classes. Our construction sheds light on\nhow transformers can simultaneously learn general-purpose representations and\nadapt dynamically to in-context examples."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuchen Jiao"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Yuting Wei"
                    },
                    {
                        "name": "Yuxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuxin Chen"
                },
                "author": "Yuxin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05191v1",
                "updated": "2025-06-05T16:04:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:04:08Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MokA: Multimodal Low-Rank Adaptation for MLLMs"
                },
                "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA."
                },
                "authors": [
                    {
                        "name": "Yake Wei"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Di Hu"
                    }
                ],
                "author_detail": {
                    "name": "Di Hu"
                },
                "author": "Di Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05188v1",
                "updated": "2025-06-05T16:02:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    2,
                    7,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:02:07Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    2,
                    7,
                    3,
                    156,
                    0
                ],
                "title": "Counterfactual reasoning: an analysis of in-context emergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual reasoning: an analysis of in-context emergence"
                },
                "summary": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git ."
                },
                "authors": [
                    {
                        "name": "Moritz Miller"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Siyuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Guo"
                },
                "author": "Siyuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05085v3",
                "updated": "2025-06-05T15:57:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    57,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Patrik Okanovic"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Michal Podstawski"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05183v1",
                "updated": "2025-06-05T15:56:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    56,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:56:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    56,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TreeRPO: Tree Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeRPO: Tree Relative Policy Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "13pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06415v3",
                "updated": "2025-06-05T15:55:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    55,
                    15,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-08T22:56:00Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    22,
                    56,
                    0,
                    1,
                    282,
                    0
                ],
                "title": "Biased AI can Influence Political Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased AI can Influence Political Decision-Making"
                },
                "summary": "As modern large language models (LLMs) become integral to everyday tasks,\nconcerns about their inherent biases and their potential impact on human\ndecision-making have emerged. While bias in models are well-documented, less is\nknown about how these biases influence human decisions. This paper presents two\ninteractive experiments investigating the effects of partisan bias in LLMs on\npolitical opinions and decision-making. Participants interacted freely with\neither a biased liberal, biased conservative, or unbiased control model while\ncompleting these tasks. We found that participants exposed to partisan biased\nmodels were significantly more likely to adopt opinions and make decisions\nwhich matched the LLM's bias. Even more surprising, this influence was seen\nwhen the model bias and personal political partisanship of the participant were\nopposite. However, we also discovered that prior knowledge of AI was weakly\ncorrelated with a reduction of the impact of the bias, highlighting the\npossible importance of AI education for robust mitigation of bias effects. Our\nfindings not only highlight the critical effects of interacting with biased\nLLMs and its ability to impact public discourse and political conduct, but also\nhighlights potential techniques for mitigating these risks in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern large language models (LLMs) become integral to everyday tasks,\nconcerns about their inherent biases and their potential impact on human\ndecision-making have emerged. While bias in models are well-documented, less is\nknown about how these biases influence human decisions. This paper presents two\ninteractive experiments investigating the effects of partisan bias in LLMs on\npolitical opinions and decision-making. Participants interacted freely with\neither a biased liberal, biased conservative, or unbiased control model while\ncompleting these tasks. We found that participants exposed to partisan biased\nmodels were significantly more likely to adopt opinions and make decisions\nwhich matched the LLM's bias. Even more surprising, this influence was seen\nwhen the model bias and personal political partisanship of the participant were\nopposite. However, we also discovered that prior knowledge of AI was weakly\ncorrelated with a reduction of the impact of the bias, highlighting the\npossible importance of AI education for robust mitigation of bias effects. Our\nfindings not only highlight the critical effects of interacting with biased\nLLMs and its ability to impact public discourse and political conduct, but also\nhighlights potential techniques for mitigating these risks in the future."
                },
                "authors": [
                    {
                        "name": "Jillian Fisher"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Robert Aron"
                    },
                    {
                        "name": "Thomas Richardson"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Daniel W. Fisher"
                    },
                    {
                        "name": "Jennifer Pan"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Katharina Reinecke"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Reinecke"
                },
                "author": "Katharina Reinecke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05182v1",
                "updated": "2025-06-05T15:52:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    52,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:52:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    52,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools"
                },
                "summary": "The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA."
                },
                "authors": [
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Messiah Ataey"
                    },
                    {
                        "name": "Shariyar Murtuza"
                    },
                    {
                        "name": "Yifan Nie"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "15 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05176v1",
                "updated": "2025-06-05T15:49:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    49,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:49:48Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    49,
                    48,
                    3,
                    156,
                    0
                ],
                "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models"
                },
                "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06854v2",
                "updated": "2025-06-05T15:48:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    48,
                    54,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-07T17:23:48Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    23,
                    48,
                    4,
                    38,
                    0
                ],
                "title": "Can Large Language Models Understand Intermediate Representations in\n  Compilers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Intermediate Representations in\n  Compilers?"
                },
                "summary": "Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at"
                },
                "authors": [
                    {
                        "name": "Hailong Jiang"
                    },
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Qiang Guan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Guan"
                },
                "author": "Qiang Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05167v2",
                "updated": "2025-06-06T07:57:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    57,
                    28,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T15:43:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    43,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECoRAG: Evidentiality-guided Compression for Long Context RAG"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG."
                },
                "authors": [
                    {
                        "name": "Yeonseok Jeong"
                    },
                    {
                        "name": "Jinsu Kim"
                    },
                    {
                        "name": "Dohyeon Lee"
                    },
                    {
                        "name": "Seung-won Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seung-won Hwang"
                },
                "author": "Seung-won Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05166v2",
                "updated": "2025-06-06T01:35:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    35,
                    43,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T15:43:34Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    43,
                    34,
                    3,
                    156,
                    0
                ],
                "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective"
                },
                "summary": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks."
                },
                "authors": [
                    {
                        "name": "Bhavik Chandna"
                    },
                    {
                        "name": "Zubair Bashir"
                    },
                    {
                        "name": "Procheta Sen"
                    }
                ],
                "author_detail": {
                    "name": "Procheta Sen"
                },
                "author": "Procheta Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11746v2",
                "updated": "2025-06-05T15:43:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    43,
                    7,
                    3,
                    156,
                    0
                ],
                "published": "2024-07-16T14:12:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    14,
                    12,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Sparse data assimilation for under-resolved large-eddy simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data assimilation for under-resolved large-eddy simulations"
                },
                "summary": "The need for accurate and fast scale-resolving simulations of fluid flows,\nwhere turbulent dispersion is a crucial physical feature, is evident.\nLarge-eddy simulations (LES) are computationally more affordable than direct\nnumerical simulations, but their accuracy depends on sub-grid scale models and\nthe quality of the computational mesh. In order to compensate related errors, a\ndata assimilation approach for LES is devised in this work.\n  The presented method is based on variational assimilation of sparse\ntime-averaged velocity reference data. Working with the time-averaged LES\nmomentum equation allows to employ a stationary discrete adjoint method.\nTherefore, a stationary corrective force in the unsteady LES momentum equation\nis iteratively updated within the gradient-based optimization framework in\nconjunction with the adjoint gradient. After data assimilation, corrected\nanisotropic Reynolds stresses are inferred from the stationary corrective\nforce. Ultimately, this corrective force that acts on the mean velocity is\nreplaced by a term that scales the velocity fluctuations through nudging of the\ncorrected anisotropic Reynolds stresses.\n  Efficacy of the proposed framework is demonstrated for turbulent flow over\nperiodic hills and around a square cylinder. Coarse meshes are leveraged to\nfurther enhance the speed of the optimization procedure. Time- and\nspanwise-averaged velocity reference data from high-fidelity simulations is\ntaken from the literature.\n  Our results demonstrate that adjoint-based assimilation of averaged velocity\nenables the optimization of the mean flow, vortex shedding frequency (i.e.,\nStrouhal number), and anisotropic Reynolds stresses. This highlights the\nsuperiority of scale-resolving simulations such as LES over simulations based\non the (unsteady) Reynolds-averaged equations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for accurate and fast scale-resolving simulations of fluid flows,\nwhere turbulent dispersion is a crucial physical feature, is evident.\nLarge-eddy simulations (LES) are computationally more affordable than direct\nnumerical simulations, but their accuracy depends on sub-grid scale models and\nthe quality of the computational mesh. In order to compensate related errors, a\ndata assimilation approach for LES is devised in this work.\n  The presented method is based on variational assimilation of sparse\ntime-averaged velocity reference data. Working with the time-averaged LES\nmomentum equation allows to employ a stationary discrete adjoint method.\nTherefore, a stationary corrective force in the unsteady LES momentum equation\nis iteratively updated within the gradient-based optimization framework in\nconjunction with the adjoint gradient. After data assimilation, corrected\nanisotropic Reynolds stresses are inferred from the stationary corrective\nforce. Ultimately, this corrective force that acts on the mean velocity is\nreplaced by a term that scales the velocity fluctuations through nudging of the\ncorrected anisotropic Reynolds stresses.\n  Efficacy of the proposed framework is demonstrated for turbulent flow over\nperiodic hills and around a square cylinder. Coarse meshes are leveraged to\nfurther enhance the speed of the optimization procedure. Time- and\nspanwise-averaged velocity reference data from high-fidelity simulations is\ntaken from the literature.\n  Our results demonstrate that adjoint-based assimilation of averaged velocity\nenables the optimization of the mean flow, vortex shedding frequency (i.e.,\nStrouhal number), and anisotropic Reynolds stresses. This highlights the\nsuperiority of scale-resolving simulations such as LES over simulations based\non the (unsteady) Reynolds-averaged equations."
                },
                "authors": [
                    {
                        "name": "Justin Plogmann"
                    },
                    {
                        "name": "Oliver Brenner"
                    },
                    {
                        "name": "Patrick Jenny"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Jenny"
                },
                "author": "Patrick Jenny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23827v2",
                "updated": "2025-06-05T15:41:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    41,
                    26,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-28T06:43:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    6,
                    43,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "ValueSim: Generating Backstories to Model Individual Value Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValueSim: Generating Backstories to Model Individual Value Systems"
                },
                "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time."
                },
                "authors": [
                    {
                        "name": "Bangde Du"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Zhijing Wu"
                    },
                    {
                        "name": "Jankowska Monika"
                    },
                    {
                        "name": "Shuqi Zhu"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05165v1",
                "updated": "2025-06-05T15:41:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    41,
                    23,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:41:23Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    41,
                    23,
                    3,
                    156,
                    0
                ],
                "title": "LiPo: A Lightweight Post-optimization Framework for Smoothing Action\n  Chunks Generated by Learned Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiPo: A Lightweight Post-optimization Framework for Smoothing Action\n  Chunks Generated by Learned Policies"
                },
                "summary": "Recent advances in imitation learning have enabled robots to perform\nincreasingly complex manipulation tasks in unstructured environments. However,\nmost learned policies rely on discrete action chunking, which introduces\ndiscontinuities at chunk boundaries. These discontinuities degrade motion\nquality and are particularly problematic in dynamic tasks such as throwing or\nlifting heavy objects, where smooth trajectories are critical for momentum\ntransfer and system stability. In this work, we present a lightweight\npost-optimization framework for smoothing chunked action sequences. Our method\ncombines three key components: (1) inference-aware chunk scheduling to\nproactively generate overlapping chunks and avoid pauses from inference delays;\n(2) linear blending in the overlap region to reduce abrupt transitions; and (3)\njerk-minimizing trajectory optimization constrained within a bounded\nperturbation space. The proposed method was validated on a position-controlled\nrobotic arm performing dynamic manipulation tasks. Experimental results\ndemonstrate that our approach significantly reduces vibration and motion\njitter, leading to smoother execution and improved mechanical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in imitation learning have enabled robots to perform\nincreasingly complex manipulation tasks in unstructured environments. However,\nmost learned policies rely on discrete action chunking, which introduces\ndiscontinuities at chunk boundaries. These discontinuities degrade motion\nquality and are particularly problematic in dynamic tasks such as throwing or\nlifting heavy objects, where smooth trajectories are critical for momentum\ntransfer and system stability. In this work, we present a lightweight\npost-optimization framework for smoothing chunked action sequences. Our method\ncombines three key components: (1) inference-aware chunk scheduling to\nproactively generate overlapping chunks and avoid pauses from inference delays;\n(2) linear blending in the overlap region to reduce abrupt transitions; and (3)\njerk-minimizing trajectory optimization constrained within a bounded\nperturbation space. The proposed method was validated on a position-controlled\nrobotic arm performing dynamic manipulation tasks. Experimental results\ndemonstrate that our approach significantly reduces vibration and motion\njitter, leading to smoother execution and improved mechanical robustness."
                },
                "authors": [
                    {
                        "name": "Dongwoo Son"
                    },
                    {
                        "name": "Suhan Park"
                    }
                ],
                "author_detail": {
                    "name": "Suhan Park"
                },
                "author": "Suhan Park",
                "arxiv_comment": "6 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v6",
                "updated": "2025-06-05T15:26:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    26,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Yingming Mao"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "ZhuoRan Liu"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Peirui Cao"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Wu Dongchao"
                    },
                    {
                        "name": "Yang Jian"
                    },
                    {
                        "name": "Zhang zhanbang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang zhanbang"
                },
                "author": "Zhang zhanbang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05142v1",
                "updated": "2025-06-05T15:24:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    24,
                    33,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:24:33Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    24,
                    33,
                    3,
                    156,
                    0
                ],
                "title": "Do Large Language Models Judge Error Severity Like Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Judge Error Severity Like Humans?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models."
                },
                "authors": [
                    {
                        "name": "Diege Sun"
                    },
                    {
                        "name": "Guanyi Chen"
                    },
                    {
                        "name": "Fan Zhao"
                    },
                    {
                        "name": "Xiaorong Cheng"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20779v3",
                "updated": "2025-06-05T15:20:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    20,
                    59,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-27T06:36:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    36,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature"
                },
                "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB"
                },
                "authors": [
                    {
                        "name": "Noy Sternlicht"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "arxiv_comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05128v1",
                "updated": "2025-06-05T15:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    16,
                    14,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    16,
                    14,
                    3,
                    156,
                    0
                ],
                "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning"
                },
                "summary": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework."
                },
                "authors": [
                    {
                        "name": "Tanmay Parekh"
                    },
                    {
                        "name": "Kartik Mehta"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Submitted at ACL ARR May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05127v1",
                "updated": "2025-06-05T15:14:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    14,
                    32,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:14:32Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    14,
                    32,
                    3,
                    156,
                    0
                ],
                "title": "PixCell: A generative foundation model for digital histopathology images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PixCell: A generative foundation model for digital histopathology images"
                },
                "summary": "The digitization of histology slides has revolutionized pathology, providing\nmassive datasets for cancer diagnosis and research. Contrastive self-supervised\nand vision-language models have been shown to effectively mine large pathology\ndatasets to learn discriminative representations. On the other hand, generative\nmodels, capable of synthesizing realistic and diverse images, present a\ncompelling solution to address unique problems in pathology that involve\nsynthesizing images; overcoming annotated data scarcity, enabling\nprivacy-preserving data sharing, and performing inherently generative tasks,\nsuch as virtual staining. We introduce PixCell, the first diffusion-based\ngenerative foundation model for histopathology. We train PixCell on PanCan-30M,\na vast, diverse dataset derived from 69,184 H\\&E-stained whole slide images\ncovering various cancer types. We employ a progressive training strategy and a\nself-supervision-based conditioning that allows us to scale up training without\nany annotated data. PixCell generates diverse and high-quality images across\nmultiple cancer types, which we find can be used in place of real data to train\na self-supervised discriminative model. Synthetic images shared between\ninstitutions are subject to fewer regulatory barriers than would be the case\nwith real clinical images. Furthermore, we showcase the ability to precisely\ncontrol image generation using a small set of annotated images, which can be\nused for both data augmentation and educational purposes. Testing on a cell\nsegmentation task, a mask-guided PixCell enables targeted data augmentation,\nimproving downstream performance. Finally, we demonstrate PixCell's ability to\nuse H\\&E structural staining to infer results from molecular marker studies; we\nuse this capability to infer IHC staining from H\\&E images. Our trained models\nare publicly released to accelerate research in computational pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The digitization of histology slides has revolutionized pathology, providing\nmassive datasets for cancer diagnosis and research. Contrastive self-supervised\nand vision-language models have been shown to effectively mine large pathology\ndatasets to learn discriminative representations. On the other hand, generative\nmodels, capable of synthesizing realistic and diverse images, present a\ncompelling solution to address unique problems in pathology that involve\nsynthesizing images; overcoming annotated data scarcity, enabling\nprivacy-preserving data sharing, and performing inherently generative tasks,\nsuch as virtual staining. We introduce PixCell, the first diffusion-based\ngenerative foundation model for histopathology. We train PixCell on PanCan-30M,\na vast, diverse dataset derived from 69,184 H\\&E-stained whole slide images\ncovering various cancer types. We employ a progressive training strategy and a\nself-supervision-based conditioning that allows us to scale up training without\nany annotated data. PixCell generates diverse and high-quality images across\nmultiple cancer types, which we find can be used in place of real data to train\na self-supervised discriminative model. Synthetic images shared between\ninstitutions are subject to fewer regulatory barriers than would be the case\nwith real clinical images. Furthermore, we showcase the ability to precisely\ncontrol image generation using a small set of annotated images, which can be\nused for both data augmentation and educational purposes. Testing on a cell\nsegmentation task, a mask-guided PixCell enables targeted data augmentation,\nimproving downstream performance. Finally, we demonstrate PixCell's ability to\nuse H\\&E structural staining to infer results from molecular marker studies; we\nuse this capability to infer IHC staining from H\\&E images. Our trained models\nare publicly released to accelerate research in computational pathology."
                },
                "authors": [
                    {
                        "name": "Srikar Yellapragada"
                    },
                    {
                        "name": "Alexandros Graikos"
                    },
                    {
                        "name": "Zilinghan Li"
                    },
                    {
                        "name": "Kostas Triaridis"
                    },
                    {
                        "name": "Varun Belagali"
                    },
                    {
                        "name": "Saarthak Kapse"
                    },
                    {
                        "name": "Tarak Nath Nandi"
                    },
                    {
                        "name": "Ravi K Madduri"
                    },
                    {
                        "name": "Prateek Prasanna"
                    },
                    {
                        "name": "Tahsin Kurc"
                    },
                    {
                        "name": "Rajarsi R. Gupta"
                    },
                    {
                        "name": "Joel Saltz"
                    },
                    {
                        "name": "Dimitris Samaras"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Samaras"
                },
                "author": "Dimitris Samaras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05126v1",
                "updated": "2025-06-05T15:13:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    13,
                    57,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:13:57Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    13,
                    57,
                    3,
                    156,
                    0
                ],
                "title": "Membership Inference Attacks on Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks on Sequence Models"
                },
                "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rossi"
                    },
                    {
                        "name": "Michael Aerni"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02234v2",
                "updated": "2025-06-05T15:12:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    12,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-03T03:01:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    3,
                    1,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "LLM Social Simulations Are a Promising Research Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Social Simulations Are a Promising Research Method"
                },
                "summary": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted this method. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a review of\nempirical comparisons between LLMs and human research subjects, commentaries on\nthe topic, and related work. We identify promising directions, including\ncontext-rich prompting and fine-tuning with social science datasets. We believe\nthat LLM social simulations can already be used for pilot and exploratory\nstudies, and more widespread use may soon be possible with rapidly advancing\nLLM capabilities. Researchers should prioritize developing conceptual models\nand iterative evaluations to make the best use of new AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted this method. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a review of\nempirical comparisons between LLMs and human research subjects, commentaries on\nthe topic, and related work. We identify promising directions, including\ncontext-rich prompting and fine-tuning with social science datasets. We believe\nthat LLM social simulations can already be used for pilot and exploratory\nstudies, and more widespread use may soon be possible with rapidly advancing\nLLM capabilities. Researchers should prioritize developing conceptual models\nand iterative evaluations to make the best use of new AI systems."
                },
                "authors": [
                    {
                        "name": "Jacy Reese Anthis"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Sean M. Richardson"
                    },
                    {
                        "name": "Austin C. Kozlowski"
                    },
                    {
                        "name": "Bernard Koch"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Erik Brynjolfsson"
                    },
                    {
                        "name": "Michael Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bernstein"
                },
                "author": "Michael Bernstein",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09755v2",
                "updated": "2025-06-05T15:08:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    8,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-13T20:25:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    20,
                    25,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "Jailbreak Attack Initializations as Extractors of Compliance Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attack Initializations as Extractors of Compliance Directions"
                },
                "summary": "Safety-aligned LLMs respond to prompts with either compliance or refusal,\neach corresponding to distinct directions in the model's activation space.\nRecent works show that initializing attacks via self-transfer from other\nprompts significantly enhances their performance. However, the underlying\nmechanisms of these initializations remain unclear, and attacks utilize\narbitrary or hand-picked initializations. This work presents that each\ngradient-based jailbreak attack and subsequent initialization gradually\nconverge to a single compliance direction that suppresses refusal, thereby\nenabling an efficient transition from refusal to compliance. Based on this\ninsight, we propose CRI, an initialization framework that aims to project\nunseen prompts further along compliance directions. We demonstrate our approach\non multiple attacks, models, and datasets, achieving an increased attack\nsuccess rate (ASR) and reduced computational overhead, highlighting the\nfragility of safety-aligned LLMs. A reference implementation is available at:\nhttps://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety-aligned LLMs respond to prompts with either compliance or refusal,\neach corresponding to distinct directions in the model's activation space.\nRecent works show that initializing attacks via self-transfer from other\nprompts significantly enhances their performance. However, the underlying\nmechanisms of these initializations remain unclear, and attacks utilize\narbitrary or hand-picked initializations. This work presents that each\ngradient-based jailbreak attack and subsequent initialization gradually\nconverge to a single compliance direction that suppresses refusal, thereby\nenabling an efficient transition from refusal to compliance. Based on this\ninsight, we propose CRI, an initialization framework that aims to project\nunseen prompts further along compliance directions. We demonstrate our approach\non multiple attacks, models, and datasets, achieving an increased attack\nsuccess rate (ASR) and reduced computational overhead, highlighting the\nfragility of safety-aligned LLMs. A reference implementation is available at:\nhttps://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation."
                },
                "authors": [
                    {
                        "name": "Amit Levi"
                    },
                    {
                        "name": "Rom Himelstein"
                    },
                    {
                        "name": "Yaniv Nemcovsky"
                    },
                    {
                        "name": "Avi Mendelson"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05120v1",
                "updated": "2025-06-05T15:08:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    8,
                    1,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:08:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    8,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "Nonlinear Causal Discovery for Grouped Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear Causal Discovery for Grouped Data"
                },
                "summary": "Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups."
                },
                "authors": [
                    {
                        "name": "Konstantin Göbler"
                    },
                    {
                        "name": "Tobias Windisch"
                    },
                    {
                        "name": "Mathias Drton"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Drton"
                },
                "author": "Mathias Drton",
                "arxiv_comment": "9 pages, 5 figures, to be published at UAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05116v1",
                "updated": "2025-06-05T15:00:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    0,
                    32,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:00:32Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    0,
                    32,
                    3,
                    156,
                    0
                ],
                "title": "The Spurious Factor Dilemma: Robust Inference in Heavy-Tailed Elliptical\n  Factor Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Spurious Factor Dilemma: Robust Inference in Heavy-Tailed Elliptical\n  Factor Models"
                },
                "summary": "Factor models are essential tools for analyzing high-dimensional data,\nparticularly in economics and finance. However, standard methods for\ndetermining the number of factors often overestimate the true number when data\nexhibit heavy-tailed randomness, misinterpreting noise-induced outliers as\ngenuine factors. This paper addresses this challenge within the framework of\nElliptical Factor Models (EFM), which accommodate both heavy tails and\npotential non-linear dependencies common in real-world data. We demonstrate\ntheoretically and empirically that heavy-tailed noise generates spurious\neigenvalues that mimic true factor signals. To distinguish these, we propose a\nnovel methodology based on a fluctuation magnification algorithm. We show that\nunder magnifying perturbations, the eigenvalues associated with real factors\nexhibit significantly less fluctuation (stabilizing asymptotically) compared to\nspurious eigenvalues arising from heavy-tailed effects. This differential\nbehavior allows the identification and detection of the true and spurious\nfactors. We develop a formal testing procedure based on this principle and\napply it to the problem of accurately selecting the number of common factors in\nheavy-tailed EFMs. Simulation studies and real data analysis confirm the\neffectiveness of our approach compared to existing methods, particularly in\nscenarios with pronounced heavy-tailedness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factor models are essential tools for analyzing high-dimensional data,\nparticularly in economics and finance. However, standard methods for\ndetermining the number of factors often overestimate the true number when data\nexhibit heavy-tailed randomness, misinterpreting noise-induced outliers as\ngenuine factors. This paper addresses this challenge within the framework of\nElliptical Factor Models (EFM), which accommodate both heavy tails and\npotential non-linear dependencies common in real-world data. We demonstrate\ntheoretically and empirically that heavy-tailed noise generates spurious\neigenvalues that mimic true factor signals. To distinguish these, we propose a\nnovel methodology based on a fluctuation magnification algorithm. We show that\nunder magnifying perturbations, the eigenvalues associated with real factors\nexhibit significantly less fluctuation (stabilizing asymptotically) compared to\nspurious eigenvalues arising from heavy-tailed effects. This differential\nbehavior allows the identification and detection of the true and spurious\nfactors. We develop a formal testing procedure based on this principle and\napply it to the problem of accurately selecting the number of common factors in\nheavy-tailed EFMs. Simulation studies and real data analysis confirm the\neffectiveness of our approach compared to existing methods, particularly in\nscenarios with pronounced heavy-tailedness."
                },
                "authors": [
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Jiahui Xie"
                    },
                    {
                        "name": "Yangchun Zhang"
                    },
                    {
                        "name": "Wang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wang Zhou"
                },
                "author": "Wang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05112v1",
                "updated": "2025-06-05T14:57:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    57,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:57:30Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    57,
                    30,
                    3,
                    156,
                    0
                ],
                "title": "At the edge of Donsker's Theorem: Asymptotics of multiscale scan\n  statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge of Donsker's Theorem: Asymptotics of multiscale scan\n  statistics"
                },
                "summary": "For nonparametric inference about a function, multiscale testing procedures\nresolve the need for bandwidth selection and achieve asymptotically optimal\ndetection performance against a broad range of alternatives. However, critical\nvalues strongly depend on the noise distribution, and we argue that existing\nmethods are either statistically infeasible, or asymptotically sub-optimal. To\naddress this methodological challenge, we show how to develop a feasible\nmultiscale test via weak convergence arguments, by replacing the additive\nmultiscale penalty with a multiplicative weighting. This new theoretical\nfoundation preserves the optimal detection properties of multiscale tests and\nextends their applicability to nonstationary nonlinear time series via a\ntailored bootstrap scheme. Inference for signal discovery, goodness-of-fit\ntesting of regression functions, and multiple changepoint detection is studied\nin detail, and we apply the new methodology to analyze the April 2025 power\nblackout on the Iberian peninsula. Our methodology is enabled by a novel\nfunctional central limit in H\\\"older spaces with critical modulus of\ncontinuity, where Donsker's theorem fails to hold due to lack of tightness.\nProbabilistically, we discover a novel form of thresholded weak convergence\nthat holds only in the upper support of the distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For nonparametric inference about a function, multiscale testing procedures\nresolve the need for bandwidth selection and achieve asymptotically optimal\ndetection performance against a broad range of alternatives. However, critical\nvalues strongly depend on the noise distribution, and we argue that existing\nmethods are either statistically infeasible, or asymptotically sub-optimal. To\naddress this methodological challenge, we show how to develop a feasible\nmultiscale test via weak convergence arguments, by replacing the additive\nmultiscale penalty with a multiplicative weighting. This new theoretical\nfoundation preserves the optimal detection properties of multiscale tests and\nextends their applicability to nonstationary nonlinear time series via a\ntailored bootstrap scheme. Inference for signal discovery, goodness-of-fit\ntesting of regression functions, and multiple changepoint detection is studied\nin detail, and we apply the new methodology to analyze the April 2025 power\nblackout on the Iberian peninsula. Our methodology is enabled by a novel\nfunctional central limit in H\\\"older spaces with critical modulus of\ncontinuity, where Donsker's theorem fails to hold due to lack of tightness.\nProbabilistically, we discover a novel form of thresholded weak convergence\nthat holds only in the upper support of the distribution."
                },
                "authors": [
                    {
                        "name": "Johann Köhne"
                    },
                    {
                        "name": "Fabian Mies"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Mies"
                },
                "author": "Fabian Mies",
                "arxiv_comment": "41 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G10 (Primary), 62M10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18334v3",
                "updated": "2025-06-05T14:44:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    44,
                    56,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-25T16:26:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    26,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Structural Alignment Improves Graph Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Alignment Improves Graph Test-Time Adaptation"
                },
                "summary": "Graph-based learning excels at capturing interaction patterns in diverse\ndomains like recommendation, fraud detection, and particle physics. However,\nits performance often degrades under distribution shifts, especially those\naltering network connectivity. Current methods to address these shifts\ntypically require retraining with the source dataset, which is often infeasible\ndue to computational or privacy limitations. We introduce Test-Time Structural\nAlignment (TSA), a novel algorithm for Graph Test-Time Adaptation (GTTA) that\naligns graph structures during inference without accessing the source data.\nGrounded in a theoretical understanding of graph data distribution shifts, TSA\nemploys three synergistic strategies: uncertainty-aware neighborhood weighting\nto accommodate neighbor label distribution shifts, adaptive balancing of\nself-node and aggregated neighborhood representations based on their\nsignal-to-noise ratio, and decision boundary refinement to correct residual\nlabel and feature shifts. Extensive experiments on synthetic and real-world\ndatasets demonstrate TSA's consistent outperformance of both non-graph TTA\nmethods and state-of-the-art GTTA baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based learning excels at capturing interaction patterns in diverse\ndomains like recommendation, fraud detection, and particle physics. However,\nits performance often degrades under distribution shifts, especially those\naltering network connectivity. Current methods to address these shifts\ntypically require retraining with the source dataset, which is often infeasible\ndue to computational or privacy limitations. We introduce Test-Time Structural\nAlignment (TSA), a novel algorithm for Graph Test-Time Adaptation (GTTA) that\naligns graph structures during inference without accessing the source data.\nGrounded in a theoretical understanding of graph data distribution shifts, TSA\nemploys three synergistic strategies: uncertainty-aware neighborhood weighting\nto accommodate neighbor label distribution shifts, adaptive balancing of\nself-node and aggregated neighborhood representations based on their\nsignal-to-noise ratio, and decision boundary refinement to correct residual\nlabel and feature shifts. Extensive experiments on synthetic and real-world\ndatasets demonstrate TSA's consistent outperformance of both non-graph TTA\nmethods and state-of-the-art GTTA baselines."
                },
                "authors": [
                    {
                        "name": "Hans Hao-Hsun Hsu"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05096v2",
                "updated": "2025-06-06T04:46:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    4,
                    46,
                    57,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T14:41:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    41,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video\n  Diffusion Transformers"
                },
                "summary": "Video diffusion transformers (vDiTs) have made impressive progress in\ntext-to-video generation, but their high computational demands present major\nchallenges for practical deployment. While existing acceleration methods reduce\nworkload at various granularities, they often rely on heuristics, limiting\ntheir applicability.\n  We introduce ASTRAEA, an automatic framework that searches for near-optimal\nconfigurations for vDiT-based video generation. At its core, ASTRAEA proposes a\nlightweight token selection mechanism and a memory-efficient, GPU-parallel\nsparse attention strategy, enabling linear reductions in execution time with\nminimal impact on generation quality. To determine optimal token reduction for\ndifferent timesteps, we further design a search framework that leverages a\nclassic evolutionary algorithm to automatically determine the distribution of\nthe token budget effectively. Together, ASTRAEA achieves up to 2.4x inference\nspeedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)\nwhile retaining better video quality compared to the state-of-the-art methods\n(<0.5% loss on the VBench score compared to the baseline vDiT models).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers (vDiTs) have made impressive progress in\ntext-to-video generation, but their high computational demands present major\nchallenges for practical deployment. While existing acceleration methods reduce\nworkload at various granularities, they often rely on heuristics, limiting\ntheir applicability.\n  We introduce ASTRAEA, an automatic framework that searches for near-optimal\nconfigurations for vDiT-based video generation. At its core, ASTRAEA proposes a\nlightweight token selection mechanism and a memory-efficient, GPU-parallel\nsparse attention strategy, enabling linear reductions in execution time with\nminimal impact on generation quality. To determine optimal token reduction for\ndifferent timesteps, we further design a search framework that leverages a\nclassic evolutionary algorithm to automatically determine the distribution of\nthe token budget effectively. Together, ASTRAEA achieves up to 2.4x inference\nspeedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)\nwhile retaining better video quality compared to the state-of-the-art methods\n(<0.5% loss on the VBench score compared to the baseline vDiT models)."
                },
                "authors": [
                    {
                        "name": "Haosong Liu"
                    },
                    {
                        "name": "Yuge Cheng"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Aiyue Chen"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03198v2",
                "updated": "2025-06-05T14:35:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    35,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2024-05-28T04:36:15Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    4,
                    36,
                    15,
                    1,
                    149,
                    0
                ],
                "title": "The Impossibility of Fair LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impossibility of Fair LLMs"
                },
                "summary": "The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jacy Anthis"
                    },
                    {
                        "name": "Kristian Lum"
                    },
                    {
                        "name": "Michael Ekstrand"
                    },
                    {
                        "name": "Avi Feller"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "Published in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12649v3",
                "updated": "2025-06-05T14:35:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    35,
                    0,
                    3,
                    156,
                    0
                ],
                "published": "2024-02-20T01:49:15Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    1,
                    49,
                    15,
                    1,
                    51,
                    0
                ],
                "title": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation"
                },
                "summary": "Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between the user attributes stated or implied by a\nprompt and the LLM's short text response, but human-AI interaction increasingly\nrequires long-form and context-specific system output to solve real-world\ntasks. In the commonly studied domain of gender-occupation bias, we test\nwhether these benchmarks are robust to lengthening the LLM responses as a\nmeasure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype) and develop analogous RUTEd evaluations from three contexts of\nreal-world use: children's bedtime stories, user personas, and English language\nlearning exercises. We find that standard bias metrics have no significant\ncorrelation with the more realistic bias metrics. For example, selecting the\nleast biased model based on the standard \"trick tests\" coincides with selecting\nthe least biased model as measured in more realistic use no more than random\nchance. We suggest that there is not yet evidence to justify standard\nbenchmarks as reliable proxies of real-world AI biases, and we encourage\nfurther development of evaluations grounded in particular contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between the user attributes stated or implied by a\nprompt and the LLM's short text response, but human-AI interaction increasingly\nrequires long-form and context-specific system output to solve real-world\ntasks. In the commonly studied domain of gender-occupation bias, we test\nwhether these benchmarks are robust to lengthening the LLM responses as a\nmeasure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype) and develop analogous RUTEd evaluations from three contexts of\nreal-world use: children's bedtime stories, user personas, and English language\nlearning exercises. We find that standard bias metrics have no significant\ncorrelation with the more realistic bias metrics. For example, selecting the\nleast biased model based on the standard \"trick tests\" coincides with selecting\nthe least biased model as measured in more realistic use no more than random\nchance. We suggest that there is not yet evidence to justify standard\nbenchmarks as reliable proxies of real-world AI biases, and we encourage\nfurther development of evaluations grounded in particular contexts."
                },
                "authors": [
                    {
                        "name": "Kristian Lum"
                    },
                    {
                        "name": "Jacy Reese Anthis"
                    },
                    {
                        "name": "Kevin Robinson"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Alexander D'Amour"
                    }
                ],
                "author_detail": {
                    "name": "Alexander D'Amour"
                },
                "author": "Alexander D'Amour",
                "arxiv_comment": "Published in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05088v1",
                "updated": "2025-06-05T14:34:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    34,
                    37,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:34:37Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    34,
                    37,
                    3,
                    156,
                    0
                ],
                "title": "Semi-Implicit Variational Inference via Kernelized Path Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Implicit Variational Inference via Kernelized Path Gradient Descent"
                },
                "summary": "Semi-implicit variational inference (SIVI) is a powerful framework for\napproximating complex posterior distributions, but training with the\nKullback-Leibler (KL) divergence can be challenging due to high variance and\nbias in high-dimensional settings. While current state-of-the-art semi-implicit\nvariational inference methods, particularly Kernel Semi-Implicit Variational\nInference (KSIVI), have been shown to work in high dimensions, training remains\nmoderately expensive. In this work, we propose a kernelized KL divergence\nestimator that stabilizes training through nonparametric smoothing. To further\nreduce the bias, we introduce an importance sampling correction. We provide a\ntheoretical connection to the amortized version of the Stein variational\ngradient descent, which estimates the score gradient via Stein's identity,\nshowing that both methods minimize the same objective, but our semi-implicit\napproach achieves lower gradient variance. In addition, our method's bias in\nfunction space is benign, leading to more stable and efficient optimization.\nEmpirical results demonstrate that our method outperforms or matches\nstate-of-the-art SIVI methods in both performance and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-implicit variational inference (SIVI) is a powerful framework for\napproximating complex posterior distributions, but training with the\nKullback-Leibler (KL) divergence can be challenging due to high variance and\nbias in high-dimensional settings. While current state-of-the-art semi-implicit\nvariational inference methods, particularly Kernel Semi-Implicit Variational\nInference (KSIVI), have been shown to work in high dimensions, training remains\nmoderately expensive. In this work, we propose a kernelized KL divergence\nestimator that stabilizes training through nonparametric smoothing. To further\nreduce the bias, we introduce an importance sampling correction. We provide a\ntheoretical connection to the amortized version of the Stein variational\ngradient descent, which estimates the score gradient via Stein's identity,\nshowing that both methods minimize the same objective, but our semi-implicit\napproach achieves lower gradient variance. In addition, our method's bias in\nfunction space is benign, leading to more stable and efficient optimization.\nEmpirical results demonstrate that our method outperforms or matches\nstate-of-the-art SIVI methods in both performance and training efficiency."
                },
                "authors": [
                    {
                        "name": "Tobias Pielok"
                    },
                    {
                        "name": "Bernd Bischl"
                    },
                    {
                        "name": "David Rügamer"
                    }
                ],
                "author_detail": {
                    "name": "David Rügamer"
                },
                "author": "David Rügamer",
                "arxiv_comment": "Preliminary version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05087v1",
                "updated": "2025-06-05T14:34:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    34,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:34:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    34,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Interpretable Multimodal Framework for Human-Centered Street Assessment:\n  Integrating Visual-Language Models for Perceptual Urban Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Multimodal Framework for Human-Centered Street Assessment:\n  Integrating Visual-Language Models for Perceptual Urban Diagnostics"
                },
                "summary": "While objective street metrics derived from imagery or GIS have become\nstandard in urban analytics, they remain insufficient to capture subjective\nperceptions essential to inclusive urban design. This study introduces a novel\nMultimodal Street Evaluation Framework (MSEF) that fuses a vision transformer\n(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable\ndual-output assessment of streetscapes. Leveraging over 15,000 annotated\nstreet-view images from Harbin, China, we fine-tune the framework using LoRA\nand P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1\nscore of 0.84 on objective features and 89.3 percent agreement with aggregated\nresident perceptions, validated across stratified socioeconomic geographies.\nBeyond classification accuracy, MSEF captures context-dependent contradictions:\nfor instance, informal commerce boosts perceived vibrancy while simultaneously\nreducing pedestrian comfort. It also identifies nonlinear and semantically\ncontingent patterns -- such as the divergent perceptual effects of\narchitectural transparency across residential and commercial zones -- revealing\nthe limits of universal spatial heuristics. By generating natural-language\nrationales grounded in attention mechanisms, the framework bridges sensory data\nwith socio-affective inference, enabling transparent diagnostics aligned with\nSDG 11. This work offers both methodological innovation in urban perception\nmodeling and practical utility for planning systems seeking to reconcile\ninfrastructural precision with lived experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While objective street metrics derived from imagery or GIS have become\nstandard in urban analytics, they remain insufficient to capture subjective\nperceptions essential to inclusive urban design. This study introduces a novel\nMultimodal Street Evaluation Framework (MSEF) that fuses a vision transformer\n(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable\ndual-output assessment of streetscapes. Leveraging over 15,000 annotated\nstreet-view images from Harbin, China, we fine-tune the framework using LoRA\nand P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1\nscore of 0.84 on objective features and 89.3 percent agreement with aggregated\nresident perceptions, validated across stratified socioeconomic geographies.\nBeyond classification accuracy, MSEF captures context-dependent contradictions:\nfor instance, informal commerce boosts perceived vibrancy while simultaneously\nreducing pedestrian comfort. It also identifies nonlinear and semantically\ncontingent patterns -- such as the divergent perceptual effects of\narchitectural transparency across residential and commercial zones -- revealing\nthe limits of universal spatial heuristics. By generating natural-language\nrationales grounded in attention mechanisms, the framework bridges sensory data\nwith socio-affective inference, enabling transparent diagnostics aligned with\nSDG 11. This work offers both methodological innovation in urban perception\nmodeling and practical utility for planning systems seeking to reconcile\ninfrastructural precision with lived experience."
                },
                "authors": [
                    {
                        "name": "HaoTian Lan"
                    }
                ],
                "author_detail": {
                    "name": "HaoTian Lan"
                },
                "author": "HaoTian Lan",
                "arxiv_comment": "24 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05079v1",
                "updated": "2025-06-05T14:27:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:27:40Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "title": "LLM-Guided Scenario-based GUI Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Scenario-based GUI Testing"
                },
                "summary": "The assurance of mobile app GUI is more and more significant. Automated GUI\ntesting approaches of different strategies have been developed, while there are\nstill huge gaps between the approaches and the app business logic, not taking\nthe completion of specific testing scenarios as the exploration target, leading\nto the exploration missing of critical app functionalities. Learning from the\nmanual testing, which takes testing scenarios with app business logic as the\nbasic granularity, in this paper, we utilize the LLMs to understand the\nsemantics presented in app GUI and how they are mapped in the testing context\nbased on specific testing scenarios. Then, scenario-based GUI tests are\ngenerated with the guidance of multi-agent collaboration. Specifically, we\npropose ScenGen, a novel LLM-guided scenario-based GUI testing approach\ninvolving five agents to respectively take responsibilities of different phases\nof the manual testing process. The Observer perceives the app GUI state by\nextracting GUI widgets and forming GUI layouts, understanding the expressed\nsemantics. Then the app GUI info is sent to the Decider to make decisions on\ntarget widgets based on the target testing scenarios. The decision-making\nprocess takes the completion of specific testing scenarios as the exploration\ntarget. The Executor then executes the demanding operations on the apps. The\nexecution results are checked by the Supervisor on whether the generated tests\nare consistent with the completion target of the testing scenarios, ensuring\nthe traceability of the test generation and execution. Furthermore, the\ncorresponding GUI test operations are recorded to the context memory by\nRecorder as an important basis for further decision-making, meanwhile\nmonitoring the runtime bug occurrences. ScenGen is evaluated and the results\nshow that ScenGen can effectively generate scenario-based GUI tests guided by\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assurance of mobile app GUI is more and more significant. Automated GUI\ntesting approaches of different strategies have been developed, while there are\nstill huge gaps between the approaches and the app business logic, not taking\nthe completion of specific testing scenarios as the exploration target, leading\nto the exploration missing of critical app functionalities. Learning from the\nmanual testing, which takes testing scenarios with app business logic as the\nbasic granularity, in this paper, we utilize the LLMs to understand the\nsemantics presented in app GUI and how they are mapped in the testing context\nbased on specific testing scenarios. Then, scenario-based GUI tests are\ngenerated with the guidance of multi-agent collaboration. Specifically, we\npropose ScenGen, a novel LLM-guided scenario-based GUI testing approach\ninvolving five agents to respectively take responsibilities of different phases\nof the manual testing process. The Observer perceives the app GUI state by\nextracting GUI widgets and forming GUI layouts, understanding the expressed\nsemantics. Then the app GUI info is sent to the Decider to make decisions on\ntarget widgets based on the target testing scenarios. The decision-making\nprocess takes the completion of specific testing scenarios as the exploration\ntarget. The Executor then executes the demanding operations on the apps. The\nexecution results are checked by the Supervisor on whether the generated tests\nare consistent with the completion target of the testing scenarios, ensuring\nthe traceability of the test generation and execution. Furthermore, the\ncorresponding GUI test operations are recorded to the context memory by\nRecorder as an important basis for further decision-making, meanwhile\nmonitoring the runtime bug occurrences. ScenGen is evaluated and the results\nshow that ScenGen can effectively generate scenario-based GUI tests guided by\nLLMs."
                },
                "authors": [
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Yuchen Ling"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Shaomin Zhu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01679v2",
                "updated": "2025-06-05T14:26:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    26,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2024-11-03T20:41:38Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    20,
                    41,
                    38,
                    6,
                    308,
                    0
                ],
                "title": "Autoformulation of Mathematical Optimization Models Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformulation of Mathematical Optimization Models Using LLMs"
                },
                "summary": "Mathematical optimization is fundamental to decision-making across diverse\ndomains, from operations research to healthcare. Yet, translating real-world\nproblems into optimization models remains a difficult task, often demanding\nspecialized expertise. This paper approaches the problem of\n$\\textit{autoformulation}$: the automated creation of solver-ready optimization\nmodels from natural language problem descriptions. We identify three core\nchallenges of autoformulation: $\\textit{(1)}$ the vast, problem-dependent\nhypothesis space, $\\textit{(2)}$ efficient and diverse exploration of this\nspace under uncertainty, and $\\textit{(3)}$ evaluation of formulation\ncorrectness against problem description. To address these challenges, we\npresent a novel method leveraging $\\textit{Large Language Models}$ (LLMs) with\n$\\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of\noptimization modeling to generate and systematically explore possible\nformulations. To enhance search efficiency, we introduce symbolic pruning to\neliminate trivially equivalent search paths (branches), and employ LLM-based\nevaluation of partial formulations to guide search. Empirical analysis on\nlinear and mixed-integer programming benchmarks demonstrates our method's\neffectiveness, with significant performance gains from both LLM-based value\nestimation and symbolic pruning techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical optimization is fundamental to decision-making across diverse\ndomains, from operations research to healthcare. Yet, translating real-world\nproblems into optimization models remains a difficult task, often demanding\nspecialized expertise. This paper approaches the problem of\n$\\textit{autoformulation}$: the automated creation of solver-ready optimization\nmodels from natural language problem descriptions. We identify three core\nchallenges of autoformulation: $\\textit{(1)}$ the vast, problem-dependent\nhypothesis space, $\\textit{(2)}$ efficient and diverse exploration of this\nspace under uncertainty, and $\\textit{(3)}$ evaluation of formulation\ncorrectness against problem description. To address these challenges, we\npresent a novel method leveraging $\\textit{Large Language Models}$ (LLMs) with\n$\\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of\noptimization modeling to generate and systematically explore possible\nformulations. To enhance search efficiency, we introduce symbolic pruning to\neliminate trivially equivalent search paths (branches), and employ LLM-based\nevaluation of partial formulations to guide search. Empirical analysis on\nlinear and mixed-integer programming benchmarks demonstrates our method's\neffectiveness, with significant performance gains from both LLM-based value\nestimation and symbolic pruning techniques."
                },
                "authors": [
                    {
                        "name": "Nicolás Astorga"
                    },
                    {
                        "name": "Tennison Liu"
                    },
                    {
                        "name": "Yuanzhang Xiao"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "*Astorga and Liu contributed equally. Published as a conference paper\n  at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05073v1",
                "updated": "2025-06-05T14:19:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:48Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    48,
                    3,
                    156,
                    0
                ],
                "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection\n  through Intent Differentiation and Emoji Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection\n  through Intent Differentiation and Emoji Interpretation"
                },
                "summary": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES ."
                },
                "authors": [
                    {
                        "name": "Soumitra Ghosh"
                    },
                    {
                        "name": "Gopendra Vikram Singh"
                    },
                    {
                        "name": "Shambhavi"
                    },
                    {
                        "name": "Sabarna Choudhury"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "arxiv_comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025 Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05070v1",
                "updated": "2025-06-05T14:18:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    18,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:18:21Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    18,
                    21,
                    3,
                    156,
                    0
                ],
                "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial\n  Optimization for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIVAL: Reinforcement Learning with Iterative and Adversarial\n  Optimization for Machine Translation"
                },
                "summary": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines."
                },
                "authors": [
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengran Yu"
                    },
                    {
                        "name": "Chenyu Shi"
                    },
                    {
                        "name": "Yanjun Zhao"
                    },
                    {
                        "name": "Xiaojing Liu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiayin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayin Wang"
                },
                "author": "Jiayin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14284v2",
                "updated": "2025-06-05T14:17:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    17,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-20T13:09:29Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    9,
                    29,
                    3,
                    172,
                    0
                ],
                "title": "Leveraging LLMs for Bangla Grammar Error Correction:Error\n  Categorization, Synthetic Data, and Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Bangla Grammar Error Correction:Error\n  Categorization, Synthetic Data, and Model Evaluation"
                },
                "summary": "Large Language Models (LLMs) perform exceedingly well in Natural Language\nUnderstanding (NLU) tasks for many languages including English. However,\ndespite being the fifth most-spoken language globally, Grammatical Error\nCorrection (GEC) in Bangla remains underdeveloped. In this work, we investigate\nhow LLMs can be leveraged for improving Bangla GEC. For that, we first do an\nextensive categorization of 12 error classes in Bangla, and take a survey of\nnative Bangla speakers to collect real-world errors. We next devise a\nrule-based noise injection method to create grammatically incorrect sentences\ncorresponding to correct ones. The Vaiyakarana dataset, thus created, consists\nof 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then\nused to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show\nthat instruction-tuning with \\name improves GEC performance of LLMs by 3-7\npercentage points as compared to the zero-shot setting, and makes them achieve\nhuman-like performance in grammatical error identification. Humans, though,\nremain superior in error correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform exceedingly well in Natural Language\nUnderstanding (NLU) tasks for many languages including English. However,\ndespite being the fifth most-spoken language globally, Grammatical Error\nCorrection (GEC) in Bangla remains underdeveloped. In this work, we investigate\nhow LLMs can be leveraged for improving Bangla GEC. For that, we first do an\nextensive categorization of 12 error classes in Bangla, and take a survey of\nnative Bangla speakers to collect real-world errors. We next devise a\nrule-based noise injection method to create grammatically incorrect sentences\ncorresponding to correct ones. The Vaiyakarana dataset, thus created, consists\nof 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then\nused to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show\nthat instruction-tuning with \\name improves GEC performance of LLMs by 3-7\npercentage points as compared to the zero-shot setting, and makes them achieve\nhuman-like performance in grammatical error identification. Humans, though,\nremain superior in error correction."
                },
                "authors": [
                    {
                        "name": "Pramit Bhattacharyya"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted at ACL Findings, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12171v2",
                "updated": "2025-06-05T14:16:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    46,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-13T10:33:58Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    33,
                    58,
                    3,
                    44,
                    0
                ],
                "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoRA: Gradient-driven Adaptive Low Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Luyang Zhou"
                    },
                    {
                        "name": "Shucun Ju"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05069v1",
                "updated": "2025-06-05T14:16:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:16:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance\n  LLM Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance\n  LLM Recommendation"
                },
                "summary": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D."
                },
                "authors": [
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05068v2",
                "updated": "2025-06-06T11:26:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    11,
                    26,
                    38,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T14:13:54Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    13,
                    54,
                    3,
                    156,
                    0
                ],
                "title": "Does It Make Sense to Speak of Introspection in Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does It Make Sense to Speak of Introspection in Large Language Models?"
                },
                "summary": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience."
                },
                "authors": [
                    {
                        "name": "Iulia M. Comsa"
                    },
                    {
                        "name": "Murray Shanahan"
                    }
                ],
                "author_detail": {
                    "name": "Murray Shanahan"
                },
                "author": "Murray Shanahan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05065v1",
                "updated": "2025-06-05T14:11:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    11,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:11:36Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    11,
                    36,
                    3,
                    156,
                    0
                ],
                "title": "UnHiPPO: Uncertainty-aware Initialization for State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnHiPPO: Uncertainty-aware Initialization for State Space Models"
                },
                "summary": "State space models are emerging as a dominant model class for sequence\nproblems with many relying on the HiPPO framework to initialize their dynamics.\nHowever, HiPPO fundamentally assumes data to be noise-free; an assumption often\nviolated in practice. We extend the HiPPO theory with measurement noise and\nderive an uncertainty-aware initialization for state space model dynamics. In\nour analysis, we interpret HiPPO as a linear stochastic control problem where\nthe data enters as a noise-free control signal. We then reformulate the problem\nso that the data become noisy outputs of a latent system and arrive at an\nalternative dynamics initialization that infers the posterior of this latent\nsystem from the data without increasing runtime. Our experiments show that our\ninitialization improves the resistance of state-space models to noise both at\ntraining and inference time. Find our implementation at\nhttps://cs.cit.tum.de/daml/unhippo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State space models are emerging as a dominant model class for sequence\nproblems with many relying on the HiPPO framework to initialize their dynamics.\nHowever, HiPPO fundamentally assumes data to be noise-free; an assumption often\nviolated in practice. We extend the HiPPO theory with measurement noise and\nderive an uncertainty-aware initialization for state space model dynamics. In\nour analysis, we interpret HiPPO as a linear stochastic control problem where\nthe data enters as a noise-free control signal. We then reformulate the problem\nso that the data become noisy outputs of a latent system and arrive at an\nalternative dynamics initialization that infers the posterior of this latent\nsystem from the data without increasing runtime. Our experiments show that our\ninitialization improves the resistance of state-space models to noise both at\ntraining and inference time. Find our implementation at\nhttps://cs.cit.tum.de/daml/unhippo."
                },
                "authors": [
                    {
                        "name": "Marten Lienen"
                    },
                    {
                        "name": "Abdullah Saydemir"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23847v2",
                "updated": "2025-06-05T14:07:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    7,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-28T18:19:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    19,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "Seven Security Challenges That Must be Solved in Cross-domain\n  Multi-agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seven Security Challenges That Must be Solved in Cross-domain\n  Multi-agent LLM Systems"
                },
                "summary": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."
                },
                "authors": [
                    {
                        "name": "Ronny Ko"
                    },
                    {
                        "name": "Jiseong Jeong"
                    },
                    {
                        "name": "Shuyuan Zheng"
                    },
                    {
                        "name": "Chuan Xiao"
                    },
                    {
                        "name": "Tae-Wan Kim"
                    },
                    {
                        "name": "Makoto Onizuka"
                    },
                    {
                        "name": "Won-Yong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Won-Yong Shin"
                },
                "author": "Won-Yong Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05062v1",
                "updated": "2025-06-05T14:06:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    6,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:06:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    6,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation"
                },
                "summary": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task."
                },
                "authors": [
                    {
                        "name": "Noy Sternlicht"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Noam Slonim"
                    }
                ],
                "author_detail": {
                    "name": "Noam Slonim"
                },
                "author": "Noam Slonim",
                "arxiv_comment": "Code: https://github.com/noy-sternlicht/Debatable-Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00847v3",
                "updated": "2025-06-05T14:03:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    53,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-02T10:49:10Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    10,
                    49,
                    10,
                    6,
                    61,
                    0
                ],
                "title": "Argument Summarization and its Evaluation in the Era of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument Summarization and its Evaluation in the Era of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum. We also show that among the four LLMs integrated in (i) and (ii),\nQwen-3-32B, despite having the fewest parameters, performs best, even\nsurpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum. We also show that among the four LLMs integrated in (i) and (ii),\nQwen-3-32B, despite having the fewest parameters, performs best, even\nsurpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms."
                },
                "authors": [
                    {
                        "name": "Moritz Altemeyer"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Johannes Daxenberger"
                    },
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Tim Altendorf"
                    },
                    {
                        "name": "Philipp Cimiano"
                    },
                    {
                        "name": "Benjamin Schiller"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Schiller"
                },
                "author": "Benjamin Schiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05061v1",
                "updated": "2025-06-05T14:03:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:03:18Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    18,
                    3,
                    156,
                    0
                ],
                "title": "A Survey on Vietnamese Document Analysis and Recognition: Challenges and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Vietnamese Document Analysis and Recognition: Challenges and\n  Future Directions"
                },
                "summary": "Vietnamese document analysis and recognition (DAR) is a crucial field with\napplications in digitization, information retrieval, and automation. Despite\nadvancements in OCR and NLP, Vietnamese text recognition faces unique\nchallenges due to its complex diacritics, tonal variations, and lack of\nlarge-scale annotated datasets. Traditional OCR methods often struggle with\nreal-world document variations, while deep learning approaches have shown\npromise but remain limited by data scarcity and generalization issues.\nRecently, large language models (LLMs) and vision-language models have\ndemonstrated remarkable improvements in text recognition and document\nunderstanding, offering a new direction for Vietnamese DAR. However, challenges\nsuch as domain adaptation, multimodal learning, and computational efficiency\npersist. This survey provide a comprehensive review of existing techniques in\nVietnamese document recognition, highlights key limitations, and explores how\nLLMs can revolutionize the field. We discuss future research directions,\nincluding dataset development, model optimization, and the integration of\nmultimodal approaches for improved document intelligence. By addressing these\ngaps, we aim to foster advancements in Vietnamese DAR and encourage\ncommunity-driven solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vietnamese document analysis and recognition (DAR) is a crucial field with\napplications in digitization, information retrieval, and automation. Despite\nadvancements in OCR and NLP, Vietnamese text recognition faces unique\nchallenges due to its complex diacritics, tonal variations, and lack of\nlarge-scale annotated datasets. Traditional OCR methods often struggle with\nreal-world document variations, while deep learning approaches have shown\npromise but remain limited by data scarcity and generalization issues.\nRecently, large language models (LLMs) and vision-language models have\ndemonstrated remarkable improvements in text recognition and document\nunderstanding, offering a new direction for Vietnamese DAR. However, challenges\nsuch as domain adaptation, multimodal learning, and computational efficiency\npersist. This survey provide a comprehensive review of existing techniques in\nVietnamese document recognition, highlights key limitations, and explores how\nLLMs can revolutionize the field. We discuss future research directions,\nincluding dataset development, model optimization, and the integration of\nmultimodal approaches for improved document intelligence. By addressing these\ngaps, we aim to foster advancements in Vietnamese DAR and encourage\ncommunity-driven solutions."
                },
                "authors": [
                    {
                        "name": "Anh Le"
                    },
                    {
                        "name": "Thanh Lam"
                    },
                    {
                        "name": "Dung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dung Nguyen"
                },
                "author": "Dung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05057v1",
                "updated": "2025-06-05T14:02:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    2,
                    12,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:02:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    2,
                    12,
                    3,
                    156,
                    0
                ],
                "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALL -- A Trainable Architecture for Enhancing LLM Performance in\n  Low-Resource Languages"
                },
                "summary": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains."
                },
                "authors": [
                    {
                        "name": "Moshe Ofer"
                    },
                    {
                        "name": "Orel Zamler"
                    },
                    {
                        "name": "Amos Azaria"
                    }
                ],
                "author_detail": {
                    "name": "Amos Azaria"
                },
                "author": "Amos Azaria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13438v2",
                "updated": "2025-06-05T13:55:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    55,
                    6,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-19T17:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
                },
                "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16691v2",
                "updated": "2025-06-05T13:52:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    52,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-20T20:19:19Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    20,
                    19,
                    19,
                    3,
                    79,
                    0
                ],
                "title": "Spatial-temporal prediction of forest attributes using latent Gaussian\n  models and inventory data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-temporal prediction of forest attributes using latent Gaussian\n  models and inventory data"
                },
                "summary": "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages and totals for plot-measured\nforest variables through design-based inference, assuming a fixed population\nand a probability sample of field plot locations. The fixed-population\nassumption and characteristics of the FIA sampling scheme make it difficult to\nestimate change in forest variables over time using design-based inference. We\npropose spatial-temporal models based on Gaussian processes as a flexible tool\nfor forest inventory data, capable of inferring forest variables and change\nthereof over arbitrary spatial and temporal domains. It is shown to be\nbeneficial for the covariance function governing the latent Gaussian process to\naccount for variation at multiple scales, separating spatially local variation\nfrom ecosystem-scale variation. We demonstrate a model for forest biomass\ndensity, inferring 20 years of biomass change within two US National Forests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages and totals for plot-measured\nforest variables through design-based inference, assuming a fixed population\nand a probability sample of field plot locations. The fixed-population\nassumption and characteristics of the FIA sampling scheme make it difficult to\nestimate change in forest variables over time using design-based inference. We\npropose spatial-temporal models based on Gaussian processes as a flexible tool\nfor forest inventory data, capable of inferring forest variables and change\nthereof over arbitrary spatial and temporal domains. It is shown to be\nbeneficial for the covariance function governing the latent Gaussian process to\naccount for variation at multiple scales, separating spatially local variation\nfrom ecosystem-scale variation. We demonstrate a model for forest biomass\ndensity, inferring 20 years of biomass change within two US National Forests."
                },
                "authors": [
                    {
                        "name": "Paul B. May"
                    },
                    {
                        "name": "Andrew O. Finley"
                    }
                ],
                "author_detail": {
                    "name": "Andrew O. Finley"
                },
                "author": "Andrew O. Finley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05041v1",
                "updated": "2025-06-05T13:45:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    45,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:45:21Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    45,
                    21,
                    3,
                    156,
                    0
                ],
                "title": "DACN: Dual-Attention Convolutional Network for Hyperspectral Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACN: Dual-Attention Convolutional Network for Hyperspectral Image\n  Super-Resolution"
                },
                "summary": "2D convolutional neural networks (CNNs) have attracted significant attention\nfor hyperspectral image super-resolution tasks. However, a key limitation is\ntheir reliance on local neighborhoods, which leads to a lack of global\ncontextual understanding. Moreover, band correlation and data scarcity continue\nto limit their performance. To mitigate these issues, we introduce DACN, a\ndual-attention convolutional network for hyperspectral image super-resolution.\nSpecifically, the model first employs augmented convolutions, integrating\nmulti-head attention to effectively capture both local and global feature\ndependencies. Next, we infer separate attention maps for the channel and\nspatial dimensions to determine where to focus across different channels and\nspatial positions. Furthermore, a custom optimized loss function is proposed\nthat combines L2 regularization with spatial-spectral gradient loss to ensure\naccurate spectral fidelity. Experimental results on two hyperspectral datasets\ndemonstrate that the combination of multi-head attention and channel attention\noutperforms either attention mechanism used individually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D convolutional neural networks (CNNs) have attracted significant attention\nfor hyperspectral image super-resolution tasks. However, a key limitation is\ntheir reliance on local neighborhoods, which leads to a lack of global\ncontextual understanding. Moreover, band correlation and data scarcity continue\nto limit their performance. To mitigate these issues, we introduce DACN, a\ndual-attention convolutional network for hyperspectral image super-resolution.\nSpecifically, the model first employs augmented convolutions, integrating\nmulti-head attention to effectively capture both local and global feature\ndependencies. Next, we infer separate attention maps for the channel and\nspatial dimensions to determine where to focus across different channels and\nspatial positions. Furthermore, a custom optimized loss function is proposed\nthat combines L2 regularization with spatial-spectral gradient loss to ensure\naccurate spectral fidelity. Experimental results on two hyperspectral datasets\ndemonstrate that the combination of multi-head attention and channel attention\noutperforms either attention mechanism used individually."
                },
                "authors": [
                    {
                        "name": "Usman Muhammad"
                    },
                    {
                        "name": "Jorma Laaksonen"
                    }
                ],
                "author_detail": {
                    "name": "Jorma Laaksonen"
                },
                "author": "Jorma Laaksonen",
                "arxiv_journal_ref": "The 33rd European Signal Processing Conference (EUSIPCO 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05038v1",
                "updated": "2025-06-05T13:42:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    42,
                    39,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:42:39Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    42,
                    39,
                    3,
                    156,
                    0
                ],
                "title": "Automatic Robustness Stress Testing of LLMs as Mathematical Problem\n  Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Robustness Stress Testing of LLMs as Mathematical Problem\n  Solvers"
                },
                "summary": "Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker."
                },
                "authors": [
                    {
                        "name": "Yutao Hou"
                    },
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Yihan Jiang"
                    },
                    {
                        "name": "Xuetao Wei"
                    },
                    {
                        "name": "Hailiang Huang"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09117v3",
                "updated": "2025-06-05T13:34:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    34,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-12T07:08:54Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    8,
                    54,
                    2,
                    71,
                    0
                ],
                "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs"
                },
                "summary": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Xiaojiang Du"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05020v1",
                "updated": "2025-06-05T13:27:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    41,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:27:41Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    41,
                    3,
                    156,
                    0
                ],
                "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System"
                },
                "summary": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning."
                },
                "authors": [
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Zhaoqi Ma"
                    },
                    {
                        "name": "Yunong Li"
                    },
                    {
                        "name": "Junichiro Sugihara"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Jinjie Li"
                    },
                    {
                        "name": "Moju Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Moju Zhao"
                },
                "author": "Moju Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05019v1",
                "updated": "2025-06-05T13:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    28,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:27:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series\n  Analysis"
                },
                "summary": "Pure time series forecasting tasks typically focus exclusively on numerical\nfeatures; however, real-world financial decision-making demands the comparison\nand analysis of heterogeneous sources of information. Recent advances in deep\nlearning and large scale language models (LLMs) have made significant strides\nin capturing sentiment and other qualitative signals, thereby enhancing the\naccuracy of financial time series predictions. Despite these advances, most\nexisting datasets consist solely of price series and news text, are confined to\na single market, and remain limited in scale. In this paper, we introduce\nFinMultiTime, the first large scale, multimodal financial time series dataset.\nFinMultiTime temporally aligns four distinct modalities financial news,\nstructured financial tables, K-line technical charts, and stock price time\nseries across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from\n2009 to 2025 in the United States and China, the dataset totals 112.6 GB and\nprovides minute-level, daily, and quarterly resolutions, thus capturing short,\nmedium, and long term market signals with high fidelity. Our experiments\ndemonstrate that (1) scale and data quality markedly boost prediction accuracy;\n(2) multimodal fusion yields moderate gains in Transformer models; and (3) a\nfully reproducible pipeline enables seamless dataset updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pure time series forecasting tasks typically focus exclusively on numerical\nfeatures; however, real-world financial decision-making demands the comparison\nand analysis of heterogeneous sources of information. Recent advances in deep\nlearning and large scale language models (LLMs) have made significant strides\nin capturing sentiment and other qualitative signals, thereby enhancing the\naccuracy of financial time series predictions. Despite these advances, most\nexisting datasets consist solely of price series and news text, are confined to\na single market, and remain limited in scale. In this paper, we introduce\nFinMultiTime, the first large scale, multimodal financial time series dataset.\nFinMultiTime temporally aligns four distinct modalities financial news,\nstructured financial tables, K-line technical charts, and stock price time\nseries across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from\n2009 to 2025 in the United States and China, the dataset totals 112.6 GB and\nprovides minute-level, daily, and quarterly resolutions, thus capturing short,\nmedium, and long term market signals with high fidelity. Our experiments\ndemonstrate that (1) scale and data quality markedly boost prediction accuracy;\n(2) multimodal fusion yields moderate gains in Transformer models; and (3) a\nfully reproducible pipeline enables seamless dataset updates."
                },
                "authors": [
                    {
                        "name": "Wenyan Xu"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Xiyu Wang"
                    },
                    {
                        "name": "Yanxiang Ma"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05346v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets"
                },
                "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers."
                },
                "authors": [
                    {
                        "name": "Lei Hsiung"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yung-Chen Tang"
                    },
                    {
                        "name": "Linyue Song"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Yaoqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaoqing Yang"
                },
                "author": "Yaoqing Yang",
                "arxiv_comment": "Project Page: https://hsiung.cc/llm-similarity-risk/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05341v1",
                "updated": "2025-06-05T17:59:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:42Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    42,
                    3,
                    156,
                    0
                ],
                "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning"
                },
                "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility."
                },
                "authors": [
                    {
                        "name": "Xingjian Ran"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Mulin Yu"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Project Page: https://directlayout.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05336v1",
                "updated": "2025-06-05T17:59:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    29,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:29Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    29,
                    3,
                    156,
                    0
                ],
                "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing"
                },
                "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo."
                },
                "authors": [
                    {
                        "name": "Ghazi Shazan Ahmad"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Hanan Gani"
                    },
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "20 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05334v1",
                "updated": "2025-06-05T17:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    26,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:26Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    26,
                    3,
                    156,
                    0
                ],
                "title": "Search Arena: Analyzing Search-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search Arena: Analyzing Search-Augmented LLMs"
                },
                "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena."
                },
                "authors": [
                    {
                        "name": "Mihran Miroyan"
                    },
                    {
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "name": "Logan King"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Xinyan Hu"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Narges Norouzi"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "arxiv_comment": "Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05331v1",
                "updated": "2025-06-05T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    2,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    2,
                    3,
                    156,
                    0
                ],
                "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT"
                },
                "authors": [
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Code is released at https://github.com/xinyan-cxy/MINT-CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05316v1",
                "updated": "2025-06-05T17:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    43,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    43,
                    3,
                    156,
                    0
                ],
                "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay"
                },
                "summary": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05314v1",
                "updated": "2025-06-05T17:55:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    23,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:55:23Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    23,
                    3,
                    156,
                    0
                ],
                "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility."
                },
                "authors": [
                    {
                        "name": "Taha Entesari"
                    },
                    {
                        "name": "Arman Hatami"
                    },
                    {
                        "name": "Rinat Khaziev"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Mahyar Fazlyab"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Fazlyab"
                },
                "author": "Mahyar Fazlyab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v2",
                "updated": "2025-06-05T17:55:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    55,
                    7,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?"
                },
                "summary": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advances in Large\nLanguage Models (LLMs) and Neural Machine Translation have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates current\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\nlimitations in LRL translation capability. We also explore alternative data\nsources, including news articles and bilingual dictionaries, and demonstrate\nhow knowledge distillation from large pre-trained teacher models can\nsignificantly improve the performance of small LLMs on LRL translation tasks.\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\ndifferent fine-tuning configurations, providing practical insights on optimal\ndata scale, training efficiency, and the preservation of generalization\ncapabilities of models under study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advances in Large\nLanguage Models (LLMs) and Neural Machine Translation have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates current\nLLMs in 200 languages using the FLORES-200 benchmark and demonstrates their\nlimitations in LRL translation capability. We also explore alternative data\nsources, including news articles and bilingual dictionaries, and demonstrate\nhow knowledge distillation from large pre-trained teacher models can\nsignificantly improve the performance of small LLMs on LRL translation tasks.\nFor example, this approach increases EN->LB with the LLM-as-a-Judge score on\nthe validation set from 0.36 to 0.89 for Llama-3.2-3B. Furthermore, we examine\ndifferent fine-tuning configurations, providing practical insights on optimal\ndata scale, training efficiency, and the preservation of generalization\ncapabilities of models under study."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05309v1",
                "updated": "2025-06-05T17:53:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    53,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:53:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    53,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games"
                },
                "summary": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated."
                },
                "authors": [
                    {
                        "name": "Niv Eckhaus"
                    },
                    {
                        "name": "Uri Berger"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05305v1",
                "updated": "2025-06-05T17:52:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    52,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:52:30Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    52,
                    30,
                    3,
                    156,
                    0
                ],
                "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProRefine: Inference-time Prompt Refinement with Textual Feedback"
                },
                "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI."
                },
                "authors": [
                    {
                        "name": "Deepak Pandita"
                    },
                    {
                        "name": "Tharindu Cyril Weerasooriya"
                    },
                    {
                        "name": "Ankit Parag Shah"
                    },
                    {
                        "name": "Christopher M. Homan"
                    },
                    {
                        "name": "Wei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wei"
                },
                "author": "Wei Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05302v1",
                "updated": "2025-06-05T17:51:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    39,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:51:39Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    51,
                    39,
                    3,
                    156,
                    0
                ],
                "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in\n  Images and Videos"
                },
                "summary": "We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Perceive Anything Model (PAM), a conceptually straightforward and\nefficient framework for comprehensive region-level visual understanding in\nimages and videos. Our approach extends the powerful segmentation model SAM 2\nby integrating Large Language Models (LLMs), enabling simultaneous object\nsegmentation with the generation of diverse, region-specific semantic outputs,\nincluding categories, label definition, functional explanations, and detailed\ncaptions. A key component, Semantic Perceiver, is introduced to efficiently\ntransform SAM 2's rich visual features, which inherently carry general vision,\nlocalization, and semantic priors into multi-modal tokens for LLM\ncomprehension. To support robust multi-granularity understanding, we also\ndevelop a dedicated data refinement and augmentation pipeline, yielding a\nhigh-quality dataset of 1.5M image and 0.6M video region-semantic annotations,\nincluding novel region-level streaming video caption data. PAM is designed for\nlightweightness and efficiency, while also demonstrates strong performance\nacross a diverse range of region understanding tasks. It runs 1.2-2.4x faster\nand consumes less GPU memory than prior approaches, offering a practical\nsolution for real-world applications. We believe that our effective approach\nwill serve as a strong baseline for future research in region-level visual\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Xinyu Wei"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Tingwei Chen"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "19 pages, 13 figures, Website: https://Perceive-Anything.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05296v1",
                "updated": "2025-06-05T17:48:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    39,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:48:39Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    39,
                    3,
                    156,
                    0
                ],
                "title": "Control Tax: The Price of Keeping AI in Check",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Tax: The Price of Keeping AI in Check"
                },
                "summary": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts."
                },
                "authors": [
                    {
                        "name": "Mikhail Terekhov"
                    },
                    {
                        "name": "Zhen Ning David Liu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Samuel Albanie"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Albanie"
                },
                "author": "Samuel Albanie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05295v1",
                "updated": "2025-06-05T17:48:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    19,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:48:19Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    48,
                    19,
                    3,
                    156,
                    0
                ],
                "title": "Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Complexity and Representation Ability of Test-time Scaling\n  Paradigms"
                },
                "summary": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods."
                },
                "authors": [
                    {
                        "name": "Baihe Huang"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Jiantao Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Jiao"
                },
                "author": "Jiantao Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05278v1",
                "updated": "2025-06-05T17:33:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    33,
                    2,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:33:02Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    33,
                    2,
                    3,
                    156,
                    0
                ],
                "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications."
                },
                "authors": [
                    {
                        "name": "Nan Huo"
                    },
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Ge Qu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Reynold Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Reynold Cheng"
                },
                "author": "Reynold Cheng",
                "arxiv_comment": "Accepted by ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05274v1",
                "updated": "2025-06-05T17:31:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    31,
                    17,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:31:17Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    31,
                    17,
                    3,
                    156,
                    0
                ],
                "title": "From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos"
                },
                "summary": "Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82."
                },
                "authors": [
                    {
                        "name": "Animesh Gupta"
                    },
                    {
                        "name": "Jay Parmar"
                    },
                    {
                        "name": "Ishan Rajendrakumar Dave"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18959v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18959v4",
                "updated": "2025-06-05T17:25:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    25,
                    49,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information"
                },
                "summary": "Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/."
                },
                "authors": [
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Jithendaraa Subramanian"
                    },
                    {
                        "name": "Roland Riachi"
                    },
                    {
                        "name": "James Requeima"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "arxiv_comment": "ICML 2025. First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18959v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18959v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05265v1",
                "updated": "2025-06-05T17:24:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    24,
                    37,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:24:37Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    24,
                    37,
                    3,
                    156,
                    0
                ],
                "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams"
                },
                "summary": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination."
                },
                "authors": [
                    {
                        "name": "Mohammed Almutairi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Almutairi"
                },
                "author": "Mohammed Almutairi",
                "arxiv_doi": "10.1145/3699682.3727574",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3699682.3727574",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, UMAP 25, June 16_19, 2025, New York City, NY, USA",
                "arxiv_journal_ref": "ACM International Conference on User Modeling, Adaptation and\n  Personalization 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05260v1",
                "updated": "2025-06-05T17:21:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    21,
                    16,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:21:16Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    21,
                    16,
                    3,
                    156,
                    0
                ],
                "title": "LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanPO: Lean Preference Optimization for Likelihood Alignment in\n  Video-LLMs"
                },
                "summary": "Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Video Large Language Models (Video-LLMs) adopt preference alignment\ntechniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin\nbetween a winning response ($y_w$) and a losing response ($y_l$). However, the\nlikelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta\n(y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training,\ninadvertently boosting the probabilities of non-target responses. In this\npaper, we systematically revisit this phenomenon from LLMs to Video-LLMs,\nshowing that it intensifies when dealing with the redundant complexity of video\ncontent. To alleviate the impact of this phenomenon, we propose \\emph{Lean\nPreference Optimization} (LeanPO), a reference-free approach that reformulates\nthe implicit reward as the average likelihood of the response with respect to\nthe policy model. A key component of LeanPO is the reward-trustworthiness\ncorrelated self-generated preference data pipeline, which carefully infuses\nrelevant prior knowledge into the model while continuously refining the\npreference data via self-reflection. This allows the policy model to obtain\nhigh-quality paired data and accurately estimate the newly defined reward, thus\nmitigating the unintended drop. In addition, we introduce a dynamic label\nsmoothing strategy that mitigates the impact of noise in responses from diverse\nvideo content, preventing the model from overfitting to spurious details.\nExtensive experiments demonstrate that LeanPO significantly enhances the\nperformance of state-of-the-art Video-LLMs, consistently boosting baselines of\nvarying capacities with minimal additional training overhead. Moreover, LeanPO\noffers a simple yet effective solution for aligning Video-LLM preferences with\nhuman trustworthiness, paving the way toward the reliable and efficient\nVideo-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Peixi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Peixi Peng"
                },
                "author": "Peixi Peng",
                "arxiv_comment": "Code: https://github.com/Wang-Xiaodong1899/LeanPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05251v1",
                "updated": "2025-06-05T17:11:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    11,
                    53,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:11:53Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    11,
                    53,
                    3,
                    156,
                    0
                ],
                "title": "Cooperation and the Design of Public Goods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperation and the Design of Public Goods"
                },
                "summary": "We consider the cooperative elements that arise in the design of public\ngoods, such as transportation policies and infrastructure. These involve a\nvariety of stakeholders: governments, businesses, advocates, and users. Their\neventual deployment depends on the decision maker's ability to garner\nsufficient support from each of these groups; we formalize these strategic\nrequirements from the perspective of cooperative game theory. Specifically, we\nintroduce non-transferable utility, linear production (NTU LP) games, which\ncombine the game-theoretic tensions inherent in public decision-making with the\nmodeling flexibility of linear programming. We derive structural properties\nregarding the non-emptiness, representability and complexity of the core, a\nsolution concept that models the viability of cooperation. In particular, we\nprovide fairly general sufficient conditions under which the core of an NTU LP\ngame is guaranteed to be non-empty, prove that determining membership in the\ncore is co-NP-complete, and develop a cutting plane algorithm to optimize\nvarious social welfare objectives subject to core membership. Lastly, we apply\nthese results in a data-driven case study on service plan optimization for the\nChicago bus system. As our study illustrates, cooperation is necessary for the\nsuccessful deployment of transportation service plans and similar public goods,\nbut it may also have adverse or counterintuitive distributive implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the cooperative elements that arise in the design of public\ngoods, such as transportation policies and infrastructure. These involve a\nvariety of stakeholders: governments, businesses, advocates, and users. Their\neventual deployment depends on the decision maker's ability to garner\nsufficient support from each of these groups; we formalize these strategic\nrequirements from the perspective of cooperative game theory. Specifically, we\nintroduce non-transferable utility, linear production (NTU LP) games, which\ncombine the game-theoretic tensions inherent in public decision-making with the\nmodeling flexibility of linear programming. We derive structural properties\nregarding the non-emptiness, representability and complexity of the core, a\nsolution concept that models the viability of cooperation. In particular, we\nprovide fairly general sufficient conditions under which the core of an NTU LP\ngame is guaranteed to be non-empty, prove that determining membership in the\ncore is co-NP-complete, and develop a cutting plane algorithm to optimize\nvarious social welfare objectives subject to core membership. Lastly, we apply\nthese results in a data-driven case study on service plan optimization for the\nChicago bus system. As our study illustrates, cooperation is necessary for the\nsuccessful deployment of transportation service plans and similar public goods,\nbut it may also have adverse or counterintuitive distributive implications."
                },
                "authors": [
                    {
                        "name": "J. Carlos Martínez Mori"
                    },
                    {
                        "name": "Alejandro Toriello"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Toriello"
                },
                "author": "Alejandro Toriello",
                "arxiv_comment": "26th ACM Conference on Economics and Computation (EC '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A12, 90C90, 90B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00038v2",
                "updated": "2025-06-05T17:10:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    10,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-25T08:41:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "From Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors"
                },
                "summary": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Teli Liu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Zhiyi Yin"
                    },
                    {
                        "name": "Jiangyu Lei"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04075v2",
                "updated": "2025-06-05T17:09:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    9,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-07T02:26:17Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    26,
                    17,
                    2,
                    127,
                    0
                ],
                "title": "Rethinking LLM Advancement: Compute-Dependent and Independent Paths to\n  Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Advancement: Compute-Dependent and Independent Paths to\n  Progress"
                },
                "summary": "Regulatory efforts to govern large language model (LLM) development have\npredominantly focused on restricting access to high-performance computational\nresources. This study evaluates the efficacy of such measures by examining\nwhether LLM capabilities can advance through algorithmic innovation in\ncompute-constrained environments. We propose a novel framework distinguishing\ncompute-dependent innovations--which yield disproportionate benefits at high\ncompute--from compute-independent innovations, which improve efficiency across\ncompute scales. The impact is quantified using Compute-Equivalent Gain (CEG).\nExperimental validation with nanoGPT models confirms that compute-independent\nadvancements yield significant performance gains (e.g., with combined CEG up to\n$3.5\\times$) across the tested scales. In contrast, compute-dependent\nadvancements were detrimental to performance at smaller experimental scales,\nbut showed improved CEG (on par with the baseline) as model size increased, a\ntrend consistent with their definition of yielding primary benefits at higher\ncompute. Crucially, these findings indicate that restrictions on computational\nhardware, while potentially slowing LLM progress, are insufficient to prevent\nall capability gains driven by algorithmic advancements. We argue that\neffective AI oversight must therefore incorporate mechanisms for understanding,\nanticipating, and potentially guiding algorithmic research, moving beyond a\nsingular focus on hardware. The proposed framework also serves as an analytical\ntool for forecasting AI progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regulatory efforts to govern large language model (LLM) development have\npredominantly focused on restricting access to high-performance computational\nresources. This study evaluates the efficacy of such measures by examining\nwhether LLM capabilities can advance through algorithmic innovation in\ncompute-constrained environments. We propose a novel framework distinguishing\ncompute-dependent innovations--which yield disproportionate benefits at high\ncompute--from compute-independent innovations, which improve efficiency across\ncompute scales. The impact is quantified using Compute-Equivalent Gain (CEG).\nExperimental validation with nanoGPT models confirms that compute-independent\nadvancements yield significant performance gains (e.g., with combined CEG up to\n$3.5\\times$) across the tested scales. In contrast, compute-dependent\nadvancements were detrimental to performance at smaller experimental scales,\nbut showed improved CEG (on par with the baseline) as model size increased, a\ntrend consistent with their definition of yielding primary benefits at higher\ncompute. Crucially, these findings indicate that restrictions on computational\nhardware, while potentially slowing LLM progress, are insufficient to prevent\nall capability gains driven by algorithmic advancements. We argue that\neffective AI oversight must therefore incorporate mechanisms for understanding,\nanticipating, and potentially guiding algorithmic research, moving beyond a\nsingular focus on hardware. The proposed framework also serves as an analytical\ntool for forecasting AI progress."
                },
                "authors": [
                    {
                        "name": "Jack Sanderson"
                    },
                    {
                        "name": "Teddy Foley"
                    },
                    {
                        "name": "Spencer Guo"
                    },
                    {
                        "name": "Anqi Qu"
                    },
                    {
                        "name": "Henry Josephson"
                    }
                ],
                "author_detail": {
                    "name": "Henry Josephson"
                },
                "author": "Henry Josephson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05244v1",
                "updated": "2025-06-05T17:03:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    3,
                    26,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:03:26Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    3,
                    26,
                    3,
                    156,
                    0
                ],
                "title": "How to Train Your Dragon: Quantum Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your Dragon: Quantum Neural Networks"
                },
                "summary": "Training of neural networks (NNs) has emerged as a major consumer of both\ncomputational and energy resources. We demonstrate that quantum annealing\nplatforms, such as D-Wave, can enable fast and efficient training of classical\nNNs, which are then deployable on conventional hardware. From a physics\nperspective, NN training can be viewed as a dynamical phase transition: the\nsystem evolves from an initial spin glass state to a highly ordered, trained\nstate. This process involves eliminating numerous undesired minima in its\nenergy landscape--akin to cutting off the ever-regenerating heads of a dragon.\nThe advantage of annealing devices is their ability to rapidly find multiple\ndeep states (dragon heads to be cut). We found that this quantum-assisted\ntraining achieves superior performance scaling compared to classical\nbackpropagation methods, with a notably higher scaling exponent (1.01 vs.\n0.78). It may be further increased up to a factor of 2 with a fully coherent\nquantum platform using a variant of the Grover algorithm. Furthermore, we argue\nthat even a modestly sized annealer can be beneficial to train a deep NN by\nbeing applied sequentially to a few layers at a time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of neural networks (NNs) has emerged as a major consumer of both\ncomputational and energy resources. We demonstrate that quantum annealing\nplatforms, such as D-Wave, can enable fast and efficient training of classical\nNNs, which are then deployable on conventional hardware. From a physics\nperspective, NN training can be viewed as a dynamical phase transition: the\nsystem evolves from an initial spin glass state to a highly ordered, trained\nstate. This process involves eliminating numerous undesired minima in its\nenergy landscape--akin to cutting off the ever-regenerating heads of a dragon.\nThe advantage of annealing devices is their ability to rapidly find multiple\ndeep states (dragon heads to be cut). We found that this quantum-assisted\ntraining achieves superior performance scaling compared to classical\nbackpropagation methods, with a notably higher scaling exponent (1.01 vs.\n0.78). It may be further increased up to a factor of 2 with a fully coherent\nquantum platform using a variant of the Grover algorithm. Furthermore, we argue\nthat even a modestly sized annealer can be beneficial to train a deep NN by\nbeing applied sequentially to a few layers at a time."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Alex Kamenev"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kamenev"
                },
                "author": "Alex Kamenev",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05243v1",
                "updated": "2025-06-05T17:02:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    2,
                    52,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:02:52Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    2,
                    52,
                    3,
                    156,
                    0
                ],
                "title": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection"
                },
                "summary": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common approach to hallucination detection casts it as a natural language\ninference (NLI) task, often using LLMs to classify whether the generated text\nis entailed by corresponding reference texts. Since entailment classification\nis a complex reasoning task, one would expect that LLMs could benefit from\ngenerating an explicit reasoning process, as in CoT reasoning or the explicit\n``thinking'' of recent reasoning models. In this work, we propose that guiding\nsuch models to perform a systematic and comprehensive reasoning process -- one\nthat both decomposes the text into smaller facts and also finds evidence in the\nsource for each fact -- allows models to execute much finer-grained and\naccurate entailment decisions, leading to increased performance. To that end,\nwe define a 3-step reasoning process, consisting of (i) claim decomposition,\n(ii) sub-claim attribution and entailment classification, and (iii) aggregated\nclassification, showing that such guided reasoning indeed yields improved\nhallucination detection. Following this reasoning framework, we introduce an\nanalysis scheme, consisting of several metrics that measure the quality of the\nintermediate reasoning steps, which provided additional empirical evidence for\nthe improved quality of our guided reasoning scheme."
                },
                "authors": [
                    {
                        "name": "Ron Eliav"
                    },
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Eran Hirsch"
                    },
                    {
                        "name": "Shahaf Bassan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05242v1",
                "updated": "2025-06-05T17:01:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    1,
                    28,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:01:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    1,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid\n  Neuron Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid\n  Neuron Encryption"
                },
                "summary": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Haohua Du"
                    },
                    {
                        "name": "Junyang Wang"
                    },
                    {
                        "name": "Haifeng Sun"
                    },
                    {
                        "name": "Kaiwen Guo"
                    },
                    {
                        "name": "Haikuo Yu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05237v1",
                "updated": "2025-06-05T16:56:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    56,
                    41,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:56:41Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    56,
                    41,
                    3,
                    156,
                    0
                ],
                "title": "CSI2Vec: Towards a Universal CSI Feature Representation for Positioning\n  and Channel Charting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSI2Vec: Towards a Universal CSI Feature Representation for Positioning\n  and Channel Charting"
                },
                "summary": "Natural language processing techniques, such as Word2Vec, have demonstrated\nexceptional capabilities in capturing semantic and syntactic relationships of\ntext through vector embeddings. Inspired by this technique, we propose CSI2Vec,\na self-supervised framework for generating universal and robust channel state\ninformation (CSI) representations tailored to CSI-based positioning (POS) and\nchannel charting (CC). CSI2Vec learns compact vector embeddings across various\nwireless scenarios, capturing spatial relationships between user equipment\npositions without relying on CSI reconstruction or ground-truth position\ninformation. We implement CSI2Vec as a neural network that is trained across\nvarious deployment setups (i.e., the spatial arrangement of radio equipment and\nscatterers) and radio setups (RSs) (i.e., the specific hardware used), ensuring\nrobustness to aspects such as differences in the environment, the number of\nused antennas, or allocated set of subcarriers. CSI2Vec abstracts the RS by\ngenerating compact vector embeddings that capture essential spatial\ninformation, avoiding the need for full CSI transmission or reconstruction\nwhile also reducing complexity and improving processing efficiency of\ndownstream tasks. Simulations with ray-tracing and real-world CSI datasets\ndemonstrate CSI2Vec's effectiveness in maintaining excellent POS and CC\nperformance while reducing computational demands and storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing techniques, such as Word2Vec, have demonstrated\nexceptional capabilities in capturing semantic and syntactic relationships of\ntext through vector embeddings. Inspired by this technique, we propose CSI2Vec,\na self-supervised framework for generating universal and robust channel state\ninformation (CSI) representations tailored to CSI-based positioning (POS) and\nchannel charting (CC). CSI2Vec learns compact vector embeddings across various\nwireless scenarios, capturing spatial relationships between user equipment\npositions without relying on CSI reconstruction or ground-truth position\ninformation. We implement CSI2Vec as a neural network that is trained across\nvarious deployment setups (i.e., the spatial arrangement of radio equipment and\nscatterers) and radio setups (RSs) (i.e., the specific hardware used), ensuring\nrobustness to aspects such as differences in the environment, the number of\nused antennas, or allocated set of subcarriers. CSI2Vec abstracts the RS by\ngenerating compact vector embeddings that capture essential spatial\ninformation, avoiding the need for full CSI transmission or reconstruction\nwhile also reducing complexity and improving processing efficiency of\ndownstream tasks. Simulations with ray-tracing and real-world CSI datasets\ndemonstrate CSI2Vec's effectiveness in maintaining excellent POS and CC\nperformance while reducing computational demands and storage."
                },
                "authors": [
                    {
                        "name": "Victoria Palhares"
                    },
                    {
                        "name": "Sueda Taner"
                    },
                    {
                        "name": "Christoph Studer"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Studer"
                },
                "author": "Christoph Studer",
                "arxiv_comment": "Submitted to a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00921v2",
                "updated": "2025-06-05T16:55:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    55,
                    6,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-02T21:19:53Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    19,
                    53,
                    6,
                    33,
                    0
                ],
                "title": "Blink of an eye: a simple theory for feature localization in generative\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blink of an eye: a simple theory for feature localization in generative\n  models"
                },
                "summary": "Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can exhibit unexpected behavior in the blink of an eye.\nIn a recent computer use demo, a language model switched from coding to\nGoogling pictures of Yellowstone, and these sudden shifts in behavior have also\nbeen observed in reasoning patterns and jailbreaks. This phenomenon is not\nunique to autoregressive models: in diffusion models, key features of the final\noutput are decided in narrow ``critical windows'' of the generation process. In\nthis work we develop a simple, unifying theory to explain this phenomenon using\nthe formalism of stochastic localization samplers. We show that it emerges\ngenerically as the generation process localizes to a sub-population of the\ndistribution it models.\n  While critical windows have been studied at length in diffusion models,\nexisting theory heavily relies on strong distributional assumptions and the\nparticulars of Gaussian diffusion. In contrast to existing work our theory (1)\napplies to autoregressive and diffusion models; (2) makes no distributional\nassumptions; (3) quantitatively improves previous bounds even when specialized\nto diffusions; and (4) requires basic tools and no stochastic calculus or\nstatistical-physics-based machinery. We also identify an intriguing connection\nto the all-or-nothing phenomenon from statistical inference. Finally, we\nvalidate our predictions empirically for LLMs and find that critical windows\noften coincide with failures in problem solving for various math and reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Marvin Li"
                    },
                    {
                        "name": "Aayush Karan"
                    },
                    {
                        "name": "Sitan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sitan Chen"
                },
                "author": "Sitan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08503v3",
                "updated": "2025-06-06T01:51:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    51,
                    44,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-12T15:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"
                },
                "summary": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs. Code and data are available at\nhttps://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs. Code and data are available at\nhttps://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks"
                },
                "authors": [
                    {
                        "name": "Jiahe Jin"
                    },
                    {
                        "name": "Yanheng He"
                    },
                    {
                        "name": "Mingyan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Yang"
                },
                "author": "Mingyan Yang",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05218v1",
                "updated": "2025-06-05T16:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    57,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    57,
                    3,
                    156,
                    0
                ],
                "title": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm"
                },
                "summary": "We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR."
                },
                "authors": [
                    {
                        "name": "Zhang Li"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Zhiyin Ma"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Zidun Guo"
                    },
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07301v2",
                "updated": "2025-06-05T16:34:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    34,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-01-13T13:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models."
                },
                "authors": [
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05213v1",
                "updated": "2025-06-05T16:27:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    27,
                    49,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:27:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    27,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "LLM-First Search: Self-Guided Exploration of the Solution Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-First Search: Self-Guided Exploration of the Solution Space"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable improvements in\nreasoning and planning through increased test-time compute, often by framing\nproblem-solving as a search process. While methods like Monte Carlo Tree Search\n(MCTS) have proven effective in some domains, their reliance on fixed\nexploration hyperparameters limits their adaptability across tasks of varying\ndifficulty, rendering them impractical or expensive in certain settings. In\nthis paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM\nSelf-Guided Search} method that removes the need for pre-defined search\nstrategies by empowering the LLM to autonomously control the search process via\nself-guided exploration. Rather than relying on external heuristics or\nhardcoded policies, the LLM evaluates whether to pursue the current search path\nor explore alternative branches based on its internal scoring mechanisms. This\nenables more flexible and context-sensitive reasoning without requiring manual\ntuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku\nagainst three classic widely-used search algorithms, Tree-of-Thoughts' Breadth\nFirst Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which\nhave been used to achieve SotA results on a range of challenging reasoning\ntasks. We found that LFS (1) performs better on more challenging tasks without\nadditional tuning, (2) is more computationally efficient compared to the other\nmethods, especially when powered by a stronger model, (3) scales better with\nstronger models, due to its LLM-First design, and (4) scales better with\nincreased compute budget. Our code is publicly available at\n\\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}."
                },
                "authors": [
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    },
                    {
                        "name": "Roberta Raileanu"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Raileanu"
                },
                "author": "Roberta Raileanu",
                "arxiv_comment": "9 main pages, 2 figures, 2 tables, 36 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02524v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02524v4",
                "updated": "2025-06-05T16:22:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    22,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-04T17:42:21Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    42,
                    21,
                    1,
                    156,
                    0
                ],
                "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks"
                },
                "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Tanja Srindran"
                    },
                    {
                        "name": "Tomasz Lehmann"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02524v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02524v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02040v2",
                "updated": "2025-06-05T16:22:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    22,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-31T08:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    8,
                    1,
                    11,
                    5,
                    151,
                    0
                ],
                "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context\n  Protocol Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context\n  Protocol Ecosystem"
                },
                "summary": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers."
                },
                "authors": [
                    {
                        "name": "Hao Song"
                    },
                    {
                        "name": "Yiming Shen"
                    },
                    {
                        "name": "Wenxuan Luo"
                    },
                    {
                        "name": "Leixin Guo"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Jiachi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiachi Chen"
                },
                "author": "Jiachi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05209v1",
                "updated": "2025-06-05T16:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:21:30Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text"
                },
                "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models."
                },
                "authors": [
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Colin Raffel"
                    },
                    {
                        "name": "Sebastian Majstorovic"
                    },
                    {
                        "name": "Stella Biderman"
                    },
                    {
                        "name": "Baber Abbasi"
                    },
                    {
                        "name": "Luca Soldaini"
                    },
                    {
                        "name": "Enrico Shippole"
                    },
                    {
                        "name": "A. Feder Cooper"
                    },
                    {
                        "name": "Aviya Skowron"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Lintang Sutawika"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Zhenlin Xu"
                    },
                    {
                        "name": "Guilherme Penedo"
                    },
                    {
                        "name": "Loubna Ben Allal"
                    },
                    {
                        "name": "Elie Bakouch"
                    },
                    {
                        "name": "John David Pressman"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Tyler Murray"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Murray"
                },
                "author": "Tyler Murray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05205v1",
                "updated": "2025-06-05T16:17:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    17,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:17:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    17,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELIC: Evaluating Compositional Instruction Following via Language\n  Recognition"
                },
                "summary": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly expected to perform tasks based\nonly on a specification of the task provided in context, without examples of\ninputs and outputs; this ability is referred to as instruction following. We\nintroduce the Recognition of Languages In-Context (RELIC) framework to evaluate\ninstruction following using language recognition: the task of determining if a\nstring is generated by formal grammar. Unlike many standard evaluations of\nLLMs' ability to use their context, this task requires composing together a\nlarge number of instructions (grammar productions) retrieved from the context.\nBecause the languages are synthetic, the task can be increased in complexity as\nLLMs' skills improve, and new instances can be automatically generated,\nmitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and\nfind that their accuracy can be reliably predicted from the complexity of the\ngrammar and the individual example strings, and that even the most advanced\nLLMs currently available show near-chance performance on more complex grammars\nand samples, in line with theoretical expectations. We also use RELIC to\ndiagnose how LLMs attempt to solve increasingly difficult reasoning tasks,\nfinding that as the complexity of the language recognition task increases,\nmodels switch to relying on shallow heuristics instead of following complex\ninstructions."
                },
                "authors": [
                    {
                        "name": "Jackson Petty"
                    },
                    {
                        "name": "Michael Y. Hu"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Tal Linzen"
                    }
                ],
                "author_detail": {
                    "name": "Tal Linzen"
                },
                "author": "Tal Linzen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05191v1",
                "updated": "2025-06-05T16:04:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T16:04:08Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MokA: Multimodal Low-Rank Adaptation for MLLMs"
                },
                "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA."
                },
                "authors": [
                    {
                        "name": "Yake Wei"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Di Hu"
                    }
                ],
                "author_detail": {
                    "name": "Di Hu"
                },
                "author": "Di Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05085v3",
                "updated": "2025-06-05T15:57:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    57,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Patrik Okanovic"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Michal Podstawski"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05183v1",
                "updated": "2025-06-05T15:56:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    56,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:56:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    56,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TreeRPO: Tree Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeRPO: Tree Relative Policy Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "13pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06415v3",
                "updated": "2025-06-05T15:55:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    55,
                    15,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-08T22:56:00Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    22,
                    56,
                    0,
                    1,
                    282,
                    0
                ],
                "title": "Biased AI can Influence Political Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased AI can Influence Political Decision-Making"
                },
                "summary": "As modern large language models (LLMs) become integral to everyday tasks,\nconcerns about their inherent biases and their potential impact on human\ndecision-making have emerged. While bias in models are well-documented, less is\nknown about how these biases influence human decisions. This paper presents two\ninteractive experiments investigating the effects of partisan bias in LLMs on\npolitical opinions and decision-making. Participants interacted freely with\neither a biased liberal, biased conservative, or unbiased control model while\ncompleting these tasks. We found that participants exposed to partisan biased\nmodels were significantly more likely to adopt opinions and make decisions\nwhich matched the LLM's bias. Even more surprising, this influence was seen\nwhen the model bias and personal political partisanship of the participant were\nopposite. However, we also discovered that prior knowledge of AI was weakly\ncorrelated with a reduction of the impact of the bias, highlighting the\npossible importance of AI education for robust mitigation of bias effects. Our\nfindings not only highlight the critical effects of interacting with biased\nLLMs and its ability to impact public discourse and political conduct, but also\nhighlights potential techniques for mitigating these risks in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern large language models (LLMs) become integral to everyday tasks,\nconcerns about their inherent biases and their potential impact on human\ndecision-making have emerged. While bias in models are well-documented, less is\nknown about how these biases influence human decisions. This paper presents two\ninteractive experiments investigating the effects of partisan bias in LLMs on\npolitical opinions and decision-making. Participants interacted freely with\neither a biased liberal, biased conservative, or unbiased control model while\ncompleting these tasks. We found that participants exposed to partisan biased\nmodels were significantly more likely to adopt opinions and make decisions\nwhich matched the LLM's bias. Even more surprising, this influence was seen\nwhen the model bias and personal political partisanship of the participant were\nopposite. However, we also discovered that prior knowledge of AI was weakly\ncorrelated with a reduction of the impact of the bias, highlighting the\npossible importance of AI education for robust mitigation of bias effects. Our\nfindings not only highlight the critical effects of interacting with biased\nLLMs and its ability to impact public discourse and political conduct, but also\nhighlights potential techniques for mitigating these risks in the future."
                },
                "authors": [
                    {
                        "name": "Jillian Fisher"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Robert Aron"
                    },
                    {
                        "name": "Thomas Richardson"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Daniel W. Fisher"
                    },
                    {
                        "name": "Jennifer Pan"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Katharina Reinecke"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Reinecke"
                },
                "author": "Katharina Reinecke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05182v1",
                "updated": "2025-06-05T15:52:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    52,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:52:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    52,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools"
                },
                "summary": "The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA."
                },
                "authors": [
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Messiah Ataey"
                    },
                    {
                        "name": "Shariyar Murtuza"
                    },
                    {
                        "name": "Yifan Nie"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "15 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05176v1",
                "updated": "2025-06-05T15:49:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    49,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:49:48Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    49,
                    48,
                    3,
                    156,
                    0
                ],
                "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models"
                },
                "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06854v2",
                "updated": "2025-06-05T15:48:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    48,
                    54,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-07T17:23:48Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    23,
                    48,
                    4,
                    38,
                    0
                ],
                "title": "Can Large Language Models Understand Intermediate Representations in\n  Compilers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Intermediate Representations in\n  Compilers?"
                },
                "summary": "Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Representations (IRs) play a critical role in compiler design\nand program analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. In this paper, we present an explorative empirical study\nevaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3,\nDeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically,\nwe assess model performance across four core tasks: control flow graph\nreconstruction, decompilation, code summarization, and execution reasoning.\nWhile LLMs exhibit competence in parsing IR syntax and identifying high-level\nstructures, they consistently struggle with instruction-level reasoning,\nespecially in control flow reasoning, loop handling, and dynamic execution.\nCommon failure modes include misinterpreting branching instructions, omitting\ncritical operations, and relying on heuristic reasoning rather than precise\ninstruction-level logic. Our findings highlight the need for IR-specific\nenhancements in LLM design. We recommend fine-tuning on structured IR datasets\nand integrating control-flow-sensitive architectures to improve model\neffectiveness. All experimental data and source code are publicly available at"
                },
                "authors": [
                    {
                        "name": "Hailong Jiang"
                    },
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Qiang Guan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Guan"
                },
                "author": "Qiang Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05171v1",
                "updated": "2025-06-05T15:46:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    46,
                    25,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:46:25Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    46,
                    25,
                    3,
                    156,
                    0
                ],
                "title": "Towards provable probabilistic safety for scalable embodied AI systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards provable probabilistic safety for scalable embodied AI systems"
                },
                "summary": "Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. To address this challenge, we introduce provable\nprobabilistic safety, which aims to ensure that the residual risk of\nlarge-scale deployment remains below a predefined threshold. Instead of\nattempting exhaustive safety proof across all corner cases, this paradigm\nestablishes a probabilistic safety boundary on overall system performance,\nleveraging statistical methods to enhance feasibility and scalability. A\nwell-defined probabilistic safety boundary enables embodied AI systems to be\ndeployed at scale while allowing for continuous refinement of safety\nguarantees. Our work focuses on three core questions: what is provable\nprobabilistic safety, how to prove the probabilistic safety, and how to achieve\nthe provable probabilistic safety. By bridging the gap between theoretical\nsafety assurance and practical deployment, our work offers a pathway toward\nsafer, large-scale adoption of embodied AI systems in safety-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. To address this challenge, we introduce provable\nprobabilistic safety, which aims to ensure that the residual risk of\nlarge-scale deployment remains below a predefined threshold. Instead of\nattempting exhaustive safety proof across all corner cases, this paradigm\nestablishes a probabilistic safety boundary on overall system performance,\nleveraging statistical methods to enhance feasibility and scalability. A\nwell-defined probabilistic safety boundary enables embodied AI systems to be\ndeployed at scale while allowing for continuous refinement of safety\nguarantees. Our work focuses on three core questions: what is provable\nprobabilistic safety, how to prove the probabilistic safety, and how to achieve\nthe provable probabilistic safety. By bridging the gap between theoretical\nsafety assurance and practical deployment, our work offers a pathway toward\nsafer, large-scale adoption of embodied AI systems in safety-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Linxuan He"
                    },
                    {
                        "name": "Qing-Shan Jia"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Hongyan Sang"
                    },
                    {
                        "name": "Ling Wang"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Henry X. Liu"
                    },
                    {
                        "name": "Shuo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Feng"
                },
                "author": "Shuo Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05167v2",
                "updated": "2025-06-06T07:57:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    57,
                    28,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T15:43:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    43,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECoRAG: Evidentiality-guided Compression for Long Context RAG"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG."
                },
                "authors": [
                    {
                        "name": "Yeonseok Jeong"
                    },
                    {
                        "name": "Jinsu Kim"
                    },
                    {
                        "name": "Dohyeon Lee"
                    },
                    {
                        "name": "Seung-won Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seung-won Hwang"
                },
                "author": "Seung-won Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05166v2",
                "updated": "2025-06-06T01:35:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    35,
                    43,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T15:43:34Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    43,
                    34,
                    3,
                    156,
                    0
                ],
                "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective"
                },
                "summary": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks."
                },
                "authors": [
                    {
                        "name": "Bhavik Chandna"
                    },
                    {
                        "name": "Zubair Bashir"
                    },
                    {
                        "name": "Procheta Sen"
                    }
                ],
                "author_detail": {
                    "name": "Procheta Sen"
                },
                "author": "Procheta Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23827v2",
                "updated": "2025-06-05T15:41:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    41,
                    26,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-28T06:43:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    6,
                    43,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "ValueSim: Generating Backstories to Model Individual Value Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValueSim: Generating Backstories to Model Individual Value Systems"
                },
                "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time."
                },
                "authors": [
                    {
                        "name": "Bangde Du"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Zhijing Wu"
                    },
                    {
                        "name": "Jankowska Monika"
                    },
                    {
                        "name": "Shuqi Zhu"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00837v2",
                "updated": "2025-06-05T15:41:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    41,
                    25,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-02T16:18:44Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    18,
                    44,
                    6,
                    33,
                    0
                ],
                "title": "Explainability in Practice: A Survey of Explainable NLP Across Various\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability in Practice: A Survey of Explainable NLP Across Various\n  Domains"
                },
                "summary": "Natural Language Processing (NLP) has become a cornerstone in many critical\nsectors, including healthcare, finance, and customer relationship management.\nThis is especially true with the development and use of advanced models such as\nGPT-based architectures and BERT, which are widely used in decision-making\nprocesses. However, the black-box nature of these advanced NLP models has\ncreated an urgent need for transparency and explainability. This review\nexplores explainable NLP (XNLP) with a focus on its practical deployment and\nreal-world applications, examining its implementation and the challenges faced\nin domain-specific contexts. The paper underscores the importance of\nexplainability in NLP and provides a comprehensive perspective on how XNLP can\nbe designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's emphasis on fraud detection and risk\nassessment. Additionally, this review aims to bridge the knowledge gap in XNLP\nliterature by offering a domain-specific exploration and discussing\nunderrepresented areas such as real-world applicability, metric evaluation, and\nthe role of human interaction in model assessment. The paper concludes by\nsuggesting future research directions that could enhance the understanding and\nbroader application of XNLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) has become a cornerstone in many critical\nsectors, including healthcare, finance, and customer relationship management.\nThis is especially true with the development and use of advanced models such as\nGPT-based architectures and BERT, which are widely used in decision-making\nprocesses. However, the black-box nature of these advanced NLP models has\ncreated an urgent need for transparency and explainability. This review\nexplores explainable NLP (XNLP) with a focus on its practical deployment and\nreal-world applications, examining its implementation and the challenges faced\nin domain-specific contexts. The paper underscores the importance of\nexplainability in NLP and provides a comprehensive perspective on how XNLP can\nbe designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's emphasis on fraud detection and risk\nassessment. Additionally, this review aims to bridge the knowledge gap in XNLP\nliterature by offering a domain-specific exploration and discussing\nunderrepresented areas such as real-world applicability, metric evaluation, and\nthe role of human interaction in model assessment. The paper concludes by\nsuggesting future research directions that could enhance the understanding and\nbroader application of XNLP."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    },
                    {
                        "name": "Daniel L. Oberski"
                    }
                ],
                "author_detail": {
                    "name": "Daniel L. Oberski"
                },
                "author": "Daniel L. Oberski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v6",
                "updated": "2025-06-05T15:26:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    26,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Yingming Mao"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "ZhuoRan Liu"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Peirui Cao"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Wu Dongchao"
                    },
                    {
                        "name": "Yang Jian"
                    },
                    {
                        "name": "Zhang zhanbang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang zhanbang"
                },
                "author": "Zhang zhanbang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05142v1",
                "updated": "2025-06-05T15:24:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    24,
                    33,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:24:33Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    24,
                    33,
                    3,
                    156,
                    0
                ],
                "title": "Do Large Language Models Judge Error Severity Like Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Judge Error Severity Like Humans?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models."
                },
                "authors": [
                    {
                        "name": "Diege Sun"
                    },
                    {
                        "name": "Guanyi Chen"
                    },
                    {
                        "name": "Fan Zhao"
                    },
                    {
                        "name": "Xiaorong Cheng"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05138v1",
                "updated": "2025-06-05T15:22:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    22,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:22:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    22,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT\n  Systems"
                },
                "summary": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning."
                },
                "authors": [
                    {
                        "name": "Pavle Vasiljevic"
                    },
                    {
                        "name": "Milica Matic"
                    },
                    {
                        "name": "Miroslav Popovic"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Popovic"
                },
                "author": "Miroslav Popovic",
                "arxiv_comment": "6 pages, 4 algorithms, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20779v3",
                "updated": "2025-06-05T15:20:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    20,
                    59,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-27T06:36:04Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    36,
                    4,
                    1,
                    147,
                    0
                ],
                "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature"
                },
                "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB"
                },
                "authors": [
                    {
                        "name": "Noy Sternlicht"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "arxiv_comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05128v1",
                "updated": "2025-06-05T15:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    16,
                    14,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    16,
                    14,
                    3,
                    156,
                    0
                ],
                "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM\n  Reasoning"
                },
                "summary": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework."
                },
                "authors": [
                    {
                        "name": "Tanmay Parekh"
                    },
                    {
                        "name": "Kartik Mehta"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Submitted at ACL ARR May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05126v1",
                "updated": "2025-06-05T15:13:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    13,
                    57,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:13:57Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    13,
                    57,
                    3,
                    156,
                    0
                ],
                "title": "Membership Inference Attacks on Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks on Sequence Models"
                },
                "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rossi"
                    },
                    {
                        "name": "Michael Aerni"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02234v2",
                "updated": "2025-06-05T15:12:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    12,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-03T03:01:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    3,
                    1,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "LLM Social Simulations Are a Promising Research Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Social Simulations Are a Promising Research Method"
                },
                "summary": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted this method. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a review of\nempirical comparisons between LLMs and human research subjects, commentaries on\nthe topic, and related work. We identify promising directions, including\ncontext-rich prompting and fine-tuning with social science datasets. We believe\nthat LLM social simulations can already be used for pilot and exploratory\nstudies, and more widespread use may soon be possible with rapidly advancing\nLLM capabilities. Researchers should prioritize developing conceptual models\nand iterative evaluations to make the best use of new AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted this method. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a review of\nempirical comparisons between LLMs and human research subjects, commentaries on\nthe topic, and related work. We identify promising directions, including\ncontext-rich prompting and fine-tuning with social science datasets. We believe\nthat LLM social simulations can already be used for pilot and exploratory\nstudies, and more widespread use may soon be possible with rapidly advancing\nLLM capabilities. Researchers should prioritize developing conceptual models\nand iterative evaluations to make the best use of new AI systems."
                },
                "authors": [
                    {
                        "name": "Jacy Reese Anthis"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Sean M. Richardson"
                    },
                    {
                        "name": "Austin C. Kozlowski"
                    },
                    {
                        "name": "Bernard Koch"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Erik Brynjolfsson"
                    },
                    {
                        "name": "Michael Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bernstein"
                },
                "author": "Michael Bernstein",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09755v2",
                "updated": "2025-06-05T15:08:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    8,
                    36,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-13T20:25:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    20,
                    25,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "Jailbreak Attack Initializations as Extractors of Compliance Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attack Initializations as Extractors of Compliance Directions"
                },
                "summary": "Safety-aligned LLMs respond to prompts with either compliance or refusal,\neach corresponding to distinct directions in the model's activation space.\nRecent works show that initializing attacks via self-transfer from other\nprompts significantly enhances their performance. However, the underlying\nmechanisms of these initializations remain unclear, and attacks utilize\narbitrary or hand-picked initializations. This work presents that each\ngradient-based jailbreak attack and subsequent initialization gradually\nconverge to a single compliance direction that suppresses refusal, thereby\nenabling an efficient transition from refusal to compliance. Based on this\ninsight, we propose CRI, an initialization framework that aims to project\nunseen prompts further along compliance directions. We demonstrate our approach\non multiple attacks, models, and datasets, achieving an increased attack\nsuccess rate (ASR) and reduced computational overhead, highlighting the\nfragility of safety-aligned LLMs. A reference implementation is available at:\nhttps://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety-aligned LLMs respond to prompts with either compliance or refusal,\neach corresponding to distinct directions in the model's activation space.\nRecent works show that initializing attacks via self-transfer from other\nprompts significantly enhances their performance. However, the underlying\nmechanisms of these initializations remain unclear, and attacks utilize\narbitrary or hand-picked initializations. This work presents that each\ngradient-based jailbreak attack and subsequent initialization gradually\nconverge to a single compliance direction that suppresses refusal, thereby\nenabling an efficient transition from refusal to compliance. Based on this\ninsight, we propose CRI, an initialization framework that aims to project\nunseen prompts further along compliance directions. We demonstrate our approach\non multiple attacks, models, and datasets, achieving an increased attack\nsuccess rate (ASR) and reduced computational overhead, highlighting the\nfragility of safety-aligned LLMs. A reference implementation is available at:\nhttps://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation."
                },
                "authors": [
                    {
                        "name": "Amit Levi"
                    },
                    {
                        "name": "Rom Himelstein"
                    },
                    {
                        "name": "Yaniv Nemcovsky"
                    },
                    {
                        "name": "Avi Mendelson"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05115v1",
                "updated": "2025-06-05T15:00:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    0,
                    27,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T15:00:27Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    15,
                    0,
                    27,
                    3,
                    156,
                    0
                ],
                "title": "Whole-Body Constrained Learning for Legged Locomotion via Hierarchical\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-Body Constrained Learning for Legged Locomotion via Hierarchical\n  Optimization"
                },
                "summary": "Reinforcement learning (RL) has demonstrated impressive performance in legged\nlocomotion over various challenging environments. However, due to the\nsim-to-real gap and lack of explainability, unconstrained RL policies deployed\nin the real world still suffer from inevitable safety issues, such as joint\ncollisions, excessive torque, or foot slippage in low-friction environments.\nThese problems limit its usage in missions with strict safety requirements,\nsuch as planetary exploration, nuclear facility inspection, and deep-sea\noperations. In this paper, we design a hierarchical optimization-based\nwhole-body follower, which integrates both hard and soft constraints into RL\nframework to make the robot move with better safety guarantees. Leveraging the\nadvantages of model-based control, our approach allows for the definition of\nvarious types of hard and soft constraints during training or deployment, which\nallows for policy fine-tuning and mitigates the challenges of sim-to-real\ntransfer. Meanwhile, it preserves the robustness of RL when dealing with\nlocomotion in complex unstructured environments. The trained policy with\nintroduced constraints was deployed in a hexapod robot and tested in various\noutdoor environments, including snow-covered slopes and stairs, demonstrating\nthe great traversability and safety of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has demonstrated impressive performance in legged\nlocomotion over various challenging environments. However, due to the\nsim-to-real gap and lack of explainability, unconstrained RL policies deployed\nin the real world still suffer from inevitable safety issues, such as joint\ncollisions, excessive torque, or foot slippage in low-friction environments.\nThese problems limit its usage in missions with strict safety requirements,\nsuch as planetary exploration, nuclear facility inspection, and deep-sea\noperations. In this paper, we design a hierarchical optimization-based\nwhole-body follower, which integrates both hard and soft constraints into RL\nframework to make the robot move with better safety guarantees. Leveraging the\nadvantages of model-based control, our approach allows for the definition of\nvarious types of hard and soft constraints during training or deployment, which\nallows for policy fine-tuning and mitigates the challenges of sim-to-real\ntransfer. Meanwhile, it preserves the robustness of RL when dealing with\nlocomotion in complex unstructured environments. The trained policy with\nintroduced constraints was deployed in a hexapod robot and tested in various\noutdoor environments, including snow-covered slopes and stairs, demonstrating\nthe great traversability and safety of our approach."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Ruyi Zhou"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Tie Liu"
                    },
                    {
                        "name": "Zhelin Zhang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Haibo Gao"
                    },
                    {
                        "name": "Zongquan Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zongquan Deng"
                },
                "author": "Zongquan Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05096v2",
                "updated": "2025-06-06T04:46:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    4,
                    46,
                    57,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T14:41:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    41,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video\n  Diffusion Transformers"
                },
                "summary": "Video diffusion transformers (vDiTs) have made impressive progress in\ntext-to-video generation, but their high computational demands present major\nchallenges for practical deployment. While existing acceleration methods reduce\nworkload at various granularities, they often rely on heuristics, limiting\ntheir applicability.\n  We introduce ASTRAEA, an automatic framework that searches for near-optimal\nconfigurations for vDiT-based video generation. At its core, ASTRAEA proposes a\nlightweight token selection mechanism and a memory-efficient, GPU-parallel\nsparse attention strategy, enabling linear reductions in execution time with\nminimal impact on generation quality. To determine optimal token reduction for\ndifferent timesteps, we further design a search framework that leverages a\nclassic evolutionary algorithm to automatically determine the distribution of\nthe token budget effectively. Together, ASTRAEA achieves up to 2.4x inference\nspeedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)\nwhile retaining better video quality compared to the state-of-the-art methods\n(<0.5% loss on the VBench score compared to the baseline vDiT models).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers (vDiTs) have made impressive progress in\ntext-to-video generation, but their high computational demands present major\nchallenges for practical deployment. While existing acceleration methods reduce\nworkload at various granularities, they often rely on heuristics, limiting\ntheir applicability.\n  We introduce ASTRAEA, an automatic framework that searches for near-optimal\nconfigurations for vDiT-based video generation. At its core, ASTRAEA proposes a\nlightweight token selection mechanism and a memory-efficient, GPU-parallel\nsparse attention strategy, enabling linear reductions in execution time with\nminimal impact on generation quality. To determine optimal token reduction for\ndifferent timesteps, we further design a search framework that leverages a\nclassic evolutionary algorithm to automatically determine the distribution of\nthe token budget effectively. Together, ASTRAEA achieves up to 2.4x inference\nspeedup on a single GPU with great scalability (up to 13.2x speedup on 8 GPUs)\nwhile retaining better video quality compared to the state-of-the-art methods\n(<0.5% loss on the VBench score compared to the baseline vDiT models)."
                },
                "authors": [
                    {
                        "name": "Haosong Liu"
                    },
                    {
                        "name": "Yuge Cheng"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Aiyue Chen"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05095v1",
                "updated": "2025-06-05T14:40:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    40,
                    49,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:40:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    40,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial\n  Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety\n  (TrustFAA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial\n  Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety\n  (TrustFAA)"
                },
                "summary": "With the increasing prevalence and deployment of Emotion AI-powered facial\naffect analysis (FAA) tools, concerns about the trustworthiness of these\nsystems have become more prominent. This first workshop on \"Towards Trustworthy\nFacial Affect Analysis: Advancing Insights of Fairness, Explainability, and\nSafety (TrustFAA)\" aims to bring together researchers who are investigating\ndifferent challenges in relation to trustworthiness-such as interpretability,\nuncertainty, biases, and privacy-across various facial affect analysis tasks,\nincluding macro/ micro-expression recognition, facial action unit detection,\nother corresponding applications such as pain and depression detection, as well\nas human-robot interaction and collaboration. In alignment with FG2025's\nemphasis on ethics, as demonstrated by the inclusion of an Ethical Impact\nStatement requirement for this year's submissions, this workshop supports\nFG2025's efforts by encouraging research, discussion and dialogue on\ntrustworthy FAA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing prevalence and deployment of Emotion AI-powered facial\naffect analysis (FAA) tools, concerns about the trustworthiness of these\nsystems have become more prominent. This first workshop on \"Towards Trustworthy\nFacial Affect Analysis: Advancing Insights of Fairness, Explainability, and\nSafety (TrustFAA)\" aims to bring together researchers who are investigating\ndifferent challenges in relation to trustworthiness-such as interpretability,\nuncertainty, biases, and privacy-across various facial affect analysis tasks,\nincluding macro/ micro-expression recognition, facial action unit detection,\nother corresponding applications such as pain and depression detection, as well\nas human-robot interaction and collaboration. In alignment with FG2025's\nemphasis on ethics, as demonstrated by the inclusion of an Ethical Impact\nStatement requirement for this year's submissions, this workshop supports\nFG2025's efforts by encouraging research, discussion and dialogue on\ntrustworthy FAA."
                },
                "authors": [
                    {
                        "name": "Jiaee Cheong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Harold Soh"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03198v2",
                "updated": "2025-06-05T14:35:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    35,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2024-05-28T04:36:15Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    4,
                    36,
                    15,
                    1,
                    149,
                    0
                ],
                "title": "The Impossibility of Fair LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impossibility of Fair LLMs"
                },
                "summary": "The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of general-purpose artificial intelligence (AI) systems,\nparticularly large language models (LLMs), has raised pressing moral questions\nabout how to reduce bias and ensure fairness at scale. Researchers have\ndocumented a sort of \"bias\" in the significant correlations between\ndemographics (e.g., race, gender) in LLM prompts and responses, but it remains\nunclear how LLM fairness could be evaluated with more rigorous definitions,\nsuch as group fairness or fair representations. We analyze a variety of\ntechnical fairness frameworks and find inherent challenges in each that make\nthe development of a fair LLM intractable. We show that each framework either\ndoes not logically extend to the general-purpose AI context or is infeasible in\npractice, primarily due to the large amounts of unstructured training data and\nthe many potential combinations of human populations, use cases, and sensitive\nattributes. These inherent challenges would persist for general-purpose AI,\nincluding LLMs, even if empirical challenges, such as limited participatory\ninput and limited measurement methods, were overcome. Nonetheless, fairness\nwill remain an important type of model evaluation, and there are still\npromising research directions, particularly the development of standards for\nthe responsibility of LLM developers, context-specific evaluations, and methods\nof iterative, participatory, and AI-assisted evaluation that could scale\nfairness across the diverse contexts of modern human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jacy Anthis"
                    },
                    {
                        "name": "Kristian Lum"
                    },
                    {
                        "name": "Michael Ekstrand"
                    },
                    {
                        "name": "Avi Feller"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "Published in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12649v3",
                "updated": "2025-06-05T14:35:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    35,
                    0,
                    3,
                    156,
                    0
                ],
                "published": "2024-02-20T01:49:15Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    1,
                    49,
                    15,
                    1,
                    51,
                    0
                ],
                "title": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation"
                },
                "summary": "Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between the user attributes stated or implied by a\nprompt and the LLM's short text response, but human-AI interaction increasingly\nrequires long-form and context-specific system output to solve real-world\ntasks. In the commonly studied domain of gender-occupation bias, we test\nwhether these benchmarks are robust to lengthening the LLM responses as a\nmeasure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype) and develop analogous RUTEd evaluations from three contexts of\nreal-world use: children's bedtime stories, user personas, and English language\nlearning exercises. We find that standard bias metrics have no significant\ncorrelation with the more realistic bias metrics. For example, selecting the\nleast biased model based on the standard \"trick tests\" coincides with selecting\nthe least biased model as measured in more realistic use no more than random\nchance. We suggest that there is not yet evidence to justify standard\nbenchmarks as reliable proxies of real-world AI biases, and we encourage\nfurther development of evaluations grounded in particular contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard benchmarks of bias and fairness in large language models (LLMs)\nmeasure the association between the user attributes stated or implied by a\nprompt and the LLM's short text response, but human-AI interaction increasingly\nrequires long-form and context-specific system output to solve real-world\ntasks. In the commonly studied domain of gender-occupation bias, we test\nwhether these benchmarks are robust to lengthening the LLM responses as a\nmeasure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From\nthe current literature, we adapt three standard bias metrics (neutrality, skew,\nand stereotype) and develop analogous RUTEd evaluations from three contexts of\nreal-world use: children's bedtime stories, user personas, and English language\nlearning exercises. We find that standard bias metrics have no significant\ncorrelation with the more realistic bias metrics. For example, selecting the\nleast biased model based on the standard \"trick tests\" coincides with selecting\nthe least biased model as measured in more realistic use no more than random\nchance. We suggest that there is not yet evidence to justify standard\nbenchmarks as reliable proxies of real-world AI biases, and we encourage\nfurther development of evaluations grounded in particular contexts."
                },
                "authors": [
                    {
                        "name": "Kristian Lum"
                    },
                    {
                        "name": "Jacy Reese Anthis"
                    },
                    {
                        "name": "Kevin Robinson"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Alexander D'Amour"
                    }
                ],
                "author_detail": {
                    "name": "Alexander D'Amour"
                },
                "author": "Alexander D'Amour",
                "arxiv_comment": "Published in ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05079v1",
                "updated": "2025-06-05T14:27:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:27:40Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "title": "LLM-Guided Scenario-based GUI Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Scenario-based GUI Testing"
                },
                "summary": "The assurance of mobile app GUI is more and more significant. Automated GUI\ntesting approaches of different strategies have been developed, while there are\nstill huge gaps between the approaches and the app business logic, not taking\nthe completion of specific testing scenarios as the exploration target, leading\nto the exploration missing of critical app functionalities. Learning from the\nmanual testing, which takes testing scenarios with app business logic as the\nbasic granularity, in this paper, we utilize the LLMs to understand the\nsemantics presented in app GUI and how they are mapped in the testing context\nbased on specific testing scenarios. Then, scenario-based GUI tests are\ngenerated with the guidance of multi-agent collaboration. Specifically, we\npropose ScenGen, a novel LLM-guided scenario-based GUI testing approach\ninvolving five agents to respectively take responsibilities of different phases\nof the manual testing process. The Observer perceives the app GUI state by\nextracting GUI widgets and forming GUI layouts, understanding the expressed\nsemantics. Then the app GUI info is sent to the Decider to make decisions on\ntarget widgets based on the target testing scenarios. The decision-making\nprocess takes the completion of specific testing scenarios as the exploration\ntarget. The Executor then executes the demanding operations on the apps. The\nexecution results are checked by the Supervisor on whether the generated tests\nare consistent with the completion target of the testing scenarios, ensuring\nthe traceability of the test generation and execution. Furthermore, the\ncorresponding GUI test operations are recorded to the context memory by\nRecorder as an important basis for further decision-making, meanwhile\nmonitoring the runtime bug occurrences. ScenGen is evaluated and the results\nshow that ScenGen can effectively generate scenario-based GUI tests guided by\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assurance of mobile app GUI is more and more significant. Automated GUI\ntesting approaches of different strategies have been developed, while there are\nstill huge gaps between the approaches and the app business logic, not taking\nthe completion of specific testing scenarios as the exploration target, leading\nto the exploration missing of critical app functionalities. Learning from the\nmanual testing, which takes testing scenarios with app business logic as the\nbasic granularity, in this paper, we utilize the LLMs to understand the\nsemantics presented in app GUI and how they are mapped in the testing context\nbased on specific testing scenarios. Then, scenario-based GUI tests are\ngenerated with the guidance of multi-agent collaboration. Specifically, we\npropose ScenGen, a novel LLM-guided scenario-based GUI testing approach\ninvolving five agents to respectively take responsibilities of different phases\nof the manual testing process. The Observer perceives the app GUI state by\nextracting GUI widgets and forming GUI layouts, understanding the expressed\nsemantics. Then the app GUI info is sent to the Decider to make decisions on\ntarget widgets based on the target testing scenarios. The decision-making\nprocess takes the completion of specific testing scenarios as the exploration\ntarget. The Executor then executes the demanding operations on the apps. The\nexecution results are checked by the Supervisor on whether the generated tests\nare consistent with the completion target of the testing scenarios, ensuring\nthe traceability of the test generation and execution. Furthermore, the\ncorresponding GUI test operations are recorded to the context memory by\nRecorder as an important basis for further decision-making, meanwhile\nmonitoring the runtime bug occurrences. ScenGen is evaluated and the results\nshow that ScenGen can effectively generate scenario-based GUI tests guided by\nLLMs."
                },
                "authors": [
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Yuchen Ling"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Shaomin Zhu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01679v2",
                "updated": "2025-06-05T14:26:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    26,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2024-11-03T20:41:38Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    20,
                    41,
                    38,
                    6,
                    308,
                    0
                ],
                "title": "Autoformulation of Mathematical Optimization Models Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformulation of Mathematical Optimization Models Using LLMs"
                },
                "summary": "Mathematical optimization is fundamental to decision-making across diverse\ndomains, from operations research to healthcare. Yet, translating real-world\nproblems into optimization models remains a difficult task, often demanding\nspecialized expertise. This paper approaches the problem of\n$\\textit{autoformulation}$: the automated creation of solver-ready optimization\nmodels from natural language problem descriptions. We identify three core\nchallenges of autoformulation: $\\textit{(1)}$ the vast, problem-dependent\nhypothesis space, $\\textit{(2)}$ efficient and diverse exploration of this\nspace under uncertainty, and $\\textit{(3)}$ evaluation of formulation\ncorrectness against problem description. To address these challenges, we\npresent a novel method leveraging $\\textit{Large Language Models}$ (LLMs) with\n$\\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of\noptimization modeling to generate and systematically explore possible\nformulations. To enhance search efficiency, we introduce symbolic pruning to\neliminate trivially equivalent search paths (branches), and employ LLM-based\nevaluation of partial formulations to guide search. Empirical analysis on\nlinear and mixed-integer programming benchmarks demonstrates our method's\neffectiveness, with significant performance gains from both LLM-based value\nestimation and symbolic pruning techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical optimization is fundamental to decision-making across diverse\ndomains, from operations research to healthcare. Yet, translating real-world\nproblems into optimization models remains a difficult task, often demanding\nspecialized expertise. This paper approaches the problem of\n$\\textit{autoformulation}$: the automated creation of solver-ready optimization\nmodels from natural language problem descriptions. We identify three core\nchallenges of autoformulation: $\\textit{(1)}$ the vast, problem-dependent\nhypothesis space, $\\textit{(2)}$ efficient and diverse exploration of this\nspace under uncertainty, and $\\textit{(3)}$ evaluation of formulation\ncorrectness against problem description. To address these challenges, we\npresent a novel method leveraging $\\textit{Large Language Models}$ (LLMs) with\n$\\textit{Monte-Carlo Tree Search}$, exploiting the hierarchical nature of\noptimization modeling to generate and systematically explore possible\nformulations. To enhance search efficiency, we introduce symbolic pruning to\neliminate trivially equivalent search paths (branches), and employ LLM-based\nevaluation of partial formulations to guide search. Empirical analysis on\nlinear and mixed-integer programming benchmarks demonstrates our method's\neffectiveness, with significant performance gains from both LLM-based value\nestimation and symbolic pruning techniques."
                },
                "authors": [
                    {
                        "name": "Nicolás Astorga"
                    },
                    {
                        "name": "Tennison Liu"
                    },
                    {
                        "name": "Yuanzhang Xiao"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "*Astorga and Liu contributed equally. Published as a conference paper\n  at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05076v1",
                "updated": "2025-06-05T14:23:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    23,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:23:34Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    23,
                    34,
                    3,
                    156,
                    0
                ],
                "title": "Cloud-Based Interoperability in Residential Energy Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Based Interoperability in Residential Energy Systems"
                },
                "summary": "As distributed energy resources (DERs) such as solar PV, batteries and\nelectric vehicles become increasingly prevalent at the edge, maintaining grid\nstability requires advanced monitoring and control mechanisms. This paper\npresents a scalable smart grid gateway architecture that enables\ninteroperability between Modbus-based inverters and IEEE 2030.5 cloud-based\ncontrol systems. The proposed solution leverages Azure cloud services and\nedge-computing gateway devices to support dynamic configuration, telemetry\ningestion, remote control and Volt-VAR Curve deployment. A microservice-based\narchitecture ensures flexibility and scalability across diverse deployment\nscenarios, including both gateway-mediated and direct-to-cloud device\ncommunication. Results demonstrate the successful mapping of a Fronius Primo\ninverter's Modbus registers to IEEE 2030.5-compliant telemetry and control\nfunctions. Additionally, we evaluate real-time VVC updates and their impact on\nlocal voltage regulation, showcasing dynamic cloud-to-edge control with minimal\nlatency. This work highlights the potential of virtualised, standards-based\ncontrol infrastructures to support DER integration and active grid\nparticipation, while remaining adaptable to evolving smart grid architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As distributed energy resources (DERs) such as solar PV, batteries and\nelectric vehicles become increasingly prevalent at the edge, maintaining grid\nstability requires advanced monitoring and control mechanisms. This paper\npresents a scalable smart grid gateway architecture that enables\ninteroperability between Modbus-based inverters and IEEE 2030.5 cloud-based\ncontrol systems. The proposed solution leverages Azure cloud services and\nedge-computing gateway devices to support dynamic configuration, telemetry\ningestion, remote control and Volt-VAR Curve deployment. A microservice-based\narchitecture ensures flexibility and scalability across diverse deployment\nscenarios, including both gateway-mediated and direct-to-cloud device\ncommunication. Results demonstrate the successful mapping of a Fronius Primo\ninverter's Modbus registers to IEEE 2030.5-compliant telemetry and control\nfunctions. Additionally, we evaluate real-time VVC updates and their impact on\nlocal voltage regulation, showcasing dynamic cloud-to-edge control with minimal\nlatency. This work highlights the potential of virtualised, standards-based\ncontrol infrastructures to support DER integration and active grid\nparticipation, while remaining adaptable to evolving smart grid architectures."
                },
                "authors": [
                    {
                        "name": "Darren Leniston"
                    },
                    {
                        "name": "David Ryan"
                    },
                    {
                        "name": "Ammar Malik"
                    },
                    {
                        "name": "Jack Jackman"
                    },
                    {
                        "name": "Terence O'Donnell"
                    }
                ],
                "author_detail": {
                    "name": "Terence O'Donnell"
                },
                "author": "Terence O'Donnell",
                "arxiv_comment": "15 pages, 12 figures, GIECS 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05073v1",
                "updated": "2025-06-05T14:19:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:48Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    48,
                    3,
                    156,
                    0
                ],
                "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection\n  through Intent Differentiation and Emoji Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection\n  through Intent Differentiation and Emoji Interpretation"
                },
                "summary": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES ."
                },
                "authors": [
                    {
                        "name": "Soumitra Ghosh"
                    },
                    {
                        "name": "Gopendra Vikram Singh"
                    },
                    {
                        "name": "Shambhavi"
                    },
                    {
                        "name": "Sabarna Choudhury"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "arxiv_comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025 Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05070v1",
                "updated": "2025-06-05T14:18:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    18,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:18:21Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    18,
                    21,
                    3,
                    156,
                    0
                ],
                "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial\n  Optimization for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIVAL: Reinforcement Learning with Iterative and Adversarial\n  Optimization for Machine Translation"
                },
                "summary": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines."
                },
                "authors": [
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengran Yu"
                    },
                    {
                        "name": "Chenyu Shi"
                    },
                    {
                        "name": "Yanjun Zhao"
                    },
                    {
                        "name": "Xiaojing Liu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiayin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayin Wang"
                },
                "author": "Jiayin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14284v2",
                "updated": "2025-06-05T14:17:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    17,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2024-06-20T13:09:29Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    9,
                    29,
                    3,
                    172,
                    0
                ],
                "title": "Leveraging LLMs for Bangla Grammar Error Correction:Error\n  Categorization, Synthetic Data, and Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Bangla Grammar Error Correction:Error\n  Categorization, Synthetic Data, and Model Evaluation"
                },
                "summary": "Large Language Models (LLMs) perform exceedingly well in Natural Language\nUnderstanding (NLU) tasks for many languages including English. However,\ndespite being the fifth most-spoken language globally, Grammatical Error\nCorrection (GEC) in Bangla remains underdeveloped. In this work, we investigate\nhow LLMs can be leveraged for improving Bangla GEC. For that, we first do an\nextensive categorization of 12 error classes in Bangla, and take a survey of\nnative Bangla speakers to collect real-world errors. We next devise a\nrule-based noise injection method to create grammatically incorrect sentences\ncorresponding to correct ones. The Vaiyakarana dataset, thus created, consists\nof 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then\nused to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show\nthat instruction-tuning with \\name improves GEC performance of LLMs by 3-7\npercentage points as compared to the zero-shot setting, and makes them achieve\nhuman-like performance in grammatical error identification. Humans, though,\nremain superior in error correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) perform exceedingly well in Natural Language\nUnderstanding (NLU) tasks for many languages including English. However,\ndespite being the fifth most-spoken language globally, Grammatical Error\nCorrection (GEC) in Bangla remains underdeveloped. In this work, we investigate\nhow LLMs can be leveraged for improving Bangla GEC. For that, we first do an\nextensive categorization of 12 error classes in Bangla, and take a survey of\nnative Bangla speakers to collect real-world errors. We next devise a\nrule-based noise injection method to create grammatically incorrect sentences\ncorresponding to correct ones. The Vaiyakarana dataset, thus created, consists\nof 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then\nused to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show\nthat instruction-tuning with \\name improves GEC performance of LLMs by 3-7\npercentage points as compared to the zero-shot setting, and makes them achieve\nhuman-like performance in grammatical error identification. Humans, though,\nremain superior in error correction."
                },
                "authors": [
                    {
                        "name": "Pramit Bhattacharyya"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted at ACL Findings, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12171v2",
                "updated": "2025-06-05T14:16:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    46,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-13T10:33:58Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    33,
                    58,
                    3,
                    44,
                    0
                ],
                "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoRA: Gradient-driven Adaptive Low Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Luyang Zhou"
                    },
                    {
                        "name": "Shucun Ju"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05069v1",
                "updated": "2025-06-05T14:16:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    44,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:16:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    16,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance\n  LLM Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance\n  LLM Recommendation"
                },
                "summary": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by advances in Large Language Models (LLMs), integrating them into\nrecommendation tasks has gained interest due to their strong semantic\nunderstanding and prompt flexibility. Prior work encoded user-item interactions\nor metadata into prompts for recommendations. In parallel, LLM reasoning,\nboosted by test-time scaling and reinforcement learning, has excelled in fields\nlike mathematics and code, where reasoning traces and correctness signals are\nclear, enabling high performance and interpretability. However, directly\napplying these reasoning methods to recommendation is ineffective because user\nfeedback is implicit and lacks reasoning supervision. To address this, we\npropose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that\nsamples interaction chains from the user-item graph and converts them into\nstructured interaction-of-thoughts via a progressive masked prompting strategy,\nwith each thought representing stepwise reasoning grounded in interaction\ncontext. This allows LLMs to simulate step-by-step decision-making based on\nimplicit patterns. We design a two-stage training pipeline: supervised\nfine-tuning teaches basic reasoning from high-quality traces, and reinforcement\nlearning refines reasoning via reward signals, alleviating sparse explicit\nsupervision. Experiments on three real-world datasets show R2Rec outperforms\nclassical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement\nin HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore,\nthe explicit reasoning chains enhance interpretability by revealing the\ndecision process. Our code is available at:\nhttps://anonymous.4open.science/r/R2Rec-7C5D."
                },
                "authors": [
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05068v2",
                "updated": "2025-06-06T11:26:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    11,
                    26,
                    38,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T14:13:54Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    13,
                    54,
                    3,
                    156,
                    0
                ],
                "title": "Does It Make Sense to Speak of Introspection in Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does It Make Sense to Speak of Introspection in Large Language Models?"
                },
                "summary": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit compelling linguistic behaviour, and\nsometimes offer self-reports, that is to say statements about their own nature,\ninner workings, or behaviour. In humans, such reports are often attributed to a\nfaculty of introspection and are typically linked to consciousness. This raises\nthe question of how to interpret self-reports produced by LLMs, given their\nincreasing linguistic fluency and cognitive capabilities. To what extent (if\nany) can the concept of introspection be meaningfully applied to LLMs? Here, we\npresent and critique two examples of apparent introspective self-report from\nLLMs. In the first example, an LLM attempts to describe the process behind its\nown \"creative\" writing, and we argue this is not a valid example of\nintrospection. In the second example, an LLM correctly infers the value of its\nown temperature parameter, and we argue that this can be legitimately\nconsidered a minimal example of introspection, albeit one that is (presumably)\nnot accompanied by conscious experience."
                },
                "authors": [
                    {
                        "name": "Iulia M. Comsa"
                    },
                    {
                        "name": "Murray Shanahan"
                    }
                ],
                "author_detail": {
                    "name": "Murray Shanahan"
                },
                "author": "Murray Shanahan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23847v2",
                "updated": "2025-06-05T14:07:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    7,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-28T18:19:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    19,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "Seven Security Challenges That Must be Solved in Cross-domain\n  Multi-agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seven Security Challenges That Must be Solved in Cross-domain\n  Multi-agent LLM Systems"
                },
                "summary": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."
                },
                "authors": [
                    {
                        "name": "Ronny Ko"
                    },
                    {
                        "name": "Jiseong Jeong"
                    },
                    {
                        "name": "Shuyuan Zheng"
                    },
                    {
                        "name": "Chuan Xiao"
                    },
                    {
                        "name": "Tae-Wan Kim"
                    },
                    {
                        "name": "Makoto Onizuka"
                    },
                    {
                        "name": "Won-Yong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Won-Yong Shin"
                },
                "author": "Won-Yong Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05062v1",
                "updated": "2025-06-05T14:06:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    6,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:06:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    6,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation"
                },
                "summary": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that have previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task."
                },
                "authors": [
                    {
                        "name": "Noy Sternlicht"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Roy Bar-Haim"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Noam Slonim"
                    }
                ],
                "author_detail": {
                    "name": "Noam Slonim"
                },
                "author": "Noam Slonim",
                "arxiv_comment": "Code: https://github.com/noy-sternlicht/Debatable-Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00847v3",
                "updated": "2025-06-05T14:03:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    53,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-02T10:49:10Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    10,
                    49,
                    10,
                    6,
                    61,
                    0
                ],
                "title": "Argument Summarization and its Evaluation in the Era of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument Summarization and its Evaluation in the Era of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum. We also show that among the four LLMs integrated in (i) and (ii),\nQwen-3-32B, despite having the fewest parameters, performs best, even\nsurpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum. We also show that among the four LLMs integrated in (i) and (ii),\nQwen-3-32B, despite having the fewest parameters, performs best, even\nsurpassing GPT-4o, while LLaMA-3.3-70B consistently underperforms."
                },
                "authors": [
                    {
                        "name": "Moritz Altemeyer"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Johannes Daxenberger"
                    },
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Tim Altendorf"
                    },
                    {
                        "name": "Philipp Cimiano"
                    },
                    {
                        "name": "Benjamin Schiller"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Schiller"
                },
                "author": "Benjamin Schiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05061v1",
                "updated": "2025-06-05T14:03:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:03:18Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    3,
                    18,
                    3,
                    156,
                    0
                ],
                "title": "A Survey on Vietnamese Document Analysis and Recognition: Challenges and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Vietnamese Document Analysis and Recognition: Challenges and\n  Future Directions"
                },
                "summary": "Vietnamese document analysis and recognition (DAR) is a crucial field with\napplications in digitization, information retrieval, and automation. Despite\nadvancements in OCR and NLP, Vietnamese text recognition faces unique\nchallenges due to its complex diacritics, tonal variations, and lack of\nlarge-scale annotated datasets. Traditional OCR methods often struggle with\nreal-world document variations, while deep learning approaches have shown\npromise but remain limited by data scarcity and generalization issues.\nRecently, large language models (LLMs) and vision-language models have\ndemonstrated remarkable improvements in text recognition and document\nunderstanding, offering a new direction for Vietnamese DAR. However, challenges\nsuch as domain adaptation, multimodal learning, and computational efficiency\npersist. This survey provide a comprehensive review of existing techniques in\nVietnamese document recognition, highlights key limitations, and explores how\nLLMs can revolutionize the field. We discuss future research directions,\nincluding dataset development, model optimization, and the integration of\nmultimodal approaches for improved document intelligence. By addressing these\ngaps, we aim to foster advancements in Vietnamese DAR and encourage\ncommunity-driven solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vietnamese document analysis and recognition (DAR) is a crucial field with\napplications in digitization, information retrieval, and automation. Despite\nadvancements in OCR and NLP, Vietnamese text recognition faces unique\nchallenges due to its complex diacritics, tonal variations, and lack of\nlarge-scale annotated datasets. Traditional OCR methods often struggle with\nreal-world document variations, while deep learning approaches have shown\npromise but remain limited by data scarcity and generalization issues.\nRecently, large language models (LLMs) and vision-language models have\ndemonstrated remarkable improvements in text recognition and document\nunderstanding, offering a new direction for Vietnamese DAR. However, challenges\nsuch as domain adaptation, multimodal learning, and computational efficiency\npersist. This survey provide a comprehensive review of existing techniques in\nVietnamese document recognition, highlights key limitations, and explores how\nLLMs can revolutionize the field. We discuss future research directions,\nincluding dataset development, model optimization, and the integration of\nmultimodal approaches for improved document intelligence. By addressing these\ngaps, we aim to foster advancements in Vietnamese DAR and encourage\ncommunity-driven solutions."
                },
                "authors": [
                    {
                        "name": "Anh Le"
                    },
                    {
                        "name": "Thanh Lam"
                    },
                    {
                        "name": "Dung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dung Nguyen"
                },
                "author": "Dung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05057v1",
                "updated": "2025-06-05T14:02:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    2,
                    12,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:02:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    2,
                    12,
                    3,
                    156,
                    0
                ],
                "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALL -- A Trainable Architecture for Enhancing LLM Performance in\n  Low-Resource Languages"
                },
                "summary": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages due to limited training data. This paper presents\nTALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource\nLanguages), which integrates an LLM with two bilingual translation models. TALL\ntransforms low-resource inputs into high-resource representations, leveraging\nthe LLM's capabilities while preserving linguistic features through dimension\nalignment layers and custom transformers. Our experiments on Hebrew demonstrate\nsignificant improvements over several baselines, including direct use, naive\ntranslation, and fine-tuning approaches. The architecture employs a\nparameter-efficient strategy, freezing pre-trained components while training\nonly lightweight adapter modules, balancing computational efficiency with\nperformance gains."
                },
                "authors": [
                    {
                        "name": "Moshe Ofer"
                    },
                    {
                        "name": "Orel Zamler"
                    },
                    {
                        "name": "Amos Azaria"
                    }
                ],
                "author_detail": {
                    "name": "Amos Azaria"
                },
                "author": "Amos Azaria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05047v1",
                "updated": "2025-06-05T13:56:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    56,
                    18,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:56:18Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    56,
                    18,
                    3,
                    156,
                    0
                ],
                "title": "Reliably detecting model failures in deployment without labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliably detecting model failures in deployment without labels"
                },
                "summary": "The distribution of data changes over time; models operating operating in\ndynamic environments need retraining. But knowing when to retrain, without\naccess to labels, is an open challenge since some, but not all shifts degrade\nmodel performance. This paper formalizes and addresses the problem of\npost-deployment deterioration (PDD) monitoring. We propose D3M, a practical and\nefficient monitoring algorithm based on the disagreement of predictive models,\nachieving low false positive rates under non-deteriorating shifts and provides\nsample complexity bounds for high true positive rates under deteriorating\nshifts. Empirical results on both standard benchmark and a real-world\nlarge-scale internal medicine dataset demonstrate the effectiveness of the\nframework and highlight its viability as an alert mechanism for high-stakes\nmachine learning pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distribution of data changes over time; models operating operating in\ndynamic environments need retraining. But knowing when to retrain, without\naccess to labels, is an open challenge since some, but not all shifts degrade\nmodel performance. This paper formalizes and addresses the problem of\npost-deployment deterioration (PDD) monitoring. We propose D3M, a practical and\nefficient monitoring algorithm based on the disagreement of predictive models,\nachieving low false positive rates under non-deteriorating shifts and provides\nsample complexity bounds for high true positive rates under deteriorating\nshifts. Empirical results on both standard benchmark and a real-world\nlarge-scale internal medicine dataset demonstrate the effectiveness of the\nframework and highlight its viability as an alert mechanism for high-stakes\nmachine learning pipelines."
                },
                "authors": [
                    {
                        "name": "Viet Nguyen Changjian Shui"
                    },
                    {
                        "name": "Vijay Giri"
                    },
                    {
                        "name": "Siddarth Arya"
                    },
                    {
                        "name": "Amol Verma"
                    },
                    {
                        "name": "Fahad Razak"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "arxiv_comment": "36 pages, 6 figures, 7 tables, submitted to NeurIPS 2025, includes\n  theoretical analysis and extensive empirical evaluation across benchmark and\n  clinical datasets. Code available at https://github.com/teivng/d3m. Viet\n  Nguyen and Changjian Shui contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13438v2",
                "updated": "2025-06-05T13:55:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    55,
                    6,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-19T17:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
                },
                "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05038v1",
                "updated": "2025-06-05T13:42:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    42,
                    39,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:42:39Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    42,
                    39,
                    3,
                    156,
                    0
                ],
                "title": "Automatic Robustness Stress Testing of LLMs as Mathematical Problem\n  Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Robustness Stress Testing of LLMs as Mathematical Problem\n  Solvers"
                },
                "summary": "Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved distinguished performance on\nvarious reasoning-intensive tasks. However, LLMs might still face the\nchallenges of robustness issues and fail unexpectedly in some simple reasoning\ntasks. Previous works evaluate the LLM robustness with hand-crafted templates\nor a limited set of perturbation rules, indicating potential data contamination\nin pre-training or fine-tuning datasets. In this work, inspired by stress\ntesting in software engineering, we propose a novel framework, Automatic\nRobustness Checker (AR-Checker), to generate mathematical problem variants that\nmaintain the semantic meanings of the original one but might fail the LLMs. The\nAR-Checker framework generates mathematical problem variants through\nmulti-round parallel streams of LLM-based rewriting and verification. Our\nframework can generate benchmark variants dynamically for each LLM, thus\nminimizing the risk of data contamination. Experiments on GSM8K and MATH-500\ndemonstrate the strong performance of AR-Checker on mathematical tasks. We also\nevaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro,\nand CommonsenseQA, where it also achieves strong performance, further proving\nthe effectiveness of AR-Checker."
                },
                "authors": [
                    {
                        "name": "Yutao Hou"
                    },
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Yihan Jiang"
                    },
                    {
                        "name": "Xuetao Wei"
                    },
                    {
                        "name": "Hailiang Huang"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09117v3",
                "updated": "2025-06-05T13:34:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    34,
                    42,
                    3,
                    156,
                    0
                ],
                "published": "2025-03-12T07:08:54Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    8,
                    54,
                    2,
                    71,
                    0
                ],
                "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs"
                },
                "summary": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. It motivates\nthis paper to explore enhanced unlearning schemes that can mitigate this\ntrade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an\nimproved framework that regulates the directions of gradient updates during the\nunlearning procedure such that their side impacts on other, unrelated responses\ncan be minimized. GRU is easy and general to implement, demonstrating practical\neffectiveness across a variety of well-established unlearning benchmarks."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Xiaojiang Du"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05020v1",
                "updated": "2025-06-05T13:27:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    41,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:27:41Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    41,
                    3,
                    156,
                    0
                ],
                "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System"
                },
                "summary": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning."
                },
                "authors": [
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Zhaoqi Ma"
                    },
                    {
                        "name": "Yunong Li"
                    },
                    {
                        "name": "Junichiro Sugihara"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Jinjie Li"
                    },
                    {
                        "name": "Moju Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Moju Zhao"
                },
                "author": "Moju Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05019v1",
                "updated": "2025-06-05T13:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    28,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:27:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    27,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series\n  Analysis"
                },
                "summary": "Pure time series forecasting tasks typically focus exclusively on numerical\nfeatures; however, real-world financial decision-making demands the comparison\nand analysis of heterogeneous sources of information. Recent advances in deep\nlearning and large scale language models (LLMs) have made significant strides\nin capturing sentiment and other qualitative signals, thereby enhancing the\naccuracy of financial time series predictions. Despite these advances, most\nexisting datasets consist solely of price series and news text, are confined to\na single market, and remain limited in scale. In this paper, we introduce\nFinMultiTime, the first large scale, multimodal financial time series dataset.\nFinMultiTime temporally aligns four distinct modalities financial news,\nstructured financial tables, K-line technical charts, and stock price time\nseries across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from\n2009 to 2025 in the United States and China, the dataset totals 112.6 GB and\nprovides minute-level, daily, and quarterly resolutions, thus capturing short,\nmedium, and long term market signals with high fidelity. Our experiments\ndemonstrate that (1) scale and data quality markedly boost prediction accuracy;\n(2) multimodal fusion yields moderate gains in Transformer models; and (3) a\nfully reproducible pipeline enables seamless dataset updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pure time series forecasting tasks typically focus exclusively on numerical\nfeatures; however, real-world financial decision-making demands the comparison\nand analysis of heterogeneous sources of information. Recent advances in deep\nlearning and large scale language models (LLMs) have made significant strides\nin capturing sentiment and other qualitative signals, thereby enhancing the\naccuracy of financial time series predictions. Despite these advances, most\nexisting datasets consist solely of price series and news text, are confined to\na single market, and remain limited in scale. In this paper, we introduce\nFinMultiTime, the first large scale, multimodal financial time series dataset.\nFinMultiTime temporally aligns four distinct modalities financial news,\nstructured financial tables, K-line technical charts, and stock price time\nseries across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from\n2009 to 2025 in the United States and China, the dataset totals 112.6 GB and\nprovides minute-level, daily, and quarterly resolutions, thus capturing short,\nmedium, and long term market signals with high fidelity. Our experiments\ndemonstrate that (1) scale and data quality markedly boost prediction accuracy;\n(2) multimodal fusion yields moderate gains in Transformer models; and (3) a\nfully reproducible pipeline enables seamless dataset updates."
                },
                "authors": [
                    {
                        "name": "Wenyan Xu"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Xiyu Wang"
                    },
                    {
                        "name": "Yanxiang Ma"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04055v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04055v3",
                "updated": "2025-06-05T13:25:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    25,
                    43,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-05T06:28:54Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    6,
                    28,
                    54,
                    5,
                    279,
                    0
                ],
                "title": "Self-Correction is More than Refinement: A Learning Framework for Visual\n  and Language Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction is More than Refinement: A Learning Framework for Visual\n  and Language Reasoning Tasks"
                },
                "summary": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement."
                },
                "authors": [
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Hehai Lin"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Yi Fung"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04055v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04055v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05017v1",
                "updated": "2025-06-05T13:25:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    25,
                    28,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:25:28Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    25,
                    28,
                    3,
                    156,
                    0
                ],
                "title": "Controlling Summarization Length Through EOS Token Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Summarization Length Through EOS Token Weighting"
                },
                "summary": "Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary."
                },
                "authors": [
                    {
                        "name": "Zeno Belligoli"
                    },
                    {
                        "name": "Emmanouil Stergiadis"
                    },
                    {
                        "name": "Eran Fainman"
                    },
                    {
                        "name": "Ilya Gusev"
                    }
                ],
                "author_detail": {
                    "name": "Ilya Gusev"
                },
                "author": "Ilya Gusev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05010v1",
                "updated": "2025-06-05T13:20:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    50,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:20:50Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    50,
                    3,
                    156,
                    0
                ],
                "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development"
                },
                "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot."
                },
                "authors": [
                    {
                        "name": "Zhenran Xu"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Qingli Hu"
                    },
                    {
                        "name": "Zijiao Wu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL 2025 Demo. Github: https://github.com/AIDC-AI/ComfyUI-Copilot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05007v1",
                "updated": "2025-06-05T13:17:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    17,
                    50,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:17:50Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    17,
                    50,
                    3,
                    156,
                    0
                ],
                "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip"
                },
                "summary": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system."
                },
                "authors": [
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Tianrui Ma"
                    },
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Huimin Cui"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21091v2",
                "updated": "2025-06-05T13:12:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    12,
                    53,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-27T12:19:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    19,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Position is Power: System Prompts as a Mechanism of Bias in Large\n  Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position is Power: System Prompts as a Mechanism of Bias in Large\n  Language Models (LLMs)"
                },
                "summary": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments."
                },
                "authors": [
                    {
                        "name": "Anna Neumann"
                    },
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    },
                    {
                        "name": "Jatinder Singh"
                    }
                ],
                "author_detail": {
                    "name": "Jatinder Singh"
                },
                "author": "Jatinder Singh",
                "arxiv_doi": "10.1145/3715275.3732038",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732038",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Forthcoming in Proceedings of ACM FAccT 2025",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05000v1",
                "updated": "2025-06-05T13:10:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    10,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:10:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    10,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "SCOP: Evaluating the Comprehension Process of Large Language Models from\n  a Cognitive View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOP: Evaluating the Comprehension Process of Large Language Models from\n  a Cognitive View"
                },
                "summary": "Despite the great potential of large language models(LLMs) in machine\ncomprehension, it is still disturbing to fully count on them in real-world\nscenarios. This is probably because there is no rational explanation for\nwhether the comprehension process of LLMs is aligned with that of experts. In\nthis paper, we propose SCOP to carefully examine how LLMs perform during the\ncomprehension process from a cognitive view. Specifically, it is equipped with\na systematical definition of five requisite skills during the comprehension\nprocess, a strict framework to construct testing data for these skills, and a\ndetailed analysis of advanced open-sourced and closed-sourced LLMs using the\ntesting data. With SCOP, we find that it is still challenging for LLMs to\nperform an expert-level comprehension process. Even so, we notice that LLMs\nshare some similarities with experts, e.g., performing better at comprehending\nlocal information than global information. Further analysis reveals that LLMs\ncan be somewhat unreliable -- they might reach correct answers through flawed\ncomprehension processes. Based on SCOP, we suggest that one direction for\nimproving LLMs is to focus more on the comprehension process, ensuring all\ncomprehension skills are thoroughly developed during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the great potential of large language models(LLMs) in machine\ncomprehension, it is still disturbing to fully count on them in real-world\nscenarios. This is probably because there is no rational explanation for\nwhether the comprehension process of LLMs is aligned with that of experts. In\nthis paper, we propose SCOP to carefully examine how LLMs perform during the\ncomprehension process from a cognitive view. Specifically, it is equipped with\na systematical definition of five requisite skills during the comprehension\nprocess, a strict framework to construct testing data for these skills, and a\ndetailed analysis of advanced open-sourced and closed-sourced LLMs using the\ntesting data. With SCOP, we find that it is still challenging for LLMs to\nperform an expert-level comprehension process. Even so, we notice that LLMs\nshare some similarities with experts, e.g., performing better at comprehending\nlocal information than global information. Further analysis reveals that LLMs\ncan be somewhat unreliable -- they might reach correct answers through flawed\ncomprehension processes. Based on SCOP, we suggest that one direction for\nimproving LLMs is to focus more on the comprehension process, ensuring all\ncomprehension skills are thoroughly developed during training."
                },
                "authors": [
                    {
                        "name": "Yongjie Xiao"
                    },
                    {
                        "name": "Hongru Liang"
                    },
                    {
                        "name": "Peixin Qin"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Wenqiang Lei"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Lei"
                },
                "author": "Wenqiang Lei",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2004.14535 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04998v1",
                "updated": "2025-06-05T13:09:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    9,
                    24,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:09:24Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    9,
                    24,
                    3,
                    156,
                    0
                ],
                "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based\n  Approach for Complex Arithmetic Reasoning"
                },
                "summary": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a small problem\nset comprising 20 UAV-centric mathematical problems across four difficulty\nlevels. Our experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5% with\nRAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders\nof magnitude for the best-performing models. This pilot study indicates that\nRAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, question\nand answer are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous UAV operation necessitates reliable mathematical reasoning for\ntasks such as trajectory planning and power management. While traditional\nflight control relies on hardcoded equations, recent Large Language Models\n(LLMs) offer potential for more flexible problem-solving but struggle with\nreliably selecting and applying correct mathematical formulations and executing\nprecise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented\ngeneration framework designed to improve the mathematical reasoning of several\nLLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in\nUAV-specific contexts by providing access to relevant domain literature. To\nconduct an initial assessment, we introduce the UAV-Math-Bench, a small problem\nset comprising 20 UAV-centric mathematical problems across four difficulty\nlevels. Our experiments demonstrate that incorporating retrieval substantially\nincreases exact answer accuracy (achieving up to 75% with o1), reduces\ninstances of incorrect formulation selection (from 25% without RAG to 5% with\nRAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders\nof magnitude for the best-performing models. This pilot study indicates that\nRAG can enable general-purpose LLMs to function as more reliable tools for\nengineering analysis, although direct real-time flight control requires further\ninvestigation and validation on a larger scale. All benchmark data, question\nand answer are publicly available."
                },
                "authors": [
                    {
                        "name": "Mehdi Azarafza"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Faezeh Pasandideh"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Achim Rettberg"
                    }
                ],
                "author_detail": {
                    "name": "Achim Rettberg"
                },
                "author": "Achim Rettberg",
                "arxiv_comment": "15 pages, 7 figures, 4 appendix subsections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04989v1",
                "updated": "2025-06-05T13:02:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    2,
                    6,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    2,
                    6,
                    3,
                    156,
                    0
                ],
                "title": "BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat\n  Assessment"
                },
                "summary": "Accessing quality preparation and feedback for the Romanian Bacalaureat exam\nis challenging, particularly for students in remote or underserved areas. This\npaper introduces BacPrep, an experimental online platform exploring Large\nLanguage Model (LLM) potential for automated assessment, aiming to offer a\nfree, accessible resource. Using official exam questions from the last 5 years,\nBacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb\n2025), guided by official grading schemes, to provide experimental feedback.\nCurrently operational, its primary research function is collecting student\nsolutions and LLM outputs. This focused dataset is vital for planned expert\nvalidation to rigorously evaluate the feasibility and accuracy of this\ncutting-edge LLM in the specific Bacalaureat context before reliable\ndeployment. We detail the design, data strategy, status, validation plan, and\nethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing quality preparation and feedback for the Romanian Bacalaureat exam\nis challenging, particularly for students in remote or underserved areas. This\npaper introduces BacPrep, an experimental online platform exploring Large\nLanguage Model (LLM) potential for automated assessment, aiming to offer a\nfree, accessible resource. Using official exam questions from the last 5 years,\nBacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb\n2025), guided by official grading schemes, to provide experimental feedback.\nCurrently operational, its primary research function is collecting student\nsolutions and LLM outputs. This focused dataset is vital for planned expert\nvalidation to rigorously evaluate the feasibility and accuracy of this\ncutting-edge LLM in the specific Bacalaureat context before reliable\ndeployment. We detail the design, data strategy, status, validation plan, and\nethics."
                },
                "authors": [
                    {
                        "name": "Dumitran Adrian Marius"
                    },
                    {
                        "name": "Dita Radu"
                    }
                ],
                "author_detail": {
                    "name": "Dita Radu"
                },
                "author": "Dita Radu",
                "arxiv_comment": "9 pages Preprint ACCEPTED at BBGI (ITS Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03785v2",
                "updated": "2025-06-05T13:01:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    1,
                    33,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T09:46:43Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    46,
                    43,
                    2,
                    155,
                    0
                ],
                "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons"
                },
                "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring."
                },
                "authors": [
                    {
                        "name": "Isik Baran Sandan"
                    },
                    {
                        "name": "Tu Anh Dinh"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted to GEM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04985v1",
                "updated": "2025-06-05T12:56:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    56,
                    12,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:56:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    56,
                    12,
                    3,
                    156,
                    0
                ],
                "title": "FPTQuant: Function-Preserving Transforms for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPTQuant: Function-Preserving Transforms for LLM Quantization"
                },
                "summary": "Large language models (LLMs) require substantial compute, and thus energy, at\ninference time. While quantizing weights and activations is effective at\nimproving efficiency, naive quantization of LLMs can significantly degrade\nperformance due to large magnitude outliers. This paper describes FPTQuant,\nwhich introduces four novel, lightweight, and expressive function-preserving\ntransforms (FPTs) to facilitate quantization of transformers: (1) a mergeable\npre-RoPE transform for queries and keys, (2) a mergeable transform for values,\n(3) a mergeable scaling transform within the MLP block, and (4) a cheap,\ndynamic scaling transform. By leveraging the equivariances and independencies\ninherent to canonical transformer operation, we designed these FPTs to maintain\nthe model's function while shaping the intermediate activation distributions to\nbe more quantization friendly. FPTQuant requires no custom kernels and adds\nvirtually no overhead during inference. The FPTs are trained both locally to\nreduce outliers, and end-to-end such that the outputs of the quantized and\nfull-precision models match. FPTQuant enables static INT4 quantization with\nminimal overhead and shows SOTA speed-up of up to 3.9 times over FP.\nEmpirically, FPTQuant has an excellent accuracy-speed trade-off -- it is\nperforming on par or exceeding most prior work and only shows slightly lower\naccuracy compared to a method that is up to 29% slower.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require substantial compute, and thus energy, at\ninference time. While quantizing weights and activations is effective at\nimproving efficiency, naive quantization of LLMs can significantly degrade\nperformance due to large magnitude outliers. This paper describes FPTQuant,\nwhich introduces four novel, lightweight, and expressive function-preserving\ntransforms (FPTs) to facilitate quantization of transformers: (1) a mergeable\npre-RoPE transform for queries and keys, (2) a mergeable transform for values,\n(3) a mergeable scaling transform within the MLP block, and (4) a cheap,\ndynamic scaling transform. By leveraging the equivariances and independencies\ninherent to canonical transformer operation, we designed these FPTs to maintain\nthe model's function while shaping the intermediate activation distributions to\nbe more quantization friendly. FPTQuant requires no custom kernels and adds\nvirtually no overhead during inference. The FPTs are trained both locally to\nreduce outliers, and end-to-end such that the outputs of the quantized and\nfull-precision models match. FPTQuant enables static INT4 quantization with\nminimal overhead and shows SOTA speed-up of up to 3.9 times over FP.\nEmpirically, FPTQuant has an excellent accuracy-speed trade-off -- it is\nperforming on par or exceeding most prior work and only shows slightly lower\naccuracy compared to a method that is up to 29% slower."
                },
                "authors": [
                    {
                        "name": "Boris van Breugel"
                    },
                    {
                        "name": "Yelysei Bondarenko"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04980v1",
                "updated": "2025-06-05T12:50:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    50,
                    54,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:50:54Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    50,
                    54,
                    3,
                    156,
                    0
                ],
                "title": "Agentic AI for Intent-Based Industrial Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Intent-Based Industrial Automation"
                },
                "summary": "The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns."
                },
                "authors": [
                    {
                        "name": "Marcos Lima Romero"
                    },
                    {
                        "name": "Ricardo Suyama"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Suyama"
                },
                "author": "Ricardo Suyama",
                "arxiv_comment": "Preprint - Submitted to 16th IEEE/IAS International Conference on\n  Industry Applications - INDUSCON 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04978v1",
                "updated": "2025-06-05T12:49:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    49,
                    22,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:49:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    49,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN\n  Intrusion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Impact of Privacy-Preserving Federated Learning on CAN\n  Intrusion Detection"
                },
                "summary": "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenges derived from the data-intensive nature of machine learning in\nconjunction with technologies that enable novel paradigms such as V2X and the\npotential offered by 5G communication, allow and justify the deployment of\nFederated Learning (FL) solutions in the vehicular intrusion detection domain.\nIn this paper, we investigate the effects of integrating FL strategies into the\nmachine learning-based intrusion detection process for on-board vehicular\nnetworks. Accordingly, we propose a FL implementation of a state-of-the-art\nIntrusion Detection System (IDS) for Controller Area Network (CAN), based on\nLSTM autoencoders. We thoroughly evaluate its detection efficiency and\ncommunication overhead, comparing it to a centralized version of the same\nalgorithm, thereby presenting it as a feasible solution."
                },
                "authors": [
                    {
                        "name": "Gabriele Digregorio"
                    },
                    {
                        "name": "Elisabetta Cainazzo"
                    },
                    {
                        "name": "Stefano Longari"
                    },
                    {
                        "name": "Michele Carminati"
                    },
                    {
                        "name": "Stefano Zanero"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Zanero"
                },
                "author": "Stefano Zanero",
                "arxiv_doi": "10.1109/VTC2024-Spring62846.2024.10683636",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/VTC2024-Spring62846.2024.10683636",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.04978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE 99th Vehicular Technology Conference (VTC2024-Spring)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04975v1",
                "updated": "2025-06-05T12:47:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    47,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:47:21Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    47,
                    21,
                    3,
                    156,
                    0
                ],
                "title": "Evaluating Prompt-Driven Chinese Large Language Models: The Influence of\n  Persona Assignment on Stereotypes and Safeguards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Prompt-Driven Chinese Large Language Models: The Influence of\n  Persona Assignment on Stereotypes and Safeguards"
                },
                "summary": "Recent research has highlighted that assigning specific personas to large\nlanguage models (LLMs) can significantly increase harmful content generation.\nYet, limited attention has been given to persona-driven toxicity in non-Western\ncontexts, particularly in Chinese-based LLMs. In this paper, we perform a\nlarge-scale, systematic analysis of how persona assignment influences refusal\nbehavior and response toxicity in Qwen, a widely-used Chinese language model.\nUtilizing fine-tuned BERT classifiers and regression analysis, our study\nreveals significant gender biases in refusal rates and demonstrates that\ncertain negative personas can amplify toxicity toward Chinese social groups by\nup to 60-fold compared to the default model. To mitigate this toxicity, we\npropose an innovative multi-model feedback strategy, employing iterative\ninteractions between Qwen and an external evaluator, which effectively reduces\ntoxic outputs without costly model retraining. Our findings emphasize the\nnecessity of culturally specific analyses for LLMs safety and offer a practical\nframework for evaluating and enhancing ethical alignment in LLM-generated\ncontent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has highlighted that assigning specific personas to large\nlanguage models (LLMs) can significantly increase harmful content generation.\nYet, limited attention has been given to persona-driven toxicity in non-Western\ncontexts, particularly in Chinese-based LLMs. In this paper, we perform a\nlarge-scale, systematic analysis of how persona assignment influences refusal\nbehavior and response toxicity in Qwen, a widely-used Chinese language model.\nUtilizing fine-tuned BERT classifiers and regression analysis, our study\nreveals significant gender biases in refusal rates and demonstrates that\ncertain negative personas can amplify toxicity toward Chinese social groups by\nup to 60-fold compared to the default model. To mitigate this toxicity, we\npropose an innovative multi-model feedback strategy, employing iterative\ninteractions between Qwen and an external evaluator, which effectively reduces\ntoxic outputs without costly model retraining. Our findings emphasize the\nnecessity of culturally specific analyses for LLMs safety and offer a practical\nframework for evaluating and enhancing ethical alignment in LLM-generated\ncontent."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Li Feng"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Songbo Yang"
                    },
                    {
                        "name": "Mengxiao Zhu"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04974v1",
                "updated": "2025-06-05T12:47:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    47,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:47:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    47,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Indoor Sharing in the Mid-Band: A Performance Study of Neutral-Host,\n  Cellular Macro, and Wi-Fi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor Sharing in the Mid-Band: A Performance Study of Neutral-Host,\n  Cellular Macro, and Wi-Fi"
                },
                "summary": "Indoor environments present a significant challenge for wireless\nconnectivity, as immense data demand strains traditional solutions. Public\nMobile Network Operators (MNOs), utilizing outdoor macro base stations (BSs),\nsuffer from poor signal penetration. Indoor Wi-Fi networks, on the other hand,\nmay face reliability issues due to spectrum contention. Shared spectrum models,\nparticularly the Citizens Broadband Radio Service (CBRS) utilized by private\n4G/5G networks, have emerged as a promising alternative to provide reliable\nindoor service. Moreover, these private networks are equipped with the\nneutral-host (NH) model, seamlessly offloading indoor MNOs' traffic to the\nprivate CBRS network. This paper presents a comprehensive, in-situ performance\nevaluation of three co-located technologies utilizing mid-bands spectrum (1-6\nGHz)--a CBRS-based NH network, public MNO macro networks, and a Wi-Fi 6\nnetwork--within a large, big-box retail store characterized by significant\nbuilding loss. Our analysis demonstrates: (i) the NH network provides superior\nindoor coverage compared to MNO macro, requiring only six CBRS devices\n(CBSDs)--versus 65 Access Points (APs) for enterprise Wi-Fi--to achieve full\ncoverage, with a median building loss of 26.6 dB ensuring interference-free\ncoexistence with outdoor federal incumbents; (ii) the NH network achieves\nsubstantial indoor throughput gains, with per-channel normalized throughput\nimprovements of 1.44x and 1.62x in downlink (DL), and 4.33x and 13x in uplink\n(UL), compared to 4G and 5G macro deployments, respectively; (iii) the NH\ndeployment achieves a median indoor aggregated physical (PHY)-layer DL\nthroughput gain of 2.08x over 5G macro deployments indoors, despite utilizing\nonly 40 MHz of aggregated bandwidth compared to 225 MHz for 5G macro; and (iv)\nthe NH deployment also outperforms Wi-Fi in application-layer HTTP DL\nperformance by 5.05x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor environments present a significant challenge for wireless\nconnectivity, as immense data demand strains traditional solutions. Public\nMobile Network Operators (MNOs), utilizing outdoor macro base stations (BSs),\nsuffer from poor signal penetration. Indoor Wi-Fi networks, on the other hand,\nmay face reliability issues due to spectrum contention. Shared spectrum models,\nparticularly the Citizens Broadband Radio Service (CBRS) utilized by private\n4G/5G networks, have emerged as a promising alternative to provide reliable\nindoor service. Moreover, these private networks are equipped with the\nneutral-host (NH) model, seamlessly offloading indoor MNOs' traffic to the\nprivate CBRS network. This paper presents a comprehensive, in-situ performance\nevaluation of three co-located technologies utilizing mid-bands spectrum (1-6\nGHz)--a CBRS-based NH network, public MNO macro networks, and a Wi-Fi 6\nnetwork--within a large, big-box retail store characterized by significant\nbuilding loss. Our analysis demonstrates: (i) the NH network provides superior\nindoor coverage compared to MNO macro, requiring only six CBRS devices\n(CBSDs)--versus 65 Access Points (APs) for enterprise Wi-Fi--to achieve full\ncoverage, with a median building loss of 26.6 dB ensuring interference-free\ncoexistence with outdoor federal incumbents; (ii) the NH network achieves\nsubstantial indoor throughput gains, with per-channel normalized throughput\nimprovements of 1.44x and 1.62x in downlink (DL), and 4.33x and 13x in uplink\n(UL), compared to 4G and 5G macro deployments, respectively; (iii) the NH\ndeployment achieves a median indoor aggregated physical (PHY)-layer DL\nthroughput gain of 2.08x over 5G macro deployments indoors, despite utilizing\nonly 40 MHz of aggregated bandwidth compared to 225 MHz for 5G macro; and (iv)\nthe NH deployment also outperforms Wi-Fi in application-layer HTTP DL\nperformance by 5.05x."
                },
                "authors": [
                    {
                        "name": "Joshua Roy Palathinkal"
                    },
                    {
                        "name": "Muhammad Iqbal Rochman"
                    },
                    {
                        "name": "Vanlin Sathya"
                    },
                    {
                        "name": "Mehmet Yavuz"
                    },
                    {
                        "name": "Monisha Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Monisha Ghosh"
                },
                "author": "Monisha Ghosh",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02672v2",
                "updated": "2025-06-05T12:44:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    44,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-03T09:18:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    18,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving"
                },
                "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Chenxiao Liu"
                    },
                    {
                        "name": "Cheng Zhong"
                    },
                    {
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Chao Xin"
                    },
                    {
                        "name": "Wei Chengzhi"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "47 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04965v1",
                "updated": "2025-06-05T12:41:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    41,
                    20,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T12:41:20Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    41,
                    20,
                    3,
                    156,
                    0
                ],
                "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced\n  Algorithm Exams and Pave the Way for Editorial Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced\n  Algorithm Exams and Pave the Way for Editorial Generation"
                },
                "summary": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education."
                },
                "authors": [
                    {
                        "name": "Adrian Marius Dumitran"
                    },
                    {
                        "name": "Theodor-Pierre Moroianu"
                    },
                    {
                        "name": "Vasile Paul Alexe"
                    }
                ],
                "author_detail": {
                    "name": "Vasile Paul Alexe"
                },
                "author": "Vasile Paul Alexe",
                "arxiv_comment": "15 pages Pre-print Paper accepted to ITS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17934v2",
                "updated": "2025-06-05T12:40:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    40,
                    53,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-24T20:51:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    20,
                    51,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered\n  GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered\n  GUI Agents"
                },
                "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Zhiping Zhang"
                    },
                    {
                        "name": "Ibrahim Khalilov"
                    },
                    {
                        "name": "Bingcan Guo"
                    },
                    {
                        "name": "Simret A Gebreegziabher"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Yaxing Yao"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04962v2",
                "updated": "2025-06-06T07:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    49,
                    53,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-05T12:37:33Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    37,
                    33,
                    3,
                    156,
                    0
                ],
                "title": "PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm\n  Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm\n  Packages"
                },
                "summary": "Security vulnerabilities in software packages are a significant concern for\ndevelopers and users alike. Patching these vulnerabilities in a timely manner\nis crucial to restoring the integrity and security of software systems.\nHowever, previous work has shown that vulnerability reports often lack\nproof-of-concept (PoC) exploits, which are essential for fixing the\nvulnerability, testing patches, and avoiding regressions. Creating a PoC\nexploit is challenging because vulnerability reports are informal and often\nincomplete, and because it requires a detailed understanding of how inputs\npassed to potentially vulnerable APIs may reach security-relevant sinks. In\nthis paper, we present PoCGen, a novel approach to autonomously generate and\nvalidate PoC exploits for vulnerabilities in npm packages. This is the first\nfully autonomous approach to use large language models (LLMs) in tandem with\nstatic and dynamic analysis techniques for PoC exploit generation. PoCGen\nleverages an LLM for understanding vulnerability reports, for generating\ncandidate PoC exploits, and for validating and refining them. Our approach\nsuccessfully generates exploits for 77% of the vulnerabilities in the\nSecBench$.$js dataset and 39% in a new, more challenging dataset of 794 recent\nvulnerabilities. This success rate significantly outperforms a recent baseline\n(by 45 absolute percentage points), while imposing an average cost of $0.02 per\ngenerated exploit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security vulnerabilities in software packages are a significant concern for\ndevelopers and users alike. Patching these vulnerabilities in a timely manner\nis crucial to restoring the integrity and security of software systems.\nHowever, previous work has shown that vulnerability reports often lack\nproof-of-concept (PoC) exploits, which are essential for fixing the\nvulnerability, testing patches, and avoiding regressions. Creating a PoC\nexploit is challenging because vulnerability reports are informal and often\nincomplete, and because it requires a detailed understanding of how inputs\npassed to potentially vulnerable APIs may reach security-relevant sinks. In\nthis paper, we present PoCGen, a novel approach to autonomously generate and\nvalidate PoC exploits for vulnerabilities in npm packages. This is the first\nfully autonomous approach to use large language models (LLMs) in tandem with\nstatic and dynamic analysis techniques for PoC exploit generation. PoCGen\nleverages an LLM for understanding vulnerability reports, for generating\ncandidate PoC exploits, and for validating and refining them. Our approach\nsuccessfully generates exploits for 77% of the vulnerabilities in the\nSecBench$.$js dataset and 39% in a new, more challenging dataset of 794 recent\nvulnerabilities. This success rate significantly outperforms a recent baseline\n(by 45 absolute percentage points), while imposing an average cost of $0.02 per\ngenerated exploit."
                },
                "authors": [
                    {
                        "name": "Deniz Simsek"
                    },
                    {
                        "name": "Aryaz Eghbali"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]