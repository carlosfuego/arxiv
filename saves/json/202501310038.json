[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v1",
                "updated": "2025-01-25T01:45:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14795v1",
                "updated": "2025-01-11T12:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T12:22:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL"
                },
                "summary": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation."
                },
                "authors": [
                    {
                        "name": "Nicolas Lee Guidotti"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Lee Guidotti"
                },
                "author": "Nicolas Lee Guidotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.17860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17860v1",
                "updated": "2025-01-29T18:58:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    58,
                    48,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:58:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    58,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "Dialogue is Better Than Monologue: Instructing Medical LLMs via\n  Strategical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue is Better Than Monologue: Instructing Medical LLMs via\n  Strategical Conversations"
                },
                "summary": "Current medical AI systems often fail to replicate real-world clinical\nreasoning, as they are predominantly trained and evaluated on static text and\nquestion-answer tasks. These tuning methods and benchmarks overlook critical\naspects like evidence-based reasoning and handling distracting information. To\nbridge this gap, we introduce a novel benchmark that simulates real-world\ndiagnostic scenarios, integrating noise and difficulty levels aligned with\nUSMLE standards. Moreover, we explore dialogue-based fine-tuning, which\ntransforms static datasets into conversational formats to better capture\niterative reasoning processes. Experiments show that dialogue-tuned models\noutperform traditional methods, with improvements of $9.64\\%$ in multi-round\nreasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our\nfindings highlight dialogue tuning as a promising approach for advancing\nclinically aligned and robust medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current medical AI systems often fail to replicate real-world clinical\nreasoning, as they are predominantly trained and evaluated on static text and\nquestion-answer tasks. These tuning methods and benchmarks overlook critical\naspects like evidence-based reasoning and handling distracting information. To\nbridge this gap, we introduce a novel benchmark that simulates real-world\ndiagnostic scenarios, integrating noise and difficulty levels aligned with\nUSMLE standards. Moreover, we explore dialogue-based fine-tuning, which\ntransforms static datasets into conversational formats to better capture\niterative reasoning processes. Experiments show that dialogue-tuned models\noutperform traditional methods, with improvements of $9.64\\%$ in multi-round\nreasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our\nfindings highlight dialogue tuning as a promising approach for advancing\nclinically aligned and robust medical AI systems."
                },
                "authors": [
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhuangdi Zhu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17858v1",
                "updated": "2025-01-29T18:57:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    57,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:57:29Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    57,
                    29,
                    2,
                    29,
                    0
                ],
                "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging"
                },
                "summary": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena."
                },
                "authors": [
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17148v2",
                "updated": "2025-01-29T18:52:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    52,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T18:51:24Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders"
                },
                "summary": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean."
                },
                "authors": [
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v3",
                "updated": "2025-01-29T18:49:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    49,
                    22,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large\n  Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker incurs on average 7%\noverhead on training while detecting and correcting all extreme errors.\nCompared with the state-of-the-art checkpoint/restore approach, ATTNChecker\nreduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker incurs on average 7%\noverhead on training while detecting and correcting all extreme errors.\nCompared with the state-of-the-art checkpoint/restore approach, ATTNChecker\nreduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15004v2",
                "updated": "2025-01-29T18:49:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    49,
                    15,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-19T16:20:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Large Language Models and Code Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Code Security: A Systematic Literature Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks."
                },
                "authors": [
                    {
                        "name": "Enna Basic"
                    },
                    {
                        "name": "Alberto Giaretta"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Giaretta"
                },
                "author": "Alberto Giaretta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00400v2",
                "updated": "2025-01-29T18:46:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    46,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-01T04:53:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    53,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "DynEx: Dynamic Code Synthesis with Structured Design Exploration for\n  Accelerated Exploratory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynEx: Dynamic Code Synthesis with Structured Design Exploration for\n  Accelerated Exploratory Programming"
                },
                "summary": "Recent advancements in large language models have significantly expedited the\nprocess of generating front-end code. This allows users to rapidly prototype\nuser interfaces and ideate through code, a process known as exploratory\nprogramming. However, existing LLM code generation tools focus more on\ntechnical implementation details rather than finding the right design given a\nparticular problem. We present DynEx, an LLM-based method for design\nexploration in accelerated exploratory programming. DynEx introduces a\ntechnique to explore the design space through a structured Design Matrix before\ncreating the prototype with a modular, stepwise approach to LLM code\ngeneration. Code is generated sequentially, and users can test and approve each\nstep before moving onto the next. A user study of 10 experts found that DynEx\nincreased design exploration and enabled the creation of more complex and\nvaried prototypes compared to a Claude Artifact baseline. We conclude with a\ndiscussion of the implications of design exploration for exploratory\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly expedited the\nprocess of generating front-end code. This allows users to rapidly prototype\nuser interfaces and ideate through code, a process known as exploratory\nprogramming. However, existing LLM code generation tools focus more on\ntechnical implementation details rather than finding the right design given a\nparticular problem. We present DynEx, an LLM-based method for design\nexploration in accelerated exploratory programming. DynEx introduces a\ntechnique to explore the design space through a structured Design Matrix before\ncreating the prototype with a modular, stepwise approach to LLM code\ngeneration. Code is generated sequentially, and users can test and approve each\nstep before moving onto the next. A user study of 10 experts found that DynEx\nincreased design exploration and enabled the creation of more complex and\nvaried prototypes compared to a Claude Artifact baseline. We conclude with a\ndiscussion of the implications of design exploration for exploratory\nprogramming."
                },
                "authors": [
                    {
                        "name": "Jenny Ma"
                    },
                    {
                        "name": "Karthik Sreedhar"
                    },
                    {
                        "name": "Vivian Liu"
                    },
                    {
                        "name": "Pedro Alejandro Perez"
                    },
                    {
                        "name": "Sitong Wang"
                    },
                    {
                        "name": "Riya Sahni"
                    },
                    {
                        "name": "Lydia B. Chilton"
                    }
                ],
                "author_detail": {
                    "name": "Lydia B. Chilton"
                },
                "author": "Lydia B. Chilton",
                "arxiv_comment": "27 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17840v1",
                "updated": "2025-01-29T18:40:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:40:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Learning Beyond the Surface: How Far Can Continual Pre-Training with\n  LoRA Enhance LLMs' Domain-Specific Insight Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Beyond the Surface: How Far Can Continual Pre-Training with\n  LoRA Enhance LLMs' Domain-Specific Insight Learning?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious tasks, yet their ability to extract and internalize deeper insights\nfrom domain-specific datasets remains underexplored. In this study, we\ninvestigate how continual pre-training can enhance LLMs' capacity for insight\nlearning across three distinct forms: declarative, statistical, and\nprobabilistic insights. Focusing on two critical domains: medicine and finance,\nwe employ LoRA to train LLMs on two existing datasets. To evaluate each insight\ntype, we create benchmarks to measure how well continual pre-training helps\nmodels go beyond surface-level knowledge. We also assess the impact of document\nmodification on capturing insights. The results show that, while continual\npre-training on original documents has a marginal effect, modifying documents\nto retain only essential information significantly enhances the\ninsight-learning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious tasks, yet their ability to extract and internalize deeper insights\nfrom domain-specific datasets remains underexplored. In this study, we\ninvestigate how continual pre-training can enhance LLMs' capacity for insight\nlearning across three distinct forms: declarative, statistical, and\nprobabilistic insights. Focusing on two critical domains: medicine and finance,\nwe employ LoRA to train LLMs on two existing datasets. To evaluate each insight\ntype, we create benchmarks to measure how well continual pre-training helps\nmodels go beyond surface-level knowledge. We also assess the impact of document\nmodification on capturing insights. The results show that, while continual\npre-training on original documents has a marginal effect, modifying documents\nto retain only essential information significantly enhances the\ninsight-learning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17834v1",
                "updated": "2025-01-29T18:30:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    30,
                    18,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:30:18Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    30,
                    18,
                    2,
                    29,
                    0
                ],
                "title": "Hierarchical Fallback Architecture for High Risk Online Machine Learning\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Fallback Architecture for High Risk Online Machine Learning\n  Inference"
                },
                "summary": "Open Banking powered machine learning applications require novel robustness\napproaches to deal with challenging stress and failure scenarios. In this paper\nwe propose an hierarchical fallback architecture for improving robustness in\nhigh risk machine learning applications with a focus in the financial domain.\nWe define generic failure scenarios often found in online inference that depend\non external data providers and we describe in detail how to apply the\nhierarchical fallback architecture to address them. Finally, we offer a real\nworld example of its applicability in the industry for near-real time\ntransactional fraud risk evaluation using Open Banking data and under extreme\nstress scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Banking powered machine learning applications require novel robustness\napproaches to deal with challenging stress and failure scenarios. In this paper\nwe propose an hierarchical fallback architecture for improving robustness in\nhigh risk machine learning applications with a focus in the financial domain.\nWe define generic failure scenarios often found in online inference that depend\non external data providers and we describe in detail how to apply the\nhierarchical fallback architecture to address them. Finally, we offer a real\nworld example of its applicability in the industry for near-real time\ntransactional fraud risk evaluation using Open Banking data and under extreme\nstress scenarios."
                },
                "authors": [
                    {
                        "name": "Gustavo Polleti"
                    },
                    {
                        "name": "Marlesson Santana"
                    },
                    {
                        "name": "Felipe Sassi Del Sant"
                    },
                    {
                        "name": "Eduardo Fontes"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Fontes"
                },
                "author": "Eduardo Fontes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11423v2",
                "updated": "2025-01-29T18:27:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    27,
                    16,
                    2,
                    29,
                    0
                ],
                "published": "2024-09-12T10:14:12Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    14,
                    12,
                    3,
                    256,
                    0
                ],
                "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large\n  Language Models on Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large\n  Language Models on Generated Data"
                },
                "summary": "Large language models (LLMs) have demonstrated significant success in various\ndomain-specific tasks, with their performance often improving substantially\nafter fine-tuning. However, fine-tuning with real-world data introduces privacy\nrisks. To mitigate these risks, developers increasingly rely on synthetic data\ngeneration as an alternative to using real data, as data generated by\ntraditional models is believed to be different from real-world data. However,\nwith the advanced capabilities of LLMs, the distinction between real data and\ndata generated by these models has become nearly indistinguishable. This\nconvergence introduces similar privacy risks for generated data to those\nassociated with real data. Our study investigates whether fine-tuning with\nLLM-generated data truly enhances privacy or introduces additional privacy\nrisks by examining the structural characteristics of data generated by LLMs,\nfocusing on two primary fine-tuning approaches: supervised fine-tuning (SFT)\nwith unstructured (plain-text) generated data and self-instruct tuning. In the\nscenario of SFT, the data is put into a particular instruction tuning format\nused by previous studies. We use Personal Information Identifier (PII) leakage\nand Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open\nPre-trained Transformer (OPT) to measure privacy risks. Notably, after\nfine-tuning with unstructured generated data, the rate of successful PII\nextractions for Pythia increased by over 20%, highlighting the potential\nprivacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs\nfor Pythia-6.9b, the second biggest model of the suite, increases over 40%\nafter self-instruct tuning. Our results indicate the potential privacy risks\nassociated with fine-tuning LLMs using generated data, underscoring the need\nfor careful consideration of privacy safeguards in such approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant success in various\ndomain-specific tasks, with their performance often improving substantially\nafter fine-tuning. However, fine-tuning with real-world data introduces privacy\nrisks. To mitigate these risks, developers increasingly rely on synthetic data\ngeneration as an alternative to using real data, as data generated by\ntraditional models is believed to be different from real-world data. However,\nwith the advanced capabilities of LLMs, the distinction between real data and\ndata generated by these models has become nearly indistinguishable. This\nconvergence introduces similar privacy risks for generated data to those\nassociated with real data. Our study investigates whether fine-tuning with\nLLM-generated data truly enhances privacy or introduces additional privacy\nrisks by examining the structural characteristics of data generated by LLMs,\nfocusing on two primary fine-tuning approaches: supervised fine-tuning (SFT)\nwith unstructured (plain-text) generated data and self-instruct tuning. In the\nscenario of SFT, the data is put into a particular instruction tuning format\nused by previous studies. We use Personal Information Identifier (PII) leakage\nand Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open\nPre-trained Transformer (OPT) to measure privacy risks. Notably, after\nfine-tuning with unstructured generated data, the rate of successful PII\nextractions for Pythia increased by over 20%, highlighting the potential\nprivacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs\nfor Pythia-6.9b, the second biggest model of the suite, increases over 40%\nafter self-instruct tuning. Our results indicate the potential privacy risks\nassociated with fine-tuning LLMs using generated data, underscoring the need\nfor careful consideration of privacy safeguards in such approaches."
                },
                "authors": [
                    {
                        "name": "Atilla Akkus"
                    },
                    {
                        "name": "Masoud Poorghaffar Aghdam"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Sinem Sav"
                    }
                ],
                "author_detail": {
                    "name": "Sinem Sav"
                },
                "author": "Sinem Sav",
                "arxiv_comment": "Accepted at 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17813v1",
                "updated": "2025-01-29T18:06:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    6,
                    8,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:06:08Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    6,
                    8,
                    2,
                    29,
                    0
                ],
                "title": "P-TAME: Explain Any Image Classifier with Trained Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-TAME: Explain Any Image Classifier with Trained Perturbations"
                },
                "summary": "The adoption of Deep Neural Networks (DNNs) in critical fields where\npredictions need to be accompanied by justifications is hindered by their\ninherent black-box nature. In this paper, we introduce P-TAME\n(Perturbation-based Trainable Attention Mechanism for Explanations), a\nmodel-agnostic method for explaining DNN-based image classifiers. P-TAME\nemploys an auxiliary image classifier to extract features from the input image,\nbypassing the need to tailor the explanation method to the internal\narchitecture of the backbone classifier being explained. Unlike traditional\nperturbation-based methods, which have high computational requirements, P-TAME\noffers an efficient alternative by generating high-resolution explanations in a\nsingle forward pass during inference. We apply P-TAME to explain the decisions\nof VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image\nclassifiers. Quantitative and qualitative results show that our method matches\nor outperforms previous explainability methods, including model-specific\napproaches. Code and trained models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Deep Neural Networks (DNNs) in critical fields where\npredictions need to be accompanied by justifications is hindered by their\ninherent black-box nature. In this paper, we introduce P-TAME\n(Perturbation-based Trainable Attention Mechanism for Explanations), a\nmodel-agnostic method for explaining DNN-based image classifiers. P-TAME\nemploys an auxiliary image classifier to extract features from the input image,\nbypassing the need to tailor the explanation method to the internal\narchitecture of the backbone classifier being explained. Unlike traditional\nperturbation-based methods, which have high computational requirements, P-TAME\noffers an efficient alternative by generating high-resolution explanations in a\nsingle forward pass during inference. We apply P-TAME to explain the decisions\nof VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image\nclassifiers. Quantitative and qualitative results show that our method matches\nor outperforms previous explainability methods, including model-specific\napproaches. Code and trained models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mariano V. Ntrougkas"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17810v1",
                "updated": "2025-01-29T17:59:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    59,
                    48,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    59,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "Cell Deformation Signatures along the Apical-Basal Axis: A 3D Continuum\n  Mechanics Shell Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell Deformation Signatures along the Apical-Basal Axis: A 3D Continuum\n  Mechanics Shell Model"
                },
                "summary": "Two-dimensional (2D) mechanical models of confluent tissues have related the\nmechanical state of a monolayer of cells to the average perimeter length of the\ncell cross sections, predicting floppiness or rigidity of the material. For the\nwell-studied system of in-vitro MDCK epithelial cells, however, we find\nexperimentally that cells in mechanically rigid tissues display long perimeters\ncharacteristic of a floppy state in 2D models. We suggest that this discrepancy\nis due to mechanical effects in the third (apical-basal) dimension, including\nthose caused by actin stress fibers near the basal membrane. To quantitatively\nunderstand cell deformations in 3D, we develop a continuum mechanics model of\nepithelial cells as elastic cylindrical shells, with appropriate boundary\nconditions reflecting both the passive confinement of neighboring cells and the\nactive stress of actomyosin contractility. This formalism yields analytical\nsolutions predicting cell cross sections along the entire cylinder axis.\nDeconvolution microscopy experimental data confirm the significant and\nsystematic change in cell shape parameters in this apical-basal direction. In\naddition to providing a wealth of detailed information on deformation on the\nsubcellular scale, the results of the approach alter our understanding of how\nactive tissues balance requirements of their stiffness and integrity,\nsuggesting they are more robust against loss of rigidity than previously\ninferred.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional (2D) mechanical models of confluent tissues have related the\nmechanical state of a monolayer of cells to the average perimeter length of the\ncell cross sections, predicting floppiness or rigidity of the material. For the\nwell-studied system of in-vitro MDCK epithelial cells, however, we find\nexperimentally that cells in mechanically rigid tissues display long perimeters\ncharacteristic of a floppy state in 2D models. We suggest that this discrepancy\nis due to mechanical effects in the third (apical-basal) dimension, including\nthose caused by actin stress fibers near the basal membrane. To quantitatively\nunderstand cell deformations in 3D, we develop a continuum mechanics model of\nepithelial cells as elastic cylindrical shells, with appropriate boundary\nconditions reflecting both the passive confinement of neighboring cells and the\nactive stress of actomyosin contractility. This formalism yields analytical\nsolutions predicting cell cross sections along the entire cylinder axis.\nDeconvolution microscopy experimental data confirm the significant and\nsystematic change in cell shape parameters in this apical-basal direction. In\naddition to providing a wealth of detailed information on deformation on the\nsubcellular scale, the results of the approach alter our understanding of how\nactive tissues balance requirements of their stiffness and integrity,\nsuggesting they are more robust against loss of rigidity than previously\ninferred."
                },
                "authors": [
                    {
                        "name": "Jairo M. Rojas"
                    },
                    {
                        "name": "Mayisha Z. Nakib"
                    },
                    {
                        "name": "Vivian W. Tang"
                    },
                    {
                        "name": "William M. Brieher"
                    },
                    {
                        "name": "Sascha Hilgenfeldt"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Hilgenfeldt"
                },
                "author": "Sascha Hilgenfeldt",
                "arxiv_comment": "9 pages, 5 figures. Supporting Information included in the same file\n  (6 additional pages, 7 additional figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17802v1",
                "updated": "2025-01-29T17:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    44,
                    57,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:44:57Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    44,
                    57,
                    2,
                    29,
                    0
                ],
                "title": "LEKA:LLM-Enhanced Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEKA:LLM-Enhanced Knowledge Augmentation"
                },
                "summary": "Humans excel in analogical learning and knowledge transfer and, more\nimportantly, possess a unique understanding of identifying appropriate sources\nof knowledge. From a model's perspective, this presents an interesting\nchallenge. If models could autonomously retrieve knowledge useful for transfer\nor decision-making to solve problems, they would transition from passively\nacquiring to actively accessing and learning from knowledge. However, filling\nmodels with knowledge is relatively straightforward -- it simply requires more\ntraining and accessible knowledge bases. The more complex task is teaching\nmodels about which knowledge can be analogized and transferred. Therefore, we\ndesign a knowledge augmentation method LEKA for knowledge transfer that\nactively searches for suitable knowledge sources that can enrich the target\ndomain's knowledge. This LEKA method extracts key information from textual\ninformation from the target domain, retrieves pertinent data from external data\nlibraries, and harmonizes retrieved data with the target domain data in feature\nspace and marginal probability measures. We validate the effectiveness of our\napproach through extensive experiments across various domains and demonstrate\nsignificant improvements over traditional methods in reducing computational\ncosts, automating data alignment, and optimizing transfer learning outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans excel in analogical learning and knowledge transfer and, more\nimportantly, possess a unique understanding of identifying appropriate sources\nof knowledge. From a model's perspective, this presents an interesting\nchallenge. If models could autonomously retrieve knowledge useful for transfer\nor decision-making to solve problems, they would transition from passively\nacquiring to actively accessing and learning from knowledge. However, filling\nmodels with knowledge is relatively straightforward -- it simply requires more\ntraining and accessible knowledge bases. The more complex task is teaching\nmodels about which knowledge can be analogized and transferred. Therefore, we\ndesign a knowledge augmentation method LEKA for knowledge transfer that\nactively searches for suitable knowledge sources that can enrich the target\ndomain's knowledge. This LEKA method extracts key information from textual\ninformation from the target domain, retrieves pertinent data from external data\nlibraries, and harmonizes retrieved data with the target domain data in feature\nspace and marginal probability measures. We validate the effectiveness of our\napproach through extensive experiments across various domains and demonstrate\nsignificant improvements over traditional methods in reducing computational\ncosts, automating data alignment, and optimizing transfer learning outcomes."
                },
                "authors": [
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Dongjie Wang"
                    },
                    {
                        "name": "Yanjie Fu"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17799v1",
                "updated": "2025-01-29T17:38:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    38,
                    39,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:38:39Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    38,
                    39,
                    2,
                    29,
                    0
                ],
                "title": "Leveraging Multimodal LLM for Inspirational User Interface Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Multimodal LLM for Inspirational User Interface Search"
                },
                "summary": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies."
                },
                "authors": [
                    {
                        "name": "Seokhyeon Park"
                    },
                    {
                        "name": "Yumin Song"
                    },
                    {
                        "name": "Soohyun Lee"
                    },
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "arxiv_doi": "10.1145/3706598.3714213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems (CHI '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17790v1",
                "updated": "2025-01-29T17:31:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    31,
                    26,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:31:26Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    31,
                    26,
                    2,
                    29,
                    0
                ],
                "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone\n  Disambiguation -- Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone\n  Disambiguation -- Challenges and Insights"
                },
                "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted\nfor Taiwanese Mandarin, highlighting phonetic control abilities to address the\nunique challenges of polyphone disambiguation in the language. Building upon\nCosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an\noptimal-transport conditional flow matching model (OT-CFM), and a grapheme to\nphoneme prediction model, to generate realistic speech that closely mimics\nhuman utterances. Our evaluation demonstrates BreezyVoice's superior\nperformance in both general and code-switching contexts, highlighting its\nrobustness and effectiveness in generating high-fidelity speech. Additionally,\nwe address the challenges of generalizability in modeling long-tail speakers\nand polyphone disambiguation. Our approach significantly enhances performance\nand offers valuable insights into the workings of neural codec TTS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted\nfor Taiwanese Mandarin, highlighting phonetic control abilities to address the\nunique challenges of polyphone disambiguation in the language. Building upon\nCosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an\noptimal-transport conditional flow matching model (OT-CFM), and a grapheme to\nphoneme prediction model, to generate realistic speech that closely mimics\nhuman utterances. Our evaluation demonstrates BreezyVoice's superior\nperformance in both general and code-switching contexts, highlighting its\nrobustness and effectiveness in generating high-fidelity speech. Additionally,\nwe address the challenges of generalizability in modeling long-tail speakers\nand polyphone disambiguation. Our approach significantly enhances performance\nand offers valuable insights into the workings of neural codec TTS systems."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Chia-Chun Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Chien-Yu Yu"
                    },
                    {
                        "name": "Ming-Ji Lee"
                    },
                    {
                        "name": "Chien-Cheng Chen"
                    },
                    {
                        "name": "Ru-Heng Huang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17788v1",
                "updated": "2025-01-29T17:26:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    26,
                    47,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:26:47Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    26,
                    47,
                    2,
                    29,
                    0
                ],
                "title": "WARP: An Efficient Engine for Multi-Vector Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WARP: An Efficient Engine for Multi-Vector Retrieval"
                },
                "summary": "We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression to bypass costly vector reconstruction, and (3) a\ntwo-stage reduction process for efficient scoring. Combined with optimized C++\nkernels and specialized inference runtimes, WARP reduces end-to-end latency by\n41x compared to XTR's reference implementation and thereby achieves a 3x\nspeedup over PLAID from the the official ColBERT implementation.\n  We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression during retrieval, and (3) a two-stage reduction process\nfor efficient scoring. Thanks also to highly-optimized C++ kernels and to the\nadoption of specialized inference runtimes, WARP can reduce end-to-end query\nlatency relative to XTR's reference implementation by 41x. And it thereby\nachieves a 3x speedup over the official ColBERTv2 PLAID engine, while\npreserving retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression to bypass costly vector reconstruction, and (3) a\ntwo-stage reduction process for efficient scoring. Combined with optimized C++\nkernels and specialized inference runtimes, WARP reduces end-to-end latency by\n41x compared to XTR's reference implementation and thereby achieves a 3x\nspeedup over PLAID from the the official ColBERT implementation.\n  We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression during retrieval, and (3) a two-stage reduction process\nfor efficient scoring. Thanks also to highly-optimized C++ kernels and to the\nadoption of specialized inference runtimes, WARP can reduce end-to-end query\nlatency relative to XTR's reference implementation by 41x. And it thereby\nachieves a 3x speedup over the official ColBERTv2 PLAID engine, while\npreserving retrieval quality."
                },
                "authors": [
                    {
                        "name": "Jan Luca Scheerer"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Gustavo Alonso"
                    },
                    {
                        "name": "Omar Khattab"
                    }
                ],
                "author_detail": {
                    "name": "Omar Khattab"
                },
                "author": "Omar Khattab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17785v1",
                "updated": "2025-01-29T17:24:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    19,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:24:19Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    19,
                    2,
                    29,
                    0
                ],
                "title": "Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare\n  Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare\n  Scripts"
                },
                "summary": "We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not\nencoded in Unicode. We introduce a novel approach to construct a multimodal\ndataset of linguistic puzzles involving such scripts, utilizing a tokenization\nmethod for language glyphs. Our methods include the Picture Method for LVLMs\nand the Description Method for LLMs, enabling these models to tackle these\nchallenges. We conduct experiments using prominent models, GPT-4o, Gemini, and\nClaude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and\nlimitations of current AI methods in linguistic decipherment, highlighting the\nimpact of Unicode encoding on model performance and the challenges of modeling\nvisual language tokens through descriptions. Our study advances understanding\nof AI's potential in linguistic decipherment and underscores the need for\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not\nencoded in Unicode. We introduce a novel approach to construct a multimodal\ndataset of linguistic puzzles involving such scripts, utilizing a tokenization\nmethod for language glyphs. Our methods include the Picture Method for LVLMs\nand the Description Method for LLMs, enabling these models to tackle these\nchallenges. We conduct experiments using prominent models, GPT-4o, Gemini, and\nClaude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and\nlimitations of current AI methods in linguistic decipherment, highlighting the\nimpact of Unicode encoding on model performance and the challenges of modeling\nvisual language tokens through descriptions. Our study advances understanding\nof AI's potential in linguistic decipherment and underscores the need for\nfurther research."
                },
                "authors": [
                    {
                        "name": "Yu-Fei Shih"
                    },
                    {
                        "name": "Zheng-Lin Lin"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Kai Hsieh"
                },
                "author": "Shu-Kai Hsieh",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11891v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11891v3",
                "updated": "2025-01-29T17:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-18T04:36:37Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    4,
                    36,
                    37,
                    3,
                    109,
                    0
                ],
                "title": "Large Language Models Can Solve Real-World Planning Rigorously with\n  Formal Verification Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Can Solve Real-World Planning Rigorously with\n  Formal Verification Tools"
                },
                "summary": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "50 pages, 6 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11891v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11891v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2112.11027v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2112.11027v6",
                "updated": "2025-01-29T17:22:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    22,
                    28,
                    2,
                    29,
                    0
                ],
                "published": "2021-12-21T07:55:55Z",
                "published_parsed": [
                    2021,
                    12,
                    21,
                    7,
                    55,
                    55,
                    1,
                    355,
                    0
                ],
                "title": "More is Less: Inducing Sparsity via Overparameterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is Less: Inducing Sparsity via Overparameterization"
                },
                "summary": "In deep learning it is common to overparameterize neural networks, that is,\nto use more parameters than training samples. Quite surprisingly training the\nneural network via (stochastic) gradient descent leads to models that\ngeneralize very well, while classical statistics would suggest overfitting. In\norder to gain understanding of this implicit bias phenomenon we study the\nspecial case of sparse recovery (compressed sensing) which is of interest on\nits own. More precisely, in order to reconstruct a vector from underdetermined\nlinear measurements, we introduce a corresponding overparameterized square loss\nfunctional, where the vector to be reconstructed is deeply factorized into\nseveral vectors. We show that, if there exists an exact solution, vanilla\ngradient flow for the overparameterized loss functional converges to a good\napproximation of the solution of minimal $\\ell_1$-norm. The latter is\nwell-known to promote sparse solutions. As a by-product, our results\nsignificantly improve the sample complexity for compressed sensing via gradient\nflow/descent on overparameterized models derived in previous works. The theory\naccurately predicts the recovery rate in numerical experiments. Our proof\nrelies on analyzing a certain Bregman divergence of the flow. This bypasses the\nobstacles caused by non-convexity and should be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In deep learning it is common to overparameterize neural networks, that is,\nto use more parameters than training samples. Quite surprisingly training the\nneural network via (stochastic) gradient descent leads to models that\ngeneralize very well, while classical statistics would suggest overfitting. In\norder to gain understanding of this implicit bias phenomenon we study the\nspecial case of sparse recovery (compressed sensing) which is of interest on\nits own. More precisely, in order to reconstruct a vector from underdetermined\nlinear measurements, we introduce a corresponding overparameterized square loss\nfunctional, where the vector to be reconstructed is deeply factorized into\nseveral vectors. We show that, if there exists an exact solution, vanilla\ngradient flow for the overparameterized loss functional converges to a good\napproximation of the solution of minimal $\\ell_1$-norm. The latter is\nwell-known to promote sparse solutions. As a by-product, our results\nsignificantly improve the sample complexity for compressed sensing via gradient\nflow/descent on overparameterized models derived in previous works. The theory\naccurately predicts the recovery rate in numerical experiments. Our proof\nrelies on analyzing a certain Bregman divergence of the flow. This bypasses the\nobstacles caused by non-convexity and should be of independent interest."
                },
                "authors": [
                    {
                        "name": "Hung-Hsu Chou"
                    },
                    {
                        "name": "Johannes Maly"
                    },
                    {
                        "name": "Holger Rauhut"
                    }
                ],
                "author_detail": {
                    "name": "Holger Rauhut"
                },
                "author": "Holger Rauhut",
                "arxiv_journal_ref": "Information and Inference: A Journal of the IMA, 12(3), 04 2023.\n  iaad012",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2112.11027v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2112.11027v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17771v1",
                "updated": "2025-01-29T17:05:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    5,
                    33,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:05:33Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    5,
                    33,
                    2,
                    29,
                    0
                ],
                "title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs"
                },
                "summary": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for\npruning Large Language Models (LLMs), which combines two different strategies\nof pruning, namely Width and Depth Pruning. The first stage (Width Pruning)\nremoves entire neurons, hence their corresponding rows and columns, aiming to\npreserve the connectivity among the pruned structures in the intermediate state\nof the Feed-Forward Networks in each Transformer block. This is done based on\nan importance score measuring the impact of each neuron over the output\nmagnitude. The second stage (Depth Pruning), instead, removes entire Attention\nsubmodules. This is done by applying an iterative process that removes the\nAttention submodules with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%),\nmeasuring the resulting perplexity over three language modeling datasets as\nwell as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at available at\n\\url{https://github.com/FabrizioSandri/2SSP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for\npruning Large Language Models (LLMs), which combines two different strategies\nof pruning, namely Width and Depth Pruning. The first stage (Width Pruning)\nremoves entire neurons, hence their corresponding rows and columns, aiming to\npreserve the connectivity among the pruned structures in the intermediate state\nof the Feed-Forward Networks in each Transformer block. This is done based on\nan importance score measuring the impact of each neuron over the output\nmagnitude. The second stage (Depth Pruning), instead, removes entire Attention\nsubmodules. This is done by applying an iterative process that removes the\nAttention submodules with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%),\nmeasuring the resulting perplexity over three language modeling datasets as\nwell as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at available at\n\\url{https://github.com/FabrizioSandri/2SSP}."
                },
                "authors": [
                    {
                        "name": "Fabrizio Sandri"
                    },
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14661v2",
                "updated": "2025-01-29T17:00:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    0,
                    25,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-20T18:29:13Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    29,
                    13,
                    3,
                    172,
                    0
                ],
                "title": "Modelling the evolution of the Galactic disc scale height traced by open\n  clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the evolution of the Galactic disc scale height traced by open\n  clusters"
                },
                "summary": "Context. The scale height of the spatial distribution of open clusters (OCs)\nin the Milky Way exhibits a well known increase with age which is usually\ninterpreted as evidence for dynamical heating of the disc or of the disc having\nbeen thicker in the past.\n  Aims. We address the increase of the scale height with age of the OC\npopulation from a different angle. We propose that the apparent thickening of\nthe disc can be largely explained as a consequence of a stronger disruption of\nOCs near the Galactic plane by disc phenomena, namely encounters with giant\nmolecular clouds (GMCs).\n  Methods. We present a computational model that forms OCs with different\ninitial masses and follows their orbits while subjecting them to different\ndisruption mechanisms. To setup the model and infer its parameters, we use and\nanalyse a Gaia-based OC catalogue (Dias et al. 2021). We investigate both the\nspatial and age distributions of the OC population and discuss the completeness\nof the sample. The simulation results are then compared to the observations.\n  Results. Consistent with previous studies, the observations reveal that the\nSH of the spatial distribution of OCs increases with age. We find that it is\nvery likely that the OC sample is incomplete even for the solar neighbourhood.\nThe model simulations successfully reproduce the SH increase with age and the\ntotal number of OCs that survive with age up to 1 Gyr. For older OCs, the\npredicted SH from the model starts deviating from the observations, although\nremaining within the uncertainties of the observations. This can be related\nwith effects of incompleteness and/or simplifications in the model.\n  Conclusions. A selective disruption of OCs near the galactic plane through\nGMC encounters is able to explain the SH evolution of the OC population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The scale height of the spatial distribution of open clusters (OCs)\nin the Milky Way exhibits a well known increase with age which is usually\ninterpreted as evidence for dynamical heating of the disc or of the disc having\nbeen thicker in the past.\n  Aims. We address the increase of the scale height with age of the OC\npopulation from a different angle. We propose that the apparent thickening of\nthe disc can be largely explained as a consequence of a stronger disruption of\nOCs near the Galactic plane by disc phenomena, namely encounters with giant\nmolecular clouds (GMCs).\n  Methods. We present a computational model that forms OCs with different\ninitial masses and follows their orbits while subjecting them to different\ndisruption mechanisms. To setup the model and infer its parameters, we use and\nanalyse a Gaia-based OC catalogue (Dias et al. 2021). We investigate both the\nspatial and age distributions of the OC population and discuss the completeness\nof the sample. The simulation results are then compared to the observations.\n  Results. Consistent with previous studies, the observations reveal that the\nSH of the spatial distribution of OCs increases with age. We find that it is\nvery likely that the OC sample is incomplete even for the solar neighbourhood.\nThe model simulations successfully reproduce the SH increase with age and the\ntotal number of OCs that survive with age up to 1 Gyr. For older OCs, the\npredicted SH from the model starts deviating from the observations, although\nremaining within the uncertainties of the observations. This can be related\nwith effects of incompleteness and/or simplifications in the model.\n  Conclusions. A selective disruption of OCs near the galactic plane through\nGMC encounters is able to explain the SH evolution of the OC population."
                },
                "authors": [
                    {
                        "name": "Sandro Moreira"
                    },
                    {
                        "name": "André Moitinho"
                    },
                    {
                        "name": "André Silva"
                    },
                    {
                        "name": "Duarte Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Duarte Almeida"
                },
                "author": "Duarte Almeida",
                "arxiv_comment": "This paper has been submitted to Astronomy and Astrophysics (A&A) on\n  15/04/2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17767v1",
                "updated": "2025-01-29T16:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    58,
                    18,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    58,
                    18,
                    2,
                    29,
                    0
                ],
                "title": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs"
                },
                "summary": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context."
                },
                "authors": [
                    {
                        "name": "Ankush Agarwal"
                    },
                    {
                        "name": "Ganesh S"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    }
                ],
                "author_detail": {
                    "name": "Chaitanya Devaguptapu"
                },
                "author": "Chaitanya Devaguptapu",
                "arxiv_comment": "Accepted at NAACL 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v3",
                "updated": "2025-01-29T16:57:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    57,
                    35,
                    2,
                    29,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. Because of their crucial impact on\nthe utility, they are often considered intellectual property, similar to the\ncode of a software product. However, extracting system prompts is easily\npossible. As of today, there is no effective countermeasure to prevent the\nstealing of system prompts and all safeguarding efforts could be evaded. In\nthis work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith only little overhead. The core idea is to find a representation of the\noriginal system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We evaluate our\napproach by comparing our obfuscated prompt output with the output of the\noriginal prompt, using eight distinct metrics, to measure the lexical,\ncharacter-level, and semantic similarity. We show that the obfuscated version\nis constantly on par with the original one. We further perform three different\ndeobfuscation attacks with varying attacker knowledge--covering both black-box\nand white-box conditions--and show that in realistic attack scenarios an\nattacker is not able to extract meaningful information. Overall, we demonstrate\nthat prompt obfuscation is an effective mechanism to safeguard the intellectual\nproperty of a system prompt while maintaining the same utility as the original\nprompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. Because of their crucial impact on\nthe utility, they are often considered intellectual property, similar to the\ncode of a software product. However, extracting system prompts is easily\npossible. As of today, there is no effective countermeasure to prevent the\nstealing of system prompts and all safeguarding efforts could be evaded. In\nthis work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith only little overhead. The core idea is to find a representation of the\noriginal system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We evaluate our\napproach by comparing our obfuscated prompt output with the output of the\noriginal prompt, using eight distinct metrics, to measure the lexical,\ncharacter-level, and semantic similarity. We show that the obfuscated version\nis constantly on par with the original one. We further perform three different\ndeobfuscation attacks with varying attacker knowledge--covering both black-box\nand white-box conditions--and show that in realistic attack scenarios an\nattacker is not able to extract meaningful information. Overall, we demonstrate\nthat prompt obfuscation is an effective mechanism to safeguard the intellectual\nproperty of a system prompt while maintaining the same utility as the original\nprompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Sina Mavali"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schönherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schönherr"
                },
                "author": "Lea Schönherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17763v1",
                "updated": "2025-01-29T16:54:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    54,
                    47,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:54:47Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    54,
                    47,
                    2,
                    29,
                    0
                ],
                "title": "Bayesian Inference of the Critical Endpoint in 2+1-Flavor System from\n  Holographic QCD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference of the Critical Endpoint in 2+1-Flavor System from\n  Holographic QCD"
                },
                "summary": "We present a Bayesian holographic model constructed by integrating the\nequation of state and baryon number susceptibility at zero chemical potential\nfrom lattice QCD. The model incorporates error estimates derived from lattice\ndata. With this model, we systematically investigate the thermodynamic\nproperties of the 2+1-flavor QCD system. Using Bayesian Inference, we perform\nprecise calibration of the model parameters and determined the critical\nendpoint (CEP) position under the maximum a posterior (MAP) estimation to be\n$(T^{c},\\mu_{B}^{c})=(0.0859\\;\\mathrm{GeV},0.742\\;\\mathrm{GeV})$. Additionally,\nwe predict the CEP positions within 68\\% and 95\\% confidence levels, yielding\n$(T^{c},\\; \\mu_{B}^{c})_{68\\%}$=$(0.0820\\text{-}0.0889,\n0.71\\text{-}0.77)\\;\\mathrm{GeV}$ and $(T^{c},\\;\n\\mu_{B}^{c})_{95\\%}$=$(0.0816\\text{-}0.0898,\\;\n0.71\\text{-}0.79)\\;\\mathrm{GeV}$, respectively. Moreover, to validate the\nreliability and predictive power of our approach, we conduct a comprehensive\ncomparison between our predictions and potential CEP locations proposed by\nother theoretical models. This work not only establishes a novel Bayesian\nframework for holographic modeling but also provides valuable insights and\ntheoretical support for exploring phase transitions in strongly-interacting\nmatter under extreme conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Bayesian holographic model constructed by integrating the\nequation of state and baryon number susceptibility at zero chemical potential\nfrom lattice QCD. The model incorporates error estimates derived from lattice\ndata. With this model, we systematically investigate the thermodynamic\nproperties of the 2+1-flavor QCD system. Using Bayesian Inference, we perform\nprecise calibration of the model parameters and determined the critical\nendpoint (CEP) position under the maximum a posterior (MAP) estimation to be\n$(T^{c},\\mu_{B}^{c})=(0.0859\\;\\mathrm{GeV},0.742\\;\\mathrm{GeV})$. Additionally,\nwe predict the CEP positions within 68\\% and 95\\% confidence levels, yielding\n$(T^{c},\\; \\mu_{B}^{c})_{68\\%}$=$(0.0820\\text{-}0.0889,\n0.71\\text{-}0.77)\\;\\mathrm{GeV}$ and $(T^{c},\\;\n\\mu_{B}^{c})_{95\\%}$=$(0.0816\\text{-}0.0898,\\;\n0.71\\text{-}0.79)\\;\\mathrm{GeV}$, respectively. Moreover, to validate the\nreliability and predictive power of our approach, we conduct a comprehensive\ncomparison between our predictions and potential CEP locations proposed by\nother theoretical models. This work not only establishes a novel Bayesian\nframework for holographic modeling but also provides valuable insights and\ntheoretical support for exploring phase transitions in strongly-interacting\nmatter under extreme conditions."
                },
                "authors": [
                    {
                        "name": "Liqiang Zhu"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Hanzhong Zhang"
                    },
                    {
                        "name": "Mei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Mei Huang"
                },
                "author": "Mei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05247v2",
                "updated": "2025-01-29T16:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    52,
                    16,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-09T13:57:09Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "title": "Online Prompt Selection for Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Prompt Selection for Program Synthesis"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2% more queries than the best single solver and\nachieves results within 4% of the virtual best solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2% more queries than the best single solver and\nachieves results within 4% of the virtual best solver."
                },
                "authors": [
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Lewis Frampton"
                    },
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth Polgreen"
                },
                "author": "Elizabeth Polgreen",
                "arxiv_comment": "Accepted at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17752v1",
                "updated": "2025-01-29T16:41:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    41,
                    15,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:41:15Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    41,
                    15,
                    2,
                    29,
                    0
                ],
                "title": "On the Partitioning of GPU Power among Multi-Instances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Partitioning of GPU Power among Multi-Instances"
                },
                "summary": "Efficient power management in cloud data centers is essential for reducing\ncosts, enhancing performance, and minimizing environmental impact. GPUs,\ncritical for tasks like machine learning (ML) and GenAI, are major contributors\nto power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU\nutilization by enabling isolated partitions with per-partition resource\ntracking, facilitating GPU sharing by multiple tenants. However, accurately\napportioning GPU power consumption among MIG instances remains challenging due\nto a lack of hardware support. This paper addresses this challenge by\ndeveloping software methods to estimate power usage per MIG partition. We\nanalyze NVIDIA GPU utilization metrics and find that light-weight methods with\ngood accuracy can be difficult to construct. We hence explore the use of\nML-based power models to enable accurate, partition-level power estimation. Our\nfindings reveal that a single generic offline power model or modeling method is\nnot applicable across diverse workloads, especially with concurrent MIG usage,\nand that online models constructed using partition-level utilization metrics of\nworkloads under execution can significantly improve accuracy. Using NVIDIA A100\nGPUs, we demonstrate this approach for accurate partition-level power\nestimation for workloads including matrix multiplication and Large Language\nModel inference, contributing to transparent and fair carbon reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient power management in cloud data centers is essential for reducing\ncosts, enhancing performance, and minimizing environmental impact. GPUs,\ncritical for tasks like machine learning (ML) and GenAI, are major contributors\nto power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU\nutilization by enabling isolated partitions with per-partition resource\ntracking, facilitating GPU sharing by multiple tenants. However, accurately\napportioning GPU power consumption among MIG instances remains challenging due\nto a lack of hardware support. This paper addresses this challenge by\ndeveloping software methods to estimate power usage per MIG partition. We\nanalyze NVIDIA GPU utilization metrics and find that light-weight methods with\ngood accuracy can be difficult to construct. We hence explore the use of\nML-based power models to enable accurate, partition-level power estimation. Our\nfindings reveal that a single generic offline power model or modeling method is\nnot applicable across diverse workloads, especially with concurrent MIG usage,\nand that online models constructed using partition-level utilization metrics of\nworkloads under execution can significantly improve accuracy. Using NVIDIA A100\nGPUs, we demonstrate this approach for accurate partition-level power\nestimation for workloads including matrix multiplication and Large Language\nModel inference, contributing to transparent and fair carbon reporting."
                },
                "authors": [
                    {
                        "name": "Tirth Vamja"
                    },
                    {
                        "name": "Kaustabha Ray"
                    },
                    {
                        "name": "Felix George"
                    },
                    {
                        "name": "UmaMaheswari C Devi"
                    }
                ],
                "author_detail": {
                    "name": "UmaMaheswari C Devi"
                },
                "author": "UmaMaheswari C Devi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17749v1",
                "updated": "2025-01-29T16:36:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    36,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:36:53Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    36,
                    53,
                    2,
                    29,
                    0
                ],
                "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the\n  Pre-Deployment Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early External Safety Testing of OpenAI's o3-mini: Insights from the\n  Pre-Deployment Evaluation"
                },
                "summary": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2501.17132",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12112v2",
                "updated": "2025-01-29T16:31:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    31,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-15T23:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    23,
                    20,
                    54,
                    1,
                    289,
                    0
                ],
                "title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming"
                },
                "summary": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "57 pages, 25 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07902v2",
                "updated": "2025-01-29T16:24:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    24,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2024-09-12T10:12:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    12,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints"
                },
                "summary": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "15 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12588v2",
                "updated": "2025-01-29T16:02:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    2,
                    14,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-22T17:54:21Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    54,
                    21,
                    3,
                    235,
                    0
                ],
                "title": "Real-Time Video Generation with Pyramid Attention Broadcast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Video Generation with Pyramid Attention Broadcast"
                },
                "summary": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates up to 10.5x speedup\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates up to 10.5x speedup\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation."
                },
                "authors": [
                    {
                        "name": "Xuanlei Zhao"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17725v1",
                "updated": "2025-01-29T15:57:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    57,
                    43,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:57:43Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    57,
                    43,
                    2,
                    29,
                    0
                ],
                "title": "Using Code Generation to Solve Open Instances of Combinatorial Design\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Code Generation to Solve Open Instances of Combinatorial Design\n  Problems"
                },
                "summary": "The Handbook of Combinatorial Designs catalogs many types of combinatorial\ndesigns, together with lists of open instances for which existence has not yet\nbeen determined. We develop a constructive protocol CPro1, which uses Large\nLanguage Models (LLMs) to generate code that constructs combinatorial designs\nand resolves some of these open instances. The protocol starts from a\ndefinition of a particular type of design, and a verifier that reliably\nconfirms whether a proposed design is valid. The LLM selects strategies and\nimplements them in code, and scaffolding provides automated hyperparameter\ntuning and execution feedback using the verifier. Most generated code fails,\nbut by generating many candidates, the protocol automates exploration of a\nvariety of standard methods (e.g. simulated annealing, genetic algorithms) and\nexperimentation with variations (e.g. cost functions) to find successful\napproaches. Testing on 16 different types of designs, CPro1 constructs\nsolutions to open instances for 6 of them: Symmetric and Skew Weighing\nMatrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary\nDesigns, and Florentine Rectangles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Handbook of Combinatorial Designs catalogs many types of combinatorial\ndesigns, together with lists of open instances for which existence has not yet\nbeen determined. We develop a constructive protocol CPro1, which uses Large\nLanguage Models (LLMs) to generate code that constructs combinatorial designs\nand resolves some of these open instances. The protocol starts from a\ndefinition of a particular type of design, and a verifier that reliably\nconfirms whether a proposed design is valid. The LLM selects strategies and\nimplements them in code, and scaffolding provides automated hyperparameter\ntuning and execution feedback using the verifier. Most generated code fails,\nbut by generating many candidates, the protocol automates exploration of a\nvariety of standard methods (e.g. simulated annealing, genetic algorithms) and\nexperimentation with variations (e.g. cost functions) to find successful\napproaches. Testing on 16 different types of designs, CPro1 constructs\nsolutions to open instances for 6 of them: Symmetric and Skew Weighing\nMatrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary\nDesigns, and Florentine Rectangles."
                },
                "authors": [
                    {
                        "name": "Christopher D. Rosin"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Rosin"
                },
                "author": "Christopher D. Rosin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17722v1",
                "updated": "2025-01-29T15:47:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    47,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:47:45Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    47,
                    45,
                    2,
                    29,
                    0
                ],
                "title": "Fundamentals of non-parametric statistical inference for integrated\n  quantiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of non-parametric statistical inference for integrated\n  quantiles"
                },
                "summary": "We present a general non-parametric statistical inference theory for\nintegrals of quantiles without assuming any specific sampling design or\ndependence structure. Technical considerations are accompanied by examples and\ndiscussions, including those pertaining to the bias of empirical estimators. To\nillustrate how the general results can be adapted to specific situations, we\nderive - at a stroke and under minimal conditions - consistency and asymptotic\nnormality of the empirical tail-value-at-risk, Lorenz and Gini curves at any\nprobability level in the case of the simple random sampling, thus facilitating\na comparison of our results with what is already known in the literature. Notes\nand references concerning underlying technicalities in the case of dependent\n(i.e., time series) data are offered. As a by-product, our general results\nprovide new and unified proofs of large-sample properties of a number of\nclassical statistical estimators, such as trimmed means, and give additional\ninsights into the origins of, and the reasons for, various necessary and\nsufficient conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a general non-parametric statistical inference theory for\nintegrals of quantiles without assuming any specific sampling design or\ndependence structure. Technical considerations are accompanied by examples and\ndiscussions, including those pertaining to the bias of empirical estimators. To\nillustrate how the general results can be adapted to specific situations, we\nderive - at a stroke and under minimal conditions - consistency and asymptotic\nnormality of the empirical tail-value-at-risk, Lorenz and Gini curves at any\nprobability level in the case of the simple random sampling, thus facilitating\na comparison of our results with what is already known in the literature. Notes\nand references concerning underlying technicalities in the case of dependent\n(i.e., time series) data are offered. As a by-product, our general results\nprovide new and unified proofs of large-sample properties of a number of\nclassical statistical estimators, such as trimmed means, and give additional\ninsights into the origins of, and the reasons for, various necessary and\nsufficient conditions."
                },
                "authors": [
                    {
                        "name": "Nadezhda Gribkova"
                    },
                    {
                        "name": "Mengqi Wang"
                    },
                    {
                        "name": "Ričardas Zitikis"
                    }
                ],
                "author_detail": {
                    "name": "Ričardas Zitikis"
                },
                "author": "Ričardas Zitikis",
                "arxiv_comment": "66 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03646v2",
                "updated": "2025-01-29T15:43:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    43,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2024-03-06T12:10:48Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    12,
                    10,
                    48,
                    2,
                    66,
                    0
                ],
                "title": "Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary\n  Quantile and Count Data Regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary\n  Quantile and Count Data Regressions"
                },
                "summary": "Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS\nhave been used for many decades in econometrics and more recently to\ninvestigate how poor air quality adversely affects human health. In this paper\nwe describe how to expand the utility of these models for Bayesian inference by\nleveraging latent variables. In particular we explain how to perform binary\nregression to better handle imbalanced data, how to incorporate negative\nbinomial regression, and how to estimate the probability of predictor\ninclusion. Extra parameters introduced through the DLM framework may require\ncalibration for the MCMC algorithm, but this will not be the case in DLM-based\nanalyses often seen in pollution exposure literature. In these cases, the\nparameters are inferred through a fully automatic Gibbs sampling procedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS\nhave been used for many decades in econometrics and more recently to\ninvestigate how poor air quality adversely affects human health. In this paper\nwe describe how to expand the utility of these models for Bayesian inference by\nleveraging latent variables. In particular we explain how to perform binary\nregression to better handle imbalanced data, how to incorporate negative\nbinomial regression, and how to estimate the probability of predictor\ninclusion. Extra parameters introduced through the DLM framework may require\ncalibration for the MCMC algorithm, but this will not be the case in DLM-based\nanalyses often seen in pollution exposure literature. In these cases, the\nparameters are inferred through a fully automatic Gibbs sampling procedure."
                },
                "authors": [
                    {
                        "name": "Daniel Dempsey"
                    },
                    {
                        "name": "Jason Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Jason Wyse"
                },
                "author": "Jason Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08561v3",
                "updated": "2025-01-29T15:41:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    41,
                    55,
                    2,
                    29,
                    0
                ],
                "published": "2024-11-13T12:18:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    18,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogLLM: Log-based Anomaly Detection Using Large Language Models"
                },
                "summary": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately."
                },
                "authors": [
                    {
                        "name": "Wei Guan"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Jianqi Gao"
                    },
                    {
                        "name": "Chun Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Ouyang"
                },
                "author": "Chun Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17717v1",
                "updated": "2025-01-29T15:36:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    36,
                    48,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:36:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    36,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "A Two-Step Procedure to Detect Cosmological Gravitational Wave\n  Backgrounds with Next-Generation Terrestrial Gravitational-Wave Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Step Procedure to Detect Cosmological Gravitational Wave\n  Backgrounds with Next-Generation Terrestrial Gravitational-Wave Detectors"
                },
                "summary": "Cosmological gravitational-wave backgrounds are an exciting science target\nfor next-generation ground-based detectors, as they encode invaluable\ninformation about the primordial Universe. However, any such background is\nexpected to be obscured by the astrophysical foreground from compact-binary\ncoalescences. We propose a novel framework to detect a cosmological\ngravitational-wave background in the presence of binary black holes and binary\nneutron star signals with next-generation ground-based detectors, including\nCosmic Explorer and the Einstein Telescope. Our procedure involves first\nremoving all the individually resolved binary black hole signals by notching\nthem out in the time-frequency domain. Then, we perform joint Bayesian\ninference on the individually resolved binary neutron star signals, the\nunresolved binary neutron star foreground, and the cosmological background. For\na flat cosmological background, we find that we can claim detection at\n$5\\,\\sigma$ level when $\\Omega_\\mathrm{ref}\\geqslant 2.7\\times\n10^{-12}/\\sqrt{T_\\mathrm{obs}/\\mathrm{yr}}$, where $T_\\mathrm{obs}$ is the\nobservation time (in years), which is within a factor of $\\lesssim2$ from the\nsensitivity reached in absence of these astrophysical foregrounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological gravitational-wave backgrounds are an exciting science target\nfor next-generation ground-based detectors, as they encode invaluable\ninformation about the primordial Universe. However, any such background is\nexpected to be obscured by the astrophysical foreground from compact-binary\ncoalescences. We propose a novel framework to detect a cosmological\ngravitational-wave background in the presence of binary black holes and binary\nneutron star signals with next-generation ground-based detectors, including\nCosmic Explorer and the Einstein Telescope. Our procedure involves first\nremoving all the individually resolved binary black hole signals by notching\nthem out in the time-frequency domain. Then, we perform joint Bayesian\ninference on the individually resolved binary neutron star signals, the\nunresolved binary neutron star foreground, and the cosmological background. For\na flat cosmological background, we find that we can claim detection at\n$5\\,\\sigma$ level when $\\Omega_\\mathrm{ref}\\geqslant 2.7\\times\n10^{-12}/\\sqrt{T_\\mathrm{obs}/\\mathrm{yr}}$, where $T_\\mathrm{obs}$ is the\nobservation time (in years), which is within a factor of $\\lesssim2$ from the\nsensitivity reached in absence of these astrophysical foregrounds."
                },
                "authors": [
                    {
                        "name": "Haowen Zhong"
                    },
                    {
                        "name": "Luca Reali"
                    },
                    {
                        "name": "Bei Zhou"
                    },
                    {
                        "name": "Emanuele Berti"
                    },
                    {
                        "name": "Vuk Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Vuk Mandic"
                },
                "author": "Vuk Mandic",
                "arxiv_comment": "Main text: 7 pages, 3 figures, 1 table. Supplemental material: 12\n  pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10996v3",
                "updated": "2025-01-29T15:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    34,
                    2,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-16T16:17:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    17,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Towards Lifelong Dialogue Agents via Timeline-based Memory Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lifelong Dialogue Agents via Timeline-based Memory Management"
                },
                "summary": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior studies focus on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present THEANINE, a framework for LLM-based lifelong dialogue\nagents. THEANINE discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, THEANINE augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts when assessing agent\nperformance in integrating past memories into RG. A supplementary video for\nTHEANINE and data for TeaFarm are at\nhttps://huggingface.co/spaces/ResearcherScholar/Theanine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior studies focus on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present THEANINE, a framework for LLM-based lifelong dialogue\nagents. THEANINE discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, THEANINE augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts when assessing agent\nperformance in integrating past memories into RG. A supplementary video for\nTHEANINE and data for TeaFarm are at\nhttps://huggingface.co/spaces/ResearcherScholar/Theanine."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Namyoung Kim"
                    },
                    {
                        "name": "Minju Gwak"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17715v1",
                "updated": "2025-01-29T15:32:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    32,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:32:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    32,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts"
                },
                "summary": "User interactions with conversational agents (CAs) evolve in the era of\nheavily guardrailed large language models (LLMs). As users push beyond\nprogrammed boundaries to explore and build relationships with these systems,\nthere is a growing concern regarding the potential for unauthorized access or\nmanipulation, commonly referred to as \"jailbreaking.\" Moreover, with CAs that\npossess highly human-like qualities, users show a tendency toward initiating\nintimate sexual interactions or attempting to tame their chatbots. To capture\nand reflect these in-the-wild interactions into chatbot designs, we propose\nRICoTA, a Korean red teaming dataset that consists of 609 prompts challenging\nLLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We\nutilize user-chatbot conversations that were self-posted on a Korean\nReddit-like community, containing specific testing and gaming intentions with a\nsocial chatbot. With these prompts, we aim to evaluate LLMs' ability to\nidentify the type of conversation and users' testing purposes to derive chatbot\ndesign implications for mitigating jailbreaking risks. Our dataset will be made\npublicly available via GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interactions with conversational agents (CAs) evolve in the era of\nheavily guardrailed large language models (LLMs). As users push beyond\nprogrammed boundaries to explore and build relationships with these systems,\nthere is a growing concern regarding the potential for unauthorized access or\nmanipulation, commonly referred to as \"jailbreaking.\" Moreover, with CAs that\npossess highly human-like qualities, users show a tendency toward initiating\nintimate sexual interactions or attempting to tame their chatbots. To capture\nand reflect these in-the-wild interactions into chatbot designs, we propose\nRICoTA, a Korean red teaming dataset that consists of 609 prompts challenging\nLLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We\nutilize user-chatbot conversations that were self-posted on a Korean\nReddit-like community, containing specific testing and gaming intentions with a\nsocial chatbot. With these prompts, we aim to evaluate LLMs' ability to\nidentify the type of conversation and users' testing purposes to derive chatbot\ndesign implications for mitigating jailbreaking risks. Our dataset will be made\npublicly available via GitHub."
                },
                "authors": [
                    {
                        "name": "Eujeong Choi"
                    },
                    {
                        "name": "Younghun Jeong"
                    },
                    {
                        "name": "Soomin Kim"
                    },
                    {
                        "name": "Won Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Won Ik Cho"
                },
                "author": "Won Ik Cho",
                "arxiv_comment": "PACLIC 38",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17711v1",
                "updated": "2025-01-29T15:28:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    28,
                    6,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:28:06Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    28,
                    6,
                    2,
                    29,
                    0
                ],
                "title": "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization"
                },
                "summary": "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts."
                },
                "authors": [
                    {
                        "name": "Yiquan Wang"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Zihao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Xu"
                },
                "author": "Zihao Xu",
                "arxiv_comment": "18pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17704v1",
                "updated": "2025-01-29T15:20:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    20,
                    43,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:20:43Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    20,
                    43,
                    2,
                    29,
                    0
                ],
                "title": "Inferring Implicit Goals Across Differing Task Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Implicit Goals Across Differing Task Models"
                },
                "summary": "One of the significant challenges to generating value-aligned behavior is to\nnot only account for the specified user objectives but also any implicit or\nunspecified user requirements. The existence of such implicit requirements\ncould be particularly common in settings where the user's understanding of the\ntask model may differ from the agent's estimate of the model. Under this\nscenario, the user may incorrectly expect some agent behavior to be inevitable\nor guaranteed. This paper addresses such expectation mismatch in the presence\nof differing models by capturing the possibility of unspecified user subgoal in\nthe context of a task captured as a Markov Decision Process (MDP) and querying\nfor it as required. Our method identifies bottleneck states and uses them as\ncandidates for potential implicit subgoals. We then introduce a querying\nstrategy that will generate the minimal number of queries required to identify\na policy guaranteed to achieve the underlying goal. Our empirical evaluations\ndemonstrate the effectiveness of our approach in inferring and achieving\nunstated goals across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the significant challenges to generating value-aligned behavior is to\nnot only account for the specified user objectives but also any implicit or\nunspecified user requirements. The existence of such implicit requirements\ncould be particularly common in settings where the user's understanding of the\ntask model may differ from the agent's estimate of the model. Under this\nscenario, the user may incorrectly expect some agent behavior to be inevitable\nor guaranteed. This paper addresses such expectation mismatch in the presence\nof differing models by capturing the possibility of unspecified user subgoal in\nthe context of a task captured as a Markov Decision Process (MDP) and querying\nfor it as required. Our method identifies bottleneck states and uses them as\ncandidates for potential implicit subgoals. We then introduce a querying\nstrategy that will generate the minimal number of queries required to identify\na policy guaranteed to achieve the underlying goal. Our empirical evaluations\ndemonstrate the effectiveness of our approach in inferring and achieving\nunstated goals across various tasks."
                },
                "authors": [
                    {
                        "name": "Silvia Tulli"
                    },
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Sarath Sreedharan"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Sreedharan"
                },
                "author": "Sarath Sreedharan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2105.07424v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2105.07424v4",
                "updated": "2025-01-29T14:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    59,
                    58,
                    2,
                    29,
                    0
                ],
                "published": "2021-05-16T12:52:18Z",
                "published_parsed": [
                    2021,
                    5,
                    16,
                    12,
                    52,
                    18,
                    6,
                    136,
                    0
                ],
                "title": "Uniform Inference on High-dimensional Spatial Panel Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Inference on High-dimensional Spatial Panel Networks"
                },
                "summary": "We propose employing a debiased-regularized, high-dimensional generalized\nmethod of moments (GMM) framework to perform inference on large-scale spatial\npanel networks. In particular, network structure with a flexible sparse\ndeviation, which can be regarded either as latent or as misspecified from a\npredetermined adjacency matrix, is estimated using debiased machine learning\napproach. The theoretical analysis establishes the consistency and asymptotic\nnormality of our proposed estimator, taking into account general temporal and\nspatial dependency inherent in the data-generating processes. A primary\ncontribution of our study is the development of uniform inference theory that\nenables hypothesis testing on the parameters of interest, including zero or\nnon-zero elements in the network structure. Additionally, the asymptotic\nproperties for the estimator are derived for both linear and nonlinear moments.\nSimulations demonstrate superior performance of our proposed approach. Lastly,\nwe apply our methodology to investigate the spatial network effect of stock\nreturns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose employing a debiased-regularized, high-dimensional generalized\nmethod of moments (GMM) framework to perform inference on large-scale spatial\npanel networks. In particular, network structure with a flexible sparse\ndeviation, which can be regarded either as latent or as misspecified from a\npredetermined adjacency matrix, is estimated using debiased machine learning\napproach. The theoretical analysis establishes the consistency and asymptotic\nnormality of our proposed estimator, taking into account general temporal and\nspatial dependency inherent in the data-generating processes. A primary\ncontribution of our study is the development of uniform inference theory that\nenables hypothesis testing on the parameters of interest, including zero or\nnon-zero elements in the network structure. Additionally, the asymptotic\nproperties for the estimator are derived for both linear and nonlinear moments.\nSimulations demonstrate superior performance of our proposed approach. Lastly,\nwe apply our methodology to investigate the spatial network effect of stock\nreturns."
                },
                "authors": [
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Weining Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weining Wang"
                },
                "author": "Weining Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2105.07424v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2105.07424v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17688v1",
                "updated": "2025-01-29T14:56:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    56,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T14:56:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    56,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation\n  Transformer"
                },
                "summary": "This paper presents Contourformer, a real-time contour-based instance\nsegmentation algorithm. The method is fully based on the DETR paradigm and\nachieves end-to-end inference through iterative and progressive mechanisms to\noptimize contours. To improve efficiency and accuracy, we develop two novel\ntechniques: sub-contour decoupling mechanisms and contour fine-grained\ndistribution refinement.In the sub-contour decoupling mechanism, we propose a\ndeformable attention-based module that adaptively selects sampling regions\nbased on the current predicted contour, enabling more effective capturing of\nobject boundary information. Additionally, we design a multi-stage optimization\nprocess to enhance segmentation precision by progressively refining\nsub-contours. The contour fine-grained distribution refinement technique aims\nto further improve the ability to express fine details of contours.These\ninnovations enable Contourformer to achieve stable and precise segmentation for\neach instance while maintaining real-time performance. Extensive experiments\ndemonstrate the superior performance of Contourformer on multiple benchmark\ndatasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations\nand comparisons with existing state-of-the-art methods, showing significant\nimprovements in both accuracy and inference speed.This work provides a new\nsolution for contour-based instance segmentation tasks and lays a foundation\nfor future research, with the potential to become a strong baseline method in\nthis field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Contourformer, a real-time contour-based instance\nsegmentation algorithm. The method is fully based on the DETR paradigm and\nachieves end-to-end inference through iterative and progressive mechanisms to\noptimize contours. To improve efficiency and accuracy, we develop two novel\ntechniques: sub-contour decoupling mechanisms and contour fine-grained\ndistribution refinement.In the sub-contour decoupling mechanism, we propose a\ndeformable attention-based module that adaptively selects sampling regions\nbased on the current predicted contour, enabling more effective capturing of\nobject boundary information. Additionally, we design a multi-stage optimization\nprocess to enhance segmentation precision by progressively refining\nsub-contours. The contour fine-grained distribution refinement technique aims\nto further improve the ability to express fine details of contours.These\ninnovations enable Contourformer to achieve stable and precise segmentation for\neach instance while maintaining real-time performance. Extensive experiments\ndemonstrate the superior performance of Contourformer on multiple benchmark\ndatasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations\nand comparisons with existing state-of-the-art methods, showing significant\nimprovements in both accuracy and inference speed.This work provides a new\nsolution for contour-based instance segmentation tasks and lays a foundation\nfor future research, with the potential to become a strong baseline method in\nthis field."
                },
                "authors": [
                    {
                        "name": "Weiwei yao"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Minjun Xiong"
                    },
                    {
                        "name": "Wenbo Dong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xiong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiong Xiao"
                },
                "author": "Xiong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04482v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04482v4",
                "updated": "2025-01-29T14:55:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    55,
                    28,
                    2,
                    29,
                    0
                ],
                "published": "2024-01-09T10:39:17Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    10,
                    39,
                    17,
                    1,
                    9,
                    0
                ],
                "title": "Continuously Learning New Words in Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously Learning New Words in Automatic Speech Recognition"
                },
                "summary": "Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model."
                },
                "authors": [
                    {
                        "name": "Christian Huber"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04482v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04482v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09580v2",
                "updated": "2025-01-29T14:41:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    41,
                    51,
                    2,
                    29,
                    0
                ],
                "published": "2024-03-14T17:14:53Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    14,
                    53,
                    3,
                    74,
                    0
                ],
                "title": "Algorithmic syntactic causal identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic syntactic causal identification"
                },
                "summary": "Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model."
                },
                "authors": [
                    {
                        "name": "Dhurim Cakiqi"
                    },
                    {
                        "name": "Max A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max A. Little"
                },
                "author": "Max A. Little",
                "arxiv_comment": "11 pages, 2 TikZ figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18311v2",
                "updated": "2025-01-29T14:21:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    21,
                    13,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-28T16:02:11Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    16,
                    2,
                    11,
                    1,
                    149,
                    0
                ],
                "title": "Deterministic and statistical calibration of constitutive models from\n  full-field data with parametric physics-informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic and statistical calibration of constitutive models from\n  full-field data with parametric physics-informed neural networks"
                },
                "summary": "The calibration of constitutive models from full-field data has recently\ngained increasing interest due to improvements in full-field measurement\ncapabilities. In addition to the experimental characterization of novel\nmaterials, continuous structural health monitoring is another application that\nis of great interest. However, monitoring is usually associated with severe\ntime constraints, difficult to meet with standard numerical approaches.\nTherefore, parametric physics-informed neural networks (PINNs) for constitutive\nmodel calibration from full-field displacement data are investigated. In an\noffline stage, a parametric PINN can be trained to learn a parameterized\nsolution of the underlying partial differential equation. In the subsequent\nonline stage, the parametric PINN then acts as a surrogate for the\nparameters-to-state map in calibration. We test the proposed approach for the\ndeterministic least-squares calibration of a linear elastic as well as a\nhyperelastic constitutive model from noisy synthetic displacement data. We\nfurther carry out Markov chain Monte Carlo-based Bayesian inference to quantify\nthe uncertainty. A proper statistical evaluation of the results underlines the\nhigh accuracy of the deterministic calibration and that the estimated\nuncertainty is valid. Finally, we consider experimental data and show that the\nresults are in good agreement with a finite element method-based calibration.\nDue to the fast evaluation of PINNs, calibration can be performed in near\nreal-time. This advantage is particularly evident in many-query applications\nsuch as Markov chain Monte Carlo-based Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The calibration of constitutive models from full-field data has recently\ngained increasing interest due to improvements in full-field measurement\ncapabilities. In addition to the experimental characterization of novel\nmaterials, continuous structural health monitoring is another application that\nis of great interest. However, monitoring is usually associated with severe\ntime constraints, difficult to meet with standard numerical approaches.\nTherefore, parametric physics-informed neural networks (PINNs) for constitutive\nmodel calibration from full-field displacement data are investigated. In an\noffline stage, a parametric PINN can be trained to learn a parameterized\nsolution of the underlying partial differential equation. In the subsequent\nonline stage, the parametric PINN then acts as a surrogate for the\nparameters-to-state map in calibration. We test the proposed approach for the\ndeterministic least-squares calibration of a linear elastic as well as a\nhyperelastic constitutive model from noisy synthetic displacement data. We\nfurther carry out Markov chain Monte Carlo-based Bayesian inference to quantify\nthe uncertainty. A proper statistical evaluation of the results underlines the\nhigh accuracy of the deterministic calibration and that the estimated\nuncertainty is valid. Finally, we consider experimental data and show that the\nresults are in good agreement with a finite element method-based calibration.\nDue to the fast evaluation of PINNs, calibration can be performed in near\nreal-time. This advantage is particularly evident in many-query applications\nsuch as Markov chain Monte Carlo-based Bayesian inference."
                },
                "authors": [
                    {
                        "name": "David Anton"
                    },
                    {
                        "name": "Jendrik-Alexander Tröger"
                    },
                    {
                        "name": "Henning Wessels"
                    },
                    {
                        "name": "Ulrich Römer"
                    },
                    {
                        "name": "Alexander Henkes"
                    },
                    {
                        "name": "Stefan Hartmann"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Hartmann"
                },
                "author": "Stefan Hartmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17665v1",
                "updated": "2025-01-29T14:04:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    4,
                    54,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T14:04:54Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    4,
                    54,
                    2,
                    29,
                    0
                ],
                "title": "Planning with Vision-Language Models and a Use Case in Robot-Assisted\n  Teaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with Vision-Language Models and a Use Case in Robot-Assisted\n  Teaching"
                },
                "summary": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder."
                },
                "authors": [
                    {
                        "name": "Xuzhe Dang"
                    },
                    {
                        "name": "Lada Kudláčková"
                    },
                    {
                        "name": "Stefan Edelkamp"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Edelkamp"
                },
                "author": "Stefan Edelkamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08701v3",
                "updated": "2025-01-29T13:52:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    52,
                    46,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-11T19:00:00Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    19,
                    0,
                    0,
                    2,
                    346,
                    0
                ],
                "title": "GalSBI: Phenomenological galaxy population model for cosmology using\n  simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GalSBI: Phenomenological galaxy population model for cosmology using\n  simulation-based inference"
                },
                "summary": "We present GalSBI, a phenomenological model of the galaxy population for\ncosmological applications using simulation-based inference. The model is based\non analytical parametrizations of galaxy luminosity functions, morphologies and\nspectral energy distributions. Model constraints are derived through iterative\nApproximate Bayesian Computation, by comparing Hyper Suprime-Cam deep field\nimages with simulations which include a forward model of instrumental,\nobservational and source extraction effects. We developed an emulator trained\non image simulations using a normalizing flow. We use it to accelerate the\ninference by predicting detection probabilities, including blending effects and\nphotometric properties of each object, while accounting for background and PSF\nvariations. This enables robustness tests for all elements of the forward model\nand the inference. The model demonstrates excellent performance when comparing\nphotometric properties from simulations with observed imaging data for key\nparameters such as magnitudes, colors and sizes. The redshift distribution of\nsimulated galaxies agrees well with high-precision photometric redshifts in the\nCOSMOS field within $1.5\\sigma$ for all magnitude cuts. Additionally, we\ndemonstrate how GalSBI's redshifts can be utilized for splitting galaxy\ncatalogs into tomographic bins, highlighting its potential for current and\nupcoming surveys. GalSBI is fully open-source, with the accompanying Python\npackage, $\\texttt{galsbi}$, offering an easy interface to quickly generate\nrealistic, survey-independent galaxy catalogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GalSBI, a phenomenological model of the galaxy population for\ncosmological applications using simulation-based inference. The model is based\non analytical parametrizations of galaxy luminosity functions, morphologies and\nspectral energy distributions. Model constraints are derived through iterative\nApproximate Bayesian Computation, by comparing Hyper Suprime-Cam deep field\nimages with simulations which include a forward model of instrumental,\nobservational and source extraction effects. We developed an emulator trained\non image simulations using a normalizing flow. We use it to accelerate the\ninference by predicting detection probabilities, including blending effects and\nphotometric properties of each object, while accounting for background and PSF\nvariations. This enables robustness tests for all elements of the forward model\nand the inference. The model demonstrates excellent performance when comparing\nphotometric properties from simulations with observed imaging data for key\nparameters such as magnitudes, colors and sizes. The redshift distribution of\nsimulated galaxies agrees well with high-precision photometric redshifts in the\nCOSMOS field within $1.5\\sigma$ for all magnitude cuts. Additionally, we\ndemonstrate how GalSBI's redshifts can be utilized for splitting galaxy\ncatalogs into tomographic bins, highlighting its potential for current and\nupcoming surveys. GalSBI is fully open-source, with the accompanying Python\npackage, $\\texttt{galsbi}$, offering an easy interface to quickly generate\nrealistic, survey-independent galaxy catalogs."
                },
                "authors": [
                    {
                        "name": "Silvan Fischbacher"
                    },
                    {
                        "name": "Tomasz Kacprzak"
                    },
                    {
                        "name": "Luca Tortorelli"
                    },
                    {
                        "name": "Beatrice Moser"
                    },
                    {
                        "name": "Alexandre Refregier"
                    },
                    {
                        "name": "Patrick Gebhardt"
                    },
                    {
                        "name": "Daniel Gruen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gruen"
                },
                "author": "Daniel Gruen",
                "arxiv_comment": "43 pages, 14 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v3",
                "updated": "2025-01-29T13:52:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    52,
                    31,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ketu Ndawula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02301v2",
                "updated": "2025-01-29T13:27:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    27,
                    37,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-05T08:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    8,
                    23,
                    59,
                    0,
                    218,
                    0
                ],
                "title": "Network Fission Ensembles for Low-Cost Self-Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Fission Ensembles for Low-Cost Self-Ensembles"
                },
                "summary": "Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE."
                },
                "authors": [
                    {
                        "name": "Hojung Lee"
                    },
                    {
                        "name": "Jong-Seok Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jong-Seok Lee"
                },
                "author": "Jong-Seok Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16287v2",
                "updated": "2025-01-29T13:22:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    22,
                    21,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-27T18:23:59Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    23,
                    59,
                    0,
                    27,
                    0
                ],
                "title": "A Unified Representation of Density-Power-Based Divergences Reducible to\n  M-Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation of Density-Power-Based Divergences Reducible to\n  M-Estimation"
                },
                "summary": "Density-power-based divergences are known to provide robust inference\nprocedures against outliers, and their extensions have been widely studied. A\ncharacteristic of successful divergences is that the estimation problem can be\nreduced to M-estimation. In this paper, we define a norm-based Bregman density\npower divergence (NB-DPD) -- density-power-based divergence with functional\nflexibility within the framework of Bregman divergences that can be reduced to\nM-estimation. We show that, by specifying the function $\\phi_\\gamma$, NB-DPD\nreduces to well-known divergences, such as the density power divergence and the\n$\\gamma$-divergence. Furthermore, by examining the combinations of functions\n$\\phi_\\gamma$ corresponding to existing divergences, we show that a new\ndivergence connecting these existing divergences can be derived. Finally, we\nshow that the redescending property, one of the key indicators of robustness,\nholds only for the $\\gamma$-divergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Density-power-based divergences are known to provide robust inference\nprocedures against outliers, and their extensions have been widely studied. A\ncharacteristic of successful divergences is that the estimation problem can be\nreduced to M-estimation. In this paper, we define a norm-based Bregman density\npower divergence (NB-DPD) -- density-power-based divergence with functional\nflexibility within the framework of Bregman divergences that can be reduced to\nM-estimation. We show that, by specifying the function $\\phi_\\gamma$, NB-DPD\nreduces to well-known divergences, such as the density power divergence and the\n$\\gamma$-divergence. Furthermore, by examining the combinations of functions\n$\\phi_\\gamma$ corresponding to existing divergences, we show that a new\ndivergence connecting these existing divergences can be derived. Finally, we\nshow that the redescending property, one of the key indicators of robustness,\nholds only for the $\\gamma$-divergence."
                },
                "authors": [
                    {
                        "name": "Masahiro Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Kobayashi"
                },
                "author": "Masahiro Kobayashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17635v1",
                "updated": "2025-01-29T13:12:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:12:01Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "title": "In-Context Meta LoRA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Meta LoRA Generation"
                },
                "summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jingcai Guo"
                },
                "author": "Jingcai Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17629v1",
                "updated": "2025-01-29T13:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "title": "The Imitation Game According To Turing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Imitation Game According To Turing"
                },
                "summary": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines."
                },
                "authors": [
                    {
                        "name": "Sharon Temtsin"
                    },
                    {
                        "name": "Diane Proudfoot"
                    },
                    {
                        "name": "David Kaber"
                    },
                    {
                        "name": "Christoph Bartneck"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Bartneck"
                },
                "arxiv_affiliation": "The University of Canterbury",
                "author": "Christoph Bartneck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17630v1",
                "updated": "2025-01-29T13:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "title": "Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation"
                },
                "summary": "Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025"
                },
                "authors": [
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Sanghwan Jang"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_doi": "10.1145/3696410.3714601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v5",
                "updated": "2025-01-29T12:52:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    52,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17617v1",
                "updated": "2025-01-29T12:46:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    46,
                    42,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:46:42Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    46,
                    42,
                    2,
                    29,
                    0
                ],
                "title": "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment"
                },
                "summary": "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications."
                },
                "authors": [
                    {
                        "name": "Jonathan Teel"
                    },
                    {
                        "name": "Jocasta Cumberbatch"
                    },
                    {
                        "name": "Raphael Benington"
                    },
                    {
                        "name": "Quentin Baskerville"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Baskerville"
                },
                "author": "Quentin Baskerville",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03664v3",
                "updated": "2025-01-29T12:36:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    36,
                    0,
                    2,
                    29,
                    0
                ],
                "published": "2024-02-16T10:56:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    56,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine"
                },
                "summary": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results."
                },
                "authors": [
                    {
                        "name": "Erblin Isaku"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Thomas Schwitalla"
                    },
                    {
                        "name": "Jan F. Nygård"
                    }
                ],
                "author_detail": {
                    "name": "Jan F. Nygård"
                },
                "author": "Jan F. Nygård",
                "arxiv_comment": "12 pages, 6 figures, 4 tables, 1 listing, revised arguments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17612v1",
                "updated": "2025-01-29T12:34:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    34,
                    58,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:34:58Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    34,
                    58,
                    2,
                    29,
                    0
                ],
                "title": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and\n  Conditional Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and\n  Conditional Flow Matching"
                },
                "summary": "Despite remarkable advancements in recent voice conversion (VC) systems,\nenhancing speaker similarity in zero-shot scenarios remains challenging. This\nchallenge arises from the difficulty of generalizing and adapting speaker\ncharacteristics in speech within zero-shot environments, which is further\ncomplicated by mismatch between the training and inference processes. To\naddress these challenges, we propose VoicePrompter, a robust zero-shot VC model\nthat leverages in-context learning with voice prompts. VoicePrompter is\ncomposed of (1) a factorization method that disentangles speech components and\n(2) a DiT-based conditional flow matching (CFM) decoder that conditions on\nthese factorized features and voice prompts. Additionally, (3) latent mixup is\nused to enhance in-context learning by combining various speaker features. This\napproach improves speaker similarity and naturalness in zero-shot VC by\napplying mixup to latent representations. Experimental results demonstrate that\nVoicePrompter outperforms existing zero-shot VC systems in terms of speaker\nsimilarity, speech intelligibility, and audio quality. Our demo is available at\n\\url{https://hayeong0.github.io/VoicePrompter-demo/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable advancements in recent voice conversion (VC) systems,\nenhancing speaker similarity in zero-shot scenarios remains challenging. This\nchallenge arises from the difficulty of generalizing and adapting speaker\ncharacteristics in speech within zero-shot environments, which is further\ncomplicated by mismatch between the training and inference processes. To\naddress these challenges, we propose VoicePrompter, a robust zero-shot VC model\nthat leverages in-context learning with voice prompts. VoicePrompter is\ncomposed of (1) a factorization method that disentangles speech components and\n(2) a DiT-based conditional flow matching (CFM) decoder that conditions on\nthese factorized features and voice prompts. Additionally, (3) latent mixup is\nused to enhance in-context learning by combining various speaker features. This\napproach improves speaker similarity and naturalness in zero-shot VC by\napplying mixup to latent representations. Experimental results demonstrate that\nVoicePrompter outperforms existing zero-shot VC systems in terms of speaker\nsimilarity, speech intelligibility, and audio quality. Our demo is available at\n\\url{https://hayeong0.github.io/VoicePrompter-demo/}."
                },
                "authors": [
                    {
                        "name": "Ha-Yeong Choi"
                    },
                    {
                        "name": "Jaehan Park"
                    }
                ],
                "author_detail": {
                    "name": "Jaehan Park"
                },
                "author": "Jaehan Park",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17610v1",
                "updated": "2025-01-29T12:33:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    33,
                    16,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:33:16Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    33,
                    16,
                    2,
                    29,
                    0
                ],
                "title": "FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models\n  with Extremely Low Communication Overhead of One Bit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models\n  with Extremely Low Communication Overhead of One Bit"
                },
                "summary": "Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with\nprivate data from distributed clients by exchanging models rather than data\nunder the orchestration of a parameter server (PS). To overcome the bottleneck\nforged by the growing communication and memory overhead for clients in such\nsystems due to the growing model sizes, we propose \\textit{FeedSign}, an FFT\nalgorithm in which the upload and download payload for an aggregation step is\nexactly $1$ bit per step, while the memory overhead is squeezed to the amount\nneeded for inference. This is realized by utilizing zeroth-order (ZO)\noptimizers on large models and shared pseudo-random number generators (PRNG)\nacross devices to represent the gradient estimates as seed-sign pairs. We\nconduct theoretical analysis on FeedSign and show that it converges at an\nexponential rate $\\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed\nsteps under widely used assumptions. Moreover, FeedSign is found to be robust\nagainst data heterogeneity and Byzantine attacks. We conducted extensive\nexperiments on models across different structures and sizes (11M to 13B) and\nfound that the proposed method performs better or closely, depending on\nscenarios, compared to its ZO and FO counterparts, albeit with an\norders-of-magnitude lower communication overhead. We also discuss some\ninteresting advantages as byproducts guaranteed by the minimalistic design of\n\\textit{FeedSign}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with\nprivate data from distributed clients by exchanging models rather than data\nunder the orchestration of a parameter server (PS). To overcome the bottleneck\nforged by the growing communication and memory overhead for clients in such\nsystems due to the growing model sizes, we propose \\textit{FeedSign}, an FFT\nalgorithm in which the upload and download payload for an aggregation step is\nexactly $1$ bit per step, while the memory overhead is squeezed to the amount\nneeded for inference. This is realized by utilizing zeroth-order (ZO)\noptimizers on large models and shared pseudo-random number generators (PRNG)\nacross devices to represent the gradient estimates as seed-sign pairs. We\nconduct theoretical analysis on FeedSign and show that it converges at an\nexponential rate $\\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed\nsteps under widely used assumptions. Moreover, FeedSign is found to be robust\nagainst data heterogeneity and Byzantine attacks. We conducted extensive\nexperiments on models across different structures and sizes (11M to 13B) and\nfound that the proposed method performs better or closely, depending on\nscenarios, compared to its ZO and FO counterparts, albeit with an\norders-of-magnitude lower communication overhead. We also discuss some\ninteresting advantages as byproducts guaranteed by the minimalistic design of\n\\textit{FeedSign}."
                },
                "authors": [
                    {
                        "name": "Zhijie Cai"
                    },
                    {
                        "name": "Haolong Chen"
                    },
                    {
                        "name": "Guangxu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Guangxu Zhu"
                },
                "author": "Guangxu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14326v2",
                "updated": "2025-01-29T12:27:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    27,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-20T13:56:52Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    56,
                    52,
                    3,
                    172,
                    0
                ],
                "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced\n  Clinical Diagnosis on EMRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced\n  Clinical Diagnosis on EMRs"
                },
                "summary": "Electronic Medical Records (EMRs), while integral to modern healthcare,\npresent challenges for clinical reasoning and diagnosis due to their complexity\nand information redundancy. To address this, we proposed medIKAL (Integrating\nKnowledge Graphs as Assistants of LLMs), a framework that combines Large\nLanguage Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic\ncapabilities. medIKAL assigns weighted importance to entities in medical\nrecords based on their type, enabling precise localization of candidate\ndiseases within KGs. It innovatively employs a residual network-like approach,\nallowing initial diagnosis by the LLM to be merged into KG search results.\nThrough a path-based reranking algorithm and a fill-in-the-blank style prompt\ntemplate, it further refined the diagnostic process. We validated medIKAL's\neffectiveness through extensive experiments on a newly introduced open-sourced\nChinese EMR dataset, demonstrating its potential to improve clinical diagnosis\nin real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Medical Records (EMRs), while integral to modern healthcare,\npresent challenges for clinical reasoning and diagnosis due to their complexity\nand information redundancy. To address this, we proposed medIKAL (Integrating\nKnowledge Graphs as Assistants of LLMs), a framework that combines Large\nLanguage Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic\ncapabilities. medIKAL assigns weighted importance to entities in medical\nrecords based on their type, enabling precise localization of candidate\ndiseases within KGs. It innovatively employs a residual network-like approach,\nallowing initial diagnosis by the LLM to be merged into KG search results.\nThrough a path-based reranking algorithm and a fill-in-the-blank style prompt\ntemplate, it further refined the diagnostic process. We validated medIKAL's\neffectiveness through extensive experiments on a newly introduced open-sourced\nChinese EMR dataset, demonstrating its potential to improve clinical diagnosis\nin real-world settings."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10304v2",
                "updated": "2025-01-29T12:18:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    18,
                    7,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-13T17:37:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "A Neyman-Orthogonalization Approach to the Incidental Parameter Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neyman-Orthogonalization Approach to the Incidental Parameter Problem"
                },
                "summary": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations."
                },
                "authors": [
                    {
                        "name": "Stéphane Bonhomme"
                    },
                    {
                        "name": "Koen Jochmans"
                    },
                    {
                        "name": "Martin Weidner"
                    }
                ],
                "author_detail": {
                    "name": "Martin Weidner"
                },
                "author": "Martin Weidner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12846v4",
                "updated": "2025-01-29T12:15:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    15,
                    49,
                    2,
                    29,
                    0
                ],
                "published": "2024-01-23T15:29:26Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    15,
                    29,
                    26,
                    1,
                    23,
                    0
                ],
                "title": "How well can a large language model explain business processes as\n  perceived by users?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well can a large language model explain business processes as\n  perceived by users?"
                },
                "summary": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation."
                },
                "authors": [
                    {
                        "name": "Dirk Fahland"
                    },
                    {
                        "name": "Fabiana Fournier"
                    },
                    {
                        "name": "Lior Limonad"
                    },
                    {
                        "name": "Inna Skarbovsky"
                    },
                    {
                        "name": "Ava J. E. Swevels"
                    }
                ],
                "author_detail": {
                    "name": "Ava J. E. Swevels"
                },
                "author": "Ava J. E. Swevels",
                "arxiv_comment": "42 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17598v1",
                "updated": "2025-01-29T12:03:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    3,
                    11,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:03:11Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    3,
                    11,
                    2,
                    29,
                    0
                ],
                "title": "Semantic Consistency Regularization with Large Language Models for\n  Semi-supervised Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Consistency Regularization with Large Language Models for\n  Semi-supervised Sentiment Analysis"
                },
                "summary": "Accurate sentiment analysis of texts is crucial for a variety of\napplications, such as understanding customer feedback, monitoring market\ntrends, and detecting public sentiment. However, manually annotating large\nsentiment corpora for supervised learning is labor-intensive and\ntime-consuming. Therefore, it is essential and effective to develop a\nsemi-supervised method for the sentiment analysis task. Although some methods\nhave been proposed for semi-supervised text classification, they rely on the\nintrinsic information within the unlabeled data and the learning capability of\nthe NLP model, which lack generalization ability to the sentiment analysis\nscenario and may prone to overfit. Inspired by the ability of pretrained Large\nLanguage Models (LLMs) in following instructions and generating coherent text,\nwe propose a Semantic Consistency Regularization with Large Language Models\n(SCR) framework for semi-supervised sentiment analysis. We introduce two\nprompting strategies to semantically enhance unlabeled text using LLMs. The\nfirst is Entity-based Enhancement (SCR-EE), which involves extracting entities\nand numerical information, and querying the LLM to reconstruct the textual\ninformation. The second is Concept-based Enhancement (SCR-CE), which directly\nqueries the LLM with the original sentence for semantic reconstruction.\nSubsequently, the LLM-augmented data is utilized for a consistency loss with\nconfidence thresholding, which preserves high-quality agreement samples to\nprovide additional supervision signals during training. Furthermore, to fully\nutilize the uncertain unlabeled data samples, we propose a class re-assembling\nstrategy inspired by the class space shrinking theorem. Experiments show our\nmethod achieves remarkable performance over prior semi-supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate sentiment analysis of texts is crucial for a variety of\napplications, such as understanding customer feedback, monitoring market\ntrends, and detecting public sentiment. However, manually annotating large\nsentiment corpora for supervised learning is labor-intensive and\ntime-consuming. Therefore, it is essential and effective to develop a\nsemi-supervised method for the sentiment analysis task. Although some methods\nhave been proposed for semi-supervised text classification, they rely on the\nintrinsic information within the unlabeled data and the learning capability of\nthe NLP model, which lack generalization ability to the sentiment analysis\nscenario and may prone to overfit. Inspired by the ability of pretrained Large\nLanguage Models (LLMs) in following instructions and generating coherent text,\nwe propose a Semantic Consistency Regularization with Large Language Models\n(SCR) framework for semi-supervised sentiment analysis. We introduce two\nprompting strategies to semantically enhance unlabeled text using LLMs. The\nfirst is Entity-based Enhancement (SCR-EE), which involves extracting entities\nand numerical information, and querying the LLM to reconstruct the textual\ninformation. The second is Concept-based Enhancement (SCR-CE), which directly\nqueries the LLM with the original sentence for semantic reconstruction.\nSubsequently, the LLM-augmented data is utilized for a consistency loss with\nconfidence thresholding, which preserves high-quality agreement samples to\nprovide additional supervision signals during training. Furthermore, to fully\nutilize the uncertain unlabeled data samples, we propose a class re-assembling\nstrategy inspired by the class space shrinking theorem. Experiments show our\nmethod achieves remarkable performance over prior semi-supervised methods."
                },
                "authors": [
                    {
                        "name": "Kunrong Li"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "arxiv_comment": "ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17584v1",
                "updated": "2025-01-29T11:40:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    40,
                    46,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T11:40:46Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    40,
                    46,
                    2,
                    29,
                    0
                ],
                "title": "GLLM: Self-Corrective G-Code Generation using Large Language Models with\n  User Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLLM: Self-Corrective G-Code Generation using Large Language Models with\n  User Feedback"
                },
                "summary": "This paper introduces GLLM, an innovative tool that leverages Large Language\nModels (LLMs) to automatically generate G-code from natural language\ninstructions for Computer Numerical Control (CNC) machining. GLLM addresses the\nchallenges of manual G-code writing by bridging the gap between human-readable\ntask descriptions and machine-executable code. The system incorporates a\nfine-tuned StarCoder-3B model, enhanced with domain-specific training data and\na Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced\nprompting strategies and a novel self-corrective code generation approach to\nensure both syntactic and semantic correctness of the generated G-code. The\narchitecture includes robust validation mechanisms, including syntax checks,\nG-code-specific verifications, and functional correctness evaluations using\nHausdorff distance. By combining these techniques, GLLM aims to democratize CNC\nprogramming, making it more accessible to users without extensive programming\nexperience while maintaining high accuracy and reliability in G-code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GLLM, an innovative tool that leverages Large Language\nModels (LLMs) to automatically generate G-code from natural language\ninstructions for Computer Numerical Control (CNC) machining. GLLM addresses the\nchallenges of manual G-code writing by bridging the gap between human-readable\ntask descriptions and machine-executable code. The system incorporates a\nfine-tuned StarCoder-3B model, enhanced with domain-specific training data and\na Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced\nprompting strategies and a novel self-corrective code generation approach to\nensure both syntactic and semantic correctness of the generated G-code. The\narchitecture includes robust validation mechanisms, including syntax checks,\nG-code-specific verifications, and functional correctness evaluations using\nHausdorff distance. By combining these techniques, GLLM aims to democratize CNC\nprogramming, making it more accessible to users without extensive programming\nexperience while maintaining high accuracy and reliability in G-code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Mohamed Abdelaal"
                    },
                    {
                        "name": "Samuel Lokadjaja"
                    },
                    {
                        "name": "Gilbert Engert"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Engert"
                },
                "author": "Gilbert Engert",
                "arxiv_journal_ref": "Industrial Track of 21st Conference on Database Systems for\n  Business, Technology and Web (BTW), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17581v1",
                "updated": "2025-01-29T11:38:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    38,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T11:38:29Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    38,
                    29,
                    2,
                    29,
                    0
                ],
                "title": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free\n  Counterspeech Evaluation using Auto-Calibrated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free\n  Counterspeech Evaluation using Auto-Calibrated LLMs"
                },
                "summary": "Counterspeech has been popular as an effective approach to counter online\nhate speech, leading to increasing research interest in automated counterspeech\ngeneration using language models. However, this field lacks standardised\nevaluation protocols and robust automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (ACE), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that ACE outperforms traditional metrics\nlike ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant advancement in automated counterspeech evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterspeech has been popular as an effective approach to counter online\nhate speech, leading to increasing research interest in automated counterspeech\ngeneration using language models. However, this field lacks standardised\nevaluation protocols and robust automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (ACE), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that ACE outperforms traditional metrics\nlike ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant advancement in automated counterspeech evaluation."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Aswini Kumar"
                    },
                    {
                        "name": "Anil Bandhakavi"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "17 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:2309.13308 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11156v2",
                "updated": "2025-01-29T11:32:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    32,
                    23,
                    2,
                    29,
                    0
                ],
                "published": "2024-07-15T18:28:15Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    28,
                    15,
                    0,
                    197,
                    0
                ],
                "title": "Magnetic reconnection: an alternative explanation of radio emission in\n  galaxy clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic reconnection: an alternative explanation of radio emission in\n  galaxy clusters"
                },
                "summary": "Observations of galaxy clusters show radio emission extended over almost the\nsystem scale, necessitating mechanisms for particle acceleration. Previous\nmodels for acceleration such as diffusive shock acceleration and that due to\nturbulence can fall short in terms of efficiency. In this letter, we propose\nthe possibility of acceleration via magnetic reconnection. In particular, we\ninvoke the plasmoid instability which has been previously applied to understand\nparticle energization in high energy systems. Turbulence in galaxy clusters\nlead to fluctuation dynamos that are known to generate magnetic fields\nstructures consisting of sharp reversals. These form natural sites of\nreconnection. We perform Particle-In-Cell (PIC) simulations of the plasmoid\ninstability in collisionless and nonrelativistic plasmas. We show that the\nresulting electron energy spectra have power law indices that are consistent\nwith that inferred from observations. Our estimates show that the acceleration\ntimescales are much smaller than the lifetime of the reconnecting magnetic\nstructures indicating the feasibility of our model. The synchrotron radio\nluminosity estimate is about $10^{41}$ ergs/s, agreeing with observations.\nFinally, we find that the maximum achievable Lorentz factor can go upto $10^5$\nindicating that acceleration due magnetic reconnection is a promising avenue\nfor understanding the origin of nonthermal emission in galaxy clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of galaxy clusters show radio emission extended over almost the\nsystem scale, necessitating mechanisms for particle acceleration. Previous\nmodels for acceleration such as diffusive shock acceleration and that due to\nturbulence can fall short in terms of efficiency. In this letter, we propose\nthe possibility of acceleration via magnetic reconnection. In particular, we\ninvoke the plasmoid instability which has been previously applied to understand\nparticle energization in high energy systems. Turbulence in galaxy clusters\nlead to fluctuation dynamos that are known to generate magnetic fields\nstructures consisting of sharp reversals. These form natural sites of\nreconnection. We perform Particle-In-Cell (PIC) simulations of the plasmoid\ninstability in collisionless and nonrelativistic plasmas. We show that the\nresulting electron energy spectra have power law indices that are consistent\nwith that inferred from observations. Our estimates show that the acceleration\ntimescales are much smaller than the lifetime of the reconnecting magnetic\nstructures indicating the feasibility of our model. The synchrotron radio\nluminosity estimate is about $10^{41}$ ergs/s, agreeing with observations.\nFinally, we find that the maximum achievable Lorentz factor can go upto $10^5$\nindicating that acceleration due magnetic reconnection is a promising avenue\nfor understanding the origin of nonthermal emission in galaxy clusters."
                },
                "authors": [
                    {
                        "name": "Subham Ghosh"
                    },
                    {
                        "name": "Pallavi Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Pallavi Bhat"
                },
                "author": "Pallavi Bhat",
                "arxiv_doi": "10.3847/2041-8213/ad9f2d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad9f2d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 6 figures, Published in The ApJ Letters",
                "arxiv_journal_ref": "ApJL, 979, L15 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08225v2",
                "updated": "2025-01-29T11:28:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    28,
                    34,
                    2,
                    29,
                    0
                ],
                "published": "2023-10-12T11:17:40Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    11,
                    17,
                    40,
                    3,
                    285,
                    0
                ],
                "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations\n  for Speech and Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Word Error Rate Estimation Using Self-Supervised Representations\n  for Speech and Text"
                },
                "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic\nspeech recognition (ASR) system's output without requiring ground-truth labels.\nThis task has gained increasing attention as advanced ASR systems are trained\non large amounts of data. In this context, the computational efficiency of a\nWER estimator becomes essential in practice. However, previous works have not\nprioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is\nintroduced, utilizing average pooling over self-supervised learning\nrepresentations for speech and text. Our results demonstrate that Fe-WER\noutperformed a baseline relatively by 14.10% in root mean square error and\n1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative\nanalysis of the distributions of target WER and WER estimates was conducted,\nincluding an examination of the average values per speaker. Lastly, the\ninference speed was approximately 3.4 times faster in the real-time factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word error rate (WER) estimation aims to evaluate the quality of an automatic\nspeech recognition (ASR) system's output without requiring ground-truth labels.\nThis task has gained increasing attention as advanced ASR systems are trained\non large amounts of data. In this context, the computational efficiency of a\nWER estimator becomes essential in practice. However, previous works have not\nprioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is\nintroduced, utilizing average pooling over self-supervised learning\nrepresentations for speech and text. Our results demonstrate that Fe-WER\noutperformed a baseline relatively by 14.10% in root mean square error and\n1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative\nanalysis of the distributions of target WER and WER estimates was conducted,\nincluding an examination of the average values per speaker. Lastly, the\ninference speed was approximately 3.4 times faster in the real-time factor."
                },
                "authors": [
                    {
                        "name": "Chanho Park"
                    },
                    {
                        "name": "Chengsong Lu"
                    },
                    {
                        "name": "Mingjie Chen"
                    },
                    {
                        "name": "Thomas Hain"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hain"
                },
                "author": "Thomas Hain",
                "arxiv_comment": "5 pages, accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17044v2",
                "updated": "2025-01-29T11:06:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    6,
                    57,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T16:09:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    9,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with\n  Transformers"
                },
                "summary": "We generate abstractions of buildings, reflecting the essential aspects of\ntheir geometry and structure, by learning to invert procedural models. We first\nbuild a dataset of abstract procedural building models paired with simulated\npoint clouds and then learn the inverse mapping through a transformer. Given a\npoint cloud, the trained transformer then infers the corresponding abstracted\nbuilding in terms of a programmatic language description. This approach\nleverages expressive procedural models developed for gaming and animation, and\nthereby retains desirable properties such as efficient rendering of the\ninferred abstractions and strong priors for regularity and symmetry. Our\napproach achieves good reconstruction accuracy in terms of geometry and\nstructure, as well as structurally consistent inpainting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We generate abstractions of buildings, reflecting the essential aspects of\ntheir geometry and structure, by learning to invert procedural models. We first\nbuild a dataset of abstract procedural building models paired with simulated\npoint clouds and then learn the inverse mapping through a transformer. Given a\npoint cloud, the trained transformer then infers the corresponding abstracted\nbuilding in terms of a programmatic language description. This approach\nleverages expressive procedural models developed for gaming and animation, and\nthereby retains desirable properties such as efficient rendering of the\ninferred abstractions and strong priors for regularity and symmetry. Our\napproach achieves good reconstruction accuracy in terms of geometry and\nstructure, as well as structurally consistent inpainting."
                },
                "authors": [
                    {
                        "name": "Maximilian Dax"
                    },
                    {
                        "name": "Jordi Berbel"
                    },
                    {
                        "name": "Jan Stria"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Urs Bergmann"
                    }
                ],
                "author_detail": {
                    "name": "Urs Bergmann"
                },
                "author": "Urs Bergmann",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17546v1",
                "updated": "2025-01-29T10:29:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    29,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T10:29:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    29,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "Is Conversational XAI All You Need? Human-AI Decision Making With a\n  Conversational XAI Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Conversational XAI All You Need? Human-AI Decision Making With a\n  Conversational XAI Assistant"
                },
                "summary": "Explainable artificial intelligence (XAI) methods are being proposed to help\ninterpret and understand how AI systems reach specific predictions. Inspired by\nprior work on conversational user interfaces, we argue that augmenting existing\nXAI methods with conversational user interfaces can increase user engagement\nand boost user understanding of the AI system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding of the AI\nsystem, their trust, and reliance on the AI system. In comparison to an XAI\ndashboard, we found that the conversational XAI interface can bring about a\nbetter understanding of the AI system among users and higher user trust.\nHowever, users of both the XAI dashboard and conversational XAI interfaces\nshowed clear overreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based on our\nfindings, we reason that the potential cause of such overreliance is the\nillusion of explanatory depth that is concomitant with both XAI interfaces. Our\nfindings have important implications for designing effective conversational XAI\ninterfaces to facilitate appropriate reliance and improve human-AI\ncollaboration. Code can be found at\nhttps://github.com/delftcrowd/IUI2025_ConvXAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable artificial intelligence (XAI) methods are being proposed to help\ninterpret and understand how AI systems reach specific predictions. Inspired by\nprior work on conversational user interfaces, we argue that augmenting existing\nXAI methods with conversational user interfaces can increase user engagement\nand boost user understanding of the AI system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding of the AI\nsystem, their trust, and reliance on the AI system. In comparison to an XAI\ndashboard, we found that the conversational XAI interface can bring about a\nbetter understanding of the AI system among users and higher user trust.\nHowever, users of both the XAI dashboard and conversational XAI interfaces\nshowed clear overreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based on our\nfindings, we reason that the potential cause of such overreliance is the\nillusion of explanatory depth that is concomitant with both XAI interfaces. Our\nfindings have important implications for designing effective conversational XAI\ninterfaces to facilitate appropriate reliance and improve human-AI\ncollaboration. Code can be found at\nhttps://github.com/delftcrowd/IUI2025_ConvXAI"
                },
                "authors": [
                    {
                        "name": "Gaole He"
                    },
                    {
                        "name": "Nilay Aishwarya"
                    },
                    {
                        "name": "Ujwal Gadiraju"
                    }
                ],
                "author_detail": {
                    "name": "Ujwal Gadiraju"
                },
                "author": "Ujwal Gadiraju",
                "arxiv_doi": "10.1145/3708359.3712133",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712133",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conditionally accepted to IUI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v3",
                "updated": "2025-01-29T10:25:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    25,
                    2,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17539v1",
                "updated": "2025-01-29T10:12:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    12,
                    13,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T10:12:13Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    12,
                    13,
                    2,
                    29,
                    0
                ],
                "title": "Towards Supporting Penetration Testing Education with Large Language\n  Models: an Evaluation and Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Supporting Penetration Testing Education with Large Language\n  Models: an Evaluation and Comparison"
                },
                "summary": "Cybersecurity education is challenging and it is helpful for educators to\nunderstand Large Language Models' (LLMs') capabilities for supporting\neducation. This study evaluates the effectiveness of LLMs in conducting a\nvariety of penetration testing tasks. Fifteen representative tasks were\nselected to cover a comprehensive range of real-world scenarios. We evaluate\nthe performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1\n405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image\nand OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the\nmost consistent support making it a valuable tool for educational purposes.\nHowever, its use in conjonction with WhiteRabbitNeo should be considered,\nbecause of its innovative approach to tool and command recommendations. This\nstudy underscores the need for continued research into optimising LLMs for\ncomplex, domain-specific tasks in cybersecurity education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity education is challenging and it is helpful for educators to\nunderstand Large Language Models' (LLMs') capabilities for supporting\neducation. This study evaluates the effectiveness of LLMs in conducting a\nvariety of penetration testing tasks. Fifteen representative tasks were\nselected to cover a comprehensive range of real-world scenarios. We evaluate\nthe performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1\n405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image\nand OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the\nmost consistent support making it a valuable tool for educational purposes.\nHowever, its use in conjonction with WhiteRabbitNeo should be considered,\nbecause of its innovative approach to tool and command recommendations. This\nstudy underscores the need for continued research into optimising LLMs for\ncomplex, domain-specific tasks in cybersecurity education."
                },
                "authors": [
                    {
                        "name": "Martin Nizon-Deladoeuille"
                    },
                    {
                        "name": "Brynjólfur Stefánsson"
                    },
                    {
                        "name": "Helmut Neukirchen"
                    },
                    {
                        "name": "Thomas Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Welsh"
                },
                "author": "Thomas Welsh",
                "arxiv_comment": "To be published in: 11th IEEE International Conference on Social\n  Networks Analysis, Management and Security (SNAMS-2024), IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01959v2",
                "updated": "2025-01-29T10:09:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    9,
                    28,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-02T19:05:12Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    5,
                    12,
                    2,
                    276,
                    0
                ],
                "title": "Scale-Invariant Learning-to-Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Invariant Learning-to-Rank"
                },
                "summary": "At Expedia, learning-to-rank (LTR) models plays a key role on our website in\nsorting and presenting information more relevant to users, such as search\nfilters, property rooms, amenities, and images. A major challenge in deploying\nthese models is ensuring consistent feature scaling between training and\nproduction data, as discrepancies can lead to unreliable rankings when\ndeployed. Normalization techniques like feature standardization and batch\nnormalization could address these issues but are impractical in production due\nto latency impacts and the difficulty of distributed real-time inference. To\naddress consistent feature scaling issue, we introduce a scale-invariant LTR\nframework which combines a deep and a wide neural network to mathematically\nguarantee scale-invariance in the model at both training and prediction time.\nWe evaluate our framework in simulated real-world scenarios with injected\nfeature scale issues by perturbing the test set at prediction time, and show\nthat even with inconsistent train-test scaling, using framework achieves better\nperformance than without.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Expedia, learning-to-rank (LTR) models plays a key role on our website in\nsorting and presenting information more relevant to users, such as search\nfilters, property rooms, amenities, and images. A major challenge in deploying\nthese models is ensuring consistent feature scaling between training and\nproduction data, as discrepancies can lead to unreliable rankings when\ndeployed. Normalization techniques like feature standardization and batch\nnormalization could address these issues but are impractical in production due\nto latency impacts and the difficulty of distributed real-time inference. To\naddress consistent feature scaling issue, we introduce a scale-invariant LTR\nframework which combines a deep and a wide neural network to mathematically\nguarantee scale-invariance in the model at both training and prediction time.\nWe evaluate our framework in simulated real-world scenarios with injected\nfeature scale issues by perturbing the test set at prediction time, and show\nthat even with inconsistent train-test scaling, using framework achieves better\nperformance than without."
                },
                "authors": [
                    {
                        "name": "Alessio Petrozziello"
                    },
                    {
                        "name": "Christian Sommeregger"
                    },
                    {
                        "name": "Ye-Sheen Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ye-Sheen Lim"
                },
                "author": "Ye-Sheen Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17532v1",
                "updated": "2025-01-29T10:04:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    4,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T10:04:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    4,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "Wireless Network Topology Inference: A Markov Chains Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Network Topology Inference: A Markov Chains Approach"
                },
                "summary": "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion."
                },
                "authors": [
                    {
                        "name": "James Martin"
                    },
                    {
                        "name": "Tristan Pryer"
                    },
                    {
                        "name": "Luca Zanetti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Zanetti"
                },
                "author": "Luca Zanetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19369v2",
                "updated": "2025-01-29T09:54:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    54,
                    0,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-30T08:55:01Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    8,
                    55,
                    1,
                    1,
                    121,
                    0
                ],
                "title": "Evaluating Telugu Proficiency in Large Language Models_ A Comparative\n  Analysis of ChatGPT and Gemini",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Telugu Proficiency in Large Language Models_ A Comparative\n  Analysis of ChatGPT and Gemini"
                },
                "summary": "The growing prominence of large language models (LLMs) necessitates the\nexploration of their capabilities beyond English. This research investigates\nthe Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.\nThrough a designed set of 20 questions encompassing greetings, grammar,\nvocabulary, common phrases, task completion, and situational reasoning, the\nstudy delves into their strengths and weaknesses in handling Telugu. The\nanalysis aims to identify the LLM that demonstrates a deeper understanding of\nTelugu grammatical structures, possesses a broader vocabulary, and exhibits\nsuperior performance in tasks like writing and reasoning. By comparing their\nability to comprehend and use everyday Telugu expressions, the research sheds\nlight on their suitability for real-world language interaction. Furthermore,\nthe evaluation of adaptability and reasoning capabilities provides insights\ninto how each LLM leverages Telugu to respond to dynamic situations. This\ncomparative analysis contributes to the ongoing discussion on multilingual\ncapabilities in AI and paves the way for future research in developing LLMs\nthat can seamlessly integrate with Telugu-speaking communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prominence of large language models (LLMs) necessitates the\nexploration of their capabilities beyond English. This research investigates\nthe Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.\nThrough a designed set of 20 questions encompassing greetings, grammar,\nvocabulary, common phrases, task completion, and situational reasoning, the\nstudy delves into their strengths and weaknesses in handling Telugu. The\nanalysis aims to identify the LLM that demonstrates a deeper understanding of\nTelugu grammatical structures, possesses a broader vocabulary, and exhibits\nsuperior performance in tasks like writing and reasoning. By comparing their\nability to comprehend and use everyday Telugu expressions, the research sheds\nlight on their suitability for real-world language interaction. Furthermore,\nthe evaluation of adaptability and reasoning capabilities provides insights\ninto how each LLM leverages Telugu to respond to dynamic situations. This\ncomparative analysis contributes to the ongoing discussion on multilingual\ncapabilities in AI and paves the way for future research in developing LLMs\nthat can seamlessly integrate with Telugu-speaking communities."
                },
                "authors": [
                    {
                        "name": "Katikela Sreeharsha Kishore"
                    },
                    {
                        "name": "Rahimanuddin Shaik"
                    }
                ],
                "author_detail": {
                    "name": "Rahimanuddin Shaik"
                },
                "author": "Rahimanuddin Shaik",
                "arxiv_comment": "Disparities in fundamental understandings about the article between\n  the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02322v2",
                "updated": "2025-01-29T09:43:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    43,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-05T09:07:36Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    36,
                    0,
                    218,
                    0
                ],
                "title": "Consistent time travel for realistic interactions with historical data:\n  reinforcement learning for market making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent time travel for realistic interactions with historical data:\n  reinforcement learning for market making"
                },
                "summary": "Reinforcement learning works best when the impact of the agent's actions on\nits environment can be perfectly simulated or fully appraised from available\ndata. Some systems are however both hard to simulate and very sensitive to\nsmall perturbations. An additional difficulty arises when a RL agent is trained\noffline to be part of a multi-agent system using only anonymous data, which\nmakes it impossible to infer the state of each agent, thus to use data\ndirectly. Typical examples are competitive systems without agent-resolved data\nsuch as financial markets. We introduce consistent data time travel for offline\nRL as a remedy for these problems: instead of using historical data in a\nsequential way, we argue that one needs to perform time travel in historical\ndata, i.e., to adjust the time index so that both the past state and the\ninfluence of the RL agent's action on the system coincide with real data. This\nboth alleviates the need to resort to imperfect models and consistently\naccounts for both the immediate and long-term reactions of the system when\nusing anonymous historical data. We apply this idea to market making in limit\norder books, a notoriously difficult task for RL; it turns out that the gain of\nthe agent is significantly higher with data time travel than with naive\nsequential data, which suggests that the difficulty of this task for RL may\nhave been overestimated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning works best when the impact of the agent's actions on\nits environment can be perfectly simulated or fully appraised from available\ndata. Some systems are however both hard to simulate and very sensitive to\nsmall perturbations. An additional difficulty arises when a RL agent is trained\noffline to be part of a multi-agent system using only anonymous data, which\nmakes it impossible to infer the state of each agent, thus to use data\ndirectly. Typical examples are competitive systems without agent-resolved data\nsuch as financial markets. We introduce consistent data time travel for offline\nRL as a remedy for these problems: instead of using historical data in a\nsequential way, we argue that one needs to perform time travel in historical\ndata, i.e., to adjust the time index so that both the past state and the\ninfluence of the RL agent's action on the system coincide with real data. This\nboth alleviates the need to resort to imperfect models and consistently\naccounts for both the immediate and long-term reactions of the system when\nusing anonymous historical data. We apply this idea to market making in limit\norder books, a notoriously difficult task for RL; it turns out that the gain of\nthe agent is significantly higher with data time travel than with naive\nsequential data, which suggests that the difficulty of this task for RL may\nhave been overestimated."
                },
                "authors": [
                    {
                        "name": "Vincent Ragel"
                    },
                    {
                        "name": "Damien Challet"
                    }
                ],
                "author_detail": {
                    "name": "Damien Challet"
                },
                "author": "Damien Challet",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10339v2",
                "updated": "2025-01-29T09:40:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    40,
                    26,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-17T18:23:16Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    23,
                    16,
                    4,
                    17,
                    0
                ],
                "title": "Principled model selection for stochastic dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principled model selection for stochastic dynamics"
                },
                "summary": "Complex dynamical systems, from macromolecules to ecosystems, are often\nmodeled by stochastic differential equations. To learn such models from data, a\ncommon approach involves sparse selection among a large function library.\nHowever, we show that overfitting arises - not just from individual model\ncomplexity, but also from the combinatorial growth of possible models. To\naddress this, we introduce Parsimonious Stochastic Inference (PASTIS), a\nprincipled method combining likelihood-estimation statistics with extreme value\ntheory to suppress superfluous parameters. PASTIS outperforms existing methods\nand reliably identifies minimal models, even with low sampling rates or\nmeasurement error. It extends to stochastic partial differential equations, and\napplies to ecological networks and reaction-diffusion dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex dynamical systems, from macromolecules to ecosystems, are often\nmodeled by stochastic differential equations. To learn such models from data, a\ncommon approach involves sparse selection among a large function library.\nHowever, we show that overfitting arises - not just from individual model\ncomplexity, but also from the combinatorial growth of possible models. To\naddress this, we introduce Parsimonious Stochastic Inference (PASTIS), a\nprincipled method combining likelihood-estimation statistics with extreme value\ntheory to suppress superfluous parameters. PASTIS outperforms existing methods\nand reliably identifies minimal models, even with low sampling rates or\nmeasurement error. It extends to stochastic partial differential equations, and\napplies to ecological networks and reaction-diffusion dynamics."
                },
                "authors": [
                    {
                        "name": "Andonis Gerardos"
                    },
                    {
                        "name": "Pierre Ronceray"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ronceray"
                },
                "author": "Pierre Ronceray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17514v1",
                "updated": "2025-01-29T09:38:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    38,
                    24,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:38:24Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    38,
                    24,
                    2,
                    29,
                    0
                ],
                "title": "Semiparametric principal stratification analysis beyond monotonicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric principal stratification analysis beyond monotonicity"
                },
                "summary": "Intercurrent events, common in clinical trials and observational studies,\naffect the existence or interpretation of final outcomes. Principal\nstratification addresses these challenges by defining local average treatment\neffects within latent subpopulations, but often relies on restrictive\nassumptions such as monotonicity and counterfactual intermediate independence.\nTo address these limitations, we propose a unified semiparametric framework for\nprincipal stratification analysis leveraging a margin-free, conditional odds\nratio sensitivity parameter. Under principal ignorability, we derive\nnonparametric identification formulas and develop efficient estimation methods,\nincluding a conditionally doubly robust parametric estimator and a de-biased\nmachine learning estimator with data-adaptive nuisance estimators. Simulations\nshow that incorrectly assuming monotonicity can often lead to suboptimal\ninference, while specifying non-trivial odds ratio sensitivity parameter can\nenable approximately valid inference under monotonicity. We apply our methods\nto a critical care trial and further suggest a semiparametric sensitivity\nanalysis approach under violation of principal ignorability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intercurrent events, common in clinical trials and observational studies,\naffect the existence or interpretation of final outcomes. Principal\nstratification addresses these challenges by defining local average treatment\neffects within latent subpopulations, but often relies on restrictive\nassumptions such as monotonicity and counterfactual intermediate independence.\nTo address these limitations, we propose a unified semiparametric framework for\nprincipal stratification analysis leveraging a margin-free, conditional odds\nratio sensitivity parameter. Under principal ignorability, we derive\nnonparametric identification formulas and develop efficient estimation methods,\nincluding a conditionally doubly robust parametric estimator and a de-biased\nmachine learning estimator with data-adaptive nuisance estimators. Simulations\nshow that incorrectly assuming monotonicity can often lead to suboptimal\ninference, while specifying non-trivial odds ratio sensitivity parameter can\nenable approximately valid inference under monotonicity. We apply our methods\nto a critical care trial and further suggest a semiparametric sensitivity\nanalysis approach under violation of principal ignorability."
                },
                "authors": [
                    {
                        "name": "Jiaqi Tong"
                    },
                    {
                        "name": "Brennan Kahan"
                    },
                    {
                        "name": "Michael O. Harhay"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15733v2",
                "updated": "2025-01-29T09:27:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    27,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2024-07-22T15:37:35Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    37,
                    35,
                    0,
                    204,
                    0
                ],
                "title": "Admissible online closed testing must employ e-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Admissible online closed testing must employ e-values"
                },
                "summary": "In contemporary research, data scientists often test an infinite sequence of\nhypotheses $H_1,H_2,\\ldots $ one by one, and are required to make real-time\ndecisions without knowing the future hypotheses or data. In this paper, we\nconsider such an online multiple testing problem with the goal of providing\nsimultaneous lower bounds for the number of true discoveries in data-adaptively\nchosen rejection sets. In offline multiple testing, it has been recently\nestablished that such simultaneous inference is admissible iff it proceeds\nthrough (offline) closed testing. We establish an analogous result in this\npaper using the recent online closure principle. In particular, we show that it\nis necessary to use an anytime-valid test for each intersection hypothesis.\nThis connects two distinct branches of the literature: online testing of\nmultiple hypotheses (where the hypotheses appear online), and sequential\nanytime-valid testing of a single hypothesis (where the data for a fixed\nhypothesis appears online). Motivated by this result, we construct a new online\nclosed testing procedure and a corresponding short-cut with a true discovery\nguarantee based on multiplying sequential e-values. This general but simple\nprocedure gives uniform improvements over the state-of-the-art methods but also\nallows to construct entirely new and powerful procedures. In addition, we\nintroduce new ideas for hedging and boosting of sequential e-values that\nprovably increase power. Finally, we also propose the first online true\ndiscovery procedures for exchangeable and arbitrarily dependent e-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary research, data scientists often test an infinite sequence of\nhypotheses $H_1,H_2,\\ldots $ one by one, and are required to make real-time\ndecisions without knowing the future hypotheses or data. In this paper, we\nconsider such an online multiple testing problem with the goal of providing\nsimultaneous lower bounds for the number of true discoveries in data-adaptively\nchosen rejection sets. In offline multiple testing, it has been recently\nestablished that such simultaneous inference is admissible iff it proceeds\nthrough (offline) closed testing. We establish an analogous result in this\npaper using the recent online closure principle. In particular, we show that it\nis necessary to use an anytime-valid test for each intersection hypothesis.\nThis connects two distinct branches of the literature: online testing of\nmultiple hypotheses (where the hypotheses appear online), and sequential\nanytime-valid testing of a single hypothesis (where the data for a fixed\nhypothesis appears online). Motivated by this result, we construct a new online\nclosed testing procedure and a corresponding short-cut with a true discovery\nguarantee based on multiplying sequential e-values. This general but simple\nprocedure gives uniform improvements over the state-of-the-art methods but also\nallows to construct entirely new and powerful procedures. In addition, we\nintroduce new ideas for hedging and boosting of sequential e-values that\nprovably increase power. Finally, we also propose the first online true\ndiscovery procedures for exchangeable and arbitrarily dependent e-values."
                },
                "authors": [
                    {
                        "name": "Lasse Fischer"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "arxiv_comment": "35 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17510v1",
                "updated": "2025-01-29T09:27:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    27,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:27:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    27,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "LLM Assistance for Pediatric Depression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assistance for Pediatric Depression"
                },
                "summary": "Traditional depression screening methods, such as the PHQ-9, are particularly\nchallenging for children in pediatric primary care due to practical\nlimitations. AI has the potential to help, but the scarcity of annotated\ndatasets in mental health, combined with the computational costs of training,\nhighlights the need for efficient, zero-shot approaches. In this work, we\ninvestigate the feasibility of state-of-the-art LLMs for depressive symptom\nextraction in pediatric settings (ages 6-24). This approach aims to complement\ntraditional screening and minimize diagnostic errors.\n  Our findings show that all LLMs are 60% more efficient than word match, with\nFlan leading in precision (average F1: 0.65, precision: 0.78), excelling in the\nextraction of more rare symptoms like \"sleep problems\" (F1: 0.92) and\n\"self-loathing\" (F1: 0.8). Phi strikes a balance between precision (0.44) and\nrecall (0.60), performing well in categories like \"Feeling depressed\" (0.69)\nand \"Weight change\" (0.78). Llama 3, with the highest recall (0.90),\novergeneralizes symptoms, making it less suitable for this type of analysis.\nChallenges include the complexity of clinical notes and overgeneralization from\nPHQ-9 scores. The main challenges faced by LLMs include navigating the complex\nstructure of clinical notes with content from different times in the patient\ntrajectory, as well as misinterpreting elevated PHQ-9 scores.\n  We finally demonstrate the utility of symptom annotations provided by Flan as\nfeatures in an ML algorithm, which differentiates depression cases from\ncontrols with high precision of 0.78, showing a major performance boost\ncompared to a baseline that does not use these features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional depression screening methods, such as the PHQ-9, are particularly\nchallenging for children in pediatric primary care due to practical\nlimitations. AI has the potential to help, but the scarcity of annotated\ndatasets in mental health, combined with the computational costs of training,\nhighlights the need for efficient, zero-shot approaches. In this work, we\ninvestigate the feasibility of state-of-the-art LLMs for depressive symptom\nextraction in pediatric settings (ages 6-24). This approach aims to complement\ntraditional screening and minimize diagnostic errors.\n  Our findings show that all LLMs are 60% more efficient than word match, with\nFlan leading in precision (average F1: 0.65, precision: 0.78), excelling in the\nextraction of more rare symptoms like \"sleep problems\" (F1: 0.92) and\n\"self-loathing\" (F1: 0.8). Phi strikes a balance between precision (0.44) and\nrecall (0.60), performing well in categories like \"Feeling depressed\" (0.69)\nand \"Weight change\" (0.78). Llama 3, with the highest recall (0.90),\novergeneralizes symptoms, making it less suitable for this type of analysis.\nChallenges include the complexity of clinical notes and overgeneralization from\nPHQ-9 scores. The main challenges faced by LLMs include navigating the complex\nstructure of clinical notes with content from different times in the patient\ntrajectory, as well as misinterpreting elevated PHQ-9 scores.\n  We finally demonstrate the utility of symptom annotations provided by Flan as\nfeatures in an ML algorithm, which differentiates depression cases from\ncontrols with high precision of 0.78, showing a major performance boost\ncompared to a baseline that does not use these features."
                },
                "authors": [
                    {
                        "name": "Mariia Ignashina"
                    },
                    {
                        "name": "Paulina Bondaronek"
                    },
                    {
                        "name": "Dan Santel"
                    },
                    {
                        "name": "John Pestian"
                    },
                    {
                        "name": "Julia Ive"
                    }
                ],
                "author_detail": {
                    "name": "Julia Ive"
                },
                "author": "Julia Ive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17499v1",
                "updated": "2025-01-29T09:12:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    12,
                    23,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:12:23Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    12,
                    23,
                    2,
                    29,
                    0
                ],
                "title": "A Sampling Complexity-aware Framework for Discrete-time Fractional-Order\n  Dynamical System Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sampling Complexity-aware Framework for Discrete-time Fractional-Order\n  Dynamical System Identification"
                },
                "summary": "A variety of complex biological, natural and man-made systems exhibit\nnon-Markovian dynamics that can be modeled through fractional order\ndifferential equations, yet, we lack sample comlexity aware system\nidentification strategies. Towards this end, we propose an affine discrete-time\nfractional order dynamical system (FoDS) identification algorithm and provide a\ndetailed sample complexity analysis. The algorithm effectively addresses the\nchallenges of FoDS identification in the presence of noisy data. The proposed\nalgorithm consists of two key steps. Firstly, it avoids solving higher-order\npolynomial equations, which would otherwise result in multiple potential\nsolutions for the fractional orders. Secondly, the identification problem is\nreformulated as a least squares estimation, allowing us to infer the system\nparameters. We derive the expectation and probabilistic bounds for the FoDS\nparameter estimation error, assuming prior knowledge of the functions \\( f \\)\nand \\( g \\) in the FoDS model. The error decays at a rate of \\( N = O\\left(\n\\frac{d}{\\epsilon} \\right) \\), where \\( N \\) is the number of samples, \\( d \\)\nis the dimension of the state variable, and \\( \\epsilon \\) represents the\ndesired estimation accuracy. Simulation results demonstrate that our\ntheoretical bounds are tight, validating the accuracy and robustness of this\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A variety of complex biological, natural and man-made systems exhibit\nnon-Markovian dynamics that can be modeled through fractional order\ndifferential equations, yet, we lack sample comlexity aware system\nidentification strategies. Towards this end, we propose an affine discrete-time\nfractional order dynamical system (FoDS) identification algorithm and provide a\ndetailed sample complexity analysis. The algorithm effectively addresses the\nchallenges of FoDS identification in the presence of noisy data. The proposed\nalgorithm consists of two key steps. Firstly, it avoids solving higher-order\npolynomial equations, which would otherwise result in multiple potential\nsolutions for the fractional orders. Secondly, the identification problem is\nreformulated as a least squares estimation, allowing us to infer the system\nparameters. We derive the expectation and probabilistic bounds for the FoDS\nparameter estimation error, assuming prior knowledge of the functions \\( f \\)\nand \\( g \\) in the FoDS model. The error decays at a rate of \\( N = O\\left(\n\\frac{d}{\\epsilon} \\right) \\), where \\( N \\) is the number of samples, \\( d \\)\nis the dimension of the state variable, and \\( \\epsilon \\) represents the\ndesired estimation accuracy. Simulation results demonstrate that our\ntheoretical bounds are tight, validating the accuracy and robustness of this\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Vijay Gupta"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17492v1",
                "updated": "2025-01-29T09:00:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    0,
                    8,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:00:08Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    0,
                    8,
                    2,
                    29,
                    0
                ],
                "title": "Velocity Structure of Circumstellar Environment around Class 0/I\n  Protostars: Uncertainty in the Protostellar Mass Estimation Using\n  Circumstellar Velocities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Velocity Structure of Circumstellar Environment around Class 0/I\n  Protostars: Uncertainty in the Protostellar Mass Estimation Using\n  Circumstellar Velocities"
                },
                "summary": "Recent high-resolution observations have enabled detailed investigations of\nthe circumstellar environments around Class 0/I protostars. Several studies\nhave reported that the infall velocity of the envelope is a few times smaller\nthan the free-fall velocity inferred from protostellar masses estimated via the\nobserved rotational velocity of their Keplerian disks. To explore the physical\norigins of the slow infall, we perform a set of three-dimensional non-ideal\nmagnetohydrodynamic simulations of the star formation process, extending to\n$10^5$ yr after protostar formation. Our simulations show that the infall\nvelocity decreases markedly at the outer edge of the pseudo-disk (at radii of\n$\\sim \\! 100-1000$ au) and is much slower than the expected free-fall velocity.\nThe degree of this reduction depends on (1) the initial magnetic field\nstrength, (2) the alignment between the initial field and the rotation axis,\nand (3) the evolutionary stage of the system. Across our parameter space, the\nratio of the infall velocity to the free-fall velocity is as small as\n$0.2-0.5$, which is consistent with the observations. We further examine the\nreliability of protostellar mass estimates derived from infall and rotational\nvelocities. While the mass derived from disk rotation closely matches the true\nvalue, deviation by a factor of $0.3-2$ is found for the estimates using the\ninfall velocity; it is underestimated due to slow infall, but could also be\noverestimated due to the contribution of disk mass. These findings underscore\nthe critical role of magnetic fields in shaping star formation dynamics and\nhighlight the uncertainties associated with protostellar mass estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent high-resolution observations have enabled detailed investigations of\nthe circumstellar environments around Class 0/I protostars. Several studies\nhave reported that the infall velocity of the envelope is a few times smaller\nthan the free-fall velocity inferred from protostellar masses estimated via the\nobserved rotational velocity of their Keplerian disks. To explore the physical\norigins of the slow infall, we perform a set of three-dimensional non-ideal\nmagnetohydrodynamic simulations of the star formation process, extending to\n$10^5$ yr after protostar formation. Our simulations show that the infall\nvelocity decreases markedly at the outer edge of the pseudo-disk (at radii of\n$\\sim \\! 100-1000$ au) and is much slower than the expected free-fall velocity.\nThe degree of this reduction depends on (1) the initial magnetic field\nstrength, (2) the alignment between the initial field and the rotation axis,\nand (3) the evolutionary stage of the system. Across our parameter space, the\nratio of the infall velocity to the free-fall velocity is as small as\n$0.2-0.5$, which is consistent with the observations. We further examine the\nreliability of protostellar mass estimates derived from infall and rotational\nvelocities. While the mass derived from disk rotation closely matches the true\nvalue, deviation by a factor of $0.3-2$ is found for the estimates using the\ninfall velocity; it is underestimated due to slow infall, but could also be\noverestimated due to the contribution of disk mass. These findings underscore\nthe critical role of magnetic fields in shaping star formation dynamics and\nhighlight the uncertainties associated with protostellar mass estimates."
                },
                "authors": [
                    {
                        "name": "Shingo Hirano"
                    },
                    {
                        "name": "Yuri Aikawa"
                    },
                    {
                        "name": "Masahiro N. Machida"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro N. Machida"
                },
                "author": "Masahiro N. Machida",
                "arxiv_comment": "13 pages, 12 figures, 1 table, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17490v1",
                "updated": "2025-01-29T09:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    0,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    0,
                    3,
                    2,
                    29,
                    0
                ],
                "title": "Pricing Carbon Allowance Options on Futures: Insights from\n  High-Frequency Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pricing Carbon Allowance Options on Futures: Insights from\n  High-Frequency Data"
                },
                "summary": "Leveraging a unique dataset of carbon futures option prices traded on the ICE\nmarket from December 2015 until December 2020, we present the results from an\nunprecedented calibration exercise. Within a multifactor stochastic volatility\nframework with jumps, we employ a three-dimensional pricing kernel compensating\nfor equity and variance components' risk to derive an analytically tractable\nand numerically practical approach to pricing. To the best of our knowledge, we\nare the first to provide an estimate of the equity and variance risk premia for\nthe carbon futures option market. We gain insights into daily option and\nfutures dynamics by exploiting the information from tick-by-tick futures trade\ndata. Decomposing the realized measure of futures volatility into continuous\nand jump components, we employ them as auxiliary variables for estimating\nfutures dynamics via indirect inference. Our approach provides a realistic\ndescription of carbon futures price, volatility, and jump dynamics and an\ninsightful understanding of the carbon option market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging a unique dataset of carbon futures option prices traded on the ICE\nmarket from December 2015 until December 2020, we present the results from an\nunprecedented calibration exercise. Within a multifactor stochastic volatility\nframework with jumps, we employ a three-dimensional pricing kernel compensating\nfor equity and variance components' risk to derive an analytically tractable\nand numerically practical approach to pricing. To the best of our knowledge, we\nare the first to provide an estimate of the equity and variance risk premia for\nthe carbon futures option market. We gain insights into daily option and\nfutures dynamics by exploiting the information from tick-by-tick futures trade\ndata. Decomposing the realized measure of futures volatility into continuous\nand jump components, we employ them as auxiliary variables for estimating\nfutures dynamics via indirect inference. Our approach provides a realistic\ndescription of carbon futures price, volatility, and jump dynamics and an\ninsightful understanding of the carbon option market."
                },
                "authors": [
                    {
                        "name": "Simone Serafini"
                    },
                    {
                        "name": "Giacomo Bormetti"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Bormetti"
                },
                "author": "Giacomo Bormetti",
                "arxiv_comment": "Main text 38 pages, supplementary online information 11 pages, 6\n  figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17489v1",
                "updated": "2025-01-29T08:57:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    57,
                    51,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T08:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    57,
                    51,
                    2,
                    29,
                    0
                ],
                "title": "Neural Spelling: A Spell-Based BCI System for Language Neural Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Spelling: A Spell-Based BCI System for Language Neural Decoding"
                },
                "summary": "Brain-computer interfaces (BCIs) present a promising avenue by translating\nneural activity directly into text, eliminating the need for physical actions.\nHowever, existing non-invasive BCI systems have not successfully covered the\nentire alphabet, limiting their practicality. In this paper, we propose a novel\nnon-invasive EEG-based BCI system with Curriculum-based Neural Spelling\nFramework, which recognizes all 26 alphabet letters by decoding neural signals\nassociated with handwriting first, and then apply a Generative AI (GenAI) to\nenhance spell-based neural language decoding tasks. Our approach combines the\nease of handwriting with the accessibility of EEG technology, utilizing\nadvanced neural decoding algorithms and pre-trained large language models\n(LLMs) to translate EEG patterns into text with high accuracy. This system show\nhow GenAI can improve the performance of typical spelling-based neural language\ndecoding task, and addresses the limitations of previous methods, offering a\nscalable and user-friendly solution for individuals with communication\nimpairments, thereby enhancing inclusive communication options.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-computer interfaces (BCIs) present a promising avenue by translating\nneural activity directly into text, eliminating the need for physical actions.\nHowever, existing non-invasive BCI systems have not successfully covered the\nentire alphabet, limiting their practicality. In this paper, we propose a novel\nnon-invasive EEG-based BCI system with Curriculum-based Neural Spelling\nFramework, which recognizes all 26 alphabet letters by decoding neural signals\nassociated with handwriting first, and then apply a Generative AI (GenAI) to\nenhance spell-based neural language decoding tasks. Our approach combines the\nease of handwriting with the accessibility of EEG technology, utilizing\nadvanced neural decoding algorithms and pre-trained large language models\n(LLMs) to translate EEG patterns into text with high accuracy. This system show\nhow GenAI can improve the performance of typical spelling-based neural language\ndecoding task, and addresses the limitations of previous methods, offering a\nscalable and user-friendly solution for individuals with communication\nimpairments, thereby enhancing inclusive communication options."
                },
                "authors": [
                    {
                        "name": "Xiaowei Jiang"
                    },
                    {
                        "name": "Charles Zhou"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Ziyi Zhao"
                    },
                    {
                        "name": "Thomas Do"
                    },
                    {
                        "name": "Chin-Teng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chin-Teng Lin"
                },
                "author": "Chin-Teng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01054v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01054v4",
                "updated": "2025-01-29T08:52:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    52,
                    40,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-01T11:26:50Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    11,
                    26,
                    50,
                    0,
                    92,
                    0
                ],
                "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for\n  Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for\n  Language Model Alignment"
                },
                "summary": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon"
                },
                "authors": [
                    {
                        "name": "Yuu Jinnai"
                    },
                    {
                        "name": "Tetsuro Morimura"
                    },
                    {
                        "name": "Kaito Ariu"
                    },
                    {
                        "name": "Kenshi Abe"
                    }
                ],
                "author_detail": {
                    "name": "Kenshi Abe"
                },
                "author": "Kenshi Abe",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01054v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01054v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17479v1",
                "updated": "2025-01-29T08:44:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    44,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T08:44:45Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    44,
                    45,
                    2,
                    29,
                    0
                ],
                "title": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious natural language processing tasks but often struggle to excel uniformly\nin diverse or complex domains. We propose a novel ensemble method - Diverse\nFingerprint Ensemble (DFPE), which leverages the complementary strengths of\nmultiple LLMs to achieve more robust performance. Our approach involves: (1)\nclustering models based on response \"fingerprints\" patterns, (2) applying a\nquantile-based filtering mechanism to remove underperforming models at a\nper-subject level, and (3) assigning adaptive weights to remaining models based\non their subject-wise validation accuracy. In experiments on the Massive\nMultitask Language Understanding (MMLU) benchmark, DFPE outperforms the best\nsingle model by 3% overall accuracy and 5% in discipline-level accuracy. This\nmethod increases the robustness and generalization of LLMs and underscores how\nmodel selection, diversity preservation, and performance-driven weighting can\neffectively address challenging, multi-faceted language understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious natural language processing tasks but often struggle to excel uniformly\nin diverse or complex domains. We propose a novel ensemble method - Diverse\nFingerprint Ensemble (DFPE), which leverages the complementary strengths of\nmultiple LLMs to achieve more robust performance. Our approach involves: (1)\nclustering models based on response \"fingerprints\" patterns, (2) applying a\nquantile-based filtering mechanism to remove underperforming models at a\nper-subject level, and (3) assigning adaptive weights to remaining models based\non their subject-wise validation accuracy. In experiments on the Massive\nMultitask Language Understanding (MMLU) benchmark, DFPE outperforms the best\nsingle model by 3% overall accuracy and 5% in discipline-level accuracy. This\nmethod increases the robustness and generalization of LLMs and underscores how\nmodel selection, diversity preservation, and performance-driven weighting can\neffectively address challenging, multi-faceted language understanding tasks."
                },
                "authors": [
                    {
                        "name": "Seffi Cohen"
                    },
                    {
                        "name": "Niv Goldshlager"
                    },
                    {
                        "name": "Nurit Cohen-Inger"
                    },
                    {
                        "name": "Bracha Shapira"
                    },
                    {
                        "name": "Lior Rokach"
                    }
                ],
                "author_detail": {
                    "name": "Lior Rokach"
                },
                "author": "Lior Rokach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17461v1",
                "updated": "2025-01-29T07:45:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    45,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T07:45:41Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    45,
                    41,
                    2,
                    29,
                    0
                ],
                "title": "AugmenTest: Enhancing Tests with LLM-Driven Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AugmenTest: Enhancing Tests with LLM-Driven Oracles"
                },
                "summary": "Automated test generation is crucial for ensuring the reliability and\nrobustness of software applications while at the same time reducing the effort\nneeded. While significant progress has been made in test generation research,\ngenerating valid test oracles still remains an open problem. To address this\nchallenge, we present AugmenTest, an approach leveraging Large Language Models\n(LLMs) to infer correct test oracles based on available documentation of the\nsoftware under test. Unlike most existing methods that rely on code, AugmenTest\nutilizes the semantic capabilities of LLMs to infer the intended behavior of a\nmethod from documentation and developer comments, without looking at the code.\nAugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a\ngeneric prompt (without the context of class or method under test), and RAG\nwith Simple Prompt, each offering different levels of contextual information to\nthe LLMs. To evaluate our work, we selected 142 Java classes and generated\nmultiple mutants for each. We then generated tests from these mutants, focusing\nonly on tests that passed on the mutant but failed on the original class, to\nensure that the tests effectively captured bugs. This resulted in 203 unique\ntests with distinct bugs, which were then used to evaluate AugmenTest. Results\nshow that in the most conservative scenario, AugmenTest's Extended Prompt\nconsistently outperformed the Simple Prompt, achieving a success rate of 30\\%\nfor generating correct assertions. In comparison, the state-of-the-art TOGA\napproach achieved 8.2\\%. Contrary to our expectations, the RAG-based approaches\ndid not lead to improvements, with performance of 18.2\\% success rate for the\nmost conservative scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation is crucial for ensuring the reliability and\nrobustness of software applications while at the same time reducing the effort\nneeded. While significant progress has been made in test generation research,\ngenerating valid test oracles still remains an open problem. To address this\nchallenge, we present AugmenTest, an approach leveraging Large Language Models\n(LLMs) to infer correct test oracles based on available documentation of the\nsoftware under test. Unlike most existing methods that rely on code, AugmenTest\nutilizes the semantic capabilities of LLMs to infer the intended behavior of a\nmethod from documentation and developer comments, without looking at the code.\nAugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a\ngeneric prompt (without the context of class or method under test), and RAG\nwith Simple Prompt, each offering different levels of contextual information to\nthe LLMs. To evaluate our work, we selected 142 Java classes and generated\nmultiple mutants for each. We then generated tests from these mutants, focusing\nonly on tests that passed on the mutant but failed on the original class, to\nensure that the tests effectively captured bugs. This resulted in 203 unique\ntests with distinct bugs, which were then used to evaluate AugmenTest. Results\nshow that in the most conservative scenario, AugmenTest's Extended Prompt\nconsistently outperformed the Simple Prompt, achieving a success rate of 30\\%\nfor generating correct assertions. In comparison, the state-of-the-art TOGA\napproach achieved 8.2\\%. Contrary to our expectations, the RAG-based approaches\ndid not lead to improvements, with performance of 18.2\\% success rate for the\nmost conservative scenario."
                },
                "authors": [
                    {
                        "name": "Shaker Mahmud Khandaker"
                    },
                    {
                        "name": "Fitsum Kifetew"
                    },
                    {
                        "name": "Davide Prandi"
                    },
                    {
                        "name": "Angelo Susi"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Susi"
                },
                "author": "Angelo Susi",
                "arxiv_comment": "Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17459v1",
                "updated": "2025-01-29T07:35:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    35,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T07:35:56Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    35,
                    56,
                    2,
                    29,
                    0
                ],
                "title": "Large Language Models for Single-Step and Multi-Step Flight Trajectory\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Single-Step and Multi-Step Flight Trajectory\n  Prediction"
                },
                "summary": "Flight trajectory prediction is a critical time series task in aviation.\nWhile deep learning methods have shown significant promise, the application of\nlarge language models (LLMs) to this domain remains underexplored. This study\npioneers the use of LLMs for flight trajectory prediction by reframing it as a\nlanguage modeling problem. Specifically, We extract features representing the\naircraft's position and status from ADS-B flight data to construct a\nprompt-based dataset, where trajectory waypoints are converted into language\ntokens. The dataset is then employed to fine-tune LLMs, enabling them to learn\ncomplex spatiotemporal patterns for accurate predictions. Comprehensive\nexperiments demonstrate that LLMs achieve notable performance improvements in\nboth single-step and multi-step predictions compared to traditional methods,\nwith LLaMA-3.1 model achieving the highest overall accuracy. However, the high\ninference latency of LLMs poses a challenge for real-time applications,\nunderscoring the need for further research in this promising direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flight trajectory prediction is a critical time series task in aviation.\nWhile deep learning methods have shown significant promise, the application of\nlarge language models (LLMs) to this domain remains underexplored. This study\npioneers the use of LLMs for flight trajectory prediction by reframing it as a\nlanguage modeling problem. Specifically, We extract features representing the\naircraft's position and status from ADS-B flight data to construct a\nprompt-based dataset, where trajectory waypoints are converted into language\ntokens. The dataset is then employed to fine-tune LLMs, enabling them to learn\ncomplex spatiotemporal patterns for accurate predictions. Comprehensive\nexperiments demonstrate that LLMs achieve notable performance improvements in\nboth single-step and multi-step predictions compared to traditional methods,\nwith LLaMA-3.1 model achieving the highest overall accuracy. However, the high\ninference latency of LLMs poses a challenge for real-time applications,\nunderscoring the need for further research in this promising direction."
                },
                "authors": [
                    {
                        "name": "Kaiwei Luo"
                    },
                    {
                        "name": "Jiliu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiliu Zhou"
                },
                "author": "Jiliu Zhou",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17455v1",
                "updated": "2025-01-29T07:32:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    32,
                    42,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T07:32:42Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    32,
                    42,
                    2,
                    29,
                    0
                ],
                "title": "Uniform Confidence Band for Marginal Treatment Effect Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Confidence Band for Marginal Treatment Effect Function"
                },
                "summary": "This paper presents a method for constructing uniform confidence bands for\nthe marginal treatment effect function. Our approach visualizes statistical\nuncertainty, facilitating inferences about the function's shape. We derive a\nGaussian approximation for a local quadratic estimator, enabling\ncomputationally inexpensive construction of these bands. Monte Carlo\nsimulations demonstrate that our bands provide the desired coverage and are\nless conservative than those based on the Gumbel approximation. An empirical\nillustration is included.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for constructing uniform confidence bands for\nthe marginal treatment effect function. Our approach visualizes statistical\nuncertainty, facilitating inferences about the function's shape. We derive a\nGaussian approximation for a local quadratic estimator, enabling\ncomputationally inexpensive construction of these bands. Monte Carlo\nsimulations demonstrate that our bands provide the desired coverage and are\nless conservative than those based on the Gumbel approximation. An empirical\nillustration is included."
                },
                "authors": [
                    {
                        "name": "Toshiki Tsuda"
                    },
                    {
                        "name": "Yanchun Jin"
                    },
                    {
                        "name": "Ryo Okui"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Okui"
                },
                "author": "Ryo Okui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16689v2",
                "updated": "2025-01-29T07:23:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    23,
                    47,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T03:57:22Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "title": "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning"
                },
                "summary": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "21 pages, 19 tables",
                "arxiv_journal_ref": "Stanford University InfoLab Technical Report, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11749v2",
                "updated": "2025-01-29T07:13:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    13,
                    0,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-16T13:06:44Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    6,
                    44,
                    0,
                    351,
                    0
                ],
                "title": "Inferring additional physics through unmodelled signal reconstructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring additional physics through unmodelled signal reconstructions"
                },
                "summary": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength."
                },
                "authors": [
                    {
                        "name": "Rimo Das"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "Sijil Jose"
                    },
                    {
                        "name": "Imre Bartos"
                    },
                    {
                        "name": "Sergey Klimenko"
                    },
                    {
                        "name": "Chandra Kant Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Kant Mishra"
                },
                "author": "Chandra Kant Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v5",
                "updated": "2025-01-29T07:07:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    7,
                    54,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming"
                },
                "summary": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00750v2",
                "updated": "2025-01-29T06:49:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    49,
                    30,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-01T06:36:56Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    6,
                    36,
                    56,
                    2,
                    1,
                    0
                ],
                "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform"
                },
                "summary": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries."
                },
                "authors": [
                    {
                        "name": "Cheonsu Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Cheonsu Jeong"
                },
                "author": "Cheonsu Jeong",
                "arxiv_comment": "22 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17437v1",
                "updated": "2025-01-29T06:32:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    32,
                    55,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T06:32:55Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    32,
                    55,
                    2,
                    29,
                    0
                ],
                "title": "Bayesian BIM-Guided Construction Robot Navigation with NLP Safety\n  Prompts in Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian BIM-Guided Construction Robot Navigation with NLP Safety\n  Prompts in Dynamic Environments"
                },
                "summary": "Construction robotics increasingly relies on natural language processing for\ntask execution, creating a need for robust methods to interpret commands in\ncomplex, dynamic environments. While existing research primarily focuses on\nwhat tasks robots should perform, less attention has been paid to how these\ntasks should be executed safely and efficiently. This paper presents a novel\nprobabilistic framework that uses sentiment analysis from natural language\ncommands to dynamically adjust robot navigation policies in construction\nenvironments. The framework leverages Building Information Modeling (BIM) data\nand natural language prompts to create adaptive navigation strategies that\naccount for varying levels of environmental risk and uncertainty. We introduce\nan object-aware path planning approach that combines exponential potential\nfields with a grid-based representation of the environment, where the potential\nfields are dynamically adjusted based on the semantic analysis of user prompts.\nThe framework employs Bayesian inference to consolidate multiple information\nsources: the static data from BIM, the semantic content of natural language\ncommands, and the implied safety constraints from user prompts. We demonstrate\nour approach through experiments comparing three scenarios: baseline\nshortest-path planning, safety-oriented navigation, and risk-aware routing.\nResults show that our method successfully adapts path planning based on natural\nlanguage sentiment, achieving a 50\\% improvement in minimum distance to\nobstacles when safety is prioritized, while maintaining reasonable path\nlengths. Scenarios with contrasting prompts, such as \"dangerous\" and \"safe\",\ndemonstrate the framework's ability to modify paths. This approach provides a\nflexible foundation for integrating human knowledge and safety considerations\ninto construction robot navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction robotics increasingly relies on natural language processing for\ntask execution, creating a need for robust methods to interpret commands in\ncomplex, dynamic environments. While existing research primarily focuses on\nwhat tasks robots should perform, less attention has been paid to how these\ntasks should be executed safely and efficiently. This paper presents a novel\nprobabilistic framework that uses sentiment analysis from natural language\ncommands to dynamically adjust robot navigation policies in construction\nenvironments. The framework leverages Building Information Modeling (BIM) data\nand natural language prompts to create adaptive navigation strategies that\naccount for varying levels of environmental risk and uncertainty. We introduce\nan object-aware path planning approach that combines exponential potential\nfields with a grid-based representation of the environment, where the potential\nfields are dynamically adjusted based on the semantic analysis of user prompts.\nThe framework employs Bayesian inference to consolidate multiple information\nsources: the static data from BIM, the semantic content of natural language\ncommands, and the implied safety constraints from user prompts. We demonstrate\nour approach through experiments comparing three scenarios: baseline\nshortest-path planning, safety-oriented navigation, and risk-aware routing.\nResults show that our method successfully adapts path planning based on natural\nlanguage sentiment, achieving a 50\\% improvement in minimum distance to\nobstacles when safety is prioritized, while maintaining reasonable path\nlengths. Scenarios with contrasting prompts, such as \"dangerous\" and \"safe\",\ndemonstrate the framework's ability to modify paths. This approach provides a\nflexible foundation for integrating human knowledge and safety considerations\ninto construction robot navigation."
                },
                "authors": [
                    {
                        "name": "Mani Amani"
                    },
                    {
                        "name": "Reza Akhavian"
                    }
                ],
                "author_detail": {
                    "name": "Reza Akhavian"
                },
                "author": "Reza Akhavian",
                "arxiv_comment": "Submitted to International Symposium on Automation and Robotics in\n  Construction (ISARC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17436v1",
                "updated": "2025-01-29T06:31:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    31,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T06:31:45Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    31,
                    45,
                    2,
                    29,
                    0
                ],
                "title": "Geodesic Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geodesic Difference-in-Differences"
                },
                "summary": "Difference-in-differences (DID) is a widely used quasi-experimental design\nfor causal inference, traditionally applied to scalar or Euclidean outcomes,\nwhile extensions to outcomes residing in non-Euclidean spaces remain limited.\nExisting methods for such outcomes have primarily focused on univariate\ndistributions, leveraging linear operations in the space of quantile functions,\nbut these approaches cannot be directly extended to outcomes in general metric\nspaces. In this paper, we propose geodesic DID, a novel DID framework for\noutcomes in geodesic metric spaces, such as distributions, networks, and\nmanifold-valued data. To address the absence of algebraic operations in these\nspaces, we use geodesics as proxies for differences and introduce the geodesic\naverage treatment effect on the treated (ATT) as the causal estimand. We\nestablish the identification of the geodesic ATT and derive the convergence\nrate of its sample versions, employing tools from metric geometry and empirical\nprocess theory. This framework is further extended to the case of staggered DID\nsettings, allowing for multiple time periods and varying treatment timings. To\nillustrate the practical utility of geodesic DID, we analyze health impacts of\nthe Soviet Union's collapse using age-at-death distributions and assess effects\nof U.S. electricity market liberalization on electricity generation\ncompositions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difference-in-differences (DID) is a widely used quasi-experimental design\nfor causal inference, traditionally applied to scalar or Euclidean outcomes,\nwhile extensions to outcomes residing in non-Euclidean spaces remain limited.\nExisting methods for such outcomes have primarily focused on univariate\ndistributions, leveraging linear operations in the space of quantile functions,\nbut these approaches cannot be directly extended to outcomes in general metric\nspaces. In this paper, we propose geodesic DID, a novel DID framework for\noutcomes in geodesic metric spaces, such as distributions, networks, and\nmanifold-valued data. To address the absence of algebraic operations in these\nspaces, we use geodesics as proxies for differences and introduce the geodesic\naverage treatment effect on the treated (ATT) as the causal estimand. We\nestablish the identification of the geodesic ATT and derive the convergence\nrate of its sample versions, employing tools from metric geometry and empirical\nprocess theory. This framework is further extended to the case of staggered DID\nsettings, allowing for multiple time periods and varying treatment timings. To\nillustrate the practical utility of geodesic DID, we analyze health impacts of\nthe Soviet Union's collapse using age-at-death distributions and assess effects\nof U.S. electricity market liberalization on electricity generation\ncompositions."
                },
                "authors": [
                    {
                        "name": "Yidong Zhou"
                    },
                    {
                        "name": "Daisuke Kurisu"
                    },
                    {
                        "name": "Taisuke Otsu"
                    },
                    {
                        "name": "Hans-Georg Müller"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Georg Müller"
                },
                "author": "Hans-Georg Müller",
                "arxiv_comment": "47 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20, 62R20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17433v1",
                "updated": "2025-01-29T06:24:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    24,
                    58,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T06:24:58Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    24,
                    58,
                    2,
                    29,
                    0
                ],
                "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation"
                },
                "summary": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07806v2",
                "updated": "2025-01-29T06:13:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    13,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-14T20:30:34Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    30,
                    34,
                    2,
                    227,
                    0
                ],
                "title": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction"
                },
                "summary": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems."
                },
                "authors": [
                    {
                        "name": "Sadra Zargarzadeh"
                    },
                    {
                        "name": "Maryam Mirzaei"
                    },
                    {
                        "name": "Yafei Ou"
                    },
                    {
                        "name": "Mahdi Tavakoli"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Tavakoli"
                },
                "author": "Mahdi Tavakoli",
                "arxiv_doi": "10.1109/LRA.2025.3535184",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3535184",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for Publication in IEEE Robotics and Automation Letters,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01968v2",
                "updated": "2025-01-29T05:41:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    41,
                    52,
                    2,
                    29,
                    0
                ],
                "published": "2024-02-03T00:27:22Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    0,
                    27,
                    22,
                    5,
                    34,
                    0
                ],
                "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges\n  and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges\n  and Future Directions"
                },
                "summary": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field."
                },
                "authors": [
                    {
                        "name": "Hung Du"
                    },
                    {
                        "name": "Srikanth Thudumu"
                    },
                    {
                        "name": "Rajesh Vasa"
                    },
                    {
                        "name": "Kon Mouzakis"
                    }
                ],
                "author_detail": {
                    "name": "Kon Mouzakis"
                },
                "author": "Kon Mouzakis",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17420v1",
                "updated": "2025-01-29T05:21:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    21,
                    31,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T05:21:31Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    21,
                    31,
                    2,
                    29,
                    0
                ],
                "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases\n  in Language Models"
                },
                "summary": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    },
                    {
                        "name": "Sauvik Das"
                    }
                ],
                "author_detail": {
                    "name": "Sauvik Das"
                },
                "author": "Sauvik Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16456v2",
                "updated": "2025-01-29T05:15:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    15,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-27T19:29:11Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    19,
                    29,
                    11,
                    0,
                    27,
                    0
                ],
                "title": "CoCoNUT: Structural Code Understanding does not fall out of a tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoNUT: Structural Code Understanding does not fall out of a tree"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance across a wide\narray of tasks involving both structured and unstructured textual data. Recent\nresults on various benchmarks for code generation, repair, or completion\nsuggest that certain models have programming abilities comparable to or even\nsurpass humans. In this work, we demonstrate that high performance on such\nbenchmarks does not correlate to humans' innate ability to understand\nstructural control flow in code. To this end, we extract solutions from the\nHumanEval benchmark, which the relevant models perform strongly on, and trace\ntheir execution path using function calls sampled from the respective test set.\nUsing this dataset, we investigate the ability of seven state-of-the-art LLMs\nto match the execution trace and find that, despite their ability to generate\nsemantically identical code, they possess limited ability to trace execution\npaths, especially for longer traces and specific control structures. We find\nthat even the top-performing model, Gemini, can fully and correctly generate\nonly 47% of HumanEval task traces. Additionally, we introduce a subset for\nthree key structures not contained in HumanEval: Recursion, Parallel\nProcessing, and Object-Oriented Programming, including concepts like\nInheritance and Polymorphism. Besides OOP, we show that none of the\ninvestigated models achieve an accuracy over 5% on the relevant traces.\nAggregating these specialized parts with HumanEval tasks, we present CoCoNUT:\nCode Control Flow for Navigation Understanding and Testing, which measures a\nmodel's ability to trace execution of code upon relevant calls, including\nadvanced structural components. We conclude that current LLMs need significant\nimprovement to enhance code reasoning abilities. We hope our dataset helps\nresearchers bridge this gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance across a wide\narray of tasks involving both structured and unstructured textual data. Recent\nresults on various benchmarks for code generation, repair, or completion\nsuggest that certain models have programming abilities comparable to or even\nsurpass humans. In this work, we demonstrate that high performance on such\nbenchmarks does not correlate to humans' innate ability to understand\nstructural control flow in code. To this end, we extract solutions from the\nHumanEval benchmark, which the relevant models perform strongly on, and trace\ntheir execution path using function calls sampled from the respective test set.\nUsing this dataset, we investigate the ability of seven state-of-the-art LLMs\nto match the execution trace and find that, despite their ability to generate\nsemantically identical code, they possess limited ability to trace execution\npaths, especially for longer traces and specific control structures. We find\nthat even the top-performing model, Gemini, can fully and correctly generate\nonly 47% of HumanEval task traces. Additionally, we introduce a subset for\nthree key structures not contained in HumanEval: Recursion, Parallel\nProcessing, and Object-Oriented Programming, including concepts like\nInheritance and Polymorphism. Besides OOP, we show that none of the\ninvestigated models achieve an accuracy over 5% on the relevant traces.\nAggregating these specialized parts with HumanEval tasks, we present CoCoNUT:\nCode Control Flow for Navigation Understanding and Testing, which measures a\nmodel's ability to trace execution of code upon relevant calls, including\nadvanced structural components. We conclude that current LLMs need significant\nimprovement to enhance code reasoning abilities. We hope our dataset helps\nresearchers bridge this gap."
                },
                "authors": [
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Saikat Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Saikat Dutta"
                },
                "author": "Saikat Dutta",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Workshop on Large Language\n  Models for Code (LLM4Code)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17415v1",
                "updated": "2025-01-29T05:04:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    4,
                    46,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T05:04:46Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    4,
                    46,
                    2,
                    29,
                    0
                ],
                "title": "si4onnx: A Python package for Selective Inference in Deep Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "si4onnx: A Python package for Selective Inference in Deep Learning\n  Models"
                },
                "summary": "In this paper, we introduce si4onnx, a package for performing selective\ninference on deep learning models. Techniques such as CAM in XAI and\nreconstruction-based anomaly detection using VAE can be interpreted as methods\nfor identifying significant regions within input images. However, the\nidentified regions may not always carry meaningful significance. Therefore,\nevaluating the statistical significance of these regions represents a crucial\nchallenge in establishing the reliability of AI systems. si4onnx is a Python\npackage that enables straightforward implementation of hypothesis testing with\ncontrolled type I error rates through selective inference. It is compatible\nwith deep learning models constructed using common frameworks such as PyTorch\nand TensorFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce si4onnx, a package for performing selective\ninference on deep learning models. Techniques such as CAM in XAI and\nreconstruction-based anomaly detection using VAE can be interpreted as methods\nfor identifying significant regions within input images. However, the\nidentified regions may not always carry meaningful significance. Therefore,\nevaluating the statistical significance of these regions represents a crucial\nchallenge in establishing the reliability of AI systems. si4onnx is a Python\npackage that enables straightforward implementation of hypothesis testing with\ncontrolled type I error rates through selective inference. It is compatible\nwith deep learning models constructed using common frameworks such as PyTorch\nand TensorFlow."
                },
                "authors": [
                    {
                        "name": "Teruyuki Katsuoka"
                    },
                    {
                        "name": "Tomohiro Shiraishi"
                    },
                    {
                        "name": "Daiki Miwa"
                    },
                    {
                        "name": "Shuichi Nishino"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "arxiv_comment": "35pages, 3figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16692v2",
                "updated": "2025-01-29T04:36:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    36,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T04:00:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation"
                },
                "summary": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization."
                },
                "authors": [
                    {
                        "name": "Manish Acharya"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Kevin Leach"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17409v1",
                "updated": "2025-01-29T04:22:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    22,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T04:22:29Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    22,
                    29,
                    2,
                    29,
                    0
                ],
                "title": "Value Function Decomposition in Markov Recommendation Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Function Decomposition in Markov Recommendation Process"
                },
                "summary": "Recent advances in recommender systems have shown that user-system\ninteraction essentially formulates long-term optimization problems, and online\nreinforcement learning can be adopted to improve recommendation performance.\nThe general solution framework incorporates a value function that estimates the\nuser's expected cumulative rewards in the future and guides the training of the\nrecommendation policy. To avoid local maxima, the policy may explore potential\nhigh-quality actions during inference to increase the chance of finding better\nfuture rewards. To accommodate the stepwise recommendation process, one widely\nadopted approach to learning the value function is learning from the difference\nbetween the values of two consecutive states of a user. However, we argue that\nthis paradigm involves an incorrect approximation in the stochastic process.\nSpecifically, between the current state and the next state in each training\nsample, there exist two separate random factors from the stochastic policy and\nthe uncertain user environment. Original temporal difference (TD) learning\nunder these mixed random factors may result in a suboptimal estimation of the\nlong-term rewards. As a solution, we show that these two factors can be\nseparately approximated by decomposing the original temporal difference loss.\nThe disentangled learning framework can achieve a more accurate estimation with\nfaster learning and improved robustness against action exploration. As\nempirical verification of our proposed method, we conduct offline experiments\nwith online simulated environments built based on public datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in recommender systems have shown that user-system\ninteraction essentially formulates long-term optimization problems, and online\nreinforcement learning can be adopted to improve recommendation performance.\nThe general solution framework incorporates a value function that estimates the\nuser's expected cumulative rewards in the future and guides the training of the\nrecommendation policy. To avoid local maxima, the policy may explore potential\nhigh-quality actions during inference to increase the chance of finding better\nfuture rewards. To accommodate the stepwise recommendation process, one widely\nadopted approach to learning the value function is learning from the difference\nbetween the values of two consecutive states of a user. However, we argue that\nthis paradigm involves an incorrect approximation in the stochastic process.\nSpecifically, between the current state and the next state in each training\nsample, there exist two separate random factors from the stochastic policy and\nthe uncertain user environment. Original temporal difference (TD) learning\nunder these mixed random factors may result in a suboptimal estimation of the\nlong-term rewards. As a solution, we show that these two factors can be\nseparately approximated by decomposing the original temporal difference loss.\nThe disentangled learning framework can achieve a more accurate estimation with\nfaster learning and improved robustness against action exploration. As\nempirical verification of our proposed method, we conduct offline experiments\nwith online simulated environments built based on public datasets."
                },
                "authors": [
                    {
                        "name": "Xiaobei Wang"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Han li"
                    },
                    {
                        "name": "Guangming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Xie"
                },
                "author": "Guangming Xie",
                "arxiv_doi": "10.1145/3696410.3714807",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714807",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.17860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17860v1",
                "updated": "2025-01-29T18:58:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    58,
                    48,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:58:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    58,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "Dialogue is Better Than Monologue: Instructing Medical LLMs via\n  Strategical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue is Better Than Monologue: Instructing Medical LLMs via\n  Strategical Conversations"
                },
                "summary": "Current medical AI systems often fail to replicate real-world clinical\nreasoning, as they are predominantly trained and evaluated on static text and\nquestion-answer tasks. These tuning methods and benchmarks overlook critical\naspects like evidence-based reasoning and handling distracting information. To\nbridge this gap, we introduce a novel benchmark that simulates real-world\ndiagnostic scenarios, integrating noise and difficulty levels aligned with\nUSMLE standards. Moreover, we explore dialogue-based fine-tuning, which\ntransforms static datasets into conversational formats to better capture\niterative reasoning processes. Experiments show that dialogue-tuned models\noutperform traditional methods, with improvements of $9.64\\%$ in multi-round\nreasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our\nfindings highlight dialogue tuning as a promising approach for advancing\nclinically aligned and robust medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current medical AI systems often fail to replicate real-world clinical\nreasoning, as they are predominantly trained and evaluated on static text and\nquestion-answer tasks. These tuning methods and benchmarks overlook critical\naspects like evidence-based reasoning and handling distracting information. To\nbridge this gap, we introduce a novel benchmark that simulates real-world\ndiagnostic scenarios, integrating noise and difficulty levels aligned with\nUSMLE standards. Moreover, we explore dialogue-based fine-tuning, which\ntransforms static datasets into conversational formats to better capture\niterative reasoning processes. Experiments show that dialogue-tuned models\noutperform traditional methods, with improvements of $9.64\\%$ in multi-round\nreasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our\nfindings highlight dialogue tuning as a promising approach for advancing\nclinically aligned and robust medical AI systems."
                },
                "authors": [
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhuangdi Zhu"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17858v1",
                "updated": "2025-01-29T18:57:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    57,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:57:29Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    57,
                    29,
                    2,
                    29,
                    0
                ],
                "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging"
                },
                "summary": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,\nwhere users vote for their preferred response from two randomly sampled\nanonymous models. While Chatbot Arena is widely regarded as a reliable LLM\nranking leaderboard, we show that crowdsourced voting can be rigged to improve\n(or decrease) the ranking of a target model $m_{t}$. We first introduce a\nstraightforward target-only rigging strategy that focuses on new battles\ninvolving $m_{t}$, identifying it via watermarking or a binary classifier, and\nexclusively voting for $m_{t}$ wins. However, this strategy is practically\ninefficient because there are over $190$ models on Chatbot Arena and on average\nonly about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we\npropose omnipresent rigging strategies, exploiting the Elo rating mechanism of\nChatbot Arena that any new vote on a battle can influence the ranking of the\ntarget model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.\nWe conduct experiments on around $1.7$ million historical votes from the\nChatbot Arena Notebook, showing that omnipresent rigging strategies can improve\nmodel rankings by rigging only hundreds of new votes. While we have evaluated\nseveral defense mechanisms, our findings highlight the importance of continued\nefforts to prevent vote rigging. Our code is available at\nhttps://github.com/sail-sg/Rigging-ChatbotArena."
                },
                "authors": [
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17148v2",
                "updated": "2025-01-29T18:52:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    52,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T18:51:24Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    51,
                    24,
                    1,
                    28,
                    0
                ],
                "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders"
                },
                "summary": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean."
                },
                "authors": [
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v3",
                "updated": "2025-01-29T18:49:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    49,
                    22,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large\n  Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker incurs on average 7%\noverhead on training while detecting and correcting all extreme errors.\nCompared with the state-of-the-art checkpoint/restore approach, ATTNChecker\nreduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints. To mitigate the\nimpact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker incurs on average 7%\noverhead on training while detecting and correcting all extreme errors.\nCompared with the state-of-the-art checkpoint/restore approach, ATTNChecker\nreduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15004v2",
                "updated": "2025-01-29T18:49:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    49,
                    15,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-19T16:20:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "Large Language Models and Code Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Code Security: A Systematic Literature Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks."
                },
                "authors": [
                    {
                        "name": "Enna Basic"
                    },
                    {
                        "name": "Alberto Giaretta"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Giaretta"
                },
                "author": "Alberto Giaretta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00400v2",
                "updated": "2025-01-29T18:46:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    46,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-01T04:53:09Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    4,
                    53,
                    9,
                    1,
                    275,
                    0
                ],
                "title": "DynEx: Dynamic Code Synthesis with Structured Design Exploration for\n  Accelerated Exploratory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynEx: Dynamic Code Synthesis with Structured Design Exploration for\n  Accelerated Exploratory Programming"
                },
                "summary": "Recent advancements in large language models have significantly expedited the\nprocess of generating front-end code. This allows users to rapidly prototype\nuser interfaces and ideate through code, a process known as exploratory\nprogramming. However, existing LLM code generation tools focus more on\ntechnical implementation details rather than finding the right design given a\nparticular problem. We present DynEx, an LLM-based method for design\nexploration in accelerated exploratory programming. DynEx introduces a\ntechnique to explore the design space through a structured Design Matrix before\ncreating the prototype with a modular, stepwise approach to LLM code\ngeneration. Code is generated sequentially, and users can test and approve each\nstep before moving onto the next. A user study of 10 experts found that DynEx\nincreased design exploration and enabled the creation of more complex and\nvaried prototypes compared to a Claude Artifact baseline. We conclude with a\ndiscussion of the implications of design exploration for exploratory\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly expedited the\nprocess of generating front-end code. This allows users to rapidly prototype\nuser interfaces and ideate through code, a process known as exploratory\nprogramming. However, existing LLM code generation tools focus more on\ntechnical implementation details rather than finding the right design given a\nparticular problem. We present DynEx, an LLM-based method for design\nexploration in accelerated exploratory programming. DynEx introduces a\ntechnique to explore the design space through a structured Design Matrix before\ncreating the prototype with a modular, stepwise approach to LLM code\ngeneration. Code is generated sequentially, and users can test and approve each\nstep before moving onto the next. A user study of 10 experts found that DynEx\nincreased design exploration and enabled the creation of more complex and\nvaried prototypes compared to a Claude Artifact baseline. We conclude with a\ndiscussion of the implications of design exploration for exploratory\nprogramming."
                },
                "authors": [
                    {
                        "name": "Jenny Ma"
                    },
                    {
                        "name": "Karthik Sreedhar"
                    },
                    {
                        "name": "Vivian Liu"
                    },
                    {
                        "name": "Pedro Alejandro Perez"
                    },
                    {
                        "name": "Sitong Wang"
                    },
                    {
                        "name": "Riya Sahni"
                    },
                    {
                        "name": "Lydia B. Chilton"
                    }
                ],
                "author_detail": {
                    "name": "Lydia B. Chilton"
                },
                "author": "Lydia B. Chilton",
                "arxiv_comment": "27 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17841v1",
                "updated": "2025-01-29T18:44:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    44,
                    48,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:44:48Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    44,
                    48,
                    2,
                    29,
                    0
                ],
                "title": "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices"
                },
                "summary": "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https://github.com/acoupi/acoupi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https://github.com/acoupi/acoupi."
                },
                "authors": [
                    {
                        "name": "Aude Vuilliomenet"
                    },
                    {
                        "name": "Santiago Martínez Balvanera"
                    },
                    {
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "name": "Kate E. Jones"
                    },
                    {
                        "name": "Duncan Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Duncan Wilson"
                },
                "author": "Duncan Wilson",
                "arxiv_comment": "21 pages, 3 figures, 1 table, to be submitted to BES Methods in\n  Ecology and Evolution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.5; I.2.m; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17840v1",
                "updated": "2025-01-29T18:40:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T18:40:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    40,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Learning Beyond the Surface: How Far Can Continual Pre-Training with\n  LoRA Enhance LLMs' Domain-Specific Insight Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Beyond the Surface: How Far Can Continual Pre-Training with\n  LoRA Enhance LLMs' Domain-Specific Insight Learning?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious tasks, yet their ability to extract and internalize deeper insights\nfrom domain-specific datasets remains underexplored. In this study, we\ninvestigate how continual pre-training can enhance LLMs' capacity for insight\nlearning across three distinct forms: declarative, statistical, and\nprobabilistic insights. Focusing on two critical domains: medicine and finance,\nwe employ LoRA to train LLMs on two existing datasets. To evaluate each insight\ntype, we create benchmarks to measure how well continual pre-training helps\nmodels go beyond surface-level knowledge. We also assess the impact of document\nmodification on capturing insights. The results show that, while continual\npre-training on original documents has a marginal effect, modifying documents\nto retain only essential information significantly enhances the\ninsight-learning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious tasks, yet their ability to extract and internalize deeper insights\nfrom domain-specific datasets remains underexplored. In this study, we\ninvestigate how continual pre-training can enhance LLMs' capacity for insight\nlearning across three distinct forms: declarative, statistical, and\nprobabilistic insights. Focusing on two critical domains: medicine and finance,\nwe employ LoRA to train LLMs on two existing datasets. To evaluate each insight\ntype, we create benchmarks to measure how well continual pre-training helps\nmodels go beyond surface-level knowledge. We also assess the impact of document\nmodification on capturing insights. The results show that, while continual\npre-training on original documents has a marginal effect, modifying documents\nto retain only essential information significantly enhances the\ninsight-learning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11423v2",
                "updated": "2025-01-29T18:27:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    18,
                    27,
                    16,
                    2,
                    29,
                    0
                ],
                "published": "2024-09-12T10:14:12Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    14,
                    12,
                    3,
                    256,
                    0
                ],
                "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large\n  Language Models on Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large\n  Language Models on Generated Data"
                },
                "summary": "Large language models (LLMs) have demonstrated significant success in various\ndomain-specific tasks, with their performance often improving substantially\nafter fine-tuning. However, fine-tuning with real-world data introduces privacy\nrisks. To mitigate these risks, developers increasingly rely on synthetic data\ngeneration as an alternative to using real data, as data generated by\ntraditional models is believed to be different from real-world data. However,\nwith the advanced capabilities of LLMs, the distinction between real data and\ndata generated by these models has become nearly indistinguishable. This\nconvergence introduces similar privacy risks for generated data to those\nassociated with real data. Our study investigates whether fine-tuning with\nLLM-generated data truly enhances privacy or introduces additional privacy\nrisks by examining the structural characteristics of data generated by LLMs,\nfocusing on two primary fine-tuning approaches: supervised fine-tuning (SFT)\nwith unstructured (plain-text) generated data and self-instruct tuning. In the\nscenario of SFT, the data is put into a particular instruction tuning format\nused by previous studies. We use Personal Information Identifier (PII) leakage\nand Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open\nPre-trained Transformer (OPT) to measure privacy risks. Notably, after\nfine-tuning with unstructured generated data, the rate of successful PII\nextractions for Pythia increased by over 20%, highlighting the potential\nprivacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs\nfor Pythia-6.9b, the second biggest model of the suite, increases over 40%\nafter self-instruct tuning. Our results indicate the potential privacy risks\nassociated with fine-tuning LLMs using generated data, underscoring the need\nfor careful consideration of privacy safeguards in such approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant success in various\ndomain-specific tasks, with their performance often improving substantially\nafter fine-tuning. However, fine-tuning with real-world data introduces privacy\nrisks. To mitigate these risks, developers increasingly rely on synthetic data\ngeneration as an alternative to using real data, as data generated by\ntraditional models is believed to be different from real-world data. However,\nwith the advanced capabilities of LLMs, the distinction between real data and\ndata generated by these models has become nearly indistinguishable. This\nconvergence introduces similar privacy risks for generated data to those\nassociated with real data. Our study investigates whether fine-tuning with\nLLM-generated data truly enhances privacy or introduces additional privacy\nrisks by examining the structural characteristics of data generated by LLMs,\nfocusing on two primary fine-tuning approaches: supervised fine-tuning (SFT)\nwith unstructured (plain-text) generated data and self-instruct tuning. In the\nscenario of SFT, the data is put into a particular instruction tuning format\nused by previous studies. We use Personal Information Identifier (PII) leakage\nand Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open\nPre-trained Transformer (OPT) to measure privacy risks. Notably, after\nfine-tuning with unstructured generated data, the rate of successful PII\nextractions for Pythia increased by over 20%, highlighting the potential\nprivacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs\nfor Pythia-6.9b, the second biggest model of the suite, increases over 40%\nafter self-instruct tuning. Our results indicate the potential privacy risks\nassociated with fine-tuning LLMs using generated data, underscoring the need\nfor careful consideration of privacy safeguards in such approaches."
                },
                "authors": [
                    {
                        "name": "Atilla Akkus"
                    },
                    {
                        "name": "Masoud Poorghaffar Aghdam"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Sinem Sav"
                    }
                ],
                "author_detail": {
                    "name": "Sinem Sav"
                },
                "author": "Sinem Sav",
                "arxiv_comment": "Accepted at 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17802v1",
                "updated": "2025-01-29T17:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    44,
                    57,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:44:57Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    44,
                    57,
                    2,
                    29,
                    0
                ],
                "title": "LEKA:LLM-Enhanced Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEKA:LLM-Enhanced Knowledge Augmentation"
                },
                "summary": "Humans excel in analogical learning and knowledge transfer and, more\nimportantly, possess a unique understanding of identifying appropriate sources\nof knowledge. From a model's perspective, this presents an interesting\nchallenge. If models could autonomously retrieve knowledge useful for transfer\nor decision-making to solve problems, they would transition from passively\nacquiring to actively accessing and learning from knowledge. However, filling\nmodels with knowledge is relatively straightforward -- it simply requires more\ntraining and accessible knowledge bases. The more complex task is teaching\nmodels about which knowledge can be analogized and transferred. Therefore, we\ndesign a knowledge augmentation method LEKA for knowledge transfer that\nactively searches for suitable knowledge sources that can enrich the target\ndomain's knowledge. This LEKA method extracts key information from textual\ninformation from the target domain, retrieves pertinent data from external data\nlibraries, and harmonizes retrieved data with the target domain data in feature\nspace and marginal probability measures. We validate the effectiveness of our\napproach through extensive experiments across various domains and demonstrate\nsignificant improvements over traditional methods in reducing computational\ncosts, automating data alignment, and optimizing transfer learning outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans excel in analogical learning and knowledge transfer and, more\nimportantly, possess a unique understanding of identifying appropriate sources\nof knowledge. From a model's perspective, this presents an interesting\nchallenge. If models could autonomously retrieve knowledge useful for transfer\nor decision-making to solve problems, they would transition from passively\nacquiring to actively accessing and learning from knowledge. However, filling\nmodels with knowledge is relatively straightforward -- it simply requires more\ntraining and accessible knowledge bases. The more complex task is teaching\nmodels about which knowledge can be analogized and transferred. Therefore, we\ndesign a knowledge augmentation method LEKA for knowledge transfer that\nactively searches for suitable knowledge sources that can enrich the target\ndomain's knowledge. This LEKA method extracts key information from textual\ninformation from the target domain, retrieves pertinent data from external data\nlibraries, and harmonizes retrieved data with the target domain data in feature\nspace and marginal probability measures. We validate the effectiveness of our\napproach through extensive experiments across various domains and demonstrate\nsignificant improvements over traditional methods in reducing computational\ncosts, automating data alignment, and optimizing transfer learning outcomes."
                },
                "authors": [
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Dongjie Wang"
                    },
                    {
                        "name": "Yanjie Fu"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17799v1",
                "updated": "2025-01-29T17:38:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    38,
                    39,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:38:39Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    38,
                    39,
                    2,
                    29,
                    0
                ],
                "title": "Leveraging Multimodal LLM for Inspirational User Interface Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Multimodal LLM for Inspirational User Interface Search"
                },
                "summary": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies."
                },
                "authors": [
                    {
                        "name": "Seokhyeon Park"
                    },
                    {
                        "name": "Yumin Song"
                    },
                    {
                        "name": "Soohyun Lee"
                    },
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "arxiv_doi": "10.1145/3706598.3714213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems (CHI '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17790v1",
                "updated": "2025-01-29T17:31:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    31,
                    26,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:31:26Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    31,
                    26,
                    2,
                    29,
                    0
                ],
                "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone\n  Disambiguation -- Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone\n  Disambiguation -- Challenges and Insights"
                },
                "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted\nfor Taiwanese Mandarin, highlighting phonetic control abilities to address the\nunique challenges of polyphone disambiguation in the language. Building upon\nCosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an\noptimal-transport conditional flow matching model (OT-CFM), and a grapheme to\nphoneme prediction model, to generate realistic speech that closely mimics\nhuman utterances. Our evaluation demonstrates BreezyVoice's superior\nperformance in both general and code-switching contexts, highlighting its\nrobustness and effectiveness in generating high-fidelity speech. Additionally,\nwe address the challenges of generalizability in modeling long-tail speakers\nand polyphone disambiguation. Our approach significantly enhances performance\nand offers valuable insights into the workings of neural codec TTS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted\nfor Taiwanese Mandarin, highlighting phonetic control abilities to address the\nunique challenges of polyphone disambiguation in the language. Building upon\nCosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an\noptimal-transport conditional flow matching model (OT-CFM), and a grapheme to\nphoneme prediction model, to generate realistic speech that closely mimics\nhuman utterances. Our evaluation demonstrates BreezyVoice's superior\nperformance in both general and code-switching contexts, highlighting its\nrobustness and effectiveness in generating high-fidelity speech. Additionally,\nwe address the challenges of generalizability in modeling long-tail speakers\nand polyphone disambiguation. Our approach significantly enhances performance\nand offers valuable insights into the workings of neural codec TTS systems."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Chia-Chun Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Chien-Yu Yu"
                    },
                    {
                        "name": "Ming-Ji Lee"
                    },
                    {
                        "name": "Chien-Cheng Chen"
                    },
                    {
                        "name": "Ru-Heng Huang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17785v1",
                "updated": "2025-01-29T17:24:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    19,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:24:19Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    19,
                    2,
                    29,
                    0
                ],
                "title": "Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare\n  Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare\n  Scripts"
                },
                "summary": "We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not\nencoded in Unicode. We introduce a novel approach to construct a multimodal\ndataset of linguistic puzzles involving such scripts, utilizing a tokenization\nmethod for language glyphs. Our methods include the Picture Method for LVLMs\nand the Description Method for LLMs, enabling these models to tackle these\nchallenges. We conduct experiments using prominent models, GPT-4o, Gemini, and\nClaude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and\nlimitations of current AI methods in linguistic decipherment, highlighting the\nimpact of Unicode encoding on model performance and the challenges of modeling\nvisual language tokens through descriptions. Our study advances understanding\nof AI's potential in linguistic decipherment and underscores the need for\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not\nencoded in Unicode. We introduce a novel approach to construct a multimodal\ndataset of linguistic puzzles involving such scripts, utilizing a tokenization\nmethod for language glyphs. Our methods include the Picture Method for LVLMs\nand the Description Method for LLMs, enabling these models to tackle these\nchallenges. We conduct experiments using prominent models, GPT-4o, Gemini, and\nClaude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and\nlimitations of current AI methods in linguistic decipherment, highlighting the\nimpact of Unicode encoding on model performance and the challenges of modeling\nvisual language tokens through descriptions. Our study advances understanding\nof AI's potential in linguistic decipherment and underscores the need for\nfurther research."
                },
                "authors": [
                    {
                        "name": "Yu-Fei Shih"
                    },
                    {
                        "name": "Zheng-Lin Lin"
                    },
                    {
                        "name": "Shu-Kai Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Kai Hsieh"
                },
                "author": "Shu-Kai Hsieh",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11891v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11891v3",
                "updated": "2025-01-29T17:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    24,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-18T04:36:37Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    4,
                    36,
                    37,
                    3,
                    109,
                    0
                ],
                "title": "Large Language Models Can Solve Real-World Planning Rigorously with\n  Formal Verification Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Can Solve Real-World Planning Rigorously with\n  Formal Verification Tools"
                },
                "summary": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle to directly generate correct plans for\ncomplex multi-constraint planning problems, even with self-verification and\nself-critique. For example, a U.S. domestic travel planning benchmark\nTravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI\no1-preview can only find viable travel plans with a 10% success rate given all\nneeded information. In this work, we tackle this by proposing an LLM-based\nplanning framework that formalizes and solves complex multi-constraint planning\nproblems as constrained satisfiability problems, which are further consumed by\nsound and complete satisfiability solvers. We start with TravelPlanner as the\nprimary use case and show that our framework achieves a success rate of 93.9%\nand is effective with diverse paraphrased prompts. More importantly, our\nframework has strong zero-shot generalizability, successfully handling unseen\nconstraints in our newly created unseen international travel dataset and\ngeneralizing well to new fundamentally different domains. Moreover, when user\ninput queries are infeasible, our framework can identify the unsatisfiable\ncore, provide failure reasons, and offers personalized modification\nsuggestions. We show that our framework can modify and solve for an average of\n81.6% and 91.7% unsatisfiable queries from two datasets and prove with\nablations that all key components of our framework are effective and necessary.\nProject page: https://sites.google.com/view/llm-rwplanning."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "50 pages, 6 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11891v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11891v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17773v1",
                "updated": "2025-01-29T17:09:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    9,
                    28,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:09:28Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    9,
                    28,
                    2,
                    29,
                    0
                ],
                "title": "SafePR: Unified Approach for Safe Parallel Robots by Contact Detection\n  and Reaction with Redundancy Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePR: Unified Approach for Safe Parallel Robots by Contact Detection\n  and Reaction with Redundancy Resolution"
                },
                "summary": "Fast and safe motion is crucial for the successful deployment of physically\ninteractive robots. Parallel robots (PRs) offer the potential for higher speeds\nwhile maintaining the same energy limits due to their low moving masses.\nHowever, they require methods for contact detection and reaction while avoiding\nsingularities and self-collisions. We address this issue and present SafePR - a\nunified approach for the detection and localization, including the distinction\nbetween collision and clamping to perform a reaction that is safe for humans\nand feasible for PRs. Our approach uses information from the encoders and motor\ncurrents to estimate forces via a generalized-momentum observer. Neural\nnetworks and particle filters classify and localize the contacts. We introduce\nreactions with redundancy resolution to avoid type-II singularities and\nself-collisions. Our approach detected and terminated 72 real-world collision\nand clamping contacts with end-effector speeds of up to 1.5 m/s, each within\n25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using\nbuilt-in sensors, SafePR enables safe interaction with already assembled PRs\nwithout the need for new hardware components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and safe motion is crucial for the successful deployment of physically\ninteractive robots. Parallel robots (PRs) offer the potential for higher speeds\nwhile maintaining the same energy limits due to their low moving masses.\nHowever, they require methods for contact detection and reaction while avoiding\nsingularities and self-collisions. We address this issue and present SafePR - a\nunified approach for the detection and localization, including the distinction\nbetween collision and clamping to perform a reaction that is safe for humans\nand feasible for PRs. Our approach uses information from the encoders and motor\ncurrents to estimate forces via a generalized-momentum observer. Neural\nnetworks and particle filters classify and localize the contacts. We introduce\nreactions with redundancy resolution to avoid type-II singularities and\nself-collisions. Our approach detected and terminated 72 real-world collision\nand clamping contacts with end-effector speeds of up to 1.5 m/s, each within\n25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using\nbuilt-in sensors, SafePR enables safe interaction with already assembled PRs\nwithout the need for new hardware components."
                },
                "authors": [
                    {
                        "name": "Aran Mohammad"
                    },
                    {
                        "name": "Tim-Lukas Habich"
                    },
                    {
                        "name": "Thomas Seel"
                    },
                    {
                        "name": "Moritz Schappler"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Schappler"
                },
                "author": "Moritz Schappler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17771v1",
                "updated": "2025-01-29T17:05:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    5,
                    33,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T17:05:33Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    5,
                    33,
                    2,
                    29,
                    0
                ],
                "title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs"
                },
                "summary": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for\npruning Large Language Models (LLMs), which combines two different strategies\nof pruning, namely Width and Depth Pruning. The first stage (Width Pruning)\nremoves entire neurons, hence their corresponding rows and columns, aiming to\npreserve the connectivity among the pruned structures in the intermediate state\nof the Feed-Forward Networks in each Transformer block. This is done based on\nan importance score measuring the impact of each neuron over the output\nmagnitude. The second stage (Depth Pruning), instead, removes entire Attention\nsubmodules. This is done by applying an iterative process that removes the\nAttention submodules with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%),\nmeasuring the resulting perplexity over three language modeling datasets as\nwell as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at available at\n\\url{https://github.com/FabrizioSandri/2SSP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for\npruning Large Language Models (LLMs), which combines two different strategies\nof pruning, namely Width and Depth Pruning. The first stage (Width Pruning)\nremoves entire neurons, hence their corresponding rows and columns, aiming to\npreserve the connectivity among the pruned structures in the intermediate state\nof the Feed-Forward Networks in each Transformer block. This is done based on\nan importance score measuring the impact of each neuron over the output\nmagnitude. The second stage (Depth Pruning), instead, removes entire Attention\nsubmodules. This is done by applying an iterative process that removes the\nAttention submodules with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%),\nmeasuring the resulting perplexity over three language modeling datasets as\nwell as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at available at\n\\url{https://github.com/FabrizioSandri/2SSP}."
                },
                "authors": [
                    {
                        "name": "Fabrizio Sandri"
                    },
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17767v1",
                "updated": "2025-01-29T16:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    58,
                    18,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    58,
                    18,
                    2,
                    29,
                    0
                ],
                "title": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs"
                },
                "summary": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context."
                },
                "authors": [
                    {
                        "name": "Ankush Agarwal"
                    },
                    {
                        "name": "Ganesh S"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    }
                ],
                "author_detail": {
                    "name": "Chaitanya Devaguptapu"
                },
                "author": "Chaitanya Devaguptapu",
                "arxiv_comment": "Accepted at NAACL 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v3",
                "updated": "2025-01-29T16:57:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    57,
                    35,
                    2,
                    29,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. Because of their crucial impact on\nthe utility, they are often considered intellectual property, similar to the\ncode of a software product. However, extracting system prompts is easily\npossible. As of today, there is no effective countermeasure to prevent the\nstealing of system prompts and all safeguarding efforts could be evaded. In\nthis work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith only little overhead. The core idea is to find a representation of the\noriginal system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We evaluate our\napproach by comparing our obfuscated prompt output with the output of the\noriginal prompt, using eight distinct metrics, to measure the lexical,\ncharacter-level, and semantic similarity. We show that the obfuscated version\nis constantly on par with the original one. We further perform three different\ndeobfuscation attacks with varying attacker knowledge--covering both black-box\nand white-box conditions--and show that in realistic attack scenarios an\nattacker is not able to extract meaningful information. Overall, we demonstrate\nthat prompt obfuscation is an effective mechanism to safeguard the intellectual\nproperty of a system prompt while maintaining the same utility as the original\nprompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying LLM can easily transform foundation models into\ntools and services with minimal overhead. Because of their crucial impact on\nthe utility, they are often considered intellectual property, similar to the\ncode of a software product. However, extracting system prompts is easily\npossible. As of today, there is no effective countermeasure to prevent the\nstealing of system prompts and all safeguarding efforts could be evaded. In\nthis work, we propose an alternative to conventional system prompts. We\nintroduce prompt obfuscation to prevent the extraction of the system prompt\nwith only little overhead. The core idea is to find a representation of the\noriginal system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We evaluate our\napproach by comparing our obfuscated prompt output with the output of the\noriginal prompt, using eight distinct metrics, to measure the lexical,\ncharacter-level, and semantic similarity. We show that the obfuscated version\nis constantly on par with the original one. We further perform three different\ndeobfuscation attacks with varying attacker knowledge--covering both black-box\nand white-box conditions--and show that in realistic attack scenarios an\nattacker is not able to extract meaningful information. Overall, we demonstrate\nthat prompt obfuscation is an effective mechanism to safeguard the intellectual\nproperty of a system prompt while maintaining the same utility as the original\nprompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Sina Mavali"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schönherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schönherr"
                },
                "author": "Lea Schönherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05247v2",
                "updated": "2025-01-29T16:52:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    52,
                    16,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-09T13:57:09Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    13,
                    57,
                    9,
                    3,
                    9,
                    0
                ],
                "title": "Online Prompt Selection for Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Prompt Selection for Program Synthesis"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2% more queries than the best single solver and\nachieves results within 4% of the virtual best solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities in the\ndomain of program synthesis. This level of performance is not, however,\nuniversal across all tasks, all LLMs and all prompting styles. There are many\nareas where one LLM dominates, one prompting style dominates, or where calling\na symbolic solver is a better choice than an LLM. A key challenge for the user\nthen, is to identify not only when an LLM is the right choice of solver, and\nthe appropriate LLM to call for a given synthesis task, but also the right way\nto call it. A non-expert user who makes the wrong choice, incurs a cost both in\nterms of results (number of tasks solved, and the time it takes to solve them)\nand financial cost, if using a closed-source language model via a commercial\nAPI. We frame this choice as an online learning problem. We use a multi-armed\nbandit algorithm to select which symbolic solver, or LLM and prompt combination\nto deploy in order to maximize a given reward function (which may prioritize\nsolving time, number of synthesis tasks solved, or financial cost of solving).\nWe implement an instance of this approach, called CYANEA, and evaluate it on\nsynthesis queries from the literature in ranking function synthesis, from the\nsyntax-guided synthesis competition, and fresh, unseen queries generated from\nSMT problems. CYANEA solves 37.2% more queries than the best single solver and\nachieves results within 4% of the virtual best solver."
                },
                "authors": [
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Lewis Frampton"
                    },
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth Polgreen"
                },
                "author": "Elizabeth Polgreen",
                "arxiv_comment": "Accepted at the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17749v1",
                "updated": "2025-01-29T16:36:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    36,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:36:53Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    36,
                    53,
                    2,
                    29,
                    0
                ],
                "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the\n  Pre-Deployment Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early External Safety Testing of OpenAI's o3-mini: Insights from the\n  Pre-Deployment Evaluation"
                },
                "summary": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM."
                },
                "authors": [
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Segura"
                },
                "author": "Sergio Segura",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2501.17132",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12112v2",
                "updated": "2025-01-29T16:31:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    31,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-15T23:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    23,
                    20,
                    54,
                    1,
                    289,
                    0
                ],
                "title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming"
                },
                "summary": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "57 pages, 25 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20207v2",
                "updated": "2025-01-29T16:21:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    21,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2024-10-26T15:53:25Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    15,
                    53,
                    25,
                    5,
                    300,
                    0
                ],
                "title": "Revisiting Differential Verification: Equivalence Verification with\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Differential Verification: Equivalence Verification with\n  Confidence"
                },
                "summary": "When validated neural networks (NNs) are pruned (and retrained) before\ndeployment, it is desirable to prove that the new NN behaves equivalently to\nthe (original) reference NN. To this end, our paper revisits the idea of\ndifferential verification which performs reasoning on differences between NNs:\nOn the one hand, our paper proposes a novel abstract domain for differential\nverification admitting more efficient reasoning about equivalence. On the other\nhand, we investigate empirically and theoretically which equivalence properties\nare (not) efficiently solved using differential reasoning. Based on the gained\ninsights, and following a recent line of work on confidence-based verification,\nwe propose a novel equivalence property that is amenable to Differential\nVerification while providing guarantees for large parts of the input space\ninstead of small-scale guarantees constructed w.r.t. predetermined input\npoints. We implement our approach in a new tool called VeryDiff and perform an\nextensive evaluation on numerous old and new benchmark families, including new\npruned NNs for particle jet classification in the context of CERN's LHC where\nwe observe median speedups >300x over the State-of-the-Art verifier\nalpha,beta-CROWN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When validated neural networks (NNs) are pruned (and retrained) before\ndeployment, it is desirable to prove that the new NN behaves equivalently to\nthe (original) reference NN. To this end, our paper revisits the idea of\ndifferential verification which performs reasoning on differences between NNs:\nOn the one hand, our paper proposes a novel abstract domain for differential\nverification admitting more efficient reasoning about equivalence. On the other\nhand, we investigate empirically and theoretically which equivalence properties\nare (not) efficiently solved using differential reasoning. Based on the gained\ninsights, and following a recent line of work on confidence-based verification,\nwe propose a novel equivalence property that is amenable to Differential\nVerification while providing guarantees for large parts of the input space\ninstead of small-scale guarantees constructed w.r.t. predetermined input\npoints. We implement our approach in a new tool called VeryDiff and perform an\nextensive evaluation on numerous old and new benchmark families, including new\npruned NNs for particle jet classification in the context of CERN's LHC where\nwe observe median speedups >300x over the State-of-the-Art verifier\nalpha,beta-CROWN."
                },
                "authors": [
                    {
                        "name": "Samuel Teuber"
                    },
                    {
                        "name": "Philipp Kern"
                    },
                    {
                        "name": "Marvin Janzen"
                    },
                    {
                        "name": "Bernhard Beckert"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Beckert"
                },
                "author": "Bernhard Beckert",
                "arxiv_comment": "Accepted at TACAS 2025, 31st International Conference on Tools and\n  Algorithms for the Construction and Analysis of Systems; 47 pages (main paper\n  has 16 pages); 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17732v1",
                "updated": "2025-01-29T16:15:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    15,
                    31,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T16:15:31Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    15,
                    31,
                    2,
                    29,
                    0
                ],
                "title": "Gateways for Institutional-Grade Commerce and Interoperability of\n  Digital Assets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gateways for Institutional-Grade Commerce and Interoperability of\n  Digital Assets"
                },
                "summary": "It is time for the legacy financial infrastructure to seamlessly connect with\nmodern, decentralized infrastructure. Although it is increasingly evident that\ndecentralized infrastructure for finance (namely distributed ledgers) will\ncoexist with and complement legacy infrastructure, it is also clear that such\ninteroperability efforts carry new risks and concerns. In particular, managing\nthe range of heterogeneous (and not well-established) infrastructure brings\nsecurity, privacy, and regulatory issues. The first step to overcome some of\nthese challenges is to recognize that in many deployment instances using\ndistributed ledgers, the purpose of the ledger is to share resources among the\ncommunity members. The second step after recognizing that borders exist is to\nunderstand that interoperability across systems can be best achieved through\nthe use of standardized service interfaces (or application programming\ninterfaces (API)). In this paper we use the term ledger gateways (or simply\ngateways) to denote the computer and software systems that implement the\nstandardized service interfaces into a distributed ledger. The main purpose of\na gateway is to communicate with other peer gateways that implement the same\nstandardized service interface. Among others, peer gateways perform the\ntransfer of data and value across borders (legal or national borders). Gateways\nalso become a mechanism to manage a permissioned environment, where abiding by\nlaws and regulations is crucial for business compliance (e.g., EU General Data\nProtection Regulations (GDPR), EU MiCa regulation on digital assets, FAFT\nRecommendation 15, ISO 27001.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is time for the legacy financial infrastructure to seamlessly connect with\nmodern, decentralized infrastructure. Although it is increasingly evident that\ndecentralized infrastructure for finance (namely distributed ledgers) will\ncoexist with and complement legacy infrastructure, it is also clear that such\ninteroperability efforts carry new risks and concerns. In particular, managing\nthe range of heterogeneous (and not well-established) infrastructure brings\nsecurity, privacy, and regulatory issues. The first step to overcome some of\nthese challenges is to recognize that in many deployment instances using\ndistributed ledgers, the purpose of the ledger is to share resources among the\ncommunity members. The second step after recognizing that borders exist is to\nunderstand that interoperability across systems can be best achieved through\nthe use of standardized service interfaces (or application programming\ninterfaces (API)). In this paper we use the term ledger gateways (or simply\ngateways) to denote the computer and software systems that implement the\nstandardized service interfaces into a distributed ledger. The main purpose of\na gateway is to communicate with other peer gateways that implement the same\nstandardized service interface. Among others, peer gateways perform the\ntransfer of data and value across borders (legal or national borders). Gateways\nalso become a mechanism to manage a permissioned environment, where abiding by\nlaws and regulations is crucial for business compliance (e.g., EU General Data\nProtection Regulations (GDPR), EU MiCa regulation on digital assets, FAFT\nRecommendation 15, ISO 27001."
                },
                "authors": [
                    {
                        "name": "Rafael Belchior"
                    },
                    {
                        "name": "Thomas Hardjono"
                    },
                    {
                        "name": "Alex Chiriac"
                    },
                    {
                        "name": "Venkatraman Ranakrishna"
                    }
                ],
                "author_detail": {
                    "name": "Venkatraman Ranakrishna"
                },
                "author": "Venkatraman Ranakrishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17725v1",
                "updated": "2025-01-29T15:57:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    57,
                    43,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:57:43Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    57,
                    43,
                    2,
                    29,
                    0
                ],
                "title": "Using Code Generation to Solve Open Instances of Combinatorial Design\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Code Generation to Solve Open Instances of Combinatorial Design\n  Problems"
                },
                "summary": "The Handbook of Combinatorial Designs catalogs many types of combinatorial\ndesigns, together with lists of open instances for which existence has not yet\nbeen determined. We develop a constructive protocol CPro1, which uses Large\nLanguage Models (LLMs) to generate code that constructs combinatorial designs\nand resolves some of these open instances. The protocol starts from a\ndefinition of a particular type of design, and a verifier that reliably\nconfirms whether a proposed design is valid. The LLM selects strategies and\nimplements them in code, and scaffolding provides automated hyperparameter\ntuning and execution feedback using the verifier. Most generated code fails,\nbut by generating many candidates, the protocol automates exploration of a\nvariety of standard methods (e.g. simulated annealing, genetic algorithms) and\nexperimentation with variations (e.g. cost functions) to find successful\napproaches. Testing on 16 different types of designs, CPro1 constructs\nsolutions to open instances for 6 of them: Symmetric and Skew Weighing\nMatrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary\nDesigns, and Florentine Rectangles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Handbook of Combinatorial Designs catalogs many types of combinatorial\ndesigns, together with lists of open instances for which existence has not yet\nbeen determined. We develop a constructive protocol CPro1, which uses Large\nLanguage Models (LLMs) to generate code that constructs combinatorial designs\nand resolves some of these open instances. The protocol starts from a\ndefinition of a particular type of design, and a verifier that reliably\nconfirms whether a proposed design is valid. The LLM selects strategies and\nimplements them in code, and scaffolding provides automated hyperparameter\ntuning and execution feedback using the verifier. Most generated code fails,\nbut by generating many candidates, the protocol automates exploration of a\nvariety of standard methods (e.g. simulated annealing, genetic algorithms) and\nexperimentation with variations (e.g. cost functions) to find successful\napproaches. Testing on 16 different types of designs, CPro1 constructs\nsolutions to open instances for 6 of them: Symmetric and Skew Weighing\nMatrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary\nDesigns, and Florentine Rectangles."
                },
                "authors": [
                    {
                        "name": "Christopher D. Rosin"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Rosin"
                },
                "author": "Christopher D. Rosin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08561v3",
                "updated": "2025-01-29T15:41:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    41,
                    55,
                    2,
                    29,
                    0
                ],
                "published": "2024-11-13T12:18:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    18,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogLLM: Log-based Anomaly Detection Using Large Language Models"
                },
                "summary": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately."
                },
                "authors": [
                    {
                        "name": "Wei Guan"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Jianqi Gao"
                    },
                    {
                        "name": "Chun Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Ouyang"
                },
                "author": "Chun Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10996v3",
                "updated": "2025-01-29T15:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    34,
                    2,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-16T16:17:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    17,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Towards Lifelong Dialogue Agents via Timeline-based Memory Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lifelong Dialogue Agents via Timeline-based Memory Management"
                },
                "summary": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior studies focus on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present THEANINE, a framework for LLM-based lifelong dialogue\nagents. THEANINE discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, THEANINE augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts when assessing agent\nperformance in integrating past memories into RG. A supplementary video for\nTHEANINE and data for TeaFarm are at\nhttps://huggingface.co/spaces/ResearcherScholar/Theanine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior studies focus on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present THEANINE, a framework for LLM-based lifelong dialogue\nagents. THEANINE discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, THEANINE augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts when assessing agent\nperformance in integrating past memories into RG. A supplementary video for\nTHEANINE and data for TeaFarm are at\nhttps://huggingface.co/spaces/ResearcherScholar/Theanine."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Namyoung Kim"
                    },
                    {
                        "name": "Minju Gwak"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17715v1",
                "updated": "2025-01-29T15:32:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    32,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T15:32:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    32,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts"
                },
                "summary": "User interactions with conversational agents (CAs) evolve in the era of\nheavily guardrailed large language models (LLMs). As users push beyond\nprogrammed boundaries to explore and build relationships with these systems,\nthere is a growing concern regarding the potential for unauthorized access or\nmanipulation, commonly referred to as \"jailbreaking.\" Moreover, with CAs that\npossess highly human-like qualities, users show a tendency toward initiating\nintimate sexual interactions or attempting to tame their chatbots. To capture\nand reflect these in-the-wild interactions into chatbot designs, we propose\nRICoTA, a Korean red teaming dataset that consists of 609 prompts challenging\nLLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We\nutilize user-chatbot conversations that were self-posted on a Korean\nReddit-like community, containing specific testing and gaming intentions with a\nsocial chatbot. With these prompts, we aim to evaluate LLMs' ability to\nidentify the type of conversation and users' testing purposes to derive chatbot\ndesign implications for mitigating jailbreaking risks. Our dataset will be made\npublicly available via GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interactions with conversational agents (CAs) evolve in the era of\nheavily guardrailed large language models (LLMs). As users push beyond\nprogrammed boundaries to explore and build relationships with these systems,\nthere is a growing concern regarding the potential for unauthorized access or\nmanipulation, commonly referred to as \"jailbreaking.\" Moreover, with CAs that\npossess highly human-like qualities, users show a tendency toward initiating\nintimate sexual interactions or attempting to tame their chatbots. To capture\nand reflect these in-the-wild interactions into chatbot designs, we propose\nRICoTA, a Korean red teaming dataset that consists of 609 prompts challenging\nLLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We\nutilize user-chatbot conversations that were self-posted on a Korean\nReddit-like community, containing specific testing and gaming intentions with a\nsocial chatbot. With these prompts, we aim to evaluate LLMs' ability to\nidentify the type of conversation and users' testing purposes to derive chatbot\ndesign implications for mitigating jailbreaking risks. Our dataset will be made\npublicly available via GitHub."
                },
                "authors": [
                    {
                        "name": "Eujeong Choi"
                    },
                    {
                        "name": "Younghun Jeong"
                    },
                    {
                        "name": "Soomin Kim"
                    },
                    {
                        "name": "Won Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Won Ik Cho"
                },
                "author": "Won Ik Cho",
                "arxiv_comment": "PACLIC 38",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17683v1",
                "updated": "2025-01-29T14:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    43,
                    21,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T14:43:21Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    43,
                    21,
                    2,
                    29,
                    0
                ],
                "title": "Temperature-Free Loss Function for Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temperature-Free Loss Function for Contrastive Learning"
                },
                "summary": "As one of the most promising methods in self-supervised learning, contrastive\nlearning has achieved a series of breakthroughs across numerous fields. A\npredominant approach to implementing contrastive learning is applying InfoNCE\nloss: By capturing the similarities between pairs, InfoNCE loss enables\nlearning the representation of data. Albeit its success, adopting InfoNCE loss\nrequires tuning a temperature, which is a core hyperparameter for calibrating\nsimilarity scores. Despite its significance and sensitivity to performance\nbeing emphasized by several studies, searching for a valid temperature requires\nextensive trial-and-error-based experiments, which increases the difficulty of\nadopting InfoNCE loss. To address this difficulty, we propose a novel method to\ndeploy InfoNCE loss without temperature. Specifically, we replace temperature\nscaling with the inverse hyperbolic tangent function, resulting in a modified\nInfoNCE loss. In addition to hyperparameter-free deployment, we observed that\nthe proposed method even yielded a performance gain in contrastive learning.\nOur detailed theoretical analysis discovers that the current practice of\ntemperature scaling in InfoNCE loss causes serious problems in gradient\ndescent, whereas our method provides desirable gradient properties. The\nproposed method was validated on five benchmarks on contrastive learning,\nyielding satisfactory results without temperature tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most promising methods in self-supervised learning, contrastive\nlearning has achieved a series of breakthroughs across numerous fields. A\npredominant approach to implementing contrastive learning is applying InfoNCE\nloss: By capturing the similarities between pairs, InfoNCE loss enables\nlearning the representation of data. Albeit its success, adopting InfoNCE loss\nrequires tuning a temperature, which is a core hyperparameter for calibrating\nsimilarity scores. Despite its significance and sensitivity to performance\nbeing emphasized by several studies, searching for a valid temperature requires\nextensive trial-and-error-based experiments, which increases the difficulty of\nadopting InfoNCE loss. To address this difficulty, we propose a novel method to\ndeploy InfoNCE loss without temperature. Specifically, we replace temperature\nscaling with the inverse hyperbolic tangent function, resulting in a modified\nInfoNCE loss. In addition to hyperparameter-free deployment, we observed that\nthe proposed method even yielded a performance gain in contrastive learning.\nOur detailed theoretical analysis discovers that the current practice of\ntemperature scaling in InfoNCE loss causes serious problems in gradient\ndescent, whereas our method provides desirable gradient properties. The\nproposed method was validated on five benchmarks on contrastive learning,\nyielding satisfactory results without temperature tuning."
                },
                "authors": [
                    {
                        "name": "Bum Jun Kim"
                    },
                    {
                        "name": "Sang Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sang Woo Kim"
                },
                "author": "Sang Woo Kim",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.12664v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.12664v4",
                "updated": "2025-01-29T14:42:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    42,
                    14,
                    2,
                    29,
                    0
                ],
                "published": "2023-07-24T10:10:24Z",
                "published_parsed": [
                    2023,
                    7,
                    24,
                    10,
                    10,
                    24,
                    0,
                    205,
                    0
                ],
                "title": "SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots\n  via Model-Based Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots\n  via Model-Based Priors"
                },
                "summary": "We present a footstep planning policy for quadrupedal locomotion that is able\nto directly take into consideration a-priori safety information in its\ndecisions. At its core, a learning process analyzes terrain patches,\nclassifying each landing location by its kinematic feasibility, shin collision,\nand terrain roughness. This information is then encoded into a small vector\nrepresentation and passed as an additional state to the footstep planning\npolicy, which furthermore proposes only safe footstep location by applying a\nmasked variant of the Proximal Policy Optimization algorithm. The performance\nof the proposed approach is shown by comparative simulations and experiments on\nan electric quadruped robot walking in different rough terrain scenarios. We\nshow that violations of the above safety conditions are greatly reduced both\nduring training and the successive deployment of the policy, resulting in an\ninherently safer footstep planner. Furthermore, we show how, as a byproduct,\nfewer reward terms are needed to shape the behavior of the policy, which in\nreturn is able to achieve both better final performances and sample efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a footstep planning policy for quadrupedal locomotion that is able\nto directly take into consideration a-priori safety information in its\ndecisions. At its core, a learning process analyzes terrain patches,\nclassifying each landing location by its kinematic feasibility, shin collision,\nand terrain roughness. This information is then encoded into a small vector\nrepresentation and passed as an additional state to the footstep planning\npolicy, which furthermore proposes only safe footstep location by applying a\nmasked variant of the Proximal Policy Optimization algorithm. The performance\nof the proposed approach is shown by comparative simulations and experiments on\nan electric quadruped robot walking in different rough terrain scenarios. We\nshow that violations of the above safety conditions are greatly reduced both\nduring training and the successive deployment of the policy, resulting in an\ninherently safer footstep planner. Furthermore, we show how, as a byproduct,\nfewer reward terms are needed to shape the behavior of the policy, which in\nreturn is able to achieve both better final performances and sample efficiency."
                },
                "authors": [
                    {
                        "name": "Shafeef Omar"
                    },
                    {
                        "name": "Lorenzo Amatucci"
                    },
                    {
                        "name": "Victor Barasuol"
                    },
                    {
                        "name": "Giulio Turrisi"
                    },
                    {
                        "name": "Claudio Semini"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Semini"
                },
                "author": "Claudio Semini",
                "arxiv_doi": "10.1109/Humanoids57100.2023.10375218",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/Humanoids57100.2023.10375218",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.12664v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.12664v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the 2023 IEEE-RAS International\n  Conference on Humanoid Robots (Humanoids)",
                "arxiv_journal_ref": "2023 IEEE-RAS 22nd International Conference on Humanoid Robots\n  (Humanoids), pp. 1-8",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17665v1",
                "updated": "2025-01-29T14:04:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    4,
                    54,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T14:04:54Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    14,
                    4,
                    54,
                    2,
                    29,
                    0
                ],
                "title": "Planning with Vision-Language Models and a Use Case in Robot-Assisted\n  Teaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with Vision-Language Models and a Use Case in Robot-Assisted\n  Teaching"
                },
                "summary": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder."
                },
                "authors": [
                    {
                        "name": "Xuzhe Dang"
                    },
                    {
                        "name": "Lada Kudláčková"
                    },
                    {
                        "name": "Stefan Edelkamp"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Edelkamp"
                },
                "author": "Stefan Edelkamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16135v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16135v3",
                "updated": "2025-01-29T13:52:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    52,
                    31,
                    2,
                    29,
                    0
                ],
                "published": "2024-12-20T18:31:24Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    18,
                    31,
                    24,
                    4,
                    355,
                    0
                ],
                "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models\n  into Assembly Code Obfuscation"
                },
                "summary": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware authors often employ code obfuscations to make their malware harder\nto detect. Existing tools for generating obfuscated code often require access\nto the original source code (e.g., C++ or Java), and adding new obfuscations is\na non-trivial, labor-intensive process. In this study, we ask the following\nquestion: Can Large Language Models (LLMs) potentially generate a new\nobfuscated assembly code? If so, this poses a risk to anti-virus engines and\npotentially increases the flexibility of attackers to create new obfuscation\npatterns. We answer this in the affirmative by developing the MetamorphASM\nbenchmark comprising MetamorphASM Dataset (MAD) along with three code\nobfuscation techniques: dead code, register substitution, and control flow\nchange. The MetamorphASM systematically evaluates the ability of LLMs to\ngenerate and analyze obfuscated code using MAD, which contains 328,200\nobfuscated assembly code samples. We release this dataset and analyze the\nsuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,\nCodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly\ncode. The evaluation was performed using established information-theoretic\nmetrics and manual human review to ensure correctness and provide the\nfoundation for researchers to study and develop remediations to this risk."
                },
                "authors": [
                    {
                        "name": "Seyedreza Mohseni"
                    },
                    {
                        "name": "Seyedali Mohammadi"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Gerald Ketu Ndawula"
                    },
                    {
                        "name": "Sriram Vema"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "To appear in AAAI 2025, Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16135v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17635v1",
                "updated": "2025-01-29T13:12:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:12:01Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    12,
                    1,
                    2,
                    29,
                    0
                ],
                "title": "In-Context Meta LoRA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Meta LoRA Generation"
                },
                "summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hao Zhao"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jingcai Guo"
                },
                "author": "Jingcai Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17629v1",
                "updated": "2025-01-29T13:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "title": "The Imitation Game According To Turing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Imitation Game According To Turing"
                },
                "summary": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current cycle of hype and anxiety concerning the benefits and risks to\nhuman society of Artificial Intelligence is fuelled, not only by the increasing\nuse of generative AI and other AI tools by the general public, but also by\nclaims made on behalf of such technology by popularizers and scientists. In\nparticular, recent studies have claimed that Large Language Models (LLMs) can\npass the Turing Test-a goal for AI since the 1950s-and therefore can \"think\".\nLarge-scale impacts on society have been predicted as a result. Upon detailed\nexamination, however, none of these studies has faithfully applied Turing's\noriginal instructions. Consequently, we conducted a rigorous Turing Test with\nGPT-4-Turbo that adhered closely to Turing's instructions for a three-player\nimitation game. We followed established scientific standards where Turing's\ninstructions were ambiguous or missing. For example, we performed a\nComputer-Imitates-Human Game (CIHG) without constraining the time duration and\nconducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one\nparticipant correctly identified the LLM, showing that one of today's most\nadvanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent\nextravagant claims for such models are unsupported, and do not warrant either\noptimism or concern about the social impact of thinking machines."
                },
                "authors": [
                    {
                        "name": "Sharon Temtsin"
                    },
                    {
                        "name": "Diane Proudfoot"
                    },
                    {
                        "name": "David Kaber"
                    },
                    {
                        "name": "Christoph Bartneck"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Bartneck"
                },
                "arxiv_affiliation": "The University of Canterbury",
                "author": "Christoph Bartneck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17630v1",
                "updated": "2025-01-29T13:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    8,
                    17,
                    2,
                    29,
                    0
                ],
                "title": "Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation"
                },
                "summary": "Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025"
                },
                "authors": [
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Sanghwan Jang"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_doi": "10.1145/3696410.3714601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17627v1",
                "updated": "2025-01-29T13:06:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    6,
                    52,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T13:06:52Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    13,
                    6,
                    52,
                    2,
                    29,
                    0
                ],
                "title": "Adaptively Weighted Averaging Over-the-Air Computation and Its\n  Application to Distributed Gaussian Process Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively Weighted Averaging Over-the-Air Computation and Its\n  Application to Distributed Gaussian Process Regression"
                },
                "summary": "This paper introduces a noise-tolerant computing method for over-the-air\ncomputation (AirComp) aimed at weighted averaging, which is critical in various\nInternet of Things (IoT) applications such as environmental monitoring.\nTraditional AirComp approaches, while efficient, suffer significantly in\naccuracy due to noise enhancement in the normalization by the sum of weights.\nOur proposed method allows nodes to adaptively truncate their weights based on\nthe channel conditions, thereby enhancing noise tolerance. Applied to\ndistributed Gaussian process regression, the method facilitates low-latency,\nlow-complexity, and high-accuracy distributed regression across a range of\nsignal-to-noise ratios. We evaluate the performance of the proposed method in a\nradio map construction problem, which is a task for visualizing the radio\nenvironment based on limited sensing information and spatial interpolation.\nNumerical results demonstrate that our approach not only maintains\ncomputational accuracy in high noise scenarios but also achieves performance\nclose to ideal conditions in high SNR environments. This advancement holds\nsignificant implications for the deployment of real-time, large-scale IoT\nsystems, providing a scalable solution to enhance reliability and efficiency in\ndata-driven environmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a noise-tolerant computing method for over-the-air\ncomputation (AirComp) aimed at weighted averaging, which is critical in various\nInternet of Things (IoT) applications such as environmental monitoring.\nTraditional AirComp approaches, while efficient, suffer significantly in\naccuracy due to noise enhancement in the normalization by the sum of weights.\nOur proposed method allows nodes to adaptively truncate their weights based on\nthe channel conditions, thereby enhancing noise tolerance. Applied to\ndistributed Gaussian process regression, the method facilitates low-latency,\nlow-complexity, and high-accuracy distributed regression across a range of\nsignal-to-noise ratios. We evaluate the performance of the proposed method in a\nradio map construction problem, which is a task for visualizing the radio\nenvironment based on limited sensing information and spatial interpolation.\nNumerical results demonstrate that our approach not only maintains\ncomputational accuracy in high noise scenarios but also achieves performance\nclose to ideal conditions in high SNR environments. This advancement holds\nsignificant implications for the deployment of real-time, large-scale IoT\nsystems, providing a scalable solution to enhance reliability and efficiency in\ndata-driven environmental monitoring."
                },
                "authors": [
                    {
                        "name": "Koya Sato"
                    },
                    {
                        "name": "Koji Ishibashi"
                    }
                ],
                "author_detail": {
                    "name": "Koji Ishibashi"
                },
                "author": "Koji Ishibashi",
                "arxiv_comment": "16 pages, 13 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v5",
                "updated": "2025-01-29T12:52:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    52,
                    53,
                    2,
                    29,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17617v1",
                "updated": "2025-01-29T12:46:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    46,
                    42,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:46:42Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    46,
                    42,
                    2,
                    29,
                    0
                ],
                "title": "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment"
                },
                "summary": "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications."
                },
                "authors": [
                    {
                        "name": "Jonathan Teel"
                    },
                    {
                        "name": "Jocasta Cumberbatch"
                    },
                    {
                        "name": "Raphael Benington"
                    },
                    {
                        "name": "Quentin Baskerville"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Baskerville"
                },
                "author": "Quentin Baskerville",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03664v3",
                "updated": "2025-01-29T12:36:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    36,
                    0,
                    2,
                    29,
                    0
                ],
                "published": "2024-02-16T10:56:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    56,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine"
                },
                "summary": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results."
                },
                "authors": [
                    {
                        "name": "Erblin Isaku"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Thomas Schwitalla"
                    },
                    {
                        "name": "Jan F. Nygård"
                    }
                ],
                "author_detail": {
                    "name": "Jan F. Nygård"
                },
                "author": "Jan F. Nygård",
                "arxiv_comment": "12 pages, 6 figures, 4 tables, 1 listing, revised arguments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14326v2",
                "updated": "2025-01-29T12:27:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    27,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-20T13:56:52Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    56,
                    52,
                    3,
                    172,
                    0
                ],
                "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced\n  Clinical Diagnosis on EMRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced\n  Clinical Diagnosis on EMRs"
                },
                "summary": "Electronic Medical Records (EMRs), while integral to modern healthcare,\npresent challenges for clinical reasoning and diagnosis due to their complexity\nand information redundancy. To address this, we proposed medIKAL (Integrating\nKnowledge Graphs as Assistants of LLMs), a framework that combines Large\nLanguage Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic\ncapabilities. medIKAL assigns weighted importance to entities in medical\nrecords based on their type, enabling precise localization of candidate\ndiseases within KGs. It innovatively employs a residual network-like approach,\nallowing initial diagnosis by the LLM to be merged into KG search results.\nThrough a path-based reranking algorithm and a fill-in-the-blank style prompt\ntemplate, it further refined the diagnostic process. We validated medIKAL's\neffectiveness through extensive experiments on a newly introduced open-sourced\nChinese EMR dataset, demonstrating its potential to improve clinical diagnosis\nin real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Medical Records (EMRs), while integral to modern healthcare,\npresent challenges for clinical reasoning and diagnosis due to their complexity\nand information redundancy. To address this, we proposed medIKAL (Integrating\nKnowledge Graphs as Assistants of LLMs), a framework that combines Large\nLanguage Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic\ncapabilities. medIKAL assigns weighted importance to entities in medical\nrecords based on their type, enabling precise localization of candidate\ndiseases within KGs. It innovatively employs a residual network-like approach,\nallowing initial diagnosis by the LLM to be merged into KG search results.\nThrough a path-based reranking algorithm and a fill-in-the-blank style prompt\ntemplate, it further refined the diagnostic process. We validated medIKAL's\neffectiveness through extensive experiments on a newly introduced open-sourced\nChinese EMR dataset, demonstrating its potential to improve clinical diagnosis\nin real-world settings."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12846v4",
                "updated": "2025-01-29T12:15:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    15,
                    49,
                    2,
                    29,
                    0
                ],
                "published": "2024-01-23T15:29:26Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    15,
                    29,
                    26,
                    1,
                    23,
                    0
                ],
                "title": "How well can a large language model explain business processes as\n  perceived by users?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well can a large language model explain business processes as\n  perceived by users?"
                },
                "summary": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation."
                },
                "authors": [
                    {
                        "name": "Dirk Fahland"
                    },
                    {
                        "name": "Fabiana Fournier"
                    },
                    {
                        "name": "Lior Limonad"
                    },
                    {
                        "name": "Inna Skarbovsky"
                    },
                    {
                        "name": "Ava J. E. Swevels"
                    }
                ],
                "author_detail": {
                    "name": "Ava J. E. Swevels"
                },
                "author": "Ava J. E. Swevels",
                "arxiv_comment": "42 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17598v1",
                "updated": "2025-01-29T12:03:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    3,
                    11,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T12:03:11Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    3,
                    11,
                    2,
                    29,
                    0
                ],
                "title": "Semantic Consistency Regularization with Large Language Models for\n  Semi-supervised Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Consistency Regularization with Large Language Models for\n  Semi-supervised Sentiment Analysis"
                },
                "summary": "Accurate sentiment analysis of texts is crucial for a variety of\napplications, such as understanding customer feedback, monitoring market\ntrends, and detecting public sentiment. However, manually annotating large\nsentiment corpora for supervised learning is labor-intensive and\ntime-consuming. Therefore, it is essential and effective to develop a\nsemi-supervised method for the sentiment analysis task. Although some methods\nhave been proposed for semi-supervised text classification, they rely on the\nintrinsic information within the unlabeled data and the learning capability of\nthe NLP model, which lack generalization ability to the sentiment analysis\nscenario and may prone to overfit. Inspired by the ability of pretrained Large\nLanguage Models (LLMs) in following instructions and generating coherent text,\nwe propose a Semantic Consistency Regularization with Large Language Models\n(SCR) framework for semi-supervised sentiment analysis. We introduce two\nprompting strategies to semantically enhance unlabeled text using LLMs. The\nfirst is Entity-based Enhancement (SCR-EE), which involves extracting entities\nand numerical information, and querying the LLM to reconstruct the textual\ninformation. The second is Concept-based Enhancement (SCR-CE), which directly\nqueries the LLM with the original sentence for semantic reconstruction.\nSubsequently, the LLM-augmented data is utilized for a consistency loss with\nconfidence thresholding, which preserves high-quality agreement samples to\nprovide additional supervision signals during training. Furthermore, to fully\nutilize the uncertain unlabeled data samples, we propose a class re-assembling\nstrategy inspired by the class space shrinking theorem. Experiments show our\nmethod achieves remarkable performance over prior semi-supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate sentiment analysis of texts is crucial for a variety of\napplications, such as understanding customer feedback, monitoring market\ntrends, and detecting public sentiment. However, manually annotating large\nsentiment corpora for supervised learning is labor-intensive and\ntime-consuming. Therefore, it is essential and effective to develop a\nsemi-supervised method for the sentiment analysis task. Although some methods\nhave been proposed for semi-supervised text classification, they rely on the\nintrinsic information within the unlabeled data and the learning capability of\nthe NLP model, which lack generalization ability to the sentiment analysis\nscenario and may prone to overfit. Inspired by the ability of pretrained Large\nLanguage Models (LLMs) in following instructions and generating coherent text,\nwe propose a Semantic Consistency Regularization with Large Language Models\n(SCR) framework for semi-supervised sentiment analysis. We introduce two\nprompting strategies to semantically enhance unlabeled text using LLMs. The\nfirst is Entity-based Enhancement (SCR-EE), which involves extracting entities\nand numerical information, and querying the LLM to reconstruct the textual\ninformation. The second is Concept-based Enhancement (SCR-CE), which directly\nqueries the LLM with the original sentence for semantic reconstruction.\nSubsequently, the LLM-augmented data is utilized for a consistency loss with\nconfidence thresholding, which preserves high-quality agreement samples to\nprovide additional supervision signals during training. Furthermore, to fully\nutilize the uncertain unlabeled data samples, we propose a class re-assembling\nstrategy inspired by the class space shrinking theorem. Experiments show our\nmethod achieves remarkable performance over prior semi-supervised methods."
                },
                "authors": [
                    {
                        "name": "Kunrong Li"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Zhen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Chen"
                },
                "author": "Zhen Chen",
                "arxiv_comment": "ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17584v1",
                "updated": "2025-01-29T11:40:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    40,
                    46,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T11:40:46Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    40,
                    46,
                    2,
                    29,
                    0
                ],
                "title": "GLLM: Self-Corrective G-Code Generation using Large Language Models with\n  User Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLLM: Self-Corrective G-Code Generation using Large Language Models with\n  User Feedback"
                },
                "summary": "This paper introduces GLLM, an innovative tool that leverages Large Language\nModels (LLMs) to automatically generate G-code from natural language\ninstructions for Computer Numerical Control (CNC) machining. GLLM addresses the\nchallenges of manual G-code writing by bridging the gap between human-readable\ntask descriptions and machine-executable code. The system incorporates a\nfine-tuned StarCoder-3B model, enhanced with domain-specific training data and\na Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced\nprompting strategies and a novel self-corrective code generation approach to\nensure both syntactic and semantic correctness of the generated G-code. The\narchitecture includes robust validation mechanisms, including syntax checks,\nG-code-specific verifications, and functional correctness evaluations using\nHausdorff distance. By combining these techniques, GLLM aims to democratize CNC\nprogramming, making it more accessible to users without extensive programming\nexperience while maintaining high accuracy and reliability in G-code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GLLM, an innovative tool that leverages Large Language\nModels (LLMs) to automatically generate G-code from natural language\ninstructions for Computer Numerical Control (CNC) machining. GLLM addresses the\nchallenges of manual G-code writing by bridging the gap between human-readable\ntask descriptions and machine-executable code. The system incorporates a\nfine-tuned StarCoder-3B model, enhanced with domain-specific training data and\na Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced\nprompting strategies and a novel self-corrective code generation approach to\nensure both syntactic and semantic correctness of the generated G-code. The\narchitecture includes robust validation mechanisms, including syntax checks,\nG-code-specific verifications, and functional correctness evaluations using\nHausdorff distance. By combining these techniques, GLLM aims to democratize CNC\nprogramming, making it more accessible to users without extensive programming\nexperience while maintaining high accuracy and reliability in G-code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Mohamed Abdelaal"
                    },
                    {
                        "name": "Samuel Lokadjaja"
                    },
                    {
                        "name": "Gilbert Engert"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Engert"
                },
                "author": "Gilbert Engert",
                "arxiv_journal_ref": "Industrial Track of 21st Conference on Database Systems for\n  Business, Technology and Web (BTW), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17581v1",
                "updated": "2025-01-29T11:38:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    38,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T11:38:29Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    11,
                    38,
                    29,
                    2,
                    29,
                    0
                ],
                "title": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free\n  Counterspeech Evaluation using Auto-Calibrated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free\n  Counterspeech Evaluation using Auto-Calibrated LLMs"
                },
                "summary": "Counterspeech has been popular as an effective approach to counter online\nhate speech, leading to increasing research interest in automated counterspeech\ngeneration using language models. However, this field lacks standardised\nevaluation protocols and robust automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (ACE), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that ACE outperforms traditional metrics\nlike ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant advancement in automated counterspeech evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterspeech has been popular as an effective approach to counter online\nhate speech, leading to increasing research interest in automated counterspeech\ngeneration using language models. However, this field lacks standardised\nevaluation protocols and robust automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (ACE), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that ACE outperforms traditional metrics\nlike ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant advancement in automated counterspeech evaluation."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Aswini Kumar"
                    },
                    {
                        "name": "Anil Bandhakavi"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "17 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:2309.13308 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17546v1",
                "updated": "2025-01-29T10:29:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    29,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T10:29:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    29,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "Is Conversational XAI All You Need? Human-AI Decision Making With a\n  Conversational XAI Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Conversational XAI All You Need? Human-AI Decision Making With a\n  Conversational XAI Assistant"
                },
                "summary": "Explainable artificial intelligence (XAI) methods are being proposed to help\ninterpret and understand how AI systems reach specific predictions. Inspired by\nprior work on conversational user interfaces, we argue that augmenting existing\nXAI methods with conversational user interfaces can increase user engagement\nand boost user understanding of the AI system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding of the AI\nsystem, their trust, and reliance on the AI system. In comparison to an XAI\ndashboard, we found that the conversational XAI interface can bring about a\nbetter understanding of the AI system among users and higher user trust.\nHowever, users of both the XAI dashboard and conversational XAI interfaces\nshowed clear overreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based on our\nfindings, we reason that the potential cause of such overreliance is the\nillusion of explanatory depth that is concomitant with both XAI interfaces. Our\nfindings have important implications for designing effective conversational XAI\ninterfaces to facilitate appropriate reliance and improve human-AI\ncollaboration. Code can be found at\nhttps://github.com/delftcrowd/IUI2025_ConvXAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable artificial intelligence (XAI) methods are being proposed to help\ninterpret and understand how AI systems reach specific predictions. Inspired by\nprior work on conversational user interfaces, we argue that augmenting existing\nXAI methods with conversational user interfaces can increase user engagement\nand boost user understanding of the AI system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding of the AI\nsystem, their trust, and reliance on the AI system. In comparison to an XAI\ndashboard, we found that the conversational XAI interface can bring about a\nbetter understanding of the AI system among users and higher user trust.\nHowever, users of both the XAI dashboard and conversational XAI interfaces\nshowed clear overreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based on our\nfindings, we reason that the potential cause of such overreliance is the\nillusion of explanatory depth that is concomitant with both XAI interfaces. Our\nfindings have important implications for designing effective conversational XAI\ninterfaces to facilitate appropriate reliance and improve human-AI\ncollaboration. Code can be found at\nhttps://github.com/delftcrowd/IUI2025_ConvXAI"
                },
                "authors": [
                    {
                        "name": "Gaole He"
                    },
                    {
                        "name": "Nilay Aishwarya"
                    },
                    {
                        "name": "Ujwal Gadiraju"
                    }
                ],
                "author_detail": {
                    "name": "Ujwal Gadiraju"
                },
                "author": "Ujwal Gadiraju",
                "arxiv_doi": "10.1145/3708359.3712133",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712133",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conditionally accepted to IUI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v3",
                "updated": "2025-01-29T10:25:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    25,
                    2,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17539v1",
                "updated": "2025-01-29T10:12:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    12,
                    13,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T10:12:13Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    10,
                    12,
                    13,
                    2,
                    29,
                    0
                ],
                "title": "Towards Supporting Penetration Testing Education with Large Language\n  Models: an Evaluation and Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Supporting Penetration Testing Education with Large Language\n  Models: an Evaluation and Comparison"
                },
                "summary": "Cybersecurity education is challenging and it is helpful for educators to\nunderstand Large Language Models' (LLMs') capabilities for supporting\neducation. This study evaluates the effectiveness of LLMs in conducting a\nvariety of penetration testing tasks. Fifteen representative tasks were\nselected to cover a comprehensive range of real-world scenarios. We evaluate\nthe performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1\n405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image\nand OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the\nmost consistent support making it a valuable tool for educational purposes.\nHowever, its use in conjonction with WhiteRabbitNeo should be considered,\nbecause of its innovative approach to tool and command recommendations. This\nstudy underscores the need for continued research into optimising LLMs for\ncomplex, domain-specific tasks in cybersecurity education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity education is challenging and it is helpful for educators to\nunderstand Large Language Models' (LLMs') capabilities for supporting\neducation. This study evaluates the effectiveness of LLMs in conducting a\nvariety of penetration testing tasks. Fifteen representative tasks were\nselected to cover a comprehensive range of real-world scenarios. We evaluate\nthe performance of 6 models (GPT-4o mini, GPT-4o, Gemini 1.5 Flash, Llama 3.1\n405B, Mixtral 8x7B and WhiteRabbitNeo) upon the Metasploitable v3 Ubuntu image\nand OWASP WebGOAT. Our findings suggest that GPT-4o mini currently offers the\nmost consistent support making it a valuable tool for educational purposes.\nHowever, its use in conjonction with WhiteRabbitNeo should be considered,\nbecause of its innovative approach to tool and command recommendations. This\nstudy underscores the need for continued research into optimising LLMs for\ncomplex, domain-specific tasks in cybersecurity education."
                },
                "authors": [
                    {
                        "name": "Martin Nizon-Deladoeuille"
                    },
                    {
                        "name": "Brynjólfur Stefánsson"
                    },
                    {
                        "name": "Helmut Neukirchen"
                    },
                    {
                        "name": "Thomas Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Welsh"
                },
                "author": "Thomas Welsh",
                "arxiv_comment": "To be published in: 11th IEEE International Conference on Social\n  Networks Analysis, Management and Security (SNAMS-2024), IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19369v2",
                "updated": "2025-01-29T09:54:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    54,
                    0,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-30T08:55:01Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    8,
                    55,
                    1,
                    1,
                    121,
                    0
                ],
                "title": "Evaluating Telugu Proficiency in Large Language Models_ A Comparative\n  Analysis of ChatGPT and Gemini",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Telugu Proficiency in Large Language Models_ A Comparative\n  Analysis of ChatGPT and Gemini"
                },
                "summary": "The growing prominence of large language models (LLMs) necessitates the\nexploration of their capabilities beyond English. This research investigates\nthe Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.\nThrough a designed set of 20 questions encompassing greetings, grammar,\nvocabulary, common phrases, task completion, and situational reasoning, the\nstudy delves into their strengths and weaknesses in handling Telugu. The\nanalysis aims to identify the LLM that demonstrates a deeper understanding of\nTelugu grammatical structures, possesses a broader vocabulary, and exhibits\nsuperior performance in tasks like writing and reasoning. By comparing their\nability to comprehend and use everyday Telugu expressions, the research sheds\nlight on their suitability for real-world language interaction. Furthermore,\nthe evaluation of adaptability and reasoning capabilities provides insights\ninto how each LLM leverages Telugu to respond to dynamic situations. This\ncomparative analysis contributes to the ongoing discussion on multilingual\ncapabilities in AI and paves the way for future research in developing LLMs\nthat can seamlessly integrate with Telugu-speaking communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prominence of large language models (LLMs) necessitates the\nexploration of their capabilities beyond English. This research investigates\nthe Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.\nThrough a designed set of 20 questions encompassing greetings, grammar,\nvocabulary, common phrases, task completion, and situational reasoning, the\nstudy delves into their strengths and weaknesses in handling Telugu. The\nanalysis aims to identify the LLM that demonstrates a deeper understanding of\nTelugu grammatical structures, possesses a broader vocabulary, and exhibits\nsuperior performance in tasks like writing and reasoning. By comparing their\nability to comprehend and use everyday Telugu expressions, the research sheds\nlight on their suitability for real-world language interaction. Furthermore,\nthe evaluation of adaptability and reasoning capabilities provides insights\ninto how each LLM leverages Telugu to respond to dynamic situations. This\ncomparative analysis contributes to the ongoing discussion on multilingual\ncapabilities in AI and paves the way for future research in developing LLMs\nthat can seamlessly integrate with Telugu-speaking communities."
                },
                "authors": [
                    {
                        "name": "Katikela Sreeharsha Kishore"
                    },
                    {
                        "name": "Rahimanuddin Shaik"
                    }
                ],
                "author_detail": {
                    "name": "Rahimanuddin Shaik"
                },
                "author": "Rahimanuddin Shaik",
                "arxiv_comment": "Disparities in fundamental understandings about the article between\n  the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17510v1",
                "updated": "2025-01-29T09:27:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    27,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T09:27:27Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    27,
                    27,
                    2,
                    29,
                    0
                ],
                "title": "LLM Assistance for Pediatric Depression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assistance for Pediatric Depression"
                },
                "summary": "Traditional depression screening methods, such as the PHQ-9, are particularly\nchallenging for children in pediatric primary care due to practical\nlimitations. AI has the potential to help, but the scarcity of annotated\ndatasets in mental health, combined with the computational costs of training,\nhighlights the need for efficient, zero-shot approaches. In this work, we\ninvestigate the feasibility of state-of-the-art LLMs for depressive symptom\nextraction in pediatric settings (ages 6-24). This approach aims to complement\ntraditional screening and minimize diagnostic errors.\n  Our findings show that all LLMs are 60% more efficient than word match, with\nFlan leading in precision (average F1: 0.65, precision: 0.78), excelling in the\nextraction of more rare symptoms like \"sleep problems\" (F1: 0.92) and\n\"self-loathing\" (F1: 0.8). Phi strikes a balance between precision (0.44) and\nrecall (0.60), performing well in categories like \"Feeling depressed\" (0.69)\nand \"Weight change\" (0.78). Llama 3, with the highest recall (0.90),\novergeneralizes symptoms, making it less suitable for this type of analysis.\nChallenges include the complexity of clinical notes and overgeneralization from\nPHQ-9 scores. The main challenges faced by LLMs include navigating the complex\nstructure of clinical notes with content from different times in the patient\ntrajectory, as well as misinterpreting elevated PHQ-9 scores.\n  We finally demonstrate the utility of symptom annotations provided by Flan as\nfeatures in an ML algorithm, which differentiates depression cases from\ncontrols with high precision of 0.78, showing a major performance boost\ncompared to a baseline that does not use these features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional depression screening methods, such as the PHQ-9, are particularly\nchallenging for children in pediatric primary care due to practical\nlimitations. AI has the potential to help, but the scarcity of annotated\ndatasets in mental health, combined with the computational costs of training,\nhighlights the need for efficient, zero-shot approaches. In this work, we\ninvestigate the feasibility of state-of-the-art LLMs for depressive symptom\nextraction in pediatric settings (ages 6-24). This approach aims to complement\ntraditional screening and minimize diagnostic errors.\n  Our findings show that all LLMs are 60% more efficient than word match, with\nFlan leading in precision (average F1: 0.65, precision: 0.78), excelling in the\nextraction of more rare symptoms like \"sleep problems\" (F1: 0.92) and\n\"self-loathing\" (F1: 0.8). Phi strikes a balance between precision (0.44) and\nrecall (0.60), performing well in categories like \"Feeling depressed\" (0.69)\nand \"Weight change\" (0.78). Llama 3, with the highest recall (0.90),\novergeneralizes symptoms, making it less suitable for this type of analysis.\nChallenges include the complexity of clinical notes and overgeneralization from\nPHQ-9 scores. The main challenges faced by LLMs include navigating the complex\nstructure of clinical notes with content from different times in the patient\ntrajectory, as well as misinterpreting elevated PHQ-9 scores.\n  We finally demonstrate the utility of symptom annotations provided by Flan as\nfeatures in an ML algorithm, which differentiates depression cases from\ncontrols with high precision of 0.78, showing a major performance boost\ncompared to a baseline that does not use these features."
                },
                "authors": [
                    {
                        "name": "Mariia Ignashina"
                    },
                    {
                        "name": "Paulina Bondaronek"
                    },
                    {
                        "name": "Dan Santel"
                    },
                    {
                        "name": "John Pestian"
                    },
                    {
                        "name": "Julia Ive"
                    }
                ],
                "author_detail": {
                    "name": "Julia Ive"
                },
                "author": "Julia Ive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17489v1",
                "updated": "2025-01-29T08:57:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    57,
                    51,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T08:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    57,
                    51,
                    2,
                    29,
                    0
                ],
                "title": "Neural Spelling: A Spell-Based BCI System for Language Neural Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Spelling: A Spell-Based BCI System for Language Neural Decoding"
                },
                "summary": "Brain-computer interfaces (BCIs) present a promising avenue by translating\nneural activity directly into text, eliminating the need for physical actions.\nHowever, existing non-invasive BCI systems have not successfully covered the\nentire alphabet, limiting their practicality. In this paper, we propose a novel\nnon-invasive EEG-based BCI system with Curriculum-based Neural Spelling\nFramework, which recognizes all 26 alphabet letters by decoding neural signals\nassociated with handwriting first, and then apply a Generative AI (GenAI) to\nenhance spell-based neural language decoding tasks. Our approach combines the\nease of handwriting with the accessibility of EEG technology, utilizing\nadvanced neural decoding algorithms and pre-trained large language models\n(LLMs) to translate EEG patterns into text with high accuracy. This system show\nhow GenAI can improve the performance of typical spelling-based neural language\ndecoding task, and addresses the limitations of previous methods, offering a\nscalable and user-friendly solution for individuals with communication\nimpairments, thereby enhancing inclusive communication options.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-computer interfaces (BCIs) present a promising avenue by translating\nneural activity directly into text, eliminating the need for physical actions.\nHowever, existing non-invasive BCI systems have not successfully covered the\nentire alphabet, limiting their practicality. In this paper, we propose a novel\nnon-invasive EEG-based BCI system with Curriculum-based Neural Spelling\nFramework, which recognizes all 26 alphabet letters by decoding neural signals\nassociated with handwriting first, and then apply a Generative AI (GenAI) to\nenhance spell-based neural language decoding tasks. Our approach combines the\nease of handwriting with the accessibility of EEG technology, utilizing\nadvanced neural decoding algorithms and pre-trained large language models\n(LLMs) to translate EEG patterns into text with high accuracy. This system show\nhow GenAI can improve the performance of typical spelling-based neural language\ndecoding task, and addresses the limitations of previous methods, offering a\nscalable and user-friendly solution for individuals with communication\nimpairments, thereby enhancing inclusive communication options."
                },
                "authors": [
                    {
                        "name": "Xiaowei Jiang"
                    },
                    {
                        "name": "Charles Zhou"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Ziyi Zhao"
                    },
                    {
                        "name": "Thomas Do"
                    },
                    {
                        "name": "Chin-Teng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chin-Teng Lin"
                },
                "author": "Chin-Teng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01054v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01054v4",
                "updated": "2025-01-29T08:52:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    52,
                    40,
                    2,
                    29,
                    0
                ],
                "published": "2024-04-01T11:26:50Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    11,
                    26,
                    50,
                    0,
                    92,
                    0
                ],
                "title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for\n  Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for\n  Language Model Alignment"
                },
                "summary": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon"
                },
                "authors": [
                    {
                        "name": "Yuu Jinnai"
                    },
                    {
                        "name": "Tetsuro Morimura"
                    },
                    {
                        "name": "Kaito Ariu"
                    },
                    {
                        "name": "Kenshi Abe"
                    }
                ],
                "author_detail": {
                    "name": "Kenshi Abe"
                },
                "author": "Kenshi Abe",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01054v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01054v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17479v1",
                "updated": "2025-01-29T08:44:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    44,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T08:44:45Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    8,
                    44,
                    45,
                    2,
                    29,
                    0
                ],
                "title": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious natural language processing tasks but often struggle to excel uniformly\nin diverse or complex domains. We propose a novel ensemble method - Diverse\nFingerprint Ensemble (DFPE), which leverages the complementary strengths of\nmultiple LLMs to achieve more robust performance. Our approach involves: (1)\nclustering models based on response \"fingerprints\" patterns, (2) applying a\nquantile-based filtering mechanism to remove underperforming models at a\nper-subject level, and (3) assigning adaptive weights to remaining models based\non their subject-wise validation accuracy. In experiments on the Massive\nMultitask Language Understanding (MMLU) benchmark, DFPE outperforms the best\nsingle model by 3% overall accuracy and 5% in discipline-level accuracy. This\nmethod increases the robustness and generalization of LLMs and underscores how\nmodel selection, diversity preservation, and performance-driven weighting can\neffectively address challenging, multi-faceted language understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious natural language processing tasks but often struggle to excel uniformly\nin diverse or complex domains. We propose a novel ensemble method - Diverse\nFingerprint Ensemble (DFPE), which leverages the complementary strengths of\nmultiple LLMs to achieve more robust performance. Our approach involves: (1)\nclustering models based on response \"fingerprints\" patterns, (2) applying a\nquantile-based filtering mechanism to remove underperforming models at a\nper-subject level, and (3) assigning adaptive weights to remaining models based\non their subject-wise validation accuracy. In experiments on the Massive\nMultitask Language Understanding (MMLU) benchmark, DFPE outperforms the best\nsingle model by 3% overall accuracy and 5% in discipline-level accuracy. This\nmethod increases the robustness and generalization of LLMs and underscores how\nmodel selection, diversity preservation, and performance-driven weighting can\neffectively address challenging, multi-faceted language understanding tasks."
                },
                "authors": [
                    {
                        "name": "Seffi Cohen"
                    },
                    {
                        "name": "Niv Goldshlager"
                    },
                    {
                        "name": "Nurit Cohen-Inger"
                    },
                    {
                        "name": "Bracha Shapira"
                    },
                    {
                        "name": "Lior Rokach"
                    }
                ],
                "author_detail": {
                    "name": "Lior Rokach"
                },
                "author": "Lior Rokach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17461v1",
                "updated": "2025-01-29T07:45:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    45,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T07:45:41Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    45,
                    41,
                    2,
                    29,
                    0
                ],
                "title": "AugmenTest: Enhancing Tests with LLM-Driven Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AugmenTest: Enhancing Tests with LLM-Driven Oracles"
                },
                "summary": "Automated test generation is crucial for ensuring the reliability and\nrobustness of software applications while at the same time reducing the effort\nneeded. While significant progress has been made in test generation research,\ngenerating valid test oracles still remains an open problem. To address this\nchallenge, we present AugmenTest, an approach leveraging Large Language Models\n(LLMs) to infer correct test oracles based on available documentation of the\nsoftware under test. Unlike most existing methods that rely on code, AugmenTest\nutilizes the semantic capabilities of LLMs to infer the intended behavior of a\nmethod from documentation and developer comments, without looking at the code.\nAugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a\ngeneric prompt (without the context of class or method under test), and RAG\nwith Simple Prompt, each offering different levels of contextual information to\nthe LLMs. To evaluate our work, we selected 142 Java classes and generated\nmultiple mutants for each. We then generated tests from these mutants, focusing\nonly on tests that passed on the mutant but failed on the original class, to\nensure that the tests effectively captured bugs. This resulted in 203 unique\ntests with distinct bugs, which were then used to evaluate AugmenTest. Results\nshow that in the most conservative scenario, AugmenTest's Extended Prompt\nconsistently outperformed the Simple Prompt, achieving a success rate of 30\\%\nfor generating correct assertions. In comparison, the state-of-the-art TOGA\napproach achieved 8.2\\%. Contrary to our expectations, the RAG-based approaches\ndid not lead to improvements, with performance of 18.2\\% success rate for the\nmost conservative scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation is crucial for ensuring the reliability and\nrobustness of software applications while at the same time reducing the effort\nneeded. While significant progress has been made in test generation research,\ngenerating valid test oracles still remains an open problem. To address this\nchallenge, we present AugmenTest, an approach leveraging Large Language Models\n(LLMs) to infer correct test oracles based on available documentation of the\nsoftware under test. Unlike most existing methods that rely on code, AugmenTest\nutilizes the semantic capabilities of LLMs to infer the intended behavior of a\nmethod from documentation and developer comments, without looking at the code.\nAugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a\ngeneric prompt (without the context of class or method under test), and RAG\nwith Simple Prompt, each offering different levels of contextual information to\nthe LLMs. To evaluate our work, we selected 142 Java classes and generated\nmultiple mutants for each. We then generated tests from these mutants, focusing\nonly on tests that passed on the mutant but failed on the original class, to\nensure that the tests effectively captured bugs. This resulted in 203 unique\ntests with distinct bugs, which were then used to evaluate AugmenTest. Results\nshow that in the most conservative scenario, AugmenTest's Extended Prompt\nconsistently outperformed the Simple Prompt, achieving a success rate of 30\\%\nfor generating correct assertions. In comparison, the state-of-the-art TOGA\napproach achieved 8.2\\%. Contrary to our expectations, the RAG-based approaches\ndid not lead to improvements, with performance of 18.2\\% success rate for the\nmost conservative scenario."
                },
                "authors": [
                    {
                        "name": "Shaker Mahmud Khandaker"
                    },
                    {
                        "name": "Fitsum Kifetew"
                    },
                    {
                        "name": "Davide Prandi"
                    },
                    {
                        "name": "Angelo Susi"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Susi"
                },
                "author": "Angelo Susi",
                "arxiv_comment": "Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17459v1",
                "updated": "2025-01-29T07:35:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    35,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T07:35:56Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    35,
                    56,
                    2,
                    29,
                    0
                ],
                "title": "Large Language Models for Single-Step and Multi-Step Flight Trajectory\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Single-Step and Multi-Step Flight Trajectory\n  Prediction"
                },
                "summary": "Flight trajectory prediction is a critical time series task in aviation.\nWhile deep learning methods have shown significant promise, the application of\nlarge language models (LLMs) to this domain remains underexplored. This study\npioneers the use of LLMs for flight trajectory prediction by reframing it as a\nlanguage modeling problem. Specifically, We extract features representing the\naircraft's position and status from ADS-B flight data to construct a\nprompt-based dataset, where trajectory waypoints are converted into language\ntokens. The dataset is then employed to fine-tune LLMs, enabling them to learn\ncomplex spatiotemporal patterns for accurate predictions. Comprehensive\nexperiments demonstrate that LLMs achieve notable performance improvements in\nboth single-step and multi-step predictions compared to traditional methods,\nwith LLaMA-3.1 model achieving the highest overall accuracy. However, the high\ninference latency of LLMs poses a challenge for real-time applications,\nunderscoring the need for further research in this promising direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flight trajectory prediction is a critical time series task in aviation.\nWhile deep learning methods have shown significant promise, the application of\nlarge language models (LLMs) to this domain remains underexplored. This study\npioneers the use of LLMs for flight trajectory prediction by reframing it as a\nlanguage modeling problem. Specifically, We extract features representing the\naircraft's position and status from ADS-B flight data to construct a\nprompt-based dataset, where trajectory waypoints are converted into language\ntokens. The dataset is then employed to fine-tune LLMs, enabling them to learn\ncomplex spatiotemporal patterns for accurate predictions. Comprehensive\nexperiments demonstrate that LLMs achieve notable performance improvements in\nboth single-step and multi-step predictions compared to traditional methods,\nwith LLaMA-3.1 model achieving the highest overall accuracy. However, the high\ninference latency of LLMs poses a challenge for real-time applications,\nunderscoring the need for further research in this promising direction."
                },
                "authors": [
                    {
                        "name": "Kaiwei Luo"
                    },
                    {
                        "name": "Jiliu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiliu Zhou"
                },
                "author": "Jiliu Zhou",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16689v2",
                "updated": "2025-01-29T07:23:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    23,
                    47,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T03:57:22Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    3,
                    57,
                    22,
                    1,
                    28,
                    0
                ],
                "title": "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning"
                },
                "summary": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "21 pages, 19 tables",
                "arxiv_journal_ref": "Stanford University InfoLab Technical Report, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v5",
                "updated": "2025-01-29T07:07:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    7,
                    54,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming"
                },
                "summary": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00750v2",
                "updated": "2025-01-29T06:49:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    49,
                    30,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-01T06:36:56Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    6,
                    36,
                    56,
                    2,
                    1,
                    0
                ],
                "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform"
                },
                "summary": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries."
                },
                "authors": [
                    {
                        "name": "Cheonsu Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Cheonsu Jeong"
                },
                "author": "Cheonsu Jeong",
                "arxiv_comment": "22 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17433v1",
                "updated": "2025-01-29T06:24:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    24,
                    58,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T06:24:58Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    24,
                    58,
                    2,
                    29,
                    0
                ],
                "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation"
                },
                "summary": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07806v2",
                "updated": "2025-01-29T06:13:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    6,
                    13,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2024-08-14T20:30:34Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    20,
                    30,
                    34,
                    2,
                    227,
                    0
                ],
                "title": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language\n  Models for Robot-Assisted Blood Suction"
                },
                "summary": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has impacted research in robotics\nand automation. While progress has been made in integrating LLMs into general\nrobotics tasks, a noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in robotic\nsurgery, which entails the ability to reason and adapt to changes in the\nenvironment, remains a significant challenge. In this work, we propose a\nmulti-modal LLM integration in robot-assisted surgery for autonomous blood\nsuction. The reasoning and prioritization are delegated to the higher-level\ntask-planning LLM, and the motion planning and execution are handled by the\nlower-level deep reinforcement learning model, creating a distributed agency\nbetween the two components. As surgical operations are highly dynamic and may\nencounter unforeseen circumstances, blood clots and active bleeding were\nintroduced to influence decision-making. Results showed that using a\nmulti-modal LLM as a higher-level reasoning unit can account for these surgical\ncomplexities to achieve a level of reasoning previously unattainable in\nrobot-assisted surgeries. These findings demonstrate the potential of\nmulti-modal LLMs to significantly enhance contextual understanding and\ndecision-making in robotic-assisted surgeries, marking a step toward autonomous\nsurgical systems."
                },
                "authors": [
                    {
                        "name": "Sadra Zargarzadeh"
                    },
                    {
                        "name": "Maryam Mirzaei"
                    },
                    {
                        "name": "Yafei Ou"
                    },
                    {
                        "name": "Mahdi Tavakoli"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Tavakoli"
                },
                "author": "Mahdi Tavakoli",
                "arxiv_doi": "10.1109/LRA.2025.3535184",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3535184",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for Publication in IEEE Robotics and Automation Letters,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16937v2",
                "updated": "2025-01-29T05:51:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    51,
                    25,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T13:31:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    31,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models"
                },
                "summary": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies."
                },
                "authors": [
                    {
                        "name": "Makoto Shing"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Sho Yokoi"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01968v2",
                "updated": "2025-01-29T05:41:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    41,
                    52,
                    2,
                    29,
                    0
                ],
                "published": "2024-02-03T00:27:22Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    0,
                    27,
                    22,
                    5,
                    34,
                    0
                ],
                "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges\n  and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges\n  and Future Directions"
                },
                "summary": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field."
                },
                "authors": [
                    {
                        "name": "Hung Du"
                    },
                    {
                        "name": "Srikanth Thudumu"
                    },
                    {
                        "name": "Rajesh Vasa"
                    },
                    {
                        "name": "Kon Mouzakis"
                    }
                ],
                "author_detail": {
                    "name": "Kon Mouzakis"
                },
                "author": "Kon Mouzakis",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17420v1",
                "updated": "2025-01-29T05:21:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    21,
                    31,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T05:21:31Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    21,
                    31,
                    2,
                    29,
                    0
                ],
                "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases\n  in Language Models"
                },
                "summary": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While advances in fairness and alignment have helped mitigate overt biases\nexhibited by large language models (LLMs) when explicitly prompted, we\nhypothesize that these models may still exhibit implicit biases when simulating\nhuman behavior. To test this hypothesis, we propose a technique to\nsystematically uncover such biases across a broad range of sociodemographic\ncategories by assessing decision-making disparities among agents with\nLLM-generated, sociodemographically-informed personas. Using our technique, we\ntested six LLMs across three sociodemographic groups and four decision-making\nscenarios. Our results show that state-of-the-art LLMs exhibit significant\nsociodemographic disparities in nearly all simulations, with more advanced\nmodels exhibiting greater implicit biases despite reducing explicit biases.\nFurthermore, when comparing our findings to real-world disparities reported in\nempirical studies, we find that the biases we uncovered are directionally\naligned but markedly amplified. This directional alignment highlights the\nutility of our technique in uncovering systematic biases in LLMs rather than\nrandom variations; moreover, the presence and amplification of implicit biases\nemphasizes the need for novel strategies to address these biases."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    },
                    {
                        "name": "Sauvik Das"
                    }
                ],
                "author_detail": {
                    "name": "Sauvik Das"
                },
                "author": "Sauvik Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16456v2",
                "updated": "2025-01-29T05:15:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    5,
                    15,
                    45,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-27T19:29:11Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    19,
                    29,
                    11,
                    0,
                    27,
                    0
                ],
                "title": "CoCoNUT: Structural Code Understanding does not fall out of a tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoNUT: Structural Code Understanding does not fall out of a tree"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance across a wide\narray of tasks involving both structured and unstructured textual data. Recent\nresults on various benchmarks for code generation, repair, or completion\nsuggest that certain models have programming abilities comparable to or even\nsurpass humans. In this work, we demonstrate that high performance on such\nbenchmarks does not correlate to humans' innate ability to understand\nstructural control flow in code. To this end, we extract solutions from the\nHumanEval benchmark, which the relevant models perform strongly on, and trace\ntheir execution path using function calls sampled from the respective test set.\nUsing this dataset, we investigate the ability of seven state-of-the-art LLMs\nto match the execution trace and find that, despite their ability to generate\nsemantically identical code, they possess limited ability to trace execution\npaths, especially for longer traces and specific control structures. We find\nthat even the top-performing model, Gemini, can fully and correctly generate\nonly 47% of HumanEval task traces. Additionally, we introduce a subset for\nthree key structures not contained in HumanEval: Recursion, Parallel\nProcessing, and Object-Oriented Programming, including concepts like\nInheritance and Polymorphism. Besides OOP, we show that none of the\ninvestigated models achieve an accuracy over 5% on the relevant traces.\nAggregating these specialized parts with HumanEval tasks, we present CoCoNUT:\nCode Control Flow for Navigation Understanding and Testing, which measures a\nmodel's ability to trace execution of code upon relevant calls, including\nadvanced structural components. We conclude that current LLMs need significant\nimprovement to enhance code reasoning abilities. We hope our dataset helps\nresearchers bridge this gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance across a wide\narray of tasks involving both structured and unstructured textual data. Recent\nresults on various benchmarks for code generation, repair, or completion\nsuggest that certain models have programming abilities comparable to or even\nsurpass humans. In this work, we demonstrate that high performance on such\nbenchmarks does not correlate to humans' innate ability to understand\nstructural control flow in code. To this end, we extract solutions from the\nHumanEval benchmark, which the relevant models perform strongly on, and trace\ntheir execution path using function calls sampled from the respective test set.\nUsing this dataset, we investigate the ability of seven state-of-the-art LLMs\nto match the execution trace and find that, despite their ability to generate\nsemantically identical code, they possess limited ability to trace execution\npaths, especially for longer traces and specific control structures. We find\nthat even the top-performing model, Gemini, can fully and correctly generate\nonly 47% of HumanEval task traces. Additionally, we introduce a subset for\nthree key structures not contained in HumanEval: Recursion, Parallel\nProcessing, and Object-Oriented Programming, including concepts like\nInheritance and Polymorphism. Besides OOP, we show that none of the\ninvestigated models achieve an accuracy over 5% on the relevant traces.\nAggregating these specialized parts with HumanEval tasks, we present CoCoNUT:\nCode Control Flow for Navigation Understanding and Testing, which measures a\nmodel's ability to trace execution of code upon relevant calls, including\nadvanced structural components. We conclude that current LLMs need significant\nimprovement to enhance code reasoning abilities. We hope our dataset helps\nresearchers bridge this gap."
                },
                "authors": [
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Saikat Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Saikat Dutta"
                },
                "author": "Saikat Dutta",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Workshop on Large Language\n  Models for Code (LLM4Code)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16692v2",
                "updated": "2025-01-29T04:36:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    36,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-28T04:00:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    4,
                    0,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Code Runtime Performance through Context-Aware\n  Retrieval-Augmented Generation"
                },
                "summary": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing software performance through automated code refinement offers a\npromising avenue for enhancing execution speed and efficiency. Despite recent\nadvancements in LLMs, a significant gap remains in their ability to perform\nin-depth program analysis. This study introduces AUTOPATCH, an in-context\nlearning approach designed to bridge this gap by enabling LLMs to automatically\ngenerate optimized code. Inspired by how programmers learn and apply knowledge\nto optimize software, AUTOPATCH incorporates three key components: (1) an\nanalogy-driven framework to align LLM optimization with human cognitive\nprocesses, (2) a unified approach that integrates historical code examples and\nCFG analysis for context-aware learning, and (3) an automated pipeline for\ngenerating optimized code through in-context prompting. Experimental results\ndemonstrate that AUTOPATCH achieves a 7.3% improvement in execution efficiency\nover GPT-4o across common generated executable code, highlighting its potential\nto advance automated program runtime optimization."
                },
                "authors": [
                    {
                        "name": "Manish Acharya"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Kevin Leach"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17403v1",
                "updated": "2025-01-29T03:57:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    57,
                    56,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T03:57:56Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    57,
                    56,
                    2,
                    29,
                    0
                ],
                "title": "General Scene Adaptation for Vision-and-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Scene Adaptation for Vision-and-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on\none-time execution of individual instructions across multiple environments,\naiming to develop agents capable of functioning in any environment in a\nzero-shot manner. However, real-world navigation robots often operate in\npersistent environments with relatively consistent physical layouts, visual\nobservations, and language styles from instructors. Such a gap in the task\nsetting presents an opportunity to improve VLN agents by incorporating\ncontinuous adaptation to specific environments. To better reflect these\nreal-world conditions, we introduce GSA-VLN, a novel task requiring agents to\nexecute navigation instructions within a specific scene and simultaneously\nadapt to it for improved performance over time. To evaluate the proposed task,\none has to address two challenges in existing VLN datasets: the lack of OOD\ndata, and the limited number and style diversity of instructions for each\nscene. Therefore, we propose a new dataset, GSA-R2R, which significantly\nexpands the diversity and quantity of environments and instructions for the R2R\ndataset to evaluate agent adaptability in both ID and OOD contexts.\nFurthermore, we design a three-stage instruction orchestration pipeline that\nleverages LLMs to refine speaker-generated instructions and apply role-playing\ntechniques to rephrase instructions into different speaking styles. This is\nmotivated by the observation that each individual user often has consistent\nsignatures or preferences in their instructions. We conducted extensive\nexperiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various\nmethods. Based on our findings, we propose a novel method, GR-DUET, which\nincorporates memory-based navigation graphs with an environment-specific\ntraining strategy, achieving state-of-the-art results on all GSA-R2R splits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on\none-time execution of individual instructions across multiple environments,\naiming to develop agents capable of functioning in any environment in a\nzero-shot manner. However, real-world navigation robots often operate in\npersistent environments with relatively consistent physical layouts, visual\nobservations, and language styles from instructors. Such a gap in the task\nsetting presents an opportunity to improve VLN agents by incorporating\ncontinuous adaptation to specific environments. To better reflect these\nreal-world conditions, we introduce GSA-VLN, a novel task requiring agents to\nexecute navigation instructions within a specific scene and simultaneously\nadapt to it for improved performance over time. To evaluate the proposed task,\none has to address two challenges in existing VLN datasets: the lack of OOD\ndata, and the limited number and style diversity of instructions for each\nscene. Therefore, we propose a new dataset, GSA-R2R, which significantly\nexpands the diversity and quantity of environments and instructions for the R2R\ndataset to evaluate agent adaptability in both ID and OOD contexts.\nFurthermore, we design a three-stage instruction orchestration pipeline that\nleverages LLMs to refine speaker-generated instructions and apply role-playing\ntechniques to rephrase instructions into different speaking styles. This is\nmotivated by the observation that each individual user often has consistent\nsignatures or preferences in their instructions. We conducted extensive\nexperiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various\nmethods. Based on our findings, we propose a novel method, GR-DUET, which\nincorporates memory-based navigation graphs with an environment-specific\ntraining strategy, achieving state-of-the-art results on all GSA-R2R splits."
                },
                "authors": [
                    {
                        "name": "Haodong Hong"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Sen Wang"
                    },
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17399v1",
                "updated": "2025-01-29T03:29:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    29,
                    24,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T03:29:24Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    29,
                    24,
                    2,
                    29,
                    0
                ],
                "title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark\n  Challenging to Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark\n  Challenging to Frontier LLMs"
                },
                "summary": "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy."
                },
                "authors": [
                    {
                        "name": "Ved Sirdeshmukh"
                    },
                    {
                        "name": "Kaustubh Deshpande"
                    },
                    {
                        "name": "Johannes Mols"
                    },
                    {
                        "name": "Lifeng Jin"
                    },
                    {
                        "name": "Ed-Yeremai Cardona"
                    },
                    {
                        "name": "Dean Lee"
                    },
                    {
                        "name": "Jeremy Kritz"
                    },
                    {
                        "name": "Willow Primack"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12570v2",
                "updated": "2025-01-29T03:11:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    11,
                    3,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-22T01:35:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    35,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning"
                },
                "summary": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended\nreasoning processes similar to how humans ponder over complex problems. This\nreasoning paradigm significantly enhances the model's problem-solving abilities\nand has achieved promising results. However, long-thought reasoning process\nleads to a substantial increase in inference time. A pressing challenge is\nreducing the inference overhead of long-thought LLMs while ensuring accuracy.\nIn this paper, we experimentally demonstrate that long-thought reasoning models\nstruggle to effectively allocate token budgets based on problem difficulty and\nreasoning redundancies. To address this, we propose Length-Harmonizing\nFine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while\nmaintaining accuracy. This effective fine-tuning method first estimates the\nLLM's baseline performance through pre-sampling and then uses RL-style\nfine-tuning to encourage the model to generate shorter reasoning processes\nunder accuracy constraints. This allows the model to achieve efficient\nreasoning with lower redundancy while maintaining accuracy. Experiments on\nvarious mathematical reasoning benchmarks show that O1-Pruner not only\nsignificantly reduces inference overhead but also achieves higher accuracy,\nproviding a novel and promising solution to this challenge. Our code is coming\nsoon at https://github.com/StarDewXXX/O1-Pruner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended\nreasoning processes similar to how humans ponder over complex problems. This\nreasoning paradigm significantly enhances the model's problem-solving abilities\nand has achieved promising results. However, long-thought reasoning process\nleads to a substantial increase in inference time. A pressing challenge is\nreducing the inference overhead of long-thought LLMs while ensuring accuracy.\nIn this paper, we experimentally demonstrate that long-thought reasoning models\nstruggle to effectively allocate token budgets based on problem difficulty and\nreasoning redundancies. To address this, we propose Length-Harmonizing\nFine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while\nmaintaining accuracy. This effective fine-tuning method first estimates the\nLLM's baseline performance through pre-sampling and then uses RL-style\nfine-tuning to encourage the model to generate shorter reasoning processes\nunder accuracy constraints. This allows the model to achieve efficient\nreasoning with lower redundancy while maintaining accuracy. Experiments on\nvarious mathematical reasoning benchmarks show that O1-Pruner not only\nsignificantly reduces inference overhead but also achieves higher accuracy,\nproviding a novel and promising solution to this challenge. Our code is coming\nsoon at https://github.com/StarDewXXX/O1-Pruner"
                },
                "authors": [
                    {
                        "name": "Haotian Luo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Haiying He"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06647v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06647v3",
                "updated": "2025-01-29T03:10:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    3,
                    10,
                    29,
                    2,
                    29,
                    0
                ],
                "published": "2024-06-10T04:19:20Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    4,
                    19,
                    20,
                    0,
                    162,
                    0
                ],
                "title": "How Efficient is LLM-Generated Code? A Rigorous & High-Standard\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient is LLM-Generated Code? A Rigorous & High-Standard\n  Benchmark"
                },
                "summary": "The emergence of large language models (LLMs) has significantly pushed the\nfrontiers of program synthesis. Advancement of LLM-based program synthesis\ncalls for a thorough evaluation of LLM-generated code. Most evaluation\nframeworks focus on the (functional) correctness of generated code; efficiency,\nas an important measure of code quality, has been overlooked in existing\nevaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a\nrigorous and high-standard benchmark for evaluating the capability of LLMs in\ngenerating efficient code. Firstly, we propose a new efficiency metric called\neff@k, which generalizes the pass@k metric from correctness to efficiency and\nappropriately handles right-censored execution time. Furthermore, we derive an\nunbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we\nalso provide a numerically stable implementation for the new estimator.\nSecondly, to set a high-standard for efficiency evaluation, we employ a human\nexpert to design best algorithms and implementations as our reference solutions\nof efficiency, many of which are much more efficient than existing canonical\nsolutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous\nevaluation, we employ a human expert to curate strong test case generators to\nfilter out wrong code and differentiate suboptimal algorithms. An extensive\nstudy across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still\nfall short of generating expert-level efficient code. Using two subsets of our\nproblem set, we demonstrate that such deficiency is because current LLMs\nstruggle in designing advanced algorithms and are barely aware of\nimplementation optimization. Our benchmark is publicly available at\nhttps://github.com/q-rz/enamel .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly pushed the\nfrontiers of program synthesis. Advancement of LLM-based program synthesis\ncalls for a thorough evaluation of LLM-generated code. Most evaluation\nframeworks focus on the (functional) correctness of generated code; efficiency,\nas an important measure of code quality, has been overlooked in existing\nevaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a\nrigorous and high-standard benchmark for evaluating the capability of LLMs in\ngenerating efficient code. Firstly, we propose a new efficiency metric called\neff@k, which generalizes the pass@k metric from correctness to efficiency and\nappropriately handles right-censored execution time. Furthermore, we derive an\nunbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we\nalso provide a numerically stable implementation for the new estimator.\nSecondly, to set a high-standard for efficiency evaluation, we employ a human\nexpert to design best algorithms and implementations as our reference solutions\nof efficiency, many of which are much more efficient than existing canonical\nsolutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous\nevaluation, we employ a human expert to curate strong test case generators to\nfilter out wrong code and differentiate suboptimal algorithms. An extensive\nstudy across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still\nfall short of generating expert-level efficient code. Using two subsets of our\nproblem set, we demonstrate that such deficiency is because current LLMs\nstruggle in designing advanced algorithms and are barely aware of\nimplementation optimization. Our benchmark is publicly available at\nhttps://github.com/q-rz/enamel ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Weiliang Will Zeng"
                    },
                    {
                        "name": "James Ezick"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06647v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06647v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01992v2",
                "updated": "2025-01-29T02:53:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    53,
                    6,
                    2,
                    29,
                    0
                ],
                "published": "2024-11-04T11:26:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    26,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Ask, and it shall be given: On the Turing completeness of prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, and it shall be given: On the Turing completeness of prompting"
                },
                "summary": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Wenxuan Bao"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17391v1",
                "updated": "2025-01-29T02:52:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    52,
                    32,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T02:52:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    52,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Learning Free Token Reduction for Multi-Modal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Free Token Reduction for Multi-Modal LLM"
                },
                "summary": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance."
                },
                "authors": [
                    {
                        "name": "Zihui Zhao"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04065v4",
                "updated": "2025-01-29T02:47:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    47,
                    24,
                    2,
                    29,
                    0
                ],
                "published": "2024-07-04T17:12:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    17,
                    12,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "On the Workflows and Smells of Leaderboard Operations (LBOps): An\n  Exploratory Study of Foundation Model Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Workflows and Smells of Leaderboard Operations (LBOps): An\n  Exploratory Study of Foundation Model Leaderboards"
                },
                "summary": "Foundation models (FM), such as large language models (LLMs), which are\nlarge-scale machine learning (ML) models, have demonstrated remarkable\nadaptability in various downstream software engineering (SE) tasks, such as\ncode completion, code understanding, and software development. As a result, FM\nleaderboards have become essential tools for SE teams to compare and select the\nbest third-party FMs for their specific products and purposes. However, the\nlack of standardized guidelines for FM evaluation and comparison threatens the\ntransparency of FM leaderboards and limits stakeholders' ability to perform\neffective FM selection. As a first step towards addressing this challenge, our\nresearch focuses on understanding how these FM leaderboards operate in\nreal-world scenarios (\"leaderboard operations\") and identifying potential\npitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we\ncollect up to 1,045 FM leaderboards from five different sources: GitHub,\nHugging Face Spaces, Papers With Code, spreadsheet and independent platform, to\nexamine their documentation and engage in direct communication with leaderboard\noperators to understand their workflows. Through card sorting and negotiated\nagreement, we identify five distinct workflow patterns and develop a domain\nmodel that captures the key components and their interactions within these\nworkflows. We then identify eight unique types of leaderboard smells in LBOps.\nBy mitigating these smells, SE teams can improve transparency, accountability,\nand collaboration in current LBOps practices, fostering a more robust and\nresponsible ecosystem for FM comparison and selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FM), such as large language models (LLMs), which are\nlarge-scale machine learning (ML) models, have demonstrated remarkable\nadaptability in various downstream software engineering (SE) tasks, such as\ncode completion, code understanding, and software development. As a result, FM\nleaderboards have become essential tools for SE teams to compare and select the\nbest third-party FMs for their specific products and purposes. However, the\nlack of standardized guidelines for FM evaluation and comparison threatens the\ntransparency of FM leaderboards and limits stakeholders' ability to perform\neffective FM selection. As a first step towards addressing this challenge, our\nresearch focuses on understanding how these FM leaderboards operate in\nreal-world scenarios (\"leaderboard operations\") and identifying potential\npitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we\ncollect up to 1,045 FM leaderboards from five different sources: GitHub,\nHugging Face Spaces, Papers With Code, spreadsheet and independent platform, to\nexamine their documentation and engage in direct communication with leaderboard\noperators to understand their workflows. Through card sorting and negotiated\nagreement, we identify five distinct workflow patterns and develop a domain\nmodel that captures the key components and their interactions within these\nworkflows. We then identify eight unique types of leaderboard smells in LBOps.\nBy mitigating these smells, SE teams can improve transparency, accountability,\nand collaboration in current LBOps practices, fostering a more robust and\nresponsible ecosystem for FM comparison and selection."
                },
                "authors": [
                    {
                        "name": "Zhimin Zhao"
                    },
                    {
                        "name": "Abdul Ali Bangash"
                    },
                    {
                        "name": "Filipe Roseiro Côgo"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_doi": "10.1109/TSE.2025.3533972",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TSE.2025.3533972",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Awesome Foundation Model Leaderboard List:\n  https://github.com/SAILResearch/awesome-foundation-model-leaderboards;\n  Foundation Model Leaderboard Search Toolkit:\n  https://huggingface.co/spaces/zhiminy/awesome-foundation-model-leaderboard-search",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17387v1",
                "updated": "2025-01-29T02:39:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    39,
                    57,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-29T02:39:57Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    39,
                    57,
                    2,
                    29,
                    0
                ],
                "title": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Capability of YOLO- and Transformer-based Object Detectors\n  for Real-time Weed Detection"
                },
                "summary": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 \\% and 72.36 \\%), as\nwell as mAP50 (73.52 \\% and 79.86 \\%), and mAP50-95 (43.82 \\% and 47.00 \\%) in\ndataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in\nprecision with reaching 82.44 \\% on dataset 1 and 81.46 \\% in dataset 2, making\nthem particularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spot spraying represents an efficient and sustainable method for reducing the\namount of pesticides, particularly herbicides, used in agricultural fields. To\nachieve this, it is of utmost importance to reliably differentiate between\ncrops and weeds, and even between individual weed species in situ and under\nreal-time conditions. To assess suitability for real-time application,\ndifferent object detection models that are currently state-of-the-art are\ncompared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are\ntrained and evaluated with images from a real field situation. The images are\nseparated into two distinct datasets: In the initial data set, each species of\nplants is trained individually; in the subsequent dataset, a distinction is\nmade between monocotyledonous weeds, dicotyledonous weeds, and three chosen\ncrops. The results demonstrate that while all models perform equally well in\nthe metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,\nstand out in terms of their strong recall scores (66.58 \\% and 72.36 \\%), as\nwell as mAP50 (73.52 \\% and 79.86 \\%), and mAP50-95 (43.82 \\% and 47.00 \\%) in\ndataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in\nprecision with reaching 82.44 \\% on dataset 1 and 81.46 \\% in dataset 2, making\nthem particularly suitable for scenarios where minimizing false positives is\ncritical. In particular, the smallest variants of the YOLO models (YOLOv8n,\nYOLOv9t, and YOLOv10n) achieve substantially faster inference times down to\n7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one\nframe, while maintaining competitive accuracy, highlighting their potential for\ndeployment in resource-constrained embedded computing devices as typically used\nin productive setups."
                },
                "authors": [
                    {
                        "name": "Alicia Allmendinger"
                    },
                    {
                        "name": "Ahmet Oğuz Saltık"
                    },
                    {
                        "name": "Gerassimos G. Peteinatos"
                    },
                    {
                        "name": "Anthony Stein"
                    },
                    {
                        "name": "Roland Gerhards"
                    }
                ],
                "author_detail": {
                    "name": "Roland Gerhards"
                },
                "author": "Roland Gerhards",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17343v1",
                "updated": "2025-01-28T23:29:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    23,
                    29,
                    40,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T23:29:40Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    23,
                    29,
                    40,
                    1,
                    28,
                    0
                ],
                "title": "Post-Training Quantization for 3D Medical Image Segmentation: A\n  Practical Study on Real Inference Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization for 3D Medical Image Segmentation: A\n  Practical Study on Real Inference Engines"
                },
                "summary": "Quantizing deep neural networks ,reducing the precision (bit-width) of their\ncomputations, can remarkably decrease memory usage and accelerate processing,\nmaking these models more suitable for large-scale medical imaging applications\nwith limited computational resources. However, many existing methods studied\n\"fake quantization\", which simulates lower precision operations during\ninference, but does not actually reduce model size or improve real-world\ninference speed. Moreover, the potential of deploying real 3D low-bit\nquantization on modern GPUs is still unexplored. In this study, we introduce a\nreal post-training quantization (PTQ) framework that successfully implements\ntrue 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation\nmodels, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,\nST-UNet,and VISTA3D. Our approach involves two main steps. First, we use\nTensorRT to perform fake quantization for both weights and activations with\nunlabeled calibration dataset. Second, we convert this fake quantization into\nreal quantization via TensorRT engine on real GPUs, resulting in real-world\nreductions in model size and inference latency. Extensive experiments\ndemonstrate that our framework effectively performs 8-bit quantization on GPUs\nwithout sacrificing model performance. This advancement enables the deployment\nof efficient deep learning models in medical imaging applications where\ncomputational resources are constrained. The code and models have been\nreleased, including U-Net, TransUNet pretrained on the BTCV dataset for\nabdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset\nfor whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and\nVISTA3D pretrained on TotalSegmentator V2 for full body (104-label)\nsegmentation. https://github.com/hrlblab/PTQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing deep neural networks ,reducing the precision (bit-width) of their\ncomputations, can remarkably decrease memory usage and accelerate processing,\nmaking these models more suitable for large-scale medical imaging applications\nwith limited computational resources. However, many existing methods studied\n\"fake quantization\", which simulates lower precision operations during\ninference, but does not actually reduce model size or improve real-world\ninference speed. Moreover, the potential of deploying real 3D low-bit\nquantization on modern GPUs is still unexplored. In this study, we introduce a\nreal post-training quantization (PTQ) framework that successfully implements\ntrue 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation\nmodels, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,\nST-UNet,and VISTA3D. Our approach involves two main steps. First, we use\nTensorRT to perform fake quantization for both weights and activations with\nunlabeled calibration dataset. Second, we convert this fake quantization into\nreal quantization via TensorRT engine on real GPUs, resulting in real-world\nreductions in model size and inference latency. Extensive experiments\ndemonstrate that our framework effectively performs 8-bit quantization on GPUs\nwithout sacrificing model performance. This advancement enables the deployment\nof efficient deep learning models in medical imaging applications where\ncomputational resources are constrained. The code and models have been\nreleased, including U-Net, TransUNet pretrained on the BTCV dataset for\nabdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset\nfor whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and\nVISTA3D pretrained on TotalSegmentator V2 for full body (104-label)\nsegmentation. https://github.com/hrlblab/PTQ."
                },
                "authors": [
                    {
                        "name": "Chongyu Qu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Ye Yu"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Tianyuan Yao"
                    },
                    {
                        "name": "Junchao Zhu"
                    },
                    {
                        "name": "Bennett A. Landman"
                    },
                    {
                        "name": "Yucheng Tang"
                    },
                    {
                        "name": "Yuankai Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yuankai Huo"
                },
                "author": "Yuankai Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10563v2",
                "updated": "2025-01-28T23:05:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    23,
                    5,
                    20,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-17T21:43:27Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    21,
                    43,
                    27,
                    4,
                    17,
                    0
                ],
                "title": "WALLABY Pilot Survey: kNN identification of perturbed galaxies through\n  HI morphometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WALLABY Pilot Survey: kNN identification of perturbed galaxies through\n  HI morphometrics"
                },
                "summary": "Galaxy morphology in stellar light can be described by a series of\n\"non-parametric\" or \"morphometric\" parameters, such as\nconcentration-asymmetry-smoothness, Gini, $M_{20}$, and Sersic fit. These\nparameters can be applied to column density maps of atomic hydrogen (HI). The\nHI distribution is susceptible to perturbations by environmental effects, e.g.\ninter-galactic medium pressure and tidal interactions. Therefore, HI morphology\ncan potentially identify galaxies undergoing ram-pressure stripping or tidal\ninteractions. We explore three fields in the WALLABY Pilot HI survey and\nidentify perturbed galaxies based on a k-nearest Neighbor (kNN) algorithm using\nan HI morphometric feature space. For training, we used labeled galaxies in the\ncombined NGC 4808 and NGC 4636 fields with six HI morphometrics to train and\ntest a kNN classifier. The kNN classification is proficient in classifying\nperturbed galaxies with all metrics -- accuracy, precision and recall -- at\n70-80%. By using the kNN method to identify perturbed galaxies in the\ndeployment field, the NGC 5044 mosaic, we find that in most regards, the\nscaling relations of perturbed and unperturbed galaxies have similar\ndistribution in the scaling relations of stellar mass vs star formation rate\nand the Baryonic Tully-Fisher relation, but the HI and stellar mass relation\nflatter than of the unperturbed galaxies. Our results for NGC 5044 provide a\nprediction for future studies on the fraction of galaxies undergoing\ninteraction in this catalogue and to build a training sample to classify such\ngalaxies in the full WALLABY survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy morphology in stellar light can be described by a series of\n\"non-parametric\" or \"morphometric\" parameters, such as\nconcentration-asymmetry-smoothness, Gini, $M_{20}$, and Sersic fit. These\nparameters can be applied to column density maps of atomic hydrogen (HI). The\nHI distribution is susceptible to perturbations by environmental effects, e.g.\ninter-galactic medium pressure and tidal interactions. Therefore, HI morphology\ncan potentially identify galaxies undergoing ram-pressure stripping or tidal\ninteractions. We explore three fields in the WALLABY Pilot HI survey and\nidentify perturbed galaxies based on a k-nearest Neighbor (kNN) algorithm using\nan HI morphometric feature space. For training, we used labeled galaxies in the\ncombined NGC 4808 and NGC 4636 fields with six HI morphometrics to train and\ntest a kNN classifier. The kNN classification is proficient in classifying\nperturbed galaxies with all metrics -- accuracy, precision and recall -- at\n70-80%. By using the kNN method to identify perturbed galaxies in the\ndeployment field, the NGC 5044 mosaic, we find that in most regards, the\nscaling relations of perturbed and unperturbed galaxies have similar\ndistribution in the scaling relations of stellar mass vs star formation rate\nand the Baryonic Tully-Fisher relation, but the HI and stellar mass relation\nflatter than of the unperturbed galaxies. Our results for NGC 5044 provide a\nprediction for future studies on the fraction of galaxies undergoing\ninteraction in this catalogue and to build a training sample to classify such\ngalaxies in the full WALLABY survey."
                },
                "authors": [
                    {
                        "name": "B. W. Holwerda"
                    },
                    {
                        "name": "Helga Dénes"
                    },
                    {
                        "name": "J. Rhee"
                    },
                    {
                        "name": "D. Leahy"
                    },
                    {
                        "name": "B. Koribalski"
                    },
                    {
                        "name": "N. Yu"
                    },
                    {
                        "name": "N. Deg"
                    },
                    {
                        "name": "T. Westmeier"
                    },
                    {
                        "name": "K. Lee-Waddell"
                    },
                    {
                        "name": "Y. Ascasibar"
                    },
                    {
                        "name": "M. Saraf"
                    },
                    {
                        "name": "X. Lin"
                    },
                    {
                        "name": "B. Catinella"
                    },
                    {
                        "name": "K. Hess"
                    }
                ],
                "author_detail": {
                    "name": "K. Hess"
                },
                "arxiv_affiliation": "Chalmers",
                "author": "K. Hess",
                "arxiv_comment": "21 pages, 15 figures, 8 tables, accepted by PASA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17332v1",
                "updated": "2025-01-28T22:51:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    51,
                    14,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T22:51:14Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    51,
                    14,
                    1,
                    28,
                    0
                ],
                "title": "Compact Neural TTS Voices for Accessibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Neural TTS Voices for Accessibility"
                },
                "summary": "Contemporary text-to-speech solutions for accessibility applications can\ntypically be classified into two categories: (i) device-based statistical\nparametric speech synthesis (SPSS) or unit selection (USEL) and (ii)\ncloud-based neural TTS. SPSS and USEL offer low latency and low disk footprint\nat the expense of naturalness and audio quality. Cloud-based neural TTS systems\nprovide significantly better audio quality and naturalness but regress in terms\nof latency and responsiveness, rendering these impractical for real-world\napplications. More recently, neural TTS models were made deployable to run on\nhandheld devices. Nevertheless, latency remains higher than SPSS and USEL,\nwhile disk footprint prohibits pre-installation for multiple voices at once. In\nthis work, we describe a high-quality compact neural TTS system achieving\nlatency on the order of 15 ms with low disk footprint. The proposed solution is\ncapable of running on low-power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary text-to-speech solutions for accessibility applications can\ntypically be classified into two categories: (i) device-based statistical\nparametric speech synthesis (SPSS) or unit selection (USEL) and (ii)\ncloud-based neural TTS. SPSS and USEL offer low latency and low disk footprint\nat the expense of naturalness and audio quality. Cloud-based neural TTS systems\nprovide significantly better audio quality and naturalness but regress in terms\nof latency and responsiveness, rendering these impractical for real-world\napplications. More recently, neural TTS models were made deployable to run on\nhandheld devices. Nevertheless, latency remains higher than SPSS and USEL,\nwhile disk footprint prohibits pre-installation for multiple voices at once. In\nthis work, we describe a high-quality compact neural TTS system achieving\nlatency on the order of 15 ms with low disk footprint. The proposed solution is\ncapable of running on low-power devices."
                },
                "authors": [
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Eoin Murphy"
                    },
                    {
                        "name": "Deepanshu Gupta"
                    },
                    {
                        "name": "Jonathan Dyke"
                    },
                    {
                        "name": "Saumya Shah"
                    },
                    {
                        "name": "Vasilieios Tsiaras"
                    },
                    {
                        "name": "Petko Petkov"
                    },
                    {
                        "name": "Alistair Conkie"
                    }
                ],
                "author_detail": {
                    "name": "Alistair Conkie"
                },
                "author": "Alistair Conkie",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17330v1",
                "updated": "2025-01-28T22:48:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    48,
                    29,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T22:48:29Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    48,
                    29,
                    1,
                    28,
                    0
                ],
                "title": "Attribution analysis of legal language as used by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution analysis of legal language as used by LLM"
                },
                "summary": "Three publicly-available LLM specifically designed for legal tasks have been\nimplemented and shown that classification accuracy can benefit from training\nover legal corpora, but why and how? Here we use two publicly-available legal\ndatasets, a simpler binary classification task of ``overruling'' texts, and a\nmore elaborate multiple choice task identifying ``holding'' judicial decisions.\nWe report on experiments contrasting the legal LLM and a generic BERT model for\ncomparison, against both datasets. We use integrated gradient attribution\ntechniques to impute ``causes'' of variation in the models' perfomance, and\ncharacterize them in terms of the tokenizations each use. We find that while\nall models can correctly classify some test examples from the casehold task,\nother examples can only be identified by only one, model, and attribution can\nbe used to highlight the reasons for this. We find that differential behavior\nof the models' tokenizers accounts for most of the difference and analyze these\ndifferences in terms of the legal language they process. Frequency analysis of\ntokens generated by dataset texts, combined with use of known ``stop word''\nlists, allow identification of tokens that are clear signifiers of legal\ntopics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three publicly-available LLM specifically designed for legal tasks have been\nimplemented and shown that classification accuracy can benefit from training\nover legal corpora, but why and how? Here we use two publicly-available legal\ndatasets, a simpler binary classification task of ``overruling'' texts, and a\nmore elaborate multiple choice task identifying ``holding'' judicial decisions.\nWe report on experiments contrasting the legal LLM and a generic BERT model for\ncomparison, against both datasets. We use integrated gradient attribution\ntechniques to impute ``causes'' of variation in the models' perfomance, and\ncharacterize them in terms of the tokenizations each use. We find that while\nall models can correctly classify some test examples from the casehold task,\nother examples can only be identified by only one, model, and attribution can\nbe used to highlight the reasons for this. We find that differential behavior\nof the models' tokenizers accounts for most of the difference and analyze these\ndifferences in terms of the legal language they process. Frequency analysis of\ntokens generated by dataset texts, combined with use of known ``stop word''\nlists, allow identification of tokens that are clear signifiers of legal\ntopics."
                },
                "authors": [
                    {
                        "name": "Richard K. Belew"
                    }
                ],
                "author_detail": {
                    "name": "Richard K. Belew"
                },
                "author": "Richard K. Belew",
                "arxiv_comment": "9 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17328v1",
                "updated": "2025-01-28T22:39:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    39,
                    3,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T22:39:03Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    39,
                    3,
                    1,
                    28,
                    0
                ],
                "title": "WASUP: Interpretable Classification with Weight-Input Alignment and\n  Class-Discriminative SUPports Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WASUP: Interpretable Classification with Weight-Input Alignment and\n  Class-Discriminative SUPports Vectors"
                },
                "summary": "The deployment of deep learning models in critical domains necessitates a\nbalance between high accuracy and interpretability. We introduce WASUP, an\ninherently interpretable neural network that provides local and global\nexplanations of its decision-making process. We prove that these explanations\nare faithful by fulfilling established axioms for explanations. Leveraging the\nconcept of case-based reasoning, WASUP extracts class-representative support\nvectors from training images, ensuring they capture relevant features while\nsuppressing irrelevant ones. Classification decisions are made by calculating\nand aggregating similarity scores between these support vectors and the input's\nlatent feature vector. We employ B-Cos transformations, which align model\nweights with inputs to enable faithful mappings of latent features back to the\ninput space, facilitating local explanations in addition to global explanations\nof case-based reasoning. We evaluate WASUP on three tasks: fine-grained\nclassification on Stanford Dogs, multi-label classification on Pascal VOC, and\npathology detection on the RSNA dataset. Results indicate that WASUP not only\nachieves competitive accuracy compared to state-of-the-art black-box models but\nalso offers insightful explanations verified through theoretical analysis. Our\nfindings underscore WASUP's potential for applications where understanding\nmodel decisions is as critical as the decisions themselves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of deep learning models in critical domains necessitates a\nbalance between high accuracy and interpretability. We introduce WASUP, an\ninherently interpretable neural network that provides local and global\nexplanations of its decision-making process. We prove that these explanations\nare faithful by fulfilling established axioms for explanations. Leveraging the\nconcept of case-based reasoning, WASUP extracts class-representative support\nvectors from training images, ensuring they capture relevant features while\nsuppressing irrelevant ones. Classification decisions are made by calculating\nand aggregating similarity scores between these support vectors and the input's\nlatent feature vector. We employ B-Cos transformations, which align model\nweights with inputs to enable faithful mappings of latent features back to the\ninput space, facilitating local explanations in addition to global explanations\nof case-based reasoning. We evaluate WASUP on three tasks: fine-grained\nclassification on Stanford Dogs, multi-label classification on Pascal VOC, and\npathology detection on the RSNA dataset. Results indicate that WASUP not only\nachieves competitive accuracy compared to state-of-the-art black-box models but\nalso offers insightful explanations verified through theoretical analysis. Our\nfindings underscore WASUP's potential for applications where understanding\nmodel decisions is as critical as the decisions themselves."
                },
                "authors": [
                    {
                        "name": "Tom Nuno Wolf"
                    },
                    {
                        "name": "Christian Wachinger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wachinger"
                },
                "author": "Christian Wachinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17326v1",
                "updated": "2025-01-28T22:38:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    38,
                    45,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T22:38:45Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    38,
                    45,
                    1,
                    28,
                    0
                ],
                "title": "Memorize and Rank: Elevating Large Language Models for Clinical\n  Diagnosis Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorize and Rank: Elevating Large Language Models for Clinical\n  Diagnosis Prediction"
                },
                "summary": "Clinical diagnosis prediction models, when provided with a patient's medical\nhistory, aim to detect potential diseases early, facilitating timely\nintervention and improving prognostic outcomes. However, the inherent scarcity\nof patient data and large disease candidate space often pose challenges in\ndeveloping satisfactory models for this intricate task. The exploration of\nleveraging Large Language Models (LLMs) for encapsulating clinical decision\nprocesses has been limited. We introduce MERA, a clinical diagnosis prediction\nmodel that bridges pertaining natural language knowledge with medical practice.\nWe apply hierarchical contrastive learning on a disease candidate ranking list\nto alleviate the large decision space issue. With concept memorization through\nfine-tuning, we bridge the natural language clinical knowledge with medical\ncodes. Experimental results on MIMIC-III and IV datasets show that MERA\nachieves the state-of-the-art diagnosis prediction performance and dramatically\nelevates the diagnosis prediction capabilities of generative LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical diagnosis prediction models, when provided with a patient's medical\nhistory, aim to detect potential diseases early, facilitating timely\nintervention and improving prognostic outcomes. However, the inherent scarcity\nof patient data and large disease candidate space often pose challenges in\ndeveloping satisfactory models for this intricate task. The exploration of\nleveraging Large Language Models (LLMs) for encapsulating clinical decision\nprocesses has been limited. We introduce MERA, a clinical diagnosis prediction\nmodel that bridges pertaining natural language knowledge with medical practice.\nWe apply hierarchical contrastive learning on a disease candidate ranking list\nto alleviate the large decision space issue. With concept memorization through\nfine-tuning, we bridge the natural language clinical knowledge with medical\ncodes. Experimental results on MIMIC-III and IV datasets show that MERA\nachieves the state-of-the-art diagnosis prediction performance and dramatically\nelevates the diagnosis prediction capabilities of generative LMs."
                },
                "authors": [
                    {
                        "name": "Mingyu Derek Ma"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Anthony Cuturrufo"
                    },
                    {
                        "name": "Vijay S Nori"
                    },
                    {
                        "name": "Eran Halperin"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "To appear at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15371v2",
                "updated": "2025-01-28T22:26:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    22,
                    26,
                    37,
                    1,
                    28,
                    0
                ],
                "published": "2024-11-22T22:59:34Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    22,
                    59,
                    34,
                    4,
                    327,
                    0
                ],
                "title": "Safe and Trustworthy Robot Pathfinding with BIM, MHA*, and NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe and Trustworthy Robot Pathfinding with BIM, MHA*, and NLP"
                },
                "summary": "Construction robots have gained significant traction in recent years in\nresearch and development. However, the application of industrial robots has\nunique challenges. Dynamic environments, domain-specific tasks, and complex\nlocalization and mapping are significant obstacles in their development. In\nconstruction job sites, moving objects and complex machinery can make\npathfinding a difficult task due to the possibility of object collisions.\nExisting methods such as simultaneous localization and mapping are viable\nsolutions to this problem, however, due to the precision and data quality\nrequired by the sensors and the processing of the information, they can be very\ncomputationally expensive. We propose using spatial and semantic information in\nbuilding information modeling (BIM) to develop domain-specific pathfinding\nstrategies. In this work, we integrate a multi-heuristic A* (MHA*) algorithm\nusing APFs from the BIM spatial information and process textual information\nfrom the BIM using large language models (LLMs) to adjust the algorithm for\ndynamic object avoidance. We show increased robot object proximity by 80% while\nmaintaining similar path lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction robots have gained significant traction in recent years in\nresearch and development. However, the application of industrial robots has\nunique challenges. Dynamic environments, domain-specific tasks, and complex\nlocalization and mapping are significant obstacles in their development. In\nconstruction job sites, moving objects and complex machinery can make\npathfinding a difficult task due to the possibility of object collisions.\nExisting methods such as simultaneous localization and mapping are viable\nsolutions to this problem, however, due to the precision and data quality\nrequired by the sensors and the processing of the information, they can be very\ncomputationally expensive. We propose using spatial and semantic information in\nbuilding information modeling (BIM) to develop domain-specific pathfinding\nstrategies. In this work, we integrate a multi-heuristic A* (MHA*) algorithm\nusing APFs from the BIM spatial information and process textual information\nfrom the BIM using large language models (LLMs) to adjust the algorithm for\ndynamic object avoidance. We show increased robot object proximity by 80% while\nmaintaining similar path lengths."
                },
                "authors": [
                    {
                        "name": "Mani Amani"
                    },
                    {
                        "name": "Reza Akhavian"
                    }
                ],
                "author_detail": {
                    "name": "Reza Akhavian"
                },
                "author": "Reza Akhavian",
                "arxiv_comment": "Submitted to IEEE Access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17315v1",
                "updated": "2025-01-28T21:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    52,
                    15,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T21:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    52,
                    15,
                    1,
                    28,
                    0
                ],
                "title": "A sketch of an AI control safety case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A sketch of an AI control safety case"
                },
                "summary": "As LLM agents gain a greater capacity to cause harm, AI developers might\nincreasingly rely on control measures such as monitoring to justify that they\nare safe. We sketch how developers could construct a \"control safety case\",\nwhich is a structured argument that models are incapable of subverting control\nmeasures in order to cause unacceptable outcomes. As a case study, we sketch an\nargument that a hypothetical LLM agent deployed internally at an AI company\nwon't exfiltrate sensitive information. The sketch relies on evidence from a\n\"control evaluation,\"' where a red team deliberately designs models to\nexfiltrate data in a proxy for the deployment environment. The safety case then\nhinges on several claims: (1) the red team adequately elicits model\ncapabilities to exfiltrate data, (2) control measures remain at least as\neffective in deployment, and (3) developers conservatively extrapolate model\nperformance to predict the probability of data exfiltration in deployment. This\nsafety case sketch is a step toward more concrete arguments that can be used to\nshow that a dangerously capable LLM agent is safe to deploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM agents gain a greater capacity to cause harm, AI developers might\nincreasingly rely on control measures such as monitoring to justify that they\nare safe. We sketch how developers could construct a \"control safety case\",\nwhich is a structured argument that models are incapable of subverting control\nmeasures in order to cause unacceptable outcomes. As a case study, we sketch an\nargument that a hypothetical LLM agent deployed internally at an AI company\nwon't exfiltrate sensitive information. The sketch relies on evidence from a\n\"control evaluation,\"' where a red team deliberately designs models to\nexfiltrate data in a proxy for the deployment environment. The safety case then\nhinges on several claims: (1) the red team adequately elicits model\ncapabilities to exfiltrate data, (2) control measures remain at least as\neffective in deployment, and (3) developers conservatively extrapolate model\nperformance to predict the probability of data exfiltration in deployment. This\nsafety case sketch is a step toward more concrete arguments that can be used to\nshow that a dangerously capable LLM agent is safe to deploy."
                },
                "authors": [
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Joshua Clymer"
                    },
                    {
                        "name": "Benjamin Hilton"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Geoffrey Irving"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Irving"
                },
                "author": "Geoffrey Irving",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17311v1",
                "updated": "2025-01-28T21:48:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    48,
                    18,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T21:48:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    48,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on\n  Scaled Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on\n  Scaled Platforms"
                },
                "summary": "Autonomous racing presents a complex environment requiring robust controllers\ncapable of making rapid decisions under dynamic conditions. While traditional\ncontrollers based on tire models are reliable, they often demand extensive\ntuning or system identification. RL methods offer significant potential due to\ntheir ability to learn directly from interaction, yet they typically suffer\nfrom the Sim-to-Reall gap, where policies trained in simulation fail to perform\neffectively in the real world. In this paper, we propose RLPP, a residual RL\nframework that enhances a PP controller with an RL-based residual. This hybrid\napproach leverages the reliability and interpretability of PP while using RL to\nfine-tune the controller's performance in real-world scenarios. Extensive\ntesting on the F1TENTH platform demonstrates that RLPP improves lap times by up\nto 6.37 %, closing the gap to the SotA methods by more than 52 % and providing\nreliable performance in zero-shot real-world deployment, overcoming key\nchallenges associated with the Sim-to-Real transfer and reducing the\nperformance gap from simulation to reality by more than 8-fold when compared to\nthe baseline RL controller. The RLPP framework is made available as an\nopen-source tool, encouraging further exploration and advancement in autonomous\nracing research. The code is available at: www.github.com/forzaeth/rlpp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous racing presents a complex environment requiring robust controllers\ncapable of making rapid decisions under dynamic conditions. While traditional\ncontrollers based on tire models are reliable, they often demand extensive\ntuning or system identification. RL methods offer significant potential due to\ntheir ability to learn directly from interaction, yet they typically suffer\nfrom the Sim-to-Reall gap, where policies trained in simulation fail to perform\neffectively in the real world. In this paper, we propose RLPP, a residual RL\nframework that enhances a PP controller with an RL-based residual. This hybrid\napproach leverages the reliability and interpretability of PP while using RL to\nfine-tune the controller's performance in real-world scenarios. Extensive\ntesting on the F1TENTH platform demonstrates that RLPP improves lap times by up\nto 6.37 %, closing the gap to the SotA methods by more than 52 % and providing\nreliable performance in zero-shot real-world deployment, overcoming key\nchallenges associated with the Sim-to-Real transfer and reducing the\nperformance gap from simulation to reality by more than 8-fold when compared to\nthe baseline RL controller. The RLPP framework is made available as an\nopen-source tool, encouraging further exploration and advancement in autonomous\nracing research. The code is available at: www.github.com/forzaeth/rlpp."
                },
                "authors": [
                    {
                        "name": "Edoardo Ghignone"
                    },
                    {
                        "name": "Nicolas Baumann"
                    },
                    {
                        "name": "Cheng Hu"
                    },
                    {
                        "name": "Jonathan Wang"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Andrea Carron"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_comment": "This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The\n  code is available at: www.github.com/forzaeth/rlpp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18062v3",
                "updated": "2025-01-28T21:46:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    46,
                    54,
                    1,
                    28,
                    0
                ],
                "published": "2024-05-28T11:21:45Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    11,
                    21,
                    45,
                    1,
                    149,
                    0
                ],
                "title": "Towards Integrating Emerging AI Applications in SE Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Integrating Emerging AI Applications in SE Education"
                },
                "summary": "Artificial Intelligence (AI) approaches have been incorporated into modern\nlearning environments and software engineering (SE) courses and curricula for\nseveral years. However, with the significant rise in popularity of large\nlanguage models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in\nparticular in the last year, educators are faced with rapidly changing\nclassroom environments and disrupted teaching principles. Examples range from\nprogramming assignment solutions that are fully generated via ChatGPT, to\nvarious forms of cheating during exams. However, despite these negative aspects\nand emerging challenges, AI tools in general, and LLM applications in\nparticular, can also provide significant opportunities in a wide variety of SE\ncourses, supporting both students and educators in meaningful ways. In this\nearly research paper, we present preliminary results of a systematic analysis\nof current trends in the area of AI, and how they can be integrated into\nuniversity-level SE curricula, guidelines, and approaches to support both\ninstructors and learners. We collected both teaching and research papers and\nanalyzed their potential usage in SE education, using the ACM Computer Science\nCurriculum Guidelines CS2023. As an initial outcome, we discuss a series of\nopportunities for AI applications and further research areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) approaches have been incorporated into modern\nlearning environments and software engineering (SE) courses and curricula for\nseveral years. However, with the significant rise in popularity of large\nlanguage models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in\nparticular in the last year, educators are faced with rapidly changing\nclassroom environments and disrupted teaching principles. Examples range from\nprogramming assignment solutions that are fully generated via ChatGPT, to\nvarious forms of cheating during exams. However, despite these negative aspects\nand emerging challenges, AI tools in general, and LLM applications in\nparticular, can also provide significant opportunities in a wide variety of SE\ncourses, supporting both students and educators in meaningful ways. In this\nearly research paper, we present preliminary results of a systematic analysis\nof current trends in the area of AI, and how they can be integrated into\nuniversity-level SE curricula, guidelines, and approaches to support both\ninstructors and learners. We collected both teaching and research papers and\nanalyzed their potential usage in SE education, using the ACM Computer Science\nCurriculum Guidelines CS2023. As an initial outcome, we discuss a series of\nopportunities for AI applications and further research areas."
                },
                "authors": [
                    {
                        "name": "Michael Vierhauser"
                    },
                    {
                        "name": "Iris Groher"
                    },
                    {
                        "name": "Tobias Antensteiner"
                    },
                    {
                        "name": "Clemens Sauerwein"
                    }
                ],
                "author_detail": {
                    "name": "Clemens Sauerwein"
                },
                "author": "Clemens Sauerwein",
                "arxiv_doi": "10.1109/CSEET62301.2024.10663045",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CSEET62301.2024.10663045",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.18062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the 36th Conference on Software\n  Engineering Education and Training (CSEE&T)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17310v1",
                "updated": "2025-01-28T21:43:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T21:43:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding"
                },
                "summary": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``{Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guesstimation, the task of making approximate quantity estimates, is a common\nreal-world challenge. However, it has been largely overlooked in large language\nmodels (LLMs) and vision language models (VLMs) research. We introduce a novel\nguesstimation dataset, MARBLES. This dataset requires one to estimate how many\nitems (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup),\nboth with and without accompanying images. Inspired by the social science\nconcept of the ``{Wisdom of Crowds'' (WOC) - taking the median from estimates\nfrom a crowd), which has proven effective in guesstimation, we propose ``WOC\ndecoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well\non guesstimation, suggesting that they possess some level of a \"world model\"\nnecessary for guesstimation. Moreover, similar to human performance, the WOC\ndecoding method improves LLM/VLM guesstimation accuracy. Furthermore, the\ninclusion of images in the multimodal condition enhances model performance.\nThese results highlight the value of WOC decoding strategy for LLMs/VLMs and\nposition guesstimation as a probe for evaluating LLMs/VLMs' world model."
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Nikunj Harlalka"
                    },
                    {
                        "name": "Sameer Narendran"
                    },
                    {
                        "name": "Alexander Cheung"
                    },
                    {
                        "name": "Sizhe Gao"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy T. Rogers"
                },
                "author": "Timothy T. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05731v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05731v3",
                "updated": "2025-01-28T21:31:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    31,
                    59,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-09T15:41:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    41,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence"
                },
                "summary": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems."
                },
                "authors": [
                    {
                        "name": "Robert Kaufman"
                    },
                    {
                        "name": "Aaron Broukhim"
                    },
                    {
                        "name": "David Kirsh"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "arxiv_doi": "10.1145/3706598.3713088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05731v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05731v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, Accepted to CHI Conference on Human Factors in Computing\n  Systems (CHI 2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10268v2",
                "updated": "2025-01-28T21:31:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    31,
                    38,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-16T14:17:26Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    17,
                    26,
                    4,
                    229,
                    0
                ],
                "title": "Generating Streamlining Constraints with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Streamlining Constraints with Large Language Models"
                },
                "summary": "Streamlining constraints (or streamliners, for short) narrow the search\nspace, enhancing the speed and feasibility of solving complex constraint\nsatisfaction problems. Traditionally, streamliners were crafted manually or\ngenerated through systematically combined atomic constraints with high-effort\noffline testing. Our approach utilizes the creativity of Large Language Models\n(LLMs) to propose effective streamliners for problems specified in the MiniZinc\nconstraint programming language and integrates feedback to the LLM with quick\nempirical tests for validation. Evaluated across seven diverse constraint\nsatisfaction problems, our method achieves substantial runtime reductions. We\ncompare the results to obfuscated and disguised variants of the problem to see\nwhether the results depend on LLM memorization. We also analyze whether longer\noff-line runs improve the quality of streamliners and whether the LLM can\npropose good combinations of streamliners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining constraints (or streamliners, for short) narrow the search\nspace, enhancing the speed and feasibility of solving complex constraint\nsatisfaction problems. Traditionally, streamliners were crafted manually or\ngenerated through systematically combined atomic constraints with high-effort\noffline testing. Our approach utilizes the creativity of Large Language Models\n(LLMs) to propose effective streamliners for problems specified in the MiniZinc\nconstraint programming language and integrates feedback to the LLM with quick\nempirical tests for validation. Evaluated across seven diverse constraint\nsatisfaction problems, our method achieves substantial runtime reductions. We\ncompare the results to obfuscated and disguised variants of the problem to see\nwhether the results depend on LLM memorization. We also analyze whether longer\noff-line runs improve the quality of streamliners and whether the LLM can\npropose good combinations of streamliners."
                },
                "authors": [
                    {
                        "name": "Florentina Voboril"
                    },
                    {
                        "name": "Vaidyanathan Peruvemba Ramaswamy"
                    },
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17299v1",
                "updated": "2025-01-28T21:06:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    6,
                    52,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T21:06:52Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    6,
                    52,
                    1,
                    28,
                    0
                ],
                "title": "\"Ownership, Not Just Happy Talk\": Co-Designing a Participatory Large\n  Language Model for Journalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Ownership, Not Just Happy Talk\": Co-Designing a Participatory Large\n  Language Model for Journalism"
                },
                "summary": "Journalism has emerged as an essential domain for understanding the uses,\nlimitations, and impacts of large language models (LLMs) in the workplace. News\norganizations face divergent financial incentives: LLMs already permeate\nnewswork processes within financially constrained organizations, even as\nongoing legal challenges assert that AI companies violate their copyright. At\nstake are key questions about what LLMs are created to do, and by whom: How\nmight a journalist-led LLM work, and what can participatory design illuminate\nabout the present-day challenges about adapting ``one-size-fits-all''\nfoundation models to a given context of use? In this paper, we undertake a\nco-design exploration to understand how a participatory approach to LLMs might\naddress opportunities and challenges around AI in journalism. Our 20 interviews\nwith reporters, data journalists, editors, labor organizers, product leads, and\nexecutives highlight macro, meso, and micro tensions that designing for this\nopportunity space must address. From these desiderata, we describe the result\nof our co-design work: organizational structures and functionality for a\njournalist-controlled LLM. In closing, we discuss the limitations of commercial\nfoundation models for workplace use, and the methodological implications of\napplying participatory methods to LLM co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Journalism has emerged as an essential domain for understanding the uses,\nlimitations, and impacts of large language models (LLMs) in the workplace. News\norganizations face divergent financial incentives: LLMs already permeate\nnewswork processes within financially constrained organizations, even as\nongoing legal challenges assert that AI companies violate their copyright. At\nstake are key questions about what LLMs are created to do, and by whom: How\nmight a journalist-led LLM work, and what can participatory design illuminate\nabout the present-day challenges about adapting ``one-size-fits-all''\nfoundation models to a given context of use? In this paper, we undertake a\nco-design exploration to understand how a participatory approach to LLMs might\naddress opportunities and challenges around AI in journalism. Our 20 interviews\nwith reporters, data journalists, editors, labor organizers, product leads, and\nexecutives highlight macro, meso, and micro tensions that designing for this\nopportunity space must address. From these desiderata, we describe the result\nof our co-design work: organizational structures and functionality for a\njournalist-controlled LLM. In closing, we discuss the limitations of commercial\nfoundation models for workplace use, and the methodological implications of\napplying participatory methods to LLM co-design."
                },
                "authors": [
                    {
                        "name": "Emily Tseng"
                    },
                    {
                        "name": "Meg Young"
                    },
                    {
                        "name": "Marianne Aubin Le Quéré"
                    },
                    {
                        "name": "Aimee Rinehart"
                    },
                    {
                        "name": "Harini Suresh"
                    }
                ],
                "author_detail": {
                    "name": "Harini Suresh"
                },
                "author": "Harini Suresh",
                "arxiv_comment": "Under review for an ACM conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17295v1",
                "updated": "2025-01-28T20:58:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    58,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T20:58:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    58,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Mitigating Hallucinated Translations in Large Language Models with\n  Hallucination-focused Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinated Translations in Large Language Models with\n  Hallucination-focused Preference Optimization"
                },
                "summary": "Machine Translation (MT) is undergoing a paradigm shift, with systems based\non fine-tuned large language models (LLM) becoming increasingly competitive\nwith traditional encoder-decoder models trained specifically for translation\ntasks. However, LLM-based systems are at a higher risk of generating\nhallucinations, which can severely undermine user's trust and safety. Most\nprior research on hallucination mitigation focuses on traditional MT models,\nwith solutions that involve post-hoc mitigation - detecting hallucinated\ntranslations and re-translating them. While effective, this approach introduces\nadditional complexity in deploying extra tools in production and also increases\nlatency. To address these limitations, we propose a method that intrinsically\nlearns to mitigate hallucinations during the model training phase.\nSpecifically, we introduce a data creation framework to generate hallucination\nfocused preference datasets. Fine-tuning LLMs on these preference datasets\nreduces the hallucination rate by an average of 96% across five language pairs,\nwhile preserving overall translation quality. In a zero-shot setting our\napproach reduces hallucinations by 89% on an average across three unseen target\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation (MT) is undergoing a paradigm shift, with systems based\non fine-tuned large language models (LLM) becoming increasingly competitive\nwith traditional encoder-decoder models trained specifically for translation\ntasks. However, LLM-based systems are at a higher risk of generating\nhallucinations, which can severely undermine user's trust and safety. Most\nprior research on hallucination mitigation focuses on traditional MT models,\nwith solutions that involve post-hoc mitigation - detecting hallucinated\ntranslations and re-translating them. While effective, this approach introduces\nadditional complexity in deploying extra tools in production and also increases\nlatency. To address these limitations, we propose a method that intrinsically\nlearns to mitigate hallucinations during the model training phase.\nSpecifically, we introduce a data creation framework to generate hallucination\nfocused preference datasets. Fine-tuning LLMs on these preference datasets\nreduces the hallucination rate by an average of 96% across five language pairs,\nwhile preserving overall translation quality. In a zero-shot setting our\napproach reduces hallucinations by 89% on an average across three unseen target\nlanguages."
                },
                "authors": [
                    {
                        "name": "Zilu Tang"
                    },
                    {
                        "name": "Rajen Chatterjee"
                    },
                    {
                        "name": "Sarthak Garg"
                    }
                ],
                "author_detail": {
                    "name": "Sarthak Garg"
                },
                "author": "Sarthak Garg",
                "arxiv_comment": "NAACL 2025 Main Conference Long paper (9 pages)",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17286v1",
                "updated": "2025-01-28T20:37:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    37,
                    32,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T20:37:32Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    37,
                    32,
                    1,
                    28,
                    0
                ],
                "title": "Fine-Tuning Open-Source Large Language Models to Improve Their\n  Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate\n  Their Potential Clinical Applications in Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Open-Source Large Language Models to Improve Their\n  Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate\n  Their Potential Clinical Applications in Radiation Oncology"
                },
                "summary": "Background: The radiation oncology clinical practice involves many steps\nrelying on the dynamic interplay of abundant text data. Large language models\nhave displayed remarkable capabilities in processing complex text information.\nBut their direct applications in specific fields like radiation oncology remain\nunderexplored.\n  Purpose: This study aims to investigate whether fine-tuning LLMs with domain\nknowledge can improve the performance on Task (1) treatment regimen generation,\nTask (2) treatment modality selection (photon, proton, electron, or\nbrachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.\n  Methods: Data for 15,724 patient cases were extracted. Cases where patients\nhad a single diagnostic record, and a clearly identifiable primary treatment\nplan were selected for preprocessing and manual annotation to have 7,903 cases\nof the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.\nEach case was used to construct a pair consisting of patient diagnostics\ndetails and an answer (treatment regimen, treatment modality, or ICD-10 code\nrespectively) for the supervised fine-tuning of these three tasks. Open source\nLLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the\nLow-Rank Approximations method. Accuracy and ROUGE-1 score were reported for\nthe fine-tuned models and original models. Clinical evaluation was performed on\nTask (1) by radiation oncologists, while precision, recall, and F-1 score were\nevaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used\nto statistically analyze the results.\n  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with\np-value <= 0.001. Clinical evaluation demonstrated that over 60% of the\nfine-tuned LLMs-generated treatment regimens were clinically acceptable.\nPrecision, recall, and F1-score showed improved performance of fine-tuned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The radiation oncology clinical practice involves many steps\nrelying on the dynamic interplay of abundant text data. Large language models\nhave displayed remarkable capabilities in processing complex text information.\nBut their direct applications in specific fields like radiation oncology remain\nunderexplored.\n  Purpose: This study aims to investigate whether fine-tuning LLMs with domain\nknowledge can improve the performance on Task (1) treatment regimen generation,\nTask (2) treatment modality selection (photon, proton, electron, or\nbrachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.\n  Methods: Data for 15,724 patient cases were extracted. Cases where patients\nhad a single diagnostic record, and a clearly identifiable primary treatment\nplan were selected for preprocessing and manual annotation to have 7,903 cases\nof the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.\nEach case was used to construct a pair consisting of patient diagnostics\ndetails and an answer (treatment regimen, treatment modality, or ICD-10 code\nrespectively) for the supervised fine-tuning of these three tasks. Open source\nLLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the\nLow-Rank Approximations method. Accuracy and ROUGE-1 score were reported for\nthe fine-tuned models and original models. Clinical evaluation was performed on\nTask (1) by radiation oncologists, while precision, recall, and F-1 score were\nevaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used\nto statistically analyze the results.\n  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with\np-value <= 0.001. Clinical evaluation demonstrated that over 60% of the\nfine-tuned LLMs-generated treatment regimens were clinically acceptable.\nPrecision, recall, and F1-score showed improved performance of fine-tuned LLMs."
                },
                "authors": [
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Brady S. Laughlin"
                    },
                    {
                        "name": "Diego Santos Toesca"
                    },
                    {
                        "name": "Sujay A. Vora"
                    },
                    {
                        "name": "Samir H. Patel"
                    },
                    {
                        "name": "Terence T. Sio"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17282v1",
                "updated": "2025-01-28T20:30:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    30,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T20:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    30,
                    36,
                    1,
                    28,
                    0
                ],
                "title": "From Natural Language to Extensive-Form Game Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Extensive-Form Game Representations"
                },
                "summary": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success."
                },
                "authors": [
                    {
                        "name": "Shilong Deng"
                    },
                    {
                        "name": "Yongzhao Wang"
                    },
                    {
                        "name": "Rahul Savani"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Savani"
                },
                "author": "Rahul Savani",
                "arxiv_comment": "This work has been accepted as a full paper for AAMAS 2025. This is a\n  full version of the AAMAS 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15014v2",
                "updated": "2025-01-28T20:29:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    29,
                    44,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-25T01:37:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    37,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments"
                },
                "summary": "Resource-constrained edge deployments demand AI solutions that balance high\nperformance with stringent compute, memory, and energy limitations. In this\nsurvey, we present a comprehensive overview of the primary strategies for\naccelerating deep learning models under such constraints. First, we examine\nmodel compression techniques-pruning, quantization, tensor decomposition, and\nknowledge distillation-that streamline large models into smaller, faster, and\nmore efficient variants. Next, we explore Neural Architecture Search (NAS), a\nclass of automated methods that discover architectures inherently optimized for\nparticular tasks and hardware budgets. We then discuss compiler and deployment\nframeworks, such as TVM, TensorRT, and OpenVINO, which provide\nhardware-tailored optimizations at inference time. By integrating these three\npillars into unified pipelines, practitioners can achieve multi-objective\ngoals, including latency reduction, memory savings, and energy efficiency-all\nwhile maintaining competitive accuracy. We also highlight emerging frontiers in\nhierarchical NAS, neurosymbolic approaches, and advanced distillation tailored\nto large language models, underscoring open challenges like pre-training\npruning for massive networks. Our survey offers practical insights, identifies\ncurrent research gaps, and outlines promising directions for building scalable,\nplatform-independent frameworks to accelerate deep learning models at the edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-constrained edge deployments demand AI solutions that balance high\nperformance with stringent compute, memory, and energy limitations. In this\nsurvey, we present a comprehensive overview of the primary strategies for\naccelerating deep learning models under such constraints. First, we examine\nmodel compression techniques-pruning, quantization, tensor decomposition, and\nknowledge distillation-that streamline large models into smaller, faster, and\nmore efficient variants. Next, we explore Neural Architecture Search (NAS), a\nclass of automated methods that discover architectures inherently optimized for\nparticular tasks and hardware budgets. We then discuss compiler and deployment\nframeworks, such as TVM, TensorRT, and OpenVINO, which provide\nhardware-tailored optimizations at inference time. By integrating these three\npillars into unified pipelines, practitioners can achieve multi-objective\ngoals, including latency reduction, memory savings, and energy efficiency-all\nwhile maintaining competitive accuracy. We also highlight emerging frontiers in\nhierarchical NAS, neurosymbolic approaches, and advanced distillation tailored\nto large language models, underscoring open challenges like pre-training\npruning for massive networks. Our survey offers practical insights, identifies\ncurrent research gaps, and outlines promising directions for building scalable,\nplatform-independent frameworks to accelerate deep learning models at the edge."
                },
                "authors": [
                    {
                        "name": "Jacob Sander"
                    },
                    {
                        "name": "Achraf Cohen"
                    },
                    {
                        "name": "Venkat R. Dasari"
                    },
                    {
                        "name": "Brent Venable"
                    },
                    {
                        "name": "Brian Jalaian"
                    }
                ],
                "author_detail": {
                    "name": "Brian Jalaian"
                },
                "author": "Brian Jalaian",
                "arxiv_comment": "26 pages, 13 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17273v1",
                "updated": "2025-01-28T20:06:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    6,
                    9,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T20:06:09Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    6,
                    9,
                    1,
                    28,
                    0
                ],
                "title": "Tailored Truths: Optimizing LLM Persuasion with Personalization and\n  Fabricated Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Truths: Optimizing LLM Persuasion with Personalization and\n  Fabricated Statistics"
                },
                "summary": "Large Language Models (LLMs) are becoming increasingly persuasive,\ndemonstrating the ability to personalize arguments in conversation with humans\nby leveraging their personal data. This may have serious impacts on the scale\nand effectiveness of disinformation campaigns. We studied the persuasiveness of\nLLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated\narguments intended to change the human's opinion. We quantified the LLM's\neffect by measuring human agreement with the debate's hypothesis pre- and\npost-debate and analyzing both the magnitude of opinion change, as well as the\nlikelihood of an update in the LLM's direction. We compare persuasiveness\nacross established persuasion strategies, including personalized arguments\ninformed by user demographics and personality, appeal to fabricated statistics,\nand a mixed strategy utilizing both personalized arguments and fabricated\nstatistics. We found that static arguments generated by humans and GPT-4o-mini\nhave comparable persuasive power. However, the LLM outperformed static\nhuman-written arguments when leveraging the mixed strategy in an interactive\ndebate setting. This approach had a $\\mathbf{51\\%}$ chance of persuading\nparticipants to modify their initial position, compared to $\\mathbf{32\\%}$ for\nthe static human-written arguments. Our results highlight the concerning\npotential for LLMs to enable inexpensive and persuasive large-scale\ndisinformation campaigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly persuasive,\ndemonstrating the ability to personalize arguments in conversation with humans\nby leveraging their personal data. This may have serious impacts on the scale\nand effectiveness of disinformation campaigns. We studied the persuasiveness of\nLLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated\narguments intended to change the human's opinion. We quantified the LLM's\neffect by measuring human agreement with the debate's hypothesis pre- and\npost-debate and analyzing both the magnitude of opinion change, as well as the\nlikelihood of an update in the LLM's direction. We compare persuasiveness\nacross established persuasion strategies, including personalized arguments\ninformed by user demographics and personality, appeal to fabricated statistics,\nand a mixed strategy utilizing both personalized arguments and fabricated\nstatistics. We found that static arguments generated by humans and GPT-4o-mini\nhave comparable persuasive power. However, the LLM outperformed static\nhuman-written arguments when leveraging the mixed strategy in an interactive\ndebate setting. This approach had a $\\mathbf{51\\%}$ chance of persuading\nparticipants to modify their initial position, compared to $\\mathbf{32\\%}$ for\nthe static human-written arguments. Our results highlight the concerning\npotential for LLMs to enable inexpensive and persuasive large-scale\ndisinformation campaigns."
                },
                "authors": [
                    {
                        "name": "Jasper Timm"
                    },
                    {
                        "name": "Chetan Talele"
                    },
                    {
                        "name": "Jacob Haimes"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Haimes"
                },
                "author": "Jacob Haimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14063v3",
                "updated": "2025-01-28T19:56:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    19,
                    56,
                    36,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-18T17:08:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification"
                },
                "summary": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven."
                },
                "authors": [
                    {
                        "name": "Kyle Thompson"
                    },
                    {
                        "name": "Nuno Saavedra"
                    },
                    {
                        "name": "Pedro Carrott"
                    },
                    {
                        "name": "Kevin Fisher"
                    },
                    {
                        "name": "Alex Sanchez-Stern"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "João F. Ferreira"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Emily First"
                    }
                ],
                "author_detail": {
                    "name": "Emily First"
                },
                "author": "Emily First",
                "arxiv_comment": "In Proceedings of the 47th International Conference on Software\n  Engineering (ICSE), Ottawa, ON, Canada, April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06131v2",
                "updated": "2025-01-28T19:18:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    19,
                    18,
                    51,
                    1,
                    28,
                    0
                ],
                "published": "2024-09-10T00:59:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    0,
                    59,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review"
                },
                "summary": "Traditional Large Language Model (LLM) pretraining relies on autoregressive\nlanguage modeling with randomly sampled data from web-scale datasets. Inspired\nby human learning techniques like spaced repetition, we hypothesize that random\nsampling leads to high training costs, lower-quality models, and significant\ndata forgetting. To address these inefficiencies, we propose the\nLearn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to\nthe model's learning progress. LFR tracks the model's learning performance\nacross data blocks (sequences of tokens) and prioritizes revisiting challenging\nregions of the dataset that are more prone to being forgotten, enabling better\nretention and more efficient learning. Using the LFR paradigm, we pretrained\nLlama and GPT models on the SlimPajama and OpenWebText datasets, respectively.\nThese models were evaluated on downstream tasks across various domains,\nincluding question answering, problem-solving, commonsense reasoning, language\nmodeling, and translation. Compared to baseline models trained on the full\ndatasets, LFR consistently achieved lower perplexity and higher accuracy, while\nusing only 5%--19% of the training tokens. Furthermore, LFR matched the\nperformance of industry-standard Pythia models with up to 2$\\times$ the\nparameter count, using just 3.2% of the training tokens, demonstrating its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Large Language Model (LLM) pretraining relies on autoregressive\nlanguage modeling with randomly sampled data from web-scale datasets. Inspired\nby human learning techniques like spaced repetition, we hypothesize that random\nsampling leads to high training costs, lower-quality models, and significant\ndata forgetting. To address these inefficiencies, we propose the\nLearn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to\nthe model's learning progress. LFR tracks the model's learning performance\nacross data blocks (sequences of tokens) and prioritizes revisiting challenging\nregions of the dataset that are more prone to being forgotten, enabling better\nretention and more efficient learning. Using the LFR paradigm, we pretrained\nLlama and GPT models on the SlimPajama and OpenWebText datasets, respectively.\nThese models were evaluated on downstream tasks across various domains,\nincluding question answering, problem-solving, commonsense reasoning, language\nmodeling, and translation. Compared to baseline models trained on the full\ndatasets, LFR consistently achieved lower perplexity and higher accuracy, while\nusing only 5%--19% of the training tokens. Furthermore, LFR matched the\nperformance of industry-standard Pythia models with up to 2$\\times$ the\nparameter count, using just 3.2% of the training tokens, demonstrating its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Jui-Nan Yen"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17144v1",
                "updated": "2025-01-28T18:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:45:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    45,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data"
                },
                "summary": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size."
                },
                "authors": [
                    {
                        "name": "Deren Lei"
                    },
                    {
                        "name": "Yaxi Li"
                    },
                    {
                        "name": "Siyao Li"
                    },
                    {
                        "name": "Mengya Hu"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Ken Archer"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Emily Ching"
                    },
                    {
                        "name": "Alex Deng"
                    }
                ],
                "author_detail": {
                    "name": "Alex Deng"
                },
                "author": "Alex Deng",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07272v2",
                "updated": "2025-01-28T18:40:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    40,
                    26,
                    1,
                    28,
                    0
                ],
                "published": "2024-08-14T03:42:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "Abstract Operations Research Modeling Using Natural Language Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Operations Research Modeling Using Natural Language Inputs"
                },
                "summary": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Ryan Wickman"
                    },
                    {
                        "name": "Sahil Bhatnagar"
                    },
                    {
                        "name": "Raj Kumar Maity"
                    },
                    {
                        "name": "Arko Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Arko Mukherjee"
                },
                "author": "Arko Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17132v1",
                "updated": "2025-01-28T18:25:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:25:11Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    25,
                    11,
                    1,
                    28,
                    0
                ],
                "title": "ASTRAL: Automated Safety Testing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRAL: Automated Safety Testing of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors."
                },
                "authors": [
                    {
                        "name": "Miriam Ugarte"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Sergio Segura"
                    },
                    {
                        "name": "Aitor Arrieta"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Arrieta"
                },
                "author": "Aitor Arrieta",
                "arxiv_journal_ref": "The 6th ACM/IEEE International Conference on Automation of\n  Software Test (AST 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17131v1",
                "updated": "2025-01-28T18:23:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    23,
                    12,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:23:12Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    23,
                    12,
                    1,
                    28,
                    0
                ],
                "title": "Scenario Understanding of Traffic Scenes Through Large Visual Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario Understanding of Traffic Scenes Through Large Visual Language\n  Models"
                },
                "summary": "Deep learning models for autonomous driving, encompassing perception,\nplanning, and control, depend on vast datasets to achieve their high\nperformance. However, their generalization often suffers due to domain-specific\ndata distributions, making an effective scene-based categorization of samples\nnecessary to improve their reliability across diverse domains. Manual\ncaptioning, though valuable, is both labor-intensive and time-consuming,\ncreating a bottleneck in the data annotation process. Large Visual Language\nModels (LVLMs) present a compelling solution by automating image analysis and\ncategorization through contextual queries, often without requiring retraining\nfor new categories. In this study, we evaluate the capabilities of LVLMs,\nincluding GPT-4 and LLaVA, to understand and classify urban traffic scenes on\nboth an in-house dataset and the BDD100K. We propose a scalable captioning\npipeline that integrates state-of-the-art models, enabling a flexible\ndeployment on new datasets. Our analysis, combining quantitative metrics with\nqualitative insights, demonstrates the effectiveness of LVLMs to understand\nurban traffic scenarios and highlights their potential as an efficient tool for\ndata-driven advancements in autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models for autonomous driving, encompassing perception,\nplanning, and control, depend on vast datasets to achieve their high\nperformance. However, their generalization often suffers due to domain-specific\ndata distributions, making an effective scene-based categorization of samples\nnecessary to improve their reliability across diverse domains. Manual\ncaptioning, though valuable, is both labor-intensive and time-consuming,\ncreating a bottleneck in the data annotation process. Large Visual Language\nModels (LVLMs) present a compelling solution by automating image analysis and\ncategorization through contextual queries, often without requiring retraining\nfor new categories. In this study, we evaluate the capabilities of LVLMs,\nincluding GPT-4 and LLaVA, to understand and classify urban traffic scenes on\nboth an in-house dataset and the BDD100K. We propose a scalable captioning\npipeline that integrates state-of-the-art models, enabling a flexible\ndeployment on new datasets. Our analysis, combining quantitative metrics with\nqualitative insights, demonstrates the effectiveness of LVLMs to understand\nurban traffic scenarios and highlights their potential as an efficient tool for\ndata-driven advancements in autonomous driving."
                },
                "authors": [
                    {
                        "name": "Rivera Esteban"
                    },
                    {
                        "name": "Lübberstedt Jannik"
                    },
                    {
                        "name": "Nico Uhlemann"
                    },
                    {
                        "name": "Markus Lienkamp"
                    }
                ],
                "author_detail": {
                    "name": "Markus Lienkamp"
                },
                "author": "Markus Lienkamp",
                "arxiv_comment": "Accepted at WACV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17117v1",
                "updated": "2025-01-28T18:07:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:07:30Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    7,
                    30,
                    1,
                    28,
                    0
                ],
                "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Histoires Morales: A French Dataset for Assessing Moral Alignment"
                },
                "summary": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata."
                },
                "authors": [
                    {
                        "name": "Thibaud Leteno"
                    },
                    {
                        "name": "Irina Proskurina"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Julien Velcin"
                    },
                    {
                        "name": "Charlotte Laclau"
                    },
                    {
                        "name": "Guillaume Metzler"
                    },
                    {
                        "name": "Christophe Gravier"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Gravier"
                },
                "author": "Christophe Gravier",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17116v1",
                "updated": "2025-01-28T18:04:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:04:50Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    4,
                    50,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Large Language Model Training Using FP4 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Model Training Using FP4 Quantization"
                },
                "summary": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training."
                },
                "authors": [
                    {
                        "name": "Ruizhe Wang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Baining Guo"
                    },
                    {
                        "name": "Zhengjun Zha"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v2",
                "updated": "2025-01-28T18:00:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    0,
                    22,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]