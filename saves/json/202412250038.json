[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v1",
                "updated": "2024-12-13T06:00:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce LSH-E, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that\nare cosine dissimilar to the current query token. This is achieved by computing\nthe Hamming distance between binarized Gaussian projections of the current\ntoken query and cached token keys, with a projection length much smaller than\nthe embedding dimension. We maintain a lightweight binary structure in GPU\nmemory to facilitate these calculations. Unlike existing compression strategies\nthat compute attention to determine token retention, LSH-E makes these\ndecisions pre-attention, thereby reducing computational costs. Additionally,\nLSH-E is dynamic - at every decoding step, the key and value of the current\ntoken replace the embeddings of a token expected to produce the lowest\nattention score. We demonstrate that LSH-E can compress the KV cache by 30%-70%\nwhile maintaining high performance across reasoning, multiple-choice,\nlong-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce LSH-E, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that\nare cosine dissimilar to the current query token. This is achieved by computing\nthe Hamming distance between binarized Gaussian projections of the current\ntoken query and cached token keys, with a projection length much smaller than\nthe embedding dimension. We maintain a lightweight binary structure in GPU\nmemory to facilitate these calculations. Unlike existing compression strategies\nthat compute attention to determine token retention, LSH-E makes these\ndecisions pre-attention, thereby reducing computational costs. Additionally,\nLSH-E is dynamic - at every decoding step, the key and value of the current\ntoken replace the embeddings of a token expected to produce the lowest\nattention score. We demonstrate that LSH-E can compress the KV cache by 30%-70%\nwhile maintaining high performance across reasoning, multiple-choice,\nlong-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.11692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11692v2",
                "updated": "2024-12-23T18:46:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    46,
                    34,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-16T12:10:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    10,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference"
                },
                "summary": "Tree-based models for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning, as\na data-dependent partition system that allows deeper expansion only in regions\nwith more observations would induce double dipping of the data and thus lead to\ninconsistent inference. We propose a simple strategy to restore coherency while\nallowing the candidate partitions to be data-dependent, using Cox's partial\nlikelihood. This strategy parametrizes the tree-based sampling model according\nto the allocation of probability mass based on the observed data, and yet under\nappropriate specification, the resulting inference remains valid. Our partial\nlikelihood approach is broadly applicable to existing likelihood-based methods\nand in particular to Bayesian inference on tree-based models. We give examples\nin density estimation in which the partial likelihood is endowed with existing\npriors on tree-based models and compare with the standard, full-likelihood\napproach. The results show substantial gains in estimation accuracy and\ncomputational efficiency from using the partial likelihood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based models for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning, as\na data-dependent partition system that allows deeper expansion only in regions\nwith more observations would induce double dipping of the data and thus lead to\ninconsistent inference. We propose a simple strategy to restore coherency while\nallowing the candidate partitions to be data-dependent, using Cox's partial\nlikelihood. This strategy parametrizes the tree-based sampling model according\nto the allocation of probability mass based on the observed data, and yet under\nappropriate specification, the resulting inference remains valid. Our partial\nlikelihood approach is broadly applicable to existing likelihood-based methods\nand in particular to Bayesian inference on tree-based models. We give examples\nin density estimation in which the partial likelihood is endowed with existing\npriors on tree-based models and compare with the standard, full-likelihood\napproach. The results show substantial gains in estimation accuracy and\ncomputational efficiency from using the partial likelihood."
                },
                "authors": [
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Benedetta Bruni"
                    }
                ],
                "author_detail": {
                    "name": "Benedetta Bruni"
                },
                "author": "Benedetta Bruni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17779v1",
                "updated": "2024-12-23T18:38:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:38:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "Ergodic Network Stochastic Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ergodic Network Stochastic Differential Equations"
                },
                "summary": "We propose a novel framework for Network Stochastic Differential Equations\n(N-SDE), where each node in a network is governed by an SDE influenced by\ninteractions with its neighbors. The evolution of each node is driven by the\ninterplay of three key components: the node's intrinsic dynamics\n(\\emph{momentum effect}), feedback from neighboring nodes (\\emph{network\neffect}), and a \\emph{stochastic volatility} term modeled by Brownian motion.\n  Our primary objective is to estimate the parameters of the N-SDE system from\nhigh-frequency discrete-time observations. The motivation behind this model\nlies in its ability to analyze very high-dimensional time series by leveraging\nthe inherent sparsity of the underlying network graph.\n  We consider two distinct scenarios: \\textit{i) known network structure}: the\ngraph is fully specified, and we establish conditions under which the\nparameters can be identified, considering the quadratic growth of the parameter\nspace with the number of edges. \\textit{ii) unknown network structure}: the\ngraph must be inferred from the data. For this, we develop an iterative\nprocedure using adaptive Lasso, tailored to a specific subclass of N-SDE\nmodels. In this work, we assume the network graph is oriented, paving the way\nfor novel applications of SDEs in causal inference, enabling the study of\ncause-effect relationships in dynamic systems.\n  Through extensive simulation studies, we demonstrate the performance of our\nestimators across various graph topologies in high-dimensional settings. We\nalso showcase the framework's applicability to real-world datasets,\nhighlighting its potential for advancing the analysis of complex networked\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for Network Stochastic Differential Equations\n(N-SDE), where each node in a network is governed by an SDE influenced by\ninteractions with its neighbors. The evolution of each node is driven by the\ninterplay of three key components: the node's intrinsic dynamics\n(\\emph{momentum effect}), feedback from neighboring nodes (\\emph{network\neffect}), and a \\emph{stochastic volatility} term modeled by Brownian motion.\n  Our primary objective is to estimate the parameters of the N-SDE system from\nhigh-frequency discrete-time observations. The motivation behind this model\nlies in its ability to analyze very high-dimensional time series by leveraging\nthe inherent sparsity of the underlying network graph.\n  We consider two distinct scenarios: \\textit{i) known network structure}: the\ngraph is fully specified, and we establish conditions under which the\nparameters can be identified, considering the quadratic growth of the parameter\nspace with the number of edges. \\textit{ii) unknown network structure}: the\ngraph must be inferred from the data. For this, we develop an iterative\nprocedure using adaptive Lasso, tailored to a specific subclass of N-SDE\nmodels. In this work, we assume the network graph is oriented, paving the way\nfor novel applications of SDEs in causal inference, enabling the study of\ncause-effect relationships in dynamic systems.\n  Through extensive simulation studies, we demonstrate the performance of our\nestimators across various graph topologies in high-dimensional settings. We\nalso showcase the framework's applicability to real-world datasets,\nhighlighting its potential for advancing the analysis of complex networked\nsystems."
                },
                "authors": [
                    {
                        "name": "Francesco Iafrate"
                    },
                    {
                        "name": "Stefano Iacus"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Iacus"
                },
                "author": "Stefano Iacus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06608v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v4",
                "updated": "2024-12-23T18:38:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompting Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17767v1",
                "updated": "2024-12-23T18:26:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    26,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:26:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    26,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "ResearchTown: Simulator of Human Research Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchTown: Simulator of Human Research Community"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\nscientific domains, yet a fundamental question remains unanswered: Can we\nsimulate human research communities with LLMs? Addressing this question can\ndeepen our understanding of the processes behind idea brainstorming and inspire\nthe automatic discovery of novel scientific insights. In this work, we propose\nResearchTown, a multi-agent framework for research community simulation. Within\nthis framework, the human research community is simplified and modeled as an\nagent-data graph, where researchers and papers are represented as agent-type\nand data-type nodes, respectively, and connected based on their collaboration\nrelationships. We also introduce TextGNN, a text-based inference framework that\nmodels various research activities (e.g., paper reading, paper writing, and\nreview writing) as special forms of a unified message-passing process on the\nagent-data graph. To evaluate the quality of the research simulation, we\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\nscalable and objective assessment based on similarity. Our experiments reveal\nthree key findings: (1) ResearchTown can provide a realistic simulation of\ncollaborative research activities, including paper writing and review writing;\n(2) ResearchTown can maintain robust simulation with multiple researchers and\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\nthat potentially inspire novel research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable potential in\nscientific domains, yet a fundamental question remains unanswered: Can we\nsimulate human research communities with LLMs? Addressing this question can\ndeepen our understanding of the processes behind idea brainstorming and inspire\nthe automatic discovery of novel scientific insights. In this work, we propose\nResearchTown, a multi-agent framework for research community simulation. Within\nthis framework, the human research community is simplified and modeled as an\nagent-data graph, where researchers and papers are represented as agent-type\nand data-type nodes, respectively, and connected based on their collaboration\nrelationships. We also introduce TextGNN, a text-based inference framework that\nmodels various research activities (e.g., paper reading, paper writing, and\nreview writing) as special forms of a unified message-passing process on the\nagent-data graph. To evaluate the quality of the research simulation, we\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\nscalable and objective assessment based on similarity. Our experiments reveal\nthree key findings: (1) ResearchTown can provide a realistic simulation of\ncollaborative research activities, including paper writing and review writing;\n(2) ResearchTown can maintain robust simulation with multiple researchers and\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\nthat potentially inspire novel research directions."
                },
                "authors": [
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Zhaochen Hong"
                    },
                    {
                        "name": "Zirui Cheng"
                    },
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Keyang Xuan"
                    },
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17762v1",
                "updated": "2024-12-23T18:18:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    18,
                    7,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:18:07Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    18,
                    7,
                    0,
                    358,
                    0
                ],
                "title": "The Superposition of Diffusion Models Using the It Density Estimator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Superposition of Diffusion Models Using the It Density Estimator"
                },
                "summary": "The Cambrian explosion of easily accessible pre-trained diffusion models\nsuggests a demand for methods that combine multiple different pre-trained\ndiffusion models without incurring the significant computational burden of\nre-training a larger combined model. In this paper, we cast the problem of\ncombining multiple pre-trained diffusion models at the generation stage under a\nnovel proposed framework termed superposition. Theoretically, we derive\nsuperposition from rigorous first principles stemming from the celebrated\ncontinuity equation and design two novel algorithms tailor-made for combining\ndiffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density\nestimator for the log likelihood of the diffusion SDE which incurs no\nadditional overhead compared to the well-known Hutchinson's estimator needed\nfor divergence calculations. We demonstrate that SuperDiff is scalable to large\npre-trained diffusion models as superposition is performed solely through\ncomposition during inference, and also enjoys painless implementation as it\ncombines different pre-trained vector fields through an automated re-weighting\nscheme. Notably, we show that SuperDiff is efficient during inference time, and\nmimics traditional composition operators such as the logical OR and the logical\nAND. We empirically demonstrate the utility of using SuperDiff for generating\nmore diverse images on CIFAR-10, more faithful prompt conditioned image editing\nusing Stable Diffusion, and improved unconditional de novo structure design of\nproteins. https://github.com/necludov/super-diffusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cambrian explosion of easily accessible pre-trained diffusion models\nsuggests a demand for methods that combine multiple different pre-trained\ndiffusion models without incurring the significant computational burden of\nre-training a larger combined model. In this paper, we cast the problem of\ncombining multiple pre-trained diffusion models at the generation stage under a\nnovel proposed framework termed superposition. Theoretically, we derive\nsuperposition from rigorous first principles stemming from the celebrated\ncontinuity equation and design two novel algorithms tailor-made for combining\ndiffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density\nestimator for the log likelihood of the diffusion SDE which incurs no\nadditional overhead compared to the well-known Hutchinson's estimator needed\nfor divergence calculations. We demonstrate that SuperDiff is scalable to large\npre-trained diffusion models as superposition is performed solely through\ncomposition during inference, and also enjoys painless implementation as it\ncombines different pre-trained vector fields through an automated re-weighting\nscheme. Notably, we show that SuperDiff is efficient during inference time, and\nmimics traditional composition operators such as the logical OR and the logical\nAND. We empirically demonstrate the utility of using SuperDiff for generating\nmore diverse images on CIFAR-10, more faithful prompt conditioned image editing\nusing Stable Diffusion, and improved unconditional de novo structure design of\nproteins. https://github.com/necludov/super-diffusion"
                },
                "authors": [
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Lazar Atanackovic"
                    },
                    {
                        "name": "Avishek Joey Bose"
                    },
                    {
                        "name": "Alexander Tong"
                    },
                    {
                        "name": "Kirill Neklyudov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Neklyudov"
                },
                "author": "Kirill Neklyudov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17758v1",
                "updated": "2024-12-23T18:14:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    14,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:14:36Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    14,
                    36,
                    0,
                    358,
                    0
                ],
                "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging"
                },
                "summary": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities."
                },
                "authors": [
                    {
                        "name": "ukasz Borchmann"
                    }
                ],
                "author_detail": {
                    "name": "ukasz Borchmann"
                },
                "author": "ukasz Borchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v2",
                "updated": "2024-12-23T18:09:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    9,
                    34,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17754v1",
                "updated": "2024-12-23T18:07:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    7,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:07:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    7,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code\n  Line-Level Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code\n  Line-Level Feedback"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing and coding, yet they struggle with robustness and accuracy\nin complex function calls. To tackle these challenges, this paper introduces\nADC, an innovative approach that enhances LLMs' ability to follow function\nformats and match complex parameters. ADC utilizes a high-quality code\nfine-tuning dataset with line-level execution feedback, providing granular\nprocess supervision that fosters strong logical reasoning and adherence to\nfunction formats. It also employs an adversarial dataset generation process to\nimprove parameter matching. The staged training methodology capitalizes on both\nenriched code datasets and refined adversarial datasets, leading to marked\nimprovements in function calling capabilities on the Berkeley Function-Calling\nLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic\ncombination of process supervision, adversarial refinement, and incremental\nlearning, setting a new standard for LLM proficiency in complex function\ncalling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing and coding, yet they struggle with robustness and accuracy\nin complex function calls. To tackle these challenges, this paper introduces\nADC, an innovative approach that enhances LLMs' ability to follow function\nformats and match complex parameters. ADC utilizes a high-quality code\nfine-tuning dataset with line-level execution feedback, providing granular\nprocess supervision that fosters strong logical reasoning and adherence to\nfunction formats. It also employs an adversarial dataset generation process to\nimprove parameter matching. The staged training methodology capitalizes on both\nenriched code datasets and refined adversarial datasets, leading to marked\nimprovements in function calling capabilities on the Berkeley Function-Calling\nLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic\ncombination of process supervision, adversarial refinement, and incremental\nlearning, setting a new standard for LLM proficiency in complex function\ncalling."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Li Zhu"
                    },
                    {
                        "name": "Qianghuai Jia"
                    },
                    {
                        "name": "Feijun Jiang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Mengping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mengping Zhou"
                },
                "author": "Mengping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03752v3",
                "updated": "2024-12-23T17:57:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    57,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-05T17:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "Attention Heads of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Heads of Large Language Models: A Survey"
                },
                "summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain as black-box systems. Understanding the reasoning\nbottlenecks of LLMs has become a critical challenge, as these limitations are\ndeeply tied to their internal architecture. Among these, attention heads have\nemerged as a focal point for investigating the underlying mechanics of LLMs. In\nthis survey, we aim to demystify the internal reasoning processes of LLMs by\nsystematically exploring the roles and mechanisms of attention heads. We first\nintroduce a novel four-stage framework inspired by the human thought process:\nKnowledge Recalling, In-Context Identification, Latent Reasoning, and\nExpression Preparation. Using this framework, we comprehensively review\nexisting research to identify and categorize the functions of specific\nattention heads. Additionally, we analyze the experimental methodologies used\nto discover these special heads, dividing them into two categories:\nModeling-Free and Modeling-Required methods. We further summarize relevant\nevaluation methods and benchmarks. Finally, we discuss the limitations of\ncurrent research and propose several potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain as black-box systems. Understanding the reasoning\nbottlenecks of LLMs has become a critical challenge, as these limitations are\ndeeply tied to their internal architecture. Among these, attention heads have\nemerged as a focal point for investigating the underlying mechanics of LLMs. In\nthis survey, we aim to demystify the internal reasoning processes of LLMs by\nsystematically exploring the roles and mechanisms of attention heads. We first\nintroduce a novel four-stage framework inspired by the human thought process:\nKnowledge Recalling, In-Context Identification, Latent Reasoning, and\nExpression Preparation. Using this framework, we comprehensively review\nexisting research to identify and categorize the functions of specific\nattention heads. Additionally, we analyze the experimental methodologies used\nto discover these special heads, dividing them into two categories:\nModeling-Free and Modeling-Required methods. We further summarize relevant\nevaluation methods and benchmarks. Finally, we discuss the limitations of\ncurrent research and propose several potential future directions."
                },
                "authors": [
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "33 pages, 11 figures, 7 tables, 7 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10430v2",
                "updated": "2024-12-23T17:56:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    56,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2023-06-17T21:47:19Z",
                "published_parsed": [
                    2023,
                    6,
                    17,
                    21,
                    47,
                    19,
                    5,
                    168,
                    0
                ],
                "title": "Variational Sequential Optimal Experimental Design using Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Sequential Optimal Experimental Design using Reinforcement\n  Learning"
                },
                "summary": "We present variational sequential optimal experimental design (vsOED), a\nnovel method for optimally designing a finite sequence of experiments within a\nBayesian framework with information-theoretic criteria. vsOED employs a\none-point reward formulation with variational posterior approximations,\nproviding a provable lower bound to the expected information gain. Numerical\nmethods are developed following an actor-critic reinforcement learning\napproach, including derivation and estimation of variational and policy\ngradients to optimize the design policy, and posterior approximation using\nGaussian mixture models and normalizing flows. vsOED accommodates nuisance\nparameters, implicit likelihoods, and multiple candidate models, while\nsupporting flexible design criteria that can target designs for model\ndiscrimination, parameter inference, goal-oriented prediction, and their\nweighted combinations. We demonstrate vsOED across various engineering and\nscience applications, illustrating its superior sample efficiency compared to\nexisting sequential experimental design algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present variational sequential optimal experimental design (vsOED), a\nnovel method for optimally designing a finite sequence of experiments within a\nBayesian framework with information-theoretic criteria. vsOED employs a\none-point reward formulation with variational posterior approximations,\nproviding a provable lower bound to the expected information gain. Numerical\nmethods are developed following an actor-critic reinforcement learning\napproach, including derivation and estimation of variational and policy\ngradients to optimize the design policy, and posterior approximation using\nGaussian mixture models and normalizing flows. vsOED accommodates nuisance\nparameters, implicit likelihoods, and multiple candidate models, while\nsupporting flexible design criteria that can target designs for model\ndiscrimination, parameter inference, goal-oriented prediction, and their\nweighted combinations. We demonstrate vsOED across various engineering and\nscience applications, illustrating its superior sample efficiency compared to\nexisting sequential experimental design algorithms."
                },
                "authors": [
                    {
                        "name": "Wanggang Shen"
                    },
                    {
                        "name": "Jiayuan Dong"
                    },
                    {
                        "name": "Xun Huan"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huan"
                },
                "author": "Xun Huan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62K05, 62L05, 62C10, 62F15, 90C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17744v1",
                "updated": "2024-12-23T17:52:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    52,
                    10,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:52:10Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    52,
                    10,
                    0,
                    358,
                    0
                ],
                "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoTransBench: A Real-World Benchmark for Repository-Level Code\n  Translation"
                },
                "summary": "Repository-level code translation refers to translating an entire code\nrepository from one programming language to another while preserving the\nfunctionality of the source repository. Many benchmarks have been proposed to\nevaluate the performance of such code translators. However, previous benchmarks\nmostly provide fine-grained samples, focusing at either code snippet, function,\nor file-level code translation. Such benchmarks do not accurately reflect\nreal-world demands, where entire repositories often need to be translated,\ninvolving longer code length and more complex functionalities. To address this\ngap, we propose a new benchmark, named RepoTransBench, which is a real-world\nrepository-level code translation benchmark with an automatically executable\ntest suite. We conduct experiments on RepoTransBench to evaluate the\ntranslation performance of 11 advanced LLMs. We find that the Success@1 score\n(test success in one attempt) of the best-performing LLM is only 7.33%. To\nfurther explore the potential of LLMs for repository-level code translation, we\nprovide LLMs with error-related feedback to perform iterative debugging and\nobserve an average 7.09% improvement on Success@1. However, even with this\nimprovement, the Success@1 score of the best-performing LLM is only 21%, which\nmay not meet the need for reliable automatic repository-level code translation.\nFinally, we conduct a detailed error analysis and highlight current LLMs'\ndeficiencies in repository-level code translation, which could provide a\nreference for further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code translation refers to translating an entire code\nrepository from one programming language to another while preserving the\nfunctionality of the source repository. Many benchmarks have been proposed to\nevaluate the performance of such code translators. However, previous benchmarks\nmostly provide fine-grained samples, focusing at either code snippet, function,\nor file-level code translation. Such benchmarks do not accurately reflect\nreal-world demands, where entire repositories often need to be translated,\ninvolving longer code length and more complex functionalities. To address this\ngap, we propose a new benchmark, named RepoTransBench, which is a real-world\nrepository-level code translation benchmark with an automatically executable\ntest suite. We conduct experiments on RepoTransBench to evaluate the\ntranslation performance of 11 advanced LLMs. We find that the Success@1 score\n(test success in one attempt) of the best-performing LLM is only 7.33%. To\nfurther explore the potential of LLMs for repository-level code translation, we\nprovide LLMs with error-related feedback to perform iterative debugging and\nobserve an average 7.09% improvement on Success@1. However, even with this\nimprovement, the Success@1 score of the best-performing LLM is only 21%, which\nmay not meet the need for reliable automatic repository-level code translation.\nFinally, we conduct a detailed error analysis and highlight current LLMs'\ndeficiencies in repository-level code translation, which could provide a\nreference for further improvements."
                },
                "authors": [
                    {
                        "name": "Yanli Wang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Suiquan Wang"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "John Grundy"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Mingzhi Mao"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17743v1",
                "updated": "2024-12-23T17:47:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:47:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "YuLan-Mini: An Open Data-efficient Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YuLan-Mini: An Open Data-efficient Language Model"
                },
                "summary": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini."
                },
                "authors": [
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12135v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12135v3",
                "updated": "2024-12-23T17:46:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    46,
                    10,
                    0,
                    358,
                    0
                ],
                "published": "2024-04-18T12:35:39Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    12,
                    35,
                    39,
                    3,
                    109,
                    0
                ],
                "title": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause\n  analysis in micro-services architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause\n  analysis in micro-services architecture"
                },
                "summary": "Root cause analysis (RCA) in Micro-services architecture (MSA) with\nescalating complexity encounters complex challenges in maintaining system\nstability and efficiency due to fault propagation and circular dependencies\namong nodes. Diverse root cause analysis faults require multi-agents with\ndiverse expertise. To mitigate the hallucination problem of large language\nmodels (LLMs), we design blockchain-inspired voting to ensure the reliability\nof the analysis by using a decentralized decision-making process. To avoid\nnon-terminating loops led by common circular dependency in MSA, we objectively\nlimit steps and standardize task processing through Agent Workflow. We propose\na pioneering framework, multi-Agent Blockchain-inspired Collaboration for root\ncause analysis in micro-services architecture (mABC), where multiple agents\nbased on the powerful LLMs follow Agent Workflow and collaborate in\nblockchain-inspired voting. Specifically, seven specialized agents derived from\nAgent Workflow each provide valuable insights towards root cause analysis based\non their expertise and the intrinsic software knowledge of LLMs collaborating\nwithin a decentralized chain. Our experiments on the AIOps challenge dataset\nand a newly created Train-Ticket dataset demonstrate superior performance in\nidentifying root causes and generating effective resolutions. The ablation\nstudy further highlights Agent Workflow, multi-agent, and blockchain-inspired\nvoting is crucial for achieving optimal performance. mABC offers a\ncomprehensive automated root cause analysis and resolution in micro-services\narchitecture and significantly improves the IT Operation domain. The code and\ndataset are in https://github.com/zwpride/mABC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root cause analysis (RCA) in Micro-services architecture (MSA) with\nescalating complexity encounters complex challenges in maintaining system\nstability and efficiency due to fault propagation and circular dependencies\namong nodes. Diverse root cause analysis faults require multi-agents with\ndiverse expertise. To mitigate the hallucination problem of large language\nmodels (LLMs), we design blockchain-inspired voting to ensure the reliability\nof the analysis by using a decentralized decision-making process. To avoid\nnon-terminating loops led by common circular dependency in MSA, we objectively\nlimit steps and standardize task processing through Agent Workflow. We propose\na pioneering framework, multi-Agent Blockchain-inspired Collaboration for root\ncause analysis in micro-services architecture (mABC), where multiple agents\nbased on the powerful LLMs follow Agent Workflow and collaborate in\nblockchain-inspired voting. Specifically, seven specialized agents derived from\nAgent Workflow each provide valuable insights towards root cause analysis based\non their expertise and the intrinsic software knowledge of LLMs collaborating\nwithin a decentralized chain. Our experiments on the AIOps challenge dataset\nand a newly created Train-Ticket dataset demonstrate superior performance in\nidentifying root causes and generating effective resolutions. The ablation\nstudy further highlights Agent Workflow, multi-agent, and blockchain-inspired\nvoting is crucial for achieving optimal performance. mABC offers a\ncomprehensive automated root cause analysis and resolution in micro-services\narchitecture and significantly improves the IT Operation domain. The code and\ndataset are in https://github.com/zwpride/mABC."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhoujin Tian"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Chaoran Yan"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Tongliang Li"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Liangfan Zheng"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12135v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v1",
                "updated": "2024-12-23T17:44:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17737v1",
                "updated": "2024-12-23T17:36:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    36,
                    51,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:36:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    36,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Backpropagation Loops: Amplifying Deep Reasoning with\n  Iterative Top-Down Feedback"
                },
                "summary": "Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks typically rely on a single forward pass for inference,\nwhich can limit their capacity to resolve ambiguous inputs. We introduce\nContextual Backpropagation Loops (CBLs) as an iterative mechanism that\nincorporates top-down feedback to refine intermediate representations, thereby\nimproving accuracy and robustness. This repeated process mirrors how humans\ncontinuously re-interpret sensory information in daily life-by checking and\nre-checking our perceptions using contextual cues. Our results suggest that\nCBLs can offer a straightforward yet powerful way to incorporate such\ncontextual reasoning in modern deep learning architectures."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Fein-Ashley"
                },
                "author": "Jacob Fein-Ashley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17734v1",
                "updated": "2024-12-23T17:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    35,
                    19,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    35,
                    19,
                    0,
                    358,
                    0
                ],
                "title": "LASE: Learned Adjacency Spectral Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASE: Learned Adjacency Spectral Embeddings"
                },
                "summary": "We put forth a principled design of a neural architecture to learn nodal\nAdjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the\ngradient descent (GD) method and leveraging the principle of algorithm\nunrolling, we truncate and re-interpret each GD iteration as a layer in a graph\nneural network (GNN) that is trained to approximate the ASE. Accordingly, we\ncall the resulting embeddings and our parametric model Learned ASE (LASE),\nwhich is interpretable, parameter efficient, robust to inputs with unobserved\nedges, and offers controllable complexity during inference. LASE layers combine\nGraph Convolutional Network (GCN) and fully-connected Graph Attention Network\n(GAT) modules, which is intuitively pleasing since GCN-based local aggregations\nalone are insufficient to express the sought graph eigenvectors. We propose\nseveral refinements to the unrolled LASE architecture (such as sparse attention\nin the GAT module and decoupled layerwise parameters) that offer favorable\napproximation error versus computation tradeoffs; even outperforming\nheavily-optimized eigendecomposition routines from scientific computing\nlibraries. Because LASE is a differentiable function with respect to its\nparameters as well as its graph input, we can seamlessly integrate it as a\ntrainable module within a larger (semi-)supervised graph representation\nlearning pipeline. The resulting end-to-end system effectively learns\n``discriminative ASEs'' that exhibit competitive performance in supervised link\nprediction and node classification tasks, outperforming a GNN even when the\nlatter is endowed with open loop, meaning task-agnostic, precomputed spectral\npositional encodings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We put forth a principled design of a neural architecture to learn nodal\nAdjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the\ngradient descent (GD) method and leveraging the principle of algorithm\nunrolling, we truncate and re-interpret each GD iteration as a layer in a graph\nneural network (GNN) that is trained to approximate the ASE. Accordingly, we\ncall the resulting embeddings and our parametric model Learned ASE (LASE),\nwhich is interpretable, parameter efficient, robust to inputs with unobserved\nedges, and offers controllable complexity during inference. LASE layers combine\nGraph Convolutional Network (GCN) and fully-connected Graph Attention Network\n(GAT) modules, which is intuitively pleasing since GCN-based local aggregations\nalone are insufficient to express the sought graph eigenvectors. We propose\nseveral refinements to the unrolled LASE architecture (such as sparse attention\nin the GAT module and decoupled layerwise parameters) that offer favorable\napproximation error versus computation tradeoffs; even outperforming\nheavily-optimized eigendecomposition routines from scientific computing\nlibraries. Because LASE is a differentiable function with respect to its\nparameters as well as its graph input, we can seamlessly integrate it as a\ntrainable module within a larger (semi-)supervised graph representation\nlearning pipeline. The resulting end-to-end system effectively learns\n``discriminative ASEs'' that exhibit competitive performance in supervised link\nprediction and node classification tasks, outperforming a GNN even when the\nlatter is endowed with open loop, meaning task-agnostic, precomputed spectral\npositional encodings."
                },
                "authors": [
                    {
                        "name": "Sofa Prez Casulo"
                    },
                    {
                        "name": "Marcelo Fiori"
                    },
                    {
                        "name": "Federico Larroca"
                    },
                    {
                        "name": "Gonzalo Mateos"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo Mateos"
                },
                "author": "Gonzalo Mateos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17729v1",
                "updated": "2024-12-23T17:19:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    19,
                    58,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:19:58Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    19,
                    58,
                    0,
                    358,
                    0
                ],
                "title": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding"
                },
                "summary": "Existing humor datasets and evaluations predominantly focus on English,\nleaving limited resources for culturally nuanced humor in non-English languages\nlike Chinese. To address this gap, we construct Chumor, the first Chinese humor\nexplanation dataset that exceeds the size of existing humor datasets. Chumor is\nsourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing\nintellectually challenging and culturally specific jokes. We test ten LLMs\nthrough direct and chain-of-thought prompting, revealing that Chumor poses\nsignificant challenges to existing LLMs, with their accuracy slightly above\nrandom and far below human. In addition, our analysis highlights that\nhuman-annotated humor explanations are significantly better than those\ngenerated by GPT-4o and ERNIE-4-turbo. We release Chumor at\nhttps://huggingface.co/datasets/dnaihao/Chumor, our project page is at\nhttps://dnaihao.github.io/Chumor-dataset/, our leaderboard is at\nhttps://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at\nhttps://github.com/dnaihao/Chumor-dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing humor datasets and evaluations predominantly focus on English,\nleaving limited resources for culturally nuanced humor in non-English languages\nlike Chinese. To address this gap, we construct Chumor, the first Chinese humor\nexplanation dataset that exceeds the size of existing humor datasets. Chumor is\nsourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing\nintellectually challenging and culturally specific jokes. We test ten LLMs\nthrough direct and chain-of-thought prompting, revealing that Chumor poses\nsignificant challenges to existing LLMs, with their accuracy slightly above\nrandom and far below human. In addition, our analysis highlights that\nhuman-annotated humor explanations are significantly better than those\ngenerated by GPT-4o and ERNIE-4-turbo. We release Chumor at\nhttps://huggingface.co/datasets/dnaihao/Chumor, our project page is at\nhttps://dnaihao.github.io/Chumor-dataset/, our leaderboard is at\nhttps://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at\nhttps://github.com/dnaihao/Chumor-dataset."
                },
                "authors": [
                    {
                        "name": "Ruiqi He"
                    },
                    {
                        "name": "Yushu He"
                    },
                    {
                        "name": "Longju Bai"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Zhenjie Sun"
                    },
                    {
                        "name": "Zenghao Tang"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Hanchen Xia"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Naihao Deng"
                    }
                ],
                "author_detail": {
                    "name": "Naihao Deng"
                },
                "author": "Naihao Deng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2406.12754",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17727v1",
                "updated": "2024-12-23T17:17:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    17,
                    50,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:17:50Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    17,
                    50,
                    0,
                    358,
                    0
                ],
                "title": "Knowledge Editing through Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing through Chain-of-Thought"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of natural language processing (NLP) tasks. However,\nkeeping these models up-to-date with evolving world knowledge remains a\nsignificant challenge due to the high costs of frequent retraining. To address\nthis challenge, knowledge editing techniques have emerged to update LLMs with\nnew information without rebuilding the model from scratch. Among these, the\nin-context editing paradigm stands out for its effectiveness in integrating new\nknowledge while preserving the model's original capabilities. Despite its\npotential, existing in-context knowledge editing methods are often\ntask-specific, focusing primarily on multi-hop QA tasks using structured\nknowledge triples. Moreover, their reliance on few-shot prompting for task\ndecomposition makes them unstable and less effective in generalizing across\ndiverse tasks.\n  In response to these limitations, we propose EditCoT, a novel knowledge\nediting framework that flexibly and efficiently updates LLMs across various\ntasks without retraining. EditCoT works by generating a chain-of-thought (CoT)\nfor a given input and then iteratively refining this CoT process using a CoT\neditor based on updated knowledge. We evaluate EditCoT across a diverse range\nof benchmarks, covering multiple languages and tasks. The results demonstrate\nthat our approach achieves state-of-the-art performance while offering superior\ngeneralization, effectiveness, and stability compared to existing methods,\nmarking a significant advancement in the field of knowledge updating. Code and\ndata are available at: https://github.com/bebr2/EditCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of natural language processing (NLP) tasks. However,\nkeeping these models up-to-date with evolving world knowledge remains a\nsignificant challenge due to the high costs of frequent retraining. To address\nthis challenge, knowledge editing techniques have emerged to update LLMs with\nnew information without rebuilding the model from scratch. Among these, the\nin-context editing paradigm stands out for its effectiveness in integrating new\nknowledge while preserving the model's original capabilities. Despite its\npotential, existing in-context knowledge editing methods are often\ntask-specific, focusing primarily on multi-hop QA tasks using structured\nknowledge triples. Moreover, their reliance on few-shot prompting for task\ndecomposition makes them unstable and less effective in generalizing across\ndiverse tasks.\n  In response to these limitations, we propose EditCoT, a novel knowledge\nediting framework that flexibly and efficiently updates LLMs across various\ntasks without retraining. EditCoT works by generating a chain-of-thought (CoT)\nfor a given input and then iteratively refining this CoT process using a CoT\neditor based on updated knowledge. We evaluate EditCoT across a diverse range\nof benchmarks, covering multiple languages and tasks. The results demonstrate\nthat our approach achieves state-of-the-art performance while offering superior\ngeneralization, effectiveness, and stability compared to existing methods,\nmarking a significant advancement in the field of knowledge updating. Code and\ndata are available at: https://github.com/bebr2/EditCoT."
                },
                "authors": [
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01328v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01328v7",
                "updated": "2024-12-23T17:13:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    13,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2023-03-02T15:06:05Z",
                "published_parsed": [
                    2023,
                    3,
                    2,
                    15,
                    6,
                    5,
                    3,
                    61,
                    0
                ],
                "title": "Effect Handlers for Programmable Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect Handlers for Programmable Inference"
                },
                "summary": "Inference algorithms for probabilistic programming are complex imperative\nprograms with many moving parts. Efficient inference often requires customising\nan algorithm to a particular probabilistic model or problem, sometimes called\ninference programming. Most inference frameworks are implemented in languages\nthat lack a disciplined approach to side effects, which can result in\nmonolithic implementations where the structure of the algorithms is obscured\nand inference programming is hard. Functional programming with typed effects\noffers a more structured and modular foundation for programmable inference,\nwith monad transformers being the primary structuring mechanism explored to\ndate.\n  This paper presents an alternative approach to inference programming based on\nalgebraic effects. Using effect signatures to specify the key operations of the\nalgorithms, and effect handlers to modularly interpret those operations for\nspecific variants, we develop two abstract algorithms, or inference patterns,\nrepresenting two important classes of inference: Metropolis-Hastings and\nparticle filtering. We show how our approach reveals the algorithms' high-level\nstructure, and makes it easy to tailor and recombine their parts into new\nvariants. We implement the two inference patterns as a Haskell library, and\ndiscuss the pros and cons of algebraic effects vis-a-vis monad transformers as\na structuring mechanism for modular imperative algorithm design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference algorithms for probabilistic programming are complex imperative\nprograms with many moving parts. Efficient inference often requires customising\nan algorithm to a particular probabilistic model or problem, sometimes called\ninference programming. Most inference frameworks are implemented in languages\nthat lack a disciplined approach to side effects, which can result in\nmonolithic implementations where the structure of the algorithms is obscured\nand inference programming is hard. Functional programming with typed effects\noffers a more structured and modular foundation for programmable inference,\nwith monad transformers being the primary structuring mechanism explored to\ndate.\n  This paper presents an alternative approach to inference programming based on\nalgebraic effects. Using effect signatures to specify the key operations of the\nalgorithms, and effect handlers to modularly interpret those operations for\nspecific variants, we develop two abstract algorithms, or inference patterns,\nrepresenting two important classes of inference: Metropolis-Hastings and\nparticle filtering. We show how our approach reveals the algorithms' high-level\nstructure, and makes it easy to tailor and recombine their parts into new\nvariants. We implement the two inference patterns as a Haskell library, and\ndiscuss the pros and cons of algebraic effects vis-a-vis monad transformers as\na structuring mechanism for modular imperative algorithm design."
                },
                "authors": [
                    {
                        "name": "Minh Nguyen"
                    },
                    {
                        "name": "Roly Perera"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Steven Ramsay"
                    }
                ],
                "author_detail": {
                    "name": "Steven Ramsay"
                },
                "author": "Steven Ramsay",
                "arxiv_doi": "10.1145/3609026.3609729",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3609026.3609729",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01328v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01328v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00314v2",
                "updated": "2024-12-23T16:48:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    48,
                    28,
                    0,
                    358,
                    0
                ],
                "published": "2023-01-01T00:47:03Z",
                "published_parsed": [
                    2023,
                    1,
                    1,
                    0,
                    47,
                    3,
                    6,
                    1,
                    0
                ],
                "title": "Causal Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Deep Learning"
                },
                "summary": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described."
                },
                "authors": [
                    {
                        "name": "M. Alex O. Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "M. Alex O. Vasilescu"
                },
                "author": "M. Alex O. Vasilescu",
                "arxiv_doi": "10.1007/978-3-031-78189-6_27",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78189-6_27",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.00314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Antonacopoulos, A., Chaudhuri, S., Chellappa, R., Liu, CL.,\n  Bhattacharya, S., Pal, U. (eds) Pattern Recognition. (ICPR 2024). Lecture\n  Notes in Computer Science, vol 15309, pp.420-438(LNCS 2025). Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.10; I.2.4; I.2.10; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.07011v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.07011v3",
                "updated": "2024-12-23T16:29:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    29,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2022-10-13T13:19:51Z",
                "published_parsed": [
                    2022,
                    10,
                    13,
                    13,
                    19,
                    51,
                    3,
                    286,
                    0
                ],
                "title": "Variational Graph Generator for Multi-View Graph Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Graph Generator for Multi-View Graph Clustering"
                },
                "summary": "Multi-view graph clustering (MGC) methods are increasingly being studied due\nto the explosion of multi-view data with graph structural information. The\ncritical point of MGC is to better utilize view-specific and view-common\ninformation in features and graphs of multiple views. However, existing works\nhave an inherent limitation that they are unable to concurrently utilize the\nconsensus graph information across multiple graphs and the view-specific\nfeature information. To address this issue, we propose Variational Graph\nGenerator for Multi-View Graph Clustering (VGMGC). Specifically, a novel\nvariational graph generator is proposed to extract common information among\nmultiple graphs. This generator infers a reliable variational consensus graph\nbased on a priori assumption over multiple graphs. Then a simple yet effective\ngraph encoder in conjunction with the multi-view clustering objective is\npresented to learn the desired graph embeddings for clustering, which embeds\nthe inferred view-common graph and view-specific graphs together with features.\nFinally, theoretical results illustrate the rationality of the VGMGC by\nanalyzing the uncertainty of the inferred consensus graph with the information\nbottleneck principle.Extensive experiments demonstrate the superior performance\nof our VGMGC over SOTAs. The source code is publicly available at\nhttps://github.com/cjpcool/VGMGC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view graph clustering (MGC) methods are increasingly being studied due\nto the explosion of multi-view data with graph structural information. The\ncritical point of MGC is to better utilize view-specific and view-common\ninformation in features and graphs of multiple views. However, existing works\nhave an inherent limitation that they are unable to concurrently utilize the\nconsensus graph information across multiple graphs and the view-specific\nfeature information. To address this issue, we propose Variational Graph\nGenerator for Multi-View Graph Clustering (VGMGC). Specifically, a novel\nvariational graph generator is proposed to extract common information among\nmultiple graphs. This generator infers a reliable variational consensus graph\nbased on a priori assumption over multiple graphs. Then a simple yet effective\ngraph encoder in conjunction with the multi-view clustering objective is\npresented to learn the desired graph embeddings for clustering, which embeds\nthe inferred view-common graph and view-specific graphs together with features.\nFinally, theoretical results illustrate the rationality of the VGMGC by\nanalyzing the uncertainty of the inferred consensus graph with the information\nbottleneck principle.Extensive experiments demonstrate the superior performance\nof our VGMGC over SOTAs. The source code is publicly available at\nhttps://github.com/cjpcool/VGMGC."
                },
                "authors": [
                    {
                        "name": "Jianpeng Chen"
                    },
                    {
                        "name": "Yawen Ling"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Yazhou Ren"
                    },
                    {
                        "name": "Shudong Huang"
                    },
                    {
                        "name": "Xiaorong Pu"
                    },
                    {
                        "name": "Zhifeng Hao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Lifang He"
                    }
                ],
                "author_detail": {
                    "name": "Lifang He"
                },
                "author": "Lifang He",
                "arxiv_comment": "accepted by TNNLS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.07011v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.07011v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17690v1",
                "updated": "2024-12-23T16:16:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:16:30Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "title": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG"
                },
                "summary": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at BTW 2025, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10571v3",
                "updated": "2024-12-23T16:12:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    12,
                    59,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-13T21:28:17Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    21,
                    28,
                    17,
                    4,
                    348,
                    0
                ],
                "title": "Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems"
                },
                "summary": "Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation typically rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions: it has 300 hand-created conversational questions, each in\nEnglish and German, coupled with ground truth URLs, completed questions, and\nanswers from 215 public Confluence pages. These documents are typical of\nenterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE\non ConfQuestions show the viability of our ideas: contextualization improves\nRAG performance, and counterfactual explanations outperform standard\nattribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation typically rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions: it has 300 hand-created conversational questions, each in\nEnglish and German, coupled with ground truth URLs, completed questions, and\nanswers from 215 public Confluence pages. These documents are typical of\nenterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE\non ConfQuestions show the viability of our ideas: contextualization improves\nRAG performance, and counterfactual explanations outperform standard\nattribution."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at WSDM 2025, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17686v1",
                "updated": "2024-12-23T16:11:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    27,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:27Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    27,
                    0,
                    358,
                    0
                ],
                "title": "Large Language Model Safety: A Holistic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Safety: A Holistic Survey"
                },
                "summary": "The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers."
                },
                "authors": [
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Zhigen Li"
                    },
                    {
                        "name": "Yongqi Leng"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Chuang Liu"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Zishan Guo"
                    },
                    {
                        "name": "Linhao Yu"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Bojian Jiang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "158 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16102v2",
                "updated": "2024-12-23T15:57:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    57,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-20T17:43:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    43,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Interleaved Speech-Text Language Models are Simple Streaming Text to\n  Speech Synthesizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved Speech-Text Language Models are Simple Streaming Text to\n  Speech Synthesizers"
                },
                "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for\nstreaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,\nIST-LM is directly trained on interleaved sequences of text and speech tokens\nwith a fixed ratio, eliminating the need for additional efforts in duration\nprediction and grapheme-to-phoneme alignment. The ratio of text chunk size to\nspeech chunk size is crucial for the performance of IST-LM. To explore this, we\nconducted a comprehensive series of statistical analyses on the training data\nand performed correlation analysis with the final performance, uncovering\nseveral key factors: 1) the distance between speech tokens and their\ncorresponding text tokens, 2) the number of future text tokens accessible to\neach speech token, and 3) the frequency of speech tokens precedes their\ncorresponding text tokens. Experimental results demonstrate how to achieve an\noptimal streaming TTS system without complicated engineering optimization,\nwhich has a limited gap with the non-streaming system. IST-LM is conceptually\nsimple and empirically powerful, paving the way for streaming TTS with minimal\noverhead while largely maintaining performance, showcasing broad prospects\ncoupled with real-time text stream from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for\nstreaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,\nIST-LM is directly trained on interleaved sequences of text and speech tokens\nwith a fixed ratio, eliminating the need for additional efforts in duration\nprediction and grapheme-to-phoneme alignment. The ratio of text chunk size to\nspeech chunk size is crucial for the performance of IST-LM. To explore this, we\nconducted a comprehensive series of statistical analyses on the training data\nand performed correlation analysis with the final performance, uncovering\nseveral key factors: 1) the distance between speech tokens and their\ncorresponding text tokens, 2) the number of future text tokens accessible to\neach speech token, and 3) the frequency of speech tokens precedes their\ncorresponding text tokens. Experimental results demonstrate how to achieve an\noptimal streaming TTS system without complicated engineering optimization,\nwhich has a limited gap with the non-streaming system. IST-LM is conceptually\nsimple and empirically powerful, paving the way for streaming TTS with minimal\noverhead while largely maintaining performance, showcasing broad prospects\ncoupled with real-time text stream from LLMs."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Ruiyang Xu"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "Submitted to ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17672v1",
                "updated": "2024-12-23T15:56:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    56,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:56:17Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    56,
                    17,
                    0,
                    358,
                    0
                ],
                "title": "Euclid: Early Release Observations of diffuse stellar structures and\n  globular clusters as probes of the mass assembly of galaxies in the Dorado\n  group",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: Early Release Observations of diffuse stellar structures and\n  globular clusters as probes of the mass assembly of galaxies in the Dorado\n  group"
                },
                "summary": "Deep surveys reveal tidal debris and associated compact stellar systems.\nEuclid's unique combination of capabilities (spatial resolution, depth, and\nwide sky coverage) will make it a groundbreaking tool for galactic archaeology\nin the local Universe, bringing low surface brightness (LSB) science into the\nera of large-scale astronomical surveys. Euclid's Early Release Observations\n(ERO) demonstrate this potential with a field of view that includes several\ngalaxies in the Dorado group. In this paper, we aim to derive from this image a\nmass assembly scenario for its main galaxies: NGC 1549, NGC 1553, and NGC 1546.\nWe detect internal and external diffuse structures, and identify candidate\nglobular clusters (GCs). By analysing the colours and distributions of the\ndiffuse structures and candidate GCs, we can place constraints on the galaxies'\nmass assembly and merger histories. The results show that feature morphology,\nsurface brightness, colours, and GC density profiles are consistent with\ngalaxies that have undergone different merger scenarios. We classify NGC 1549\nas a pure elliptical galaxy that has undergone a major merger. NGC 1553 appears\nto have recently transitioned from a late-type galaxy to early type, after a\nseries of radial minor to intermediate mergers. NGC 1546 is a rare specimen of\ngalaxy with an undisturbed disk and a prominent diffuse stellar halo, which we\ninfer has been fed by minor mergers and then disturbed by the tidal effect from\nNGC 1553. Finally, we identify limitations specific to the observing conditions\nof this ERO, in particular stray light in the visible and persistence in the\nnear-infrared bands. Once these issues are addressed and the extended emission\nfrom LSB objects is preserved by the data-processing pipeline, the Euclid Wide\nSurvey will allow studies of the local Universe to be extended to statistical\nensembles over a large part of the extragalactic sky.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep surveys reveal tidal debris and associated compact stellar systems.\nEuclid's unique combination of capabilities (spatial resolution, depth, and\nwide sky coverage) will make it a groundbreaking tool for galactic archaeology\nin the local Universe, bringing low surface brightness (LSB) science into the\nera of large-scale astronomical surveys. Euclid's Early Release Observations\n(ERO) demonstrate this potential with a field of view that includes several\ngalaxies in the Dorado group. In this paper, we aim to derive from this image a\nmass assembly scenario for its main galaxies: NGC 1549, NGC 1553, and NGC 1546.\nWe detect internal and external diffuse structures, and identify candidate\nglobular clusters (GCs). By analysing the colours and distributions of the\ndiffuse structures and candidate GCs, we can place constraints on the galaxies'\nmass assembly and merger histories. The results show that feature morphology,\nsurface brightness, colours, and GC density profiles are consistent with\ngalaxies that have undergone different merger scenarios. We classify NGC 1549\nas a pure elliptical galaxy that has undergone a major merger. NGC 1553 appears\nto have recently transitioned from a late-type galaxy to early type, after a\nseries of radial minor to intermediate mergers. NGC 1546 is a rare specimen of\ngalaxy with an undisturbed disk and a prominent diffuse stellar halo, which we\ninfer has been fed by minor mergers and then disturbed by the tidal effect from\nNGC 1553. Finally, we identify limitations specific to the observing conditions\nof this ERO, in particular stray light in the visible and persistence in the\nnear-infrared bands. Once these issues are addressed and the extended emission\nfrom LSB objects is preserved by the data-processing pipeline, the Euclid Wide\nSurvey will allow studies of the local Universe to be extended to statistical\nensembles over a large part of the extragalactic sky."
                },
                "authors": [
                    {
                        "name": "M. Urbano"
                    },
                    {
                        "name": "P. -A. Duc"
                    },
                    {
                        "name": "T. Saifollahi"
                    },
                    {
                        "name": "E. Sola"
                    },
                    {
                        "name": "A. Lanon"
                    },
                    {
                        "name": "K. Voggel"
                    },
                    {
                        "name": "F. Annibali"
                    },
                    {
                        "name": "M. Baes"
                    },
                    {
                        "name": "H. Bouy"
                    },
                    {
                        "name": "Michele Cantiello"
                    },
                    {
                        "name": "D. Carollo"
                    },
                    {
                        "name": "J. -C. Cuillandre"
                    },
                    {
                        "name": "P. Dimauro"
                    },
                    {
                        "name": "P. Erwin"
                    },
                    {
                        "name": "A. M. N. Ferguson"
                    },
                    {
                        "name": "R. Habas"
                    },
                    {
                        "name": "M. Hilker"
                    },
                    {
                        "name": "L. K. Hunt"
                    },
                    {
                        "name": "M. Kluge"
                    },
                    {
                        "name": "S. S. Larsen"
                    },
                    {
                        "name": "Q. Liu"
                    },
                    {
                        "name": "O. Marchal"
                    },
                    {
                        "name": "F. R. Marleau"
                    },
                    {
                        "name": "D. Massari"
                    },
                    {
                        "name": "O. Mller"
                    },
                    {
                        "name": "R. F. Peletier"
                    },
                    {
                        "name": "M. Poulain"
                    },
                    {
                        "name": "M. Rejkuba"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "C. Stone"
                    },
                    {
                        "name": "R. Zller"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Basset"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. Gmez-Alvarez"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "J. Hoar"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kmmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "D. Le Mignant"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "E. Rossetti"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "R. Scaramella"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "O. R. Williams"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "V. Scottez"
                    }
                ],
                "author_detail": {
                    "name": "V. Scottez"
                },
                "arxiv_affiliation": "ICL, Junia, Universit Catholique de Lille, LITL, 59000 Lille, France",
                "author": "V. Scottez",
                "arxiv_comment": "25 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17669v1",
                "updated": "2024-12-23T15:54:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    54,
                    15,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:54:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    54,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "Generating Completions for Fragmented Broca's Aphasic Sentences Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Completions for Fragmented Broca's Aphasic Sentences Using\n  Large Language Models"
                },
                "summary": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful\nand fragmented speech production with relatively good comprehension. Since\ntraditional aphasia treatment methods are often time-consuming,\nlabour-intensive, and do not reflect real-world conversations, applying natural\nlanguage processing based approaches such as Large Language Models (LLMs) could\npotentially contribute to improving existing treatment approaches. To address\nthis issue, we explore the use of sequence-to-sequence LLMs for completing\nfragmented Broca's aphasic sentences. We first generate synthetic Broca's\naphasic data using a rule-based system designed to mirror the linguistic\ncharacteristics of Broca's aphasic speech. Using this synthetic data, we then\nfine-tune four pre-trained LLMs on the task of completing fragmented sentences.\nWe evaluate our fine-tuned models on both synthetic and authentic Broca's\naphasic data. We demonstrate LLMs' capability for reconstructing fragmented\nsentences, with the models showing improved performance with longer input\nutterances. Our result highlights the LLMs' potential in advancing\ncommunication aids for individuals with Broca's aphasia and possibly other\nclinical populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful\nand fragmented speech production with relatively good comprehension. Since\ntraditional aphasia treatment methods are often time-consuming,\nlabour-intensive, and do not reflect real-world conversations, applying natural\nlanguage processing based approaches such as Large Language Models (LLMs) could\npotentially contribute to improving existing treatment approaches. To address\nthis issue, we explore the use of sequence-to-sequence LLMs for completing\nfragmented Broca's aphasic sentences. We first generate synthetic Broca's\naphasic data using a rule-based system designed to mirror the linguistic\ncharacteristics of Broca's aphasic speech. Using this synthetic data, we then\nfine-tune four pre-trained LLMs on the task of completing fragmented sentences.\nWe evaluate our fine-tuned models on both synthetic and authentic Broca's\naphasic data. We demonstrate LLMs' capability for reconstructing fragmented\nsentences, with the models showing improved performance with longer input\nutterances. Our result highlights the LLMs' potential in advancing\ncommunication aids for individuals with Broca's aphasia and possibly other\nclinical populations."
                },
                "authors": [
                    {
                        "name": "Sijbren van Vaals"
                    },
                    {
                        "name": "Yevgen Matusevych"
                    },
                    {
                        "name": "Frank Tsiwah"
                    }
                ],
                "author_detail": {
                    "name": "Frank Tsiwah"
                },
                "author": "Frank Tsiwah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11745v2",
                "updated": "2024-12-23T15:36:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    36,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-21T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "title": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing"
                },
                "summary": "Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Zhichao Duan"
                    },
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Rong Han"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyong Wang"
                },
                "author": "Jianyong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07499v2",
                "updated": "2024-12-23T15:34:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    34,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-10T13:27:58Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    13,
                    27,
                    58,
                    1,
                    345,
                    0
                ],
                "title": "EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap\n  Expansion"
                },
                "summary": "Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD\nsamples from the multi-label In-Distribution (ID) ones. Compared with its\nmulticlass counterpart, it is crucial to model the joint information among\nclasses. To this end, JointEnergy, which is a representative multi-label OOD\ninference criterion, summarizes the logits of all the classes. However, we find\nthat JointEnergy can produce an imbalance problem in OOD detection, especially\nwhen the model lacks enough discrimination ability. Specifically, we find that\nthe samples only related to minority classes tend to be classified as OOD\nsamples due to the ambiguous energy decision boundary. Besides, imbalanced\nmulti-label learning methods, originally designed for ID ones, would not be\nsuitable for OOD detection scenarios, even producing a serious negative\ntransfer effect. In this paper, we resort to auxiliary outlier exposure (OE)\nand propose an unknown-aware multi-label learning framework to reshape the\nuncertainty energy space layout. In this framework, the energy score is\nseparately optimized for tail ID samples and unknown samples, and the energy\ndistribution gap between them is expanded, such that the tail ID samples can\nhave a significantly larger energy score than the OOD ones. What's more, a\nsimple yet effective measure is designed to select more informative OE\ndatasets. Finally, comprehensive experimental results on multiple multi-label\nand OOD datasets reveal the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Zitai Wang"
                    },
                    {
                        "name": "Zhiyong Yang"
                    },
                    {
                        "name": "Junwei He"
                    }
                ],
                "author_detail": {
                    "name": "Junwei He"
                },
                "author": "Junwei He",
                "arxiv_comment": "9 pages, 5 figures, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17651v1",
                "updated": "2024-12-23T15:29:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:29:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Detecting anxiety and depression in dialogues: a multi-label and\n  explainable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anxiety and depression in dialogues: a multi-label and\n  explainable approach"
                },
                "summary": "Anxiety and depression are the most common mental health issues worldwide,\naffecting a non-negligible part of the population. Accordingly, stakeholders,\nincluding governments' health systems, are developing new strategies to promote\nearly detection and prevention from a holistic perspective (i.e., addressing\nseveral disorders simultaneously). In this work, an entirely novel system for\nthe multi-label classification of anxiety and depression is proposed. The input\ndata consists of dialogues from user interactions with an assistant chatbot.\nAnother relevant contribution lies in using Large Language Models (LLMs) for\nfeature extraction, provided the complexity and variability of language. The\ncombination of LLMs, given their high capability for language understanding,\nand Machine Learning (ML) models, provided their contextual knowledge about the\nclassification problem thanks to the labeled data, constitute a promising\napproach towards mental health assessment. To promote the solution's\ntrustworthiness, reliability, and accountability, explainability descriptions\nof the model's decision are provided in a graphical dashboard. Experimental\nresults on a real dataset attain 90 % accuracy, improving those in the prior\nliterature. The ultimate objective is to contribute in an accessible and\nscalable way before formal treatment occurs in the healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anxiety and depression are the most common mental health issues worldwide,\naffecting a non-negligible part of the population. Accordingly, stakeholders,\nincluding governments' health systems, are developing new strategies to promote\nearly detection and prevention from a holistic perspective (i.e., addressing\nseveral disorders simultaneously). In this work, an entirely novel system for\nthe multi-label classification of anxiety and depression is proposed. The input\ndata consists of dialogues from user interactions with an assistant chatbot.\nAnother relevant contribution lies in using Large Language Models (LLMs) for\nfeature extraction, provided the complexity and variability of language. The\ncombination of LLMs, given their high capability for language understanding,\nand Machine Learning (ML) models, provided their contextual knowledge about the\nclassification problem thanks to the labeled data, constitute a promising\napproach towards mental health assessment. To promote the solution's\ntrustworthiness, reliability, and accountability, explainability descriptions\nof the model's decision are provided in a graphical dashboard. Experimental\nresults on a real dataset attain 90 % accuracy, improving those in the prior\nliterature. The ultimate objective is to contribute in an accessible and\nscalable way before formal treatment occurs in the healthcare systems."
                },
                "authors": [
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Silvia Garca-Mndez"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Garca-Mndez"
                },
                "author": "Silvia Garca-Mndez",
                "arxiv_journal_ref": "de Arriba-P\\'erez, F., Garc\\'ia-M\\'endez, S. (2024). Detecting\n  anxiety and depression in dialogues: a multi-label and explainable approach.\n  In Proceedings of the 3rd AIxIA Workshop on Artificial Intelligence For\n  Healthcare (pp. 257-271)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06540v2",
                "updated": "2024-12-23T15:29:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    22,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-09T14:51:26Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families"
                },
                "summary": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications."
                },
                "authors": [
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Yuekai Sun"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17644v1",
                "updated": "2024-12-23T15:21:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    21,
                    28,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:21:28Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    21,
                    28,
                    0,
                    358,
                    0
                ],
                "title": "DreamFit: Garment-Centric Human Generation via a Lightweight\n  Anything-Dressing Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamFit: Garment-Centric Human Generation via a Lightweight\n  Anything-Dressing Encoder"
                },
                "summary": "Diffusion models for garment-centric human generation from text or image\nprompts have garnered emerging attention for their great application potential.\nHowever, existing methods often face a dilemma: lightweight approaches, such as\nadapters, are prone to generate inconsistent textures; while finetune-based\nmethods involve high training costs and struggle to maintain the generalization\ncapabilities of pretrained diffusion models, limiting their performance across\ndiverse scenarios. To address these challenges, we propose DreamFit, which\nincorporates a lightweight Anything-Dressing Encoder specifically tailored for\nthe garment-centric human generation. DreamFit has three key advantages: (1)\n\\textbf{Lightweight training}: with the proposed adaptive attention and LoRA\nmodules, DreamFit significantly minimizes the model complexity to 83.4M\ntrainable parameters. (2)\\textbf{Anything-Dressing}: Our model generalizes\nsurprisingly well to a wide range of (non-)garments, creative styles, and\nprompt instructions, consistently delivering high-quality results across\ndiverse scenarios. (3) \\textbf{Plug-and-play}: DreamFit is engineered for\nsmooth integration with any community control plugins for diffusion models,\nensuring easy compatibility and minimizing adoption barriers. To further\nenhance generation quality, DreamFit leverages pretrained large multi-modal\nmodels (LMMs) to enrich the prompt with fine-grained garment descriptions,\nthereby reducing the prompt gap between training and inference. We conduct\ncomprehensive experiments on both $768 \\times 512$ high-resolution benchmarks\nand in-the-wild images. DreamFit surpasses all existing methods, highlighting\nits state-of-the-art capabilities of garment-centric human generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models for garment-centric human generation from text or image\nprompts have garnered emerging attention for their great application potential.\nHowever, existing methods often face a dilemma: lightweight approaches, such as\nadapters, are prone to generate inconsistent textures; while finetune-based\nmethods involve high training costs and struggle to maintain the generalization\ncapabilities of pretrained diffusion models, limiting their performance across\ndiverse scenarios. To address these challenges, we propose DreamFit, which\nincorporates a lightweight Anything-Dressing Encoder specifically tailored for\nthe garment-centric human generation. DreamFit has three key advantages: (1)\n\\textbf{Lightweight training}: with the proposed adaptive attention and LoRA\nmodules, DreamFit significantly minimizes the model complexity to 83.4M\ntrainable parameters. (2)\\textbf{Anything-Dressing}: Our model generalizes\nsurprisingly well to a wide range of (non-)garments, creative styles, and\nprompt instructions, consistently delivering high-quality results across\ndiverse scenarios. (3) \\textbf{Plug-and-play}: DreamFit is engineered for\nsmooth integration with any community control plugins for diffusion models,\nensuring easy compatibility and minimizing adoption barriers. To further\nenhance generation quality, DreamFit leverages pretrained large multi-modal\nmodels (LMMs) to enrich the prompt with fine-grained garment descriptions,\nthereby reducing the prompt gap between training and inference. We conduct\ncomprehensive experiments on both $768 \\times 512$ high-resolution benchmarks\nand in-the-wild images. DreamFit surpasses all existing methods, highlighting\nits state-of-the-art capabilities of garment-centric human generation."
                },
                "authors": [
                    {
                        "name": "Ente Lin"
                    },
                    {
                        "name": "Xujie Zhang"
                    },
                    {
                        "name": "Fuwei Zhao"
                    },
                    {
                        "name": "Yuxuan Luo"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Long Zeng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17637v1",
                "updated": "2024-12-23T15:13:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    13,
                    56,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:13:56Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    13,
                    56,
                    0,
                    358,
                    0
                ],
                "title": "SCBench: A Sports Commentary Benchmark for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A Sports Commentary Benchmark for Video LLMs"
                },
                "summary": "Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Kuangzhi Ge"
                    },
                    {
                        "name": "Lingjun Chen"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Liaoyuan Fan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Guanqun Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17626v1",
                "updated": "2024-12-23T14:58:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    58,
                    37,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:58:37Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    58,
                    37,
                    0,
                    358,
                    0
                ],
                "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study"
                },
                "summary": "Understanding training dynamics and feature evolution is crucial for the\nmechanistic interpretability of large language models (LLMs). Although sparse\nautoencoders (SAEs) have been used to identify features within LLMs, a clear\npicture of how these features evolve during training remains elusive. In this\nstudy, we: (1) introduce SAE-Track, a method to efficiently obtain a continual\nseries of SAEs; (2) formulate the process of feature formation and conduct a\nmechanistic analysis; and (3) analyze and visualize feature drift during\ntraining. Our work provides new insights into the dynamics of features in LLMs,\nenhancing our understanding of training mechanisms and feature evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding training dynamics and feature evolution is crucial for the\nmechanistic interpretability of large language models (LLMs). Although sparse\nautoencoders (SAEs) have been used to identify features within LLMs, a clear\npicture of how these features evolve during training remains elusive. In this\nstudy, we: (1) introduce SAE-Track, a method to efficiently obtain a continual\nseries of SAEs; (2) formulate the process of feature formation and conduct a\nmechanistic analysis; and (3) analyze and visualize feature drift during\ntraining. Our work provides new insights into the dynamics of features in LLMs,\nenhancing our understanding of training mechanisms and feature evolution."
                },
                "authors": [
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17614v1",
                "updated": "2024-12-23T14:36:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    36,
                    37,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    36,
                    37,
                    0,
                    358,
                    0
                ],
                "title": "Emerging Security Challenges of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Security Challenges of Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved record adoption in a short period\nof time across many different sectors including high importance areas such as\neducation [4] and healthcare [23]. LLMs are open-ended models trained on\ndiverse data without being tailored for specific downstream tasks, enabling\nbroad applicability across various domains. They are commonly used for text\ngeneration, but also widely used to assist with code generation [3], and even\nanalysis of security information, as Microsoft Security Copilot demonstrates\n[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial\nattacks [9]. So the concerns on the potential security implications of such\nwide scale adoption of LLMs have led to the creation of this working group on\nthe security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection\nand Defense - AI-Powered Threats and Responses\", the working group discussions\nfocused on the vulnerability of LLMs to adversarial attacks, rather than their\npotential use in generating malware or enabling cyberattacks. Although we note\nthe potential threat represented by the latter, the role of the LLMs in such\nuses is mostly as an accelerator for development, similar to what it is in\nbenign use. To make the analysis more specific, the working group employed\nChatGPT as a concrete example of an LLM and addressed the following points,\nwhich also form the structure of this report: 1. How do LLMs differ in\nvulnerabilities from traditional ML models? 2. What are the attack objectives\nin LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities\nof LLMs? 4. What is the supply chain in LLMs, how data flow in and out of\nsystems and what are the security implications? We conclude with an overview of\nopen challenges and outlook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved record adoption in a short period\nof time across many different sectors including high importance areas such as\neducation [4] and healthcare [23]. LLMs are open-ended models trained on\ndiverse data without being tailored for specific downstream tasks, enabling\nbroad applicability across various domains. They are commonly used for text\ngeneration, but also widely used to assist with code generation [3], and even\nanalysis of security information, as Microsoft Security Copilot demonstrates\n[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial\nattacks [9]. So the concerns on the potential security implications of such\nwide scale adoption of LLMs have led to the creation of this working group on\nthe security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection\nand Defense - AI-Powered Threats and Responses\", the working group discussions\nfocused on the vulnerability of LLMs to adversarial attacks, rather than their\npotential use in generating malware or enabling cyberattacks. Although we note\nthe potential threat represented by the latter, the role of the LLMs in such\nuses is mostly as an accelerator for development, similar to what it is in\nbenign use. To make the analysis more specific, the working group employed\nChatGPT as a concrete example of an LLM and addressed the following points,\nwhich also form the structure of this report: 1. How do LLMs differ in\nvulnerabilities from traditional ML models? 2. What are the attack objectives\nin LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities\nof LLMs? 4. What is the supply chain in LLMs, how data flow in and out of\nsystems and what are the security implications? We conclude with an overview of\nopen challenges and outlook."
                },
                "authors": [
                    {
                        "name": "Herve Debar"
                    },
                    {
                        "name": "Sven Dietrich"
                    },
                    {
                        "name": "Pavel Laskov"
                    },
                    {
                        "name": "Emil C. Lupu"
                    },
                    {
                        "name": "Eirini Ntoutsi"
                    }
                ],
                "author_detail": {
                    "name": "Eirini Ntoutsi"
                },
                "author": "Eirini Ntoutsi",
                "arxiv_comment": "A version of this appeared in the larger Dagstuhl seminar 23431\n  report (https://doi.org/10.4230/DagRep.13.10.90)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17610v1",
                "updated": "2024-12-23T14:29:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    29,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:29:41Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    29,
                    41,
                    0,
                    358,
                    0
                ],
                "title": "Personalized Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Vision-Language Models"
                },
                "summary": "The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM."
                },
                "authors": [
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Hoang Phan"
                    },
                    {
                        "name": "David Doermann"
                    },
                    {
                        "name": "Yunjie Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yunjie Tian"
                },
                "author": "Yunjie Tian",
                "arxiv_comment": "A simple way to personalize your LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17606v1",
                "updated": "2024-12-23T14:25:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    25,
                    33,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:25:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    25,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\n  Images"
                },
                "summary": "Building a large-scale figure QA dataset requires a considerable amount of\nwork, from gathering and selecting figures to extracting attributes like text,\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\nhave led to efforts to synthesize figures, most of these focus primarily on QA\ngeneration. Additionally, creating figures directly using LLMs often encounters\nissues such as code errors, similar-looking figures, and repetitive content in\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\nthe creation of chart figures with complete annotations of the visualized data\nand dense QA annotations without any manual annotation process. Our\nstage-by-stage pipeline makes it possible to create diverse topic and\nappearance figures efficiently while minimizing code errors. Our SBSFigures\ndemonstrate a strong pre-training effect, making it possible to achieve\nefficient training with a limited amount of real-world chart data starting from\nour pre-trained weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a large-scale figure QA dataset requires a considerable amount of\nwork, from gathering and selecting figures to extracting attributes like text,\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\nhave led to efforts to synthesize figures, most of these focus primarily on QA\ngeneration. Additionally, creating figures directly using LLMs often encounters\nissues such as code errors, similar-looking figures, and repetitive content in\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\nthe creation of chart figures with complete annotations of the visualized data\nand dense QA annotations without any manual annotation process. Our\nstage-by-stage pipeline makes it possible to create diverse topic and\nappearance figures efficiently while minimizing code errors. Our SBSFigures\ndemonstrate a strong pre-training effect, making it possible to achieve\nefficient training with a limited amount of real-world chart data starting from\nour pre-trained weights."
                },
                "authors": [
                    {
                        "name": "Risa Shinoda"
                    },
                    {
                        "name": "Kuniaki Saito"
                    },
                    {
                        "name": "Shohei Tanaka"
                    },
                    {
                        "name": "Tosho Hirasawa"
                    },
                    {
                        "name": "Yoshitaka Ushiku"
                    }
                ],
                "author_detail": {
                    "name": "Yoshitaka Ushiku"
                },
                "author": "Yoshitaka Ushiku",
                "arxiv_comment": "AAAI-25 Workshop on Document Understanding and Intelligence. Dataset\n  and code: https://github.com/omron-sinicx/SBSFigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17596v1",
                "updated": "2024-12-23T14:13:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13945v2",
                "updated": "2024-12-23T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-20T02:25:07Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    25,
                    7,
                    3,
                    172,
                    0
                ],
                "title": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks"
                },
                "summary": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench."
                },
                "authors": [
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Tianhui Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Junbo Yan"
                    },
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Siqi Guo"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "26 pages, https://github.com/tsinghua-fib-lab/CityBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17593v1",
                "updated": "2024-12-23T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "title": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation"
                },
                "summary": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation."
                },
                "authors": [
                    {
                        "name": "Chengbing Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14368v2",
                "updated": "2024-12-23T14:08:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    8,
                    16,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-26T15:46:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    46,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal\n  Goal-Conditioned Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal\n  Goal-Conditioned Policy"
                },
                "summary": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One primary challenge\nis that obtaining robot trajectories fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially-annotated data,\nsuch as human activity videos without action labels and robot trajectories\nwithout text labels, are much easier to collect. Can we leverage these data to\nenhance the generalization capabilities of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on a text instruction and a\ngoal image. During training, GR-MG samples goal images from trajectories and\nconditions on both the text and the goal image or solely on the image when text\nis not available. During inference, where only the text is provided, GR-MG\ngenerates the goal image via a diffusion-based image-editing model and\nconditions on both the text and the generated image. This approach enables\nGR-MG to leverage large amounts of partially-annotated data while still using\nlanguages to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process. In simulation experiments,\nGR-MG improves the average number of tasks completed in a row of 5 from 3.35 to\n4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks\nand improves the success rate from 68.7\\% to 78.1\\% and 44.4\\% to 60.6\\% in\nsimple and generalization settings, respectively. It also outperforms comparing\nbaseline methods in few-shot learning of novel skills. Video demos, code, and\ncheckpoints are available on the project page: https://gr-mg.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One primary challenge\nis that obtaining robot trajectories fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially-annotated data,\nsuch as human activity videos without action labels and robot trajectories\nwithout text labels, are much easier to collect. Can we leverage these data to\nenhance the generalization capabilities of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on a text instruction and a\ngoal image. During training, GR-MG samples goal images from trajectories and\nconditions on both the text and the goal image or solely on the image when text\nis not available. During inference, where only the text is provided, GR-MG\ngenerates the goal image via a diffusion-based image-editing model and\nconditions on both the text and the generated image. This approach enables\nGR-MG to leverage large amounts of partially-annotated data while still using\nlanguages to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process. In simulation experiments,\nGR-MG improves the average number of tasks completed in a row of 5 from 3.35 to\n4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks\nand improves the success rate from 68.7\\% to 78.1\\% and 44.4\\% to 60.6\\% in\nsimple and generalization settings, respectively. It also outperforms comparing\nbaseline methods in few-shot learning of novel skills. Video demos, code, and\ncheckpoints are available on the project page: https://gr-mg.github.io/."
                },
                "authors": [
                    {
                        "name": "Peiyan Li"
                    },
                    {
                        "name": "Hongtao Wu"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Chilam Cheang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tao Kong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Kong"
                },
                "author": "Tao Kong",
                "arxiv_comment": "8 pages, 5 figures, RA-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12404v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12404v6",
                "updated": "2024-12-23T14:04:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    4,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-07-17T08:32:03Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    8,
                    32,
                    3,
                    2,
                    199,
                    0
                ],
                "title": "Analyzing the Generalization and Reliability of Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the Generalization and Reliability of Steering Vectors"
                },
                "summary": "Steering vectors (SVs) have been proposed as an effective approach to adjust\nlanguage model behaviour at inference time by intervening on intermediate model\nactivations. They have shown promise in terms of improving both capabilities\nand model alignment. However, the reliability and generalisation properties of\nthis approach are unknown. In this work, we rigorously investigate these\nproperties, and show that steering vectors have substantial limitations both\nin- and out-of-distribution. In-distribution, steerability is highly variable\nacross different inputs. Depending on the concept, spurious biases can\nsubstantially contribute to how effective steering is for each input,\npresenting a challenge for the widespread use of steering vectors.\nOut-of-distribution, while steering vectors often generalise well, for several\nconcepts they are brittle to reasonable changes in the prompt, resulting in\nthem failing to generalise well. Overall, our findings show that while steering\ncan work well in the right circumstances, there remain technical difficulties\nof applying steering vectors to guide models' behaviour at scale. Our code is\navailable at https://github.com/dtch1997/steering-bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering vectors (SVs) have been proposed as an effective approach to adjust\nlanguage model behaviour at inference time by intervening on intermediate model\nactivations. They have shown promise in terms of improving both capabilities\nand model alignment. However, the reliability and generalisation properties of\nthis approach are unknown. In this work, we rigorously investigate these\nproperties, and show that steering vectors have substantial limitations both\nin- and out-of-distribution. In-distribution, steerability is highly variable\nacross different inputs. Depending on the concept, spurious biases can\nsubstantially contribute to how effective steering is for each input,\npresenting a challenge for the widespread use of steering vectors.\nOut-of-distribution, while steering vectors often generalise well, for several\nconcepts they are brittle to reasonable changes in the prompt, resulting in\nthem failing to generalise well. Overall, our findings show that while steering\ncan work well in the right circumstances, there remain technical difficulties\nof applying steering vectors to guide models' behaviour at scale. Our code is\navailable at https://github.com/dtch1997/steering-bench"
                },
                "authors": [
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "Aengus Lynch"
                    },
                    {
                        "name": "Dimitrios Kanoulas"
                    },
                    {
                        "name": "Brooks Paige"
                    },
                    {
                        "name": "Adria Garriga-Alonso"
                    },
                    {
                        "name": "Robert Kirk"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kirk"
                },
                "author": "Robert Kirk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12404v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12404v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19509v2",
                "updated": "2024-12-23T14:04:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    4,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-29T07:01:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    1,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis"
                },
                "summary": "Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance."
                },
                "authors": [
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Ruobing Zheng"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "arxiv_comment": "Project Page: https://digital-avatar.github.io/ai/Ditto/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06846v2",
                "updated": "2024-12-23T13:53:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    53,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-10-09T13:06:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity"
                },
                "summary": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04135v2",
                "updated": "2024-12-23T13:48:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    48,
                    55,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-07T09:04:52Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    4,
                    52,
                    1,
                    128,
                    0
                ],
                "title": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models"
                },
                "summary": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here"
                },
                "authors": [
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Jingyue Zhang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Boyue Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Alaa Khamis"
                    }
                ],
                "author_detail": {
                    "name": "Alaa Khamis"
                },
                "author": "Alaa Khamis",
                "arxiv_comment": "9 pages, 6 figures, 34 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17560v1",
                "updated": "2024-12-23T13:28:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:28:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference"
                },
                "summary": "With the rapid growth in the scale and complexity of large language models\n(LLMs), the costs of training and inference have risen substantially. Model\ncompression has emerged as a mainstream solution to reduce memory usage and\ncomputational overhead. This paper presents Group Quantization and Sparse\nAcceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. The proposed method consists of three key steps. First, GQSA\napplies group structured pruning to adhere to GPU-friendly sparse pattern\nconstraints. Second, a two-stage sparsity-aware training process is employed to\nmaximize performance retention after compression. Finally, the framework adopts\nthe Block Sparse Row (BSR) format to enable practical deployment and efficient\nexecution. Experimental results on the LLaMA model family show that GQSA\nachieves an excellent balance between model speed and accuracy. Furthermore, on\nthe latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\ncompression techniques significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the scale and complexity of large language models\n(LLMs), the costs of training and inference have risen substantially. Model\ncompression has emerged as a mainstream solution to reduce memory usage and\ncomputational overhead. This paper presents Group Quantization and Sparse\nAcceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. The proposed method consists of three key steps. First, GQSA\napplies group structured pruning to adhere to GPU-friendly sparse pattern\nconstraints. Second, a two-stage sparsity-aware training process is employed to\nmaximize performance retention after compression. Finally, the framework adopts\nthe Block Sparse Row (BSR) format to enable practical deployment and efficient\nexecution. Experimental results on the LLaMA model family show that GQSA\nachieves an excellent balance between model speed and accuracy. Furthermore, on\nthe latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\ncompression techniques significantly."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    },
                    {
                        "name": "Lean Fu"
                    }
                ],
                "author_detail": {
                    "name": "Lean Fu"
                },
                "author": "Lean Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17558v1",
                "updated": "2024-12-23T13:26:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    26,
                    4,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:26:04Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    26,
                    4,
                    0,
                    358,
                    0
                ],
                "title": "A Survey of Query Optimization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Query Optimization in Large Language Models"
                },
                "summary": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the\nefficiency and quality of Large Language Models (LLMs) in understanding and\nanswering queries, especially complex ones in scenarios like\nRetrieval-Augmented Generation (RAG). Specifically, RAG mitigates the\nlimitations of LLMs by dynamically retrieving and leveraging up-to-date\nrelevant information, which provides a cost-effective solution to the challenge\nof LLMs producing plausible but potentially inaccurate responses. Recently, as\nRAG evolves and incorporates multiple components that influence its\nperformance, QO has emerged as a critical element, playing a pivotal role in\ndetermining the effectiveness of RAG's retrieval stage in accurately sourcing\nthe necessary multiple pieces of evidence to answer queries correctly. In this\npaper, we trace the evolution of QO techniques by summarizing and analyzing\nsignificant studies. Through an organized framework and categorization, we aim\nto consolidate existing QO techniques in RAG, elucidate their technological\nfoundations, and highlight their potential to enhance the versatility and\napplications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the\nefficiency and quality of Large Language Models (LLMs) in understanding and\nanswering queries, especially complex ones in scenarios like\nRetrieval-Augmented Generation (RAG). Specifically, RAG mitigates the\nlimitations of LLMs by dynamically retrieving and leveraging up-to-date\nrelevant information, which provides a cost-effective solution to the challenge\nof LLMs producing plausible but potentially inaccurate responses. Recently, as\nRAG evolves and incorporates multiple components that influence its\nperformance, QO has emerged as a critical element, playing a pivotal role in\ndetermining the effectiveness of RAG's retrieval stage in accurately sourcing\nthe necessary multiple pieces of evidence to answer queries correctly. In this\npaper, we trace the evolution of QO techniques by summarizing and analyzing\nsignificant studies. Through an organized framework and categorization, we aim\nto consolidate existing QO techniques in RAG, elucidate their technological\nfoundations, and highlight their potential to enhance the versatility and\napplications of LLMs."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Mao Zheng"
                },
                "author": "Mao Zheng",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17548v1",
                "updated": "2024-12-23T13:08:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    8,
                    48,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:08:48Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    8,
                    48,
                    0,
                    358,
                    0
                ],
                "title": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and\n  Multi-Domain Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and\n  Multi-Domain Testing"
                },
                "summary": "This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for\nArabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a\nsystem with only 4GB VRAM. We detail the process of adapting this large\nlanguage model to the Arabic domain, using diverse datasets including Bactrian,\nOpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom\ndata preprocessing, model configuration, and training optimization techniques\nsuch as gradient accumulation and mixed-precision training. We address specific\nchallenges in Arabic NLP, including morphological complexity, dialectal\nvariations, and diacritical mark handling. Experimental results over 10,000\ntraining steps show significant performance improvements, with the final loss\nconverging to 0.1083. We provide comprehensive analysis of GPU memory usage,\ntraining dynamics, and model evaluation across various Arabic language tasks,\nincluding text classification, question answering, and dialect identification.\nThe fine-tuned model demonstrates robustness to input perturbations and\nimproved handling of Arabic-specific linguistic phenomena. This research\ncontributes to multilingual AI by demonstrating a resource-efficient approach\nfor creating specialized language models, potentially democratizing access to\nadvanced NLP technologies for diverse linguistic communities. Our work paves\nthe way for future research in low-resource language adaptation and efficient\nfine-tuning of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for\nArabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a\nsystem with only 4GB VRAM. We detail the process of adapting this large\nlanguage model to the Arabic domain, using diverse datasets including Bactrian,\nOpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom\ndata preprocessing, model configuration, and training optimization techniques\nsuch as gradient accumulation and mixed-precision training. We address specific\nchallenges in Arabic NLP, including morphological complexity, dialectal\nvariations, and diacritical mark handling. Experimental results over 10,000\ntraining steps show significant performance improvements, with the final loss\nconverging to 0.1083. We provide comprehensive analysis of GPU memory usage,\ntraining dynamics, and model evaluation across various Arabic language tasks,\nincluding text classification, question answering, and dialect identification.\nThe fine-tuned model demonstrates robustness to input perturbations and\nimproved handling of Arabic-specific linguistic phenomena. This research\ncontributes to multilingual AI by demonstrating a resource-efficient approach\nfor creating specialized language models, potentially democratizing access to\nadvanced NLP technologies for diverse linguistic communities. Our work paves\nthe way for future research in low-resource language adaptation and efficient\nfine-tuning of large language models."
                },
                "authors": [
                    {
                        "name": "Prakash Aryan"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Aryan"
                },
                "author": "Prakash Aryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17544v1",
                "updated": "2024-12-23T13:05:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    51,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:05:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models"
                },
                "summary": "The emergence of Vision-Language Models (VLMs) is a significant advancement\nin integrating computer vision with Large Language Models (LLMs) to enhance\nmulti-modal machine learning capabilities. However, this progress has also made\nVLMs vulnerable to sophisticated adversarial attacks, raising concerns about\ntheir reliability. The objective of this paper is to assess the resilience of\nVLMs against jailbreak attacks that can compromise model safety compliance and\nresult in harmful outputs. To evaluate a VLM's ability to maintain its\nrobustness against adversarial input perturbations, we propose a novel metric\ncalled the \\textbf{Retention Score}. Retention Score is a multi-modal\nevaluation metric that includes Retention-I and Retention-T scores for\nquantifying jailbreak risks in visual and textual components of VLMs. Our\nprocess involves generating synthetic image-text pairs using a conditional\ndiffusion model. These pairs are then predicted for toxicity score by a VLM\nalongside a toxicity judgment classifier. By calculating the margin in toxicity\nscores, we can quantify the robustness of the VLM in an attack-agnostic manner.\nOur work has four main contributions. First, we prove that Retention Score can\nserve as a certified robustness metric. Second, we demonstrate that most VLMs\nwith visual components are less robust against jailbreak attacks than the\ncorresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find\nthat the security settings in Google Gemini significantly affect the score and\nrobustness. Moreover, the robustness of GPT4V is similar to the medium settings\nof Gemini. Finally, our approach offers a time-efficient alternative to\nexisting adversarial attack methods and provides consistent model robustness\nrankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Vision-Language Models (VLMs) is a significant advancement\nin integrating computer vision with Large Language Models (LLMs) to enhance\nmulti-modal machine learning capabilities. However, this progress has also made\nVLMs vulnerable to sophisticated adversarial attacks, raising concerns about\ntheir reliability. The objective of this paper is to assess the resilience of\nVLMs against jailbreak attacks that can compromise model safety compliance and\nresult in harmful outputs. To evaluate a VLM's ability to maintain its\nrobustness against adversarial input perturbations, we propose a novel metric\ncalled the \\textbf{Retention Score}. Retention Score is a multi-modal\nevaluation metric that includes Retention-I and Retention-T scores for\nquantifying jailbreak risks in visual and textual components of VLMs. Our\nprocess involves generating synthetic image-text pairs using a conditional\ndiffusion model. These pairs are then predicted for toxicity score by a VLM\nalongside a toxicity judgment classifier. By calculating the margin in toxicity\nscores, we can quantify the robustness of the VLM in an attack-agnostic manner.\nOur work has four main contributions. First, we prove that Retention Score can\nserve as a certified robustness metric. Second, we demonstrate that most VLMs\nwith visual components are less robust against jailbreak attacks than the\ncorresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find\nthat the security settings in Google Gemini significantly affect the score and\nrobustness. Moreover, the robustness of GPT4V is similar to the medium settings\nof Gemini. Finally, our approach offers a time-efficient alternative to\nexisting adversarial attack methods and provides consistent model robustness\nrankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA."
                },
                "authors": [
                    {
                        "name": "Zaitang Li"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "14 pages, 8 figures, AAAI 2025",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17542v1",
                "updated": "2024-12-23T13:05:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:05:17Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    17,
                    0,
                    358,
                    0
                ],
                "title": "Leveraging Cardiovascular Simulations for In-Vivo Prediction of Cardiac\n  Biomarkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Cardiovascular Simulations for In-Vivo Prediction of Cardiac\n  Biomarkers"
                },
                "summary": "Whole-body hemodynamics simulators, which model blood flow and pressure\nwaveforms as functions of physiological parameters, are now essential tools for\nstudying cardiovascular systems. However, solving the corresponding inverse\nproblem of mapping observations (e.g., arterial pressure waveforms at specific\nlocations in the arterial network) back to plausible physiological parameters\nremains challenging. Leveraging recent advances in simulation-based inference,\nwe cast this problem as statistical inference by training an amortized neural\nposterior estimator on a newly built large dataset of cardiac simulations that\nwe publicly release. To better align simulated data with real-world\nmeasurements, we incorporate stochastic elements modeling exogenous effects.\nThe proposed framework can further integrate in-vivo data sources to refine its\npredictive capabilities on real-world data. In silico, we demonstrate that the\nproposed framework enables finely quantifying uncertainty associated with\nindividual measurements, allowing trustworthy prediction of four biomarkers of\nclinical interest--namely Heart Rate, Cardiac Output, Systemic Vascular\nResistance, and Left Ventricular Ejection Time--from arterial pressure\nwaveforms and photoplethysmograms. Furthermore, we validate the framework in\nvivo, where our method accurately captures temporal trends in CO and SVR\nmonitoring on the VitalDB dataset. Finally, the predictive error made by the\nmodel monotonically increases with the predicted uncertainty, thereby directly\nsupporting the automatic rejection of unusable measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body hemodynamics simulators, which model blood flow and pressure\nwaveforms as functions of physiological parameters, are now essential tools for\nstudying cardiovascular systems. However, solving the corresponding inverse\nproblem of mapping observations (e.g., arterial pressure waveforms at specific\nlocations in the arterial network) back to plausible physiological parameters\nremains challenging. Leveraging recent advances in simulation-based inference,\nwe cast this problem as statistical inference by training an amortized neural\nposterior estimator on a newly built large dataset of cardiac simulations that\nwe publicly release. To better align simulated data with real-world\nmeasurements, we incorporate stochastic elements modeling exogenous effects.\nThe proposed framework can further integrate in-vivo data sources to refine its\npredictive capabilities on real-world data. In silico, we demonstrate that the\nproposed framework enables finely quantifying uncertainty associated with\nindividual measurements, allowing trustworthy prediction of four biomarkers of\nclinical interest--namely Heart Rate, Cardiac Output, Systemic Vascular\nResistance, and Left Ventricular Ejection Time--from arterial pressure\nwaveforms and photoplethysmograms. Furthermore, we validate the framework in\nvivo, where our method accurately captures temporal trends in CO and SVR\nmonitoring on the VitalDB dataset. Finally, the predictive error made by the\nmodel monotonically increases with the predicted uncertainty, thereby directly\nsupporting the automatic rejection of unusable measurements."
                },
                "authors": [
                    {
                        "name": "Laura Manduchi"
                    },
                    {
                        "name": "Antoine Wehenkel"
                    },
                    {
                        "name": "Jens Behrmann"
                    },
                    {
                        "name": "Luca Pegolotti"
                    },
                    {
                        "name": "Andy C. Miller"
                    },
                    {
                        "name": "Ozan Sener"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    },
                    {
                        "name": "Jrn-Henrik Jacobsen"
                    }
                ],
                "author_detail": {
                    "name": "Jrn-Henrik Jacobsen"
                },
                "author": "Jrn-Henrik Jacobsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v5",
                "updated": "2024-12-23T12:48:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    48,
                    43,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17522v1",
                "updated": "2024-12-23T12:44:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    44,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T12:44:54Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    44,
                    54,
                    0,
                    358,
                    0
                ],
                "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM\n  Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM\n  Jailbreak"
                },
                "summary": "Large Language Models (LLMs) are susceptible to generating harmful content\nwhen prompted with carefully crafted inputs, a vulnerability known as LLM\njailbreaking. As LLMs become more powerful, studying jailbreak methods is\ncritical to enhancing security and aligning models with human values.\nTraditionally, jailbreak techniques have relied on suffix addition or prompt\ntemplates, but these methods suffer from limited attack diversity. This paper\nintroduces DiffusionAttacker, an end-to-end generative approach for jailbreak\nrewriting inspired by diffusion models. Our method employs a\nsequence-to-sequence (seq2seq) text diffusion model as a generator,\nconditioning on the original prompt and guiding the denoising process with a\nnovel attack loss. Unlike previous approaches that use autoregressive LLMs to\ngenerate jailbreak prompts, which limit the modification of already generated\ntokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq\ndiffusion model, allowing more flexible token modifications. This approach\npreserves the semantic content of the original prompt while producing harmful\ncontent. Additionally, we leverage the Gumbel-Softmax technique to make the\nsampling process from the diffusion model's output distribution differentiable,\neliminating the need for iterative token search. Extensive experiments on\nAdvbench and Harmbench demonstrate that DiffusionAttacker outperforms previous\nmethods across various evaluation metrics, including attack success rate (ASR),\nfluency, and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to generating harmful content\nwhen prompted with carefully crafted inputs, a vulnerability known as LLM\njailbreaking. As LLMs become more powerful, studying jailbreak methods is\ncritical to enhancing security and aligning models with human values.\nTraditionally, jailbreak techniques have relied on suffix addition or prompt\ntemplates, but these methods suffer from limited attack diversity. This paper\nintroduces DiffusionAttacker, an end-to-end generative approach for jailbreak\nrewriting inspired by diffusion models. Our method employs a\nsequence-to-sequence (seq2seq) text diffusion model as a generator,\nconditioning on the original prompt and guiding the denoising process with a\nnovel attack loss. Unlike previous approaches that use autoregressive LLMs to\ngenerate jailbreak prompts, which limit the modification of already generated\ntokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq\ndiffusion model, allowing more flexible token modifications. This approach\npreserves the semantic content of the original prompt while producing harmful\ncontent. Additionally, we leverage the Gumbel-Softmax technique to make the\nsampling process from the diffusion model's output distribution differentiable,\neliminating the need for iterative token search. Extensive experiments on\nAdvbench and Harmbench demonstrate that DiffusionAttacker outperforms previous\nmethods across various evaluation metrics, including attack success rate (ASR),\nfluency, and diversity."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Junda Zhu"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "MinLie Huang"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21315v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21315v4",
                "updated": "2024-12-23T12:35:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    35,
                    12,
                    0,
                    358,
                    0
                ],
                "published": "2024-07-31T03:53:14Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    3,
                    53,
                    14,
                    2,
                    213,
                    0
                ],
                "title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances"
                },
                "summary": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%)."
                },
                "authors": [
                    {
                        "name": "Zehui Wu"
                    },
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Pengyuan Shi"
                    },
                    {
                        "name": "Kaan Donbekci"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21315v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21315v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02597v3",
                "updated": "2024-12-23T12:12:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    12,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-04T10:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    28,
                    5,
                    2,
                    248,
                    0
                ],
                "title": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate-Adaptive Generative Semantic Communication Using Conditional\n  Diffusion Models"
                },
                "summary": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning-based joint source-channel coding (DJSCC)\nhave shown promise for end-to-end semantic image transmission. However, most\nexisting schemes primarily focus on optimizing pixel-wise metrics, which often\nfail to align with human perception, leading to lower perceptual quality. In\nthis letter, we propose a novel generative DJSCC approach using conditional\ndiffusion models to enhance the perceptual quality of transmitted images.\nSpecifically, by utilizing entropy models, we effectively manage transmission\nbandwidth based on the estimated entropy of transmitted sym-bols. These symbols\nare then used at the receiver as conditional information to guide a conditional\ndiffusion decoder in image reconstruction. Our model is built upon the emerging\nadvanced mamba-like linear attention (MLLA) skeleton, which excels in image\nprocessing tasks while also offering fast inference speed. Besides, we\nintroduce a multi-stage training strategy to ensure the stability and improve\nthe overall performance of the model. Simulation results demonstrate that our\nproposed method significantly outperforms existing approaches in terms of\nperceptual quality."
                },
                "authors": [
                    {
                        "name": "Pujing Yang"
                    },
                    {
                        "name": "Guangyi Zhang"
                    },
                    {
                        "name": "Yunlong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yunlong Cai"
                },
                "author": "Yunlong Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12134v2",
                "updated": "2024-12-23T12:04:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    4,
                    58,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-20T15:53:50Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    15,
                    53,
                    50,
                    0,
                    141,
                    0
                ],
                "title": "Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system\n  and its means field derivation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system\n  and its means field derivation"
                },
                "summary": "In this paper, the well-posedness of two-dimensional signal-dependent\nKeller-Segel system and its mean field derivation from a interacting particle\nsystem on the whole space are investigated. The signal dependence effect is\nreflected by the fact that the diffusion coefficient in the particle system\ndepends nonlinearly on the interactions between the individuals. Therefore, the\nmathematical challenge in studying the well-posedness of this system lies in\nthe possible degeneracy and the aggregation effect when the concentration of\nsignal becomes unbounded. The well-established method on bounded domain, to\nobtain the appropriate estimates for the signal concentration, is invalid for\nthe whole space case. Motivated by the entropy minimization method and Onofri's\ninequality, which has been successfully applied for parabolic-parabolic\nKeller-Segel system, we establish a complete entropy estimate benefited from\nlinear diffusion term, which plays important role in obtaining the Lp estimates\nfor the solution. Furthermore, the upper bound for the concentration of signal\nis obtained. Based on estimates we obtained for the density of cells, the\nrigorous mean-field derivation is proved by introducing an intermediate\nparticle system with a mollified interaction potential with logarithmic\nscaling. By using this mollification, we obtain the convergence of the particle\ntrajectories in expectation, which implies the weak propagation of chaos.\nAdditionally, under a regularity assumption of the initial data, we infer\nhigher regularity for the solutions, which allows us to use relative entropy\nmethod to derive the strong L1 convergence for the propagation of chaos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, the well-posedness of two-dimensional signal-dependent\nKeller-Segel system and its mean field derivation from a interacting particle\nsystem on the whole space are investigated. The signal dependence effect is\nreflected by the fact that the diffusion coefficient in the particle system\ndepends nonlinearly on the interactions between the individuals. Therefore, the\nmathematical challenge in studying the well-posedness of this system lies in\nthe possible degeneracy and the aggregation effect when the concentration of\nsignal becomes unbounded. The well-established method on bounded domain, to\nobtain the appropriate estimates for the signal concentration, is invalid for\nthe whole space case. Motivated by the entropy minimization method and Onofri's\ninequality, which has been successfully applied for parabolic-parabolic\nKeller-Segel system, we establish a complete entropy estimate benefited from\nlinear diffusion term, which plays important role in obtaining the Lp estimates\nfor the solution. Furthermore, the upper bound for the concentration of signal\nis obtained. Based on estimates we obtained for the density of cells, the\nrigorous mean-field derivation is proved by introducing an intermediate\nparticle system with a mollified interaction potential with logarithmic\nscaling. By using this mollification, we obtain the convergence of the particle\ntrajectories in expectation, which implies the weak propagation of chaos.\nAdditionally, under a regularity assumption of the initial data, we infer\nhigher regularity for the solutions, which allows us to use relative entropy\nmethod to derive the strong L1 convergence for the propagation of chaos."
                },
                "authors": [
                    {
                        "name": "Lukas Bol"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yue Li"
                    }
                ],
                "author_detail": {
                    "name": "Yue Li"
                },
                "author": "Yue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14546v3",
                "updated": "2024-12-23T12:01:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    1,
                    28,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-20T17:55:04Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    55,
                    4,
                    3,
                    172,
                    0
                ],
                "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data"
                },
                "summary": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs."
                },
                "authors": [
                    {
                        "name": "Johannes Treutlein"
                    },
                    {
                        "name": "Dami Choi"
                    },
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Cem Anil"
                    },
                    {
                        "name": "Roger Grosse"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "Accepted at NeurIPS 2024. 10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v1",
                "updated": "2024-12-23T11:55:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to judge whether the translation in the current round is better\nthan the previous one or not. In this manner, we collect tens of thousands of\nlong-thought MT data, which is used to train our DRT-o1. The experimental\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to judge whether the translation in the current round is better\nthan the previous one or not. In this manner, we collect tens of thousands of\nlong-thought MT data, which is used to train our DRT-o1. The experimental\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1"
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05780v3",
                "updated": "2024-12-23T11:42:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    42,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T02:23:40Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    2,
                    23,
                    40,
                    6,
                    343,
                    0
                ],
                "title": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models"
                },
                "summary": "Diffusion models have shown unprecedented success in the task of\ntext-to-image generation. While these models are capable of generating\nhigh-quality and realistic images, the complexity of sequential denoising has\nraised societal concerns regarding high computational demands and energy\nconsumption. In response, various efforts have been made to improve inference\nefficiency. However, most of the existing efforts have taken a fixed approach\nwith neural network simplification or text prompt optimization. Are the quality\nimprovements from all denoising computations equally perceivable to humans? We\nobserved that images from different text prompts may require different\ncomputational efforts given the desired content. The observation motivates us\nto present BudgetFusion, a novel model that suggests the most perceptually\nefficient number of diffusion steps before a diffusion model starts to generate\nan image. This is achieved by predicting multi-level perceptual metrics\nrelative to diffusion steps. With the popular Stable Diffusion as an example,\nwe conduct both numerical analyses and user studies. Our experiments show that\nBudgetFusion saves up to five seconds per prompt without compromising\nperceptual similarity. We hope this work can initiate efforts toward answering\na core question: how much do humans perceptually gain from images created by a\ngenerative model, per watt of energy?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown unprecedented success in the task of\ntext-to-image generation. While these models are capable of generating\nhigh-quality and realistic images, the complexity of sequential denoising has\nraised societal concerns regarding high computational demands and energy\nconsumption. In response, various efforts have been made to improve inference\nefficiency. However, most of the existing efforts have taken a fixed approach\nwith neural network simplification or text prompt optimization. Are the quality\nimprovements from all denoising computations equally perceivable to humans? We\nobserved that images from different text prompts may require different\ncomputational efforts given the desired content. The observation motivates us\nto present BudgetFusion, a novel model that suggests the most perceptually\nefficient number of diffusion steps before a diffusion model starts to generate\nan image. This is achieved by predicting multi-level perceptual metrics\nrelative to diffusion steps. With the popular Stable Diffusion as an example,\nwe conduct both numerical analyses and user studies. Our experiments show that\nBudgetFusion saves up to five seconds per prompt without compromising\nperceptual similarity. We hope this work can initiate efforts toward answering\na core question: how much do humans perceptually gain from images created by a\ngenerative model, per watt of energy?"
                },
                "authors": [
                    {
                        "name": "Qinchan Li"
                    },
                    {
                        "name": "Kenneth Chen"
                    },
                    {
                        "name": "Changyue Su"
                    },
                    {
                        "name": "Qi Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qi Sun"
                },
                "author": "Qi Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05805v2",
                "updated": "2024-12-23T11:36:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    36,
                    22,
                    0,
                    358,
                    0
                ],
                "published": "2023-10-09T15:43:46Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    15,
                    43,
                    46,
                    0,
                    282,
                    0
                ],
                "title": "Boosted Control Functions: Distribution generalization and invariance in\n  confounded models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosted Control Functions: Distribution generalization and invariance in\n  confounded models"
                },
                "summary": "Modern machine learning methods and the availability of large-scale data have\nsignificantly advanced our ability to predict target quantities from large sets\nof covariates. However, these methods often struggle under distributional\nshifts, particularly in the presence of hidden confounding. While the impact of\nhidden confounding is well-studied in causal effect estimation, e.g.,\ninstrumental variables, its implications for prediction tasks under shifting\ndistributions remain underexplored. This work addresses this gap by introducing\na strong notion of invariance that, unlike existing weaker notions, allows for\ndistribution generalization even in the presence of nonlinear, non-identifiable\nstructural functions. Central to this framework is the Boosted Control Function\n(BCF), a novel, identifiable target of inference that satisfies the proposed\nstrong invariance notion and is provably worst-case optimal under\ndistributional shifts. The theoretical foundation of our work lies in\nSimultaneous Equation Models for Distribution Generalization (SIMDGs), which\nbridge machine learning with econometrics by describing data-generating\nprocesses under distributional shifts. To put these insights into practice, we\npropose the ControlTwicing algorithm to estimate the BCF using flexible\nmachine-learning techniques and demonstrate its generalization performance on\nsynthetic and real-world datasets compared to traditional empirical risk\nminimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning methods and the availability of large-scale data have\nsignificantly advanced our ability to predict target quantities from large sets\nof covariates. However, these methods often struggle under distributional\nshifts, particularly in the presence of hidden confounding. While the impact of\nhidden confounding is well-studied in causal effect estimation, e.g.,\ninstrumental variables, its implications for prediction tasks under shifting\ndistributions remain underexplored. This work addresses this gap by introducing\na strong notion of invariance that, unlike existing weaker notions, allows for\ndistribution generalization even in the presence of nonlinear, non-identifiable\nstructural functions. Central to this framework is the Boosted Control Function\n(BCF), a novel, identifiable target of inference that satisfies the proposed\nstrong invariance notion and is provably worst-case optimal under\ndistributional shifts. The theoretical foundation of our work lies in\nSimultaneous Equation Models for Distribution Generalization (SIMDGs), which\nbridge machine learning with econometrics by describing data-generating\nprocesses under distributional shifts. To put these insights into practice, we\npropose the ControlTwicing algorithm to estimate the BCF using flexible\nmachine-learning techniques and demonstrate its generalization performance on\nsynthetic and real-world datasets compared to traditional empirical risk\nminimization approaches."
                },
                "authors": [
                    {
                        "name": "Nicola Gnecco"
                    },
                    {
                        "name": "Jonas Peters"
                    },
                    {
                        "name": "Sebastian Engelke"
                    },
                    {
                        "name": "Niklas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Pfister"
                },
                "author": "Niklas Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17486v1",
                "updated": "2024-12-23T11:29:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    29,
                    44,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:29:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    29,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of\n  Large Language Models such as ChatGPT in Educational Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of\n  Large Language Models such as ChatGPT in Educational Settings"
                },
                "summary": "The rapid adoption of Generative AI (GenAI) based on Large Language Models\n(LLMs) such as ChatGPT has recently and profoundly impacted education, offering\ntransformative opportunities while raising significant concerns. In this study\nwe present the results of a survey that investigates how 395 students aged 13\nto 25 years old in France and Italy integrate LLMs into their educational\nroutines.\n  Key findings include the widespread use of these tools across all age groups\nand disciplines, with older students and male students demonstrating higher\nusage frequencies, particularly in scientific contexts. The results also show\ngender disparities, raising concerns about an emerging AI literacy and\ntechnological gender gap. Additionally, while most students utilise LLMs\nconstructively, the lack of systematic proofreading and critical evaluation\namong younger users suggests potential risks to cognitive skills development,\nincluding critical thinking and foundational knowledge. The survey results\nunderscore the need for educational institutions to adapt their curricula to\nintegrate AI tools effectively, promoting ethical use, critical thinking, and\nawareness of AI limitations and environmental costs. This paper provides\nactionable recommendations for fostering equitable and effective cohabitation\nof LLMs and education while addressing emerging challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Generative AI (GenAI) based on Large Language Models\n(LLMs) such as ChatGPT has recently and profoundly impacted education, offering\ntransformative opportunities while raising significant concerns. In this study\nwe present the results of a survey that investigates how 395 students aged 13\nto 25 years old in France and Italy integrate LLMs into their educational\nroutines.\n  Key findings include the widespread use of these tools across all age groups\nand disciplines, with older students and male students demonstrating higher\nusage frequencies, particularly in scientific contexts. The results also show\ngender disparities, raising concerns about an emerging AI literacy and\ntechnological gender gap. Additionally, while most students utilise LLMs\nconstructively, the lack of systematic proofreading and critical evaluation\namong younger users suggests potential risks to cognitive skills development,\nincluding critical thinking and foundational knowledge. The survey results\nunderscore the need for educational institutions to adapt their curricula to\nintegrate AI tools effectively, promoting ethical use, critical thinking, and\nawareness of AI limitations and environmental costs. This paper provides\nactionable recommendations for fostering equitable and effective cohabitation\nof LLMs and education while addressing emerging challenges."
                },
                "authors": [
                    {
                        "name": "Jrmie Sublime"
                    },
                    {
                        "name": "Ilaria Renna"
                    }
                ],
                "author_detail": {
                    "name": "Ilaria Renna"
                },
                "author": "Ilaria Renna",
                "arxiv_comment": "33 pages + references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17485v1",
                "updated": "2024-12-23T11:28:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    28,
                    44,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:28:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    28,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "Distribution-Adaptive Dynamic Shot Optimization for Variational Quantum\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-Adaptive Dynamic Shot Optimization for Variational Quantum\n  Algorithms"
                },
                "summary": "Variational quantum algorithms (VQAs) have attracted remarkable interest over\nthe past few years because of their potential computational advantages on\nnear-term quantum devices. They leverage a hybrid approach that integrates\nclassical and quantum computing resources to solve high-dimensional problems\nthat are challenging for classical approaches alone. In the training process of\nvariational circuits, constructing an accurate probability distribution for\neach epoch is not always necessary, creating opportunities to reduce\ncomputational costs through shot reduction. However, existing shot-allocation\nmethods that capitalize on this potential often lack adaptive feedback or are\ntied to specific classical optimizers, which limits their applicability to\ncommon VQAs and broader optimization techniques. Our observations indicate that\nthe information entropy of a quantum circuit's output distribution exhibits an\napproximately exponential relationship with the number of shots needed to\nachieve a target Hellinger distance. In this work, we propose a\ndistribution-adaptive dynamic shot (DDS) framework that efficiently adjusts the\nnumber of shots per iteration in VQAs using the entropy distribution from the\nprior training epoch. Our results demonstrate that the DDS framework sustains\ninference accuracy while achieving a ~50% reduction in average shot count\ncompared to fixed-shot training, and ~60% higher accuracy than recently\nproposed tiered shot allocation methods. Furthermore, in noisy simulations that\nreflect the error rates of actual IBM quantum systems, DDS achieves\napproximately a ~30% reduction in the total number of shots compared to the\nfixed-shot method with minimal degradation in accuracy, and offers about ~70%\nhigher computational accuracy than tiered shot allocation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational quantum algorithms (VQAs) have attracted remarkable interest over\nthe past few years because of their potential computational advantages on\nnear-term quantum devices. They leverage a hybrid approach that integrates\nclassical and quantum computing resources to solve high-dimensional problems\nthat are challenging for classical approaches alone. In the training process of\nvariational circuits, constructing an accurate probability distribution for\neach epoch is not always necessary, creating opportunities to reduce\ncomputational costs through shot reduction. However, existing shot-allocation\nmethods that capitalize on this potential often lack adaptive feedback or are\ntied to specific classical optimizers, which limits their applicability to\ncommon VQAs and broader optimization techniques. Our observations indicate that\nthe information entropy of a quantum circuit's output distribution exhibits an\napproximately exponential relationship with the number of shots needed to\nachieve a target Hellinger distance. In this work, we propose a\ndistribution-adaptive dynamic shot (DDS) framework that efficiently adjusts the\nnumber of shots per iteration in VQAs using the entropy distribution from the\nprior training epoch. Our results demonstrate that the DDS framework sustains\ninference accuracy while achieving a ~50% reduction in average shot count\ncompared to fixed-shot training, and ~60% higher accuracy than recently\nproposed tiered shot allocation methods. Furthermore, in noisy simulations that\nreflect the error rates of actual IBM quantum systems, DDS achieves\napproximately a ~30% reduction in the total number of shots compared to the\nfixed-shot method with minimal degradation in accuracy, and offers about ~70%\nhigher computational accuracy than tiered shot allocation methods."
                },
                "authors": [
                    {
                        "name": "Youngmin Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hyungseok Kim"
                    },
                    {
                        "name": "Seungwoo Choi"
                    },
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Donghwi Kim"
                    },
                    {
                        "name": "Woomin Kyoung"
                    },
                    {
                        "name": "Kyujin Shin"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "author": "Won Woo Ro",
                "arxiv_comment": "18 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17484v1",
                "updated": "2024-12-23T11:27:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    27,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:27:17Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    27,
                    17,
                    0,
                    358,
                    0
                ],
                "title": "Power- and Fragmentation-aware Online Scheduling for GPU Datacenters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power- and Fragmentation-aware Online Scheduling for GPU Datacenters"
                },
                "summary": "The rise of Artificial Intelligence and Large Language Models is driving\nincreased GPU usage in data centers for complex training and inference tasks,\nimpacting operational costs, energy demands, and the environmental footprint of\nlarge-scale computing infrastructures. This work addresses the online\nscheduling problem in GPU datacenters, which involves scheduling tasks without\nknowledge of their future arrivals. We focus on two objectives: minimizing GPU\nfragmentation and reducing power consumption. GPU fragmentation occurs when\npartial GPU allocations hinder the efficient use of remaining resources,\nespecially as the datacenter nears full capacity. A recent scheduling policy,\nFragmentation Gradient Descent (FGD), leverages a fragmentation metric to\naddress this issue. Reducing power consumption is also crucial due to the\nsignificant power demands of GPUs. To this end, we propose PWR, a novel\nscheduling policy to minimize power usage by selecting power-efficient GPU and\nCPU combinations. This involves a simplified model for measuring power\nconsumption integrated into a Kubernetes score plugin. Through an extensive\nexperimental evaluation in a simulated cluster, we show how PWR, when combined\nwith FGD, achieves a balanced trade-off between reducing power consumption and\nminimizing GPU fragmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Artificial Intelligence and Large Language Models is driving\nincreased GPU usage in data centers for complex training and inference tasks,\nimpacting operational costs, energy demands, and the environmental footprint of\nlarge-scale computing infrastructures. This work addresses the online\nscheduling problem in GPU datacenters, which involves scheduling tasks without\nknowledge of their future arrivals. We focus on two objectives: minimizing GPU\nfragmentation and reducing power consumption. GPU fragmentation occurs when\npartial GPU allocations hinder the efficient use of remaining resources,\nespecially as the datacenter nears full capacity. A recent scheduling policy,\nFragmentation Gradient Descent (FGD), leverages a fragmentation metric to\naddress this issue. Reducing power consumption is also crucial due to the\nsignificant power demands of GPUs. To this end, we propose PWR, a novel\nscheduling policy to minimize power usage by selecting power-efficient GPU and\nCPU combinations. This involves a simplified model for measuring power\nconsumption integrated into a Kubernetes score plugin. Through an extensive\nexperimental evaluation in a simulated cluster, we show how PWR, when combined\nwith FGD, achieves a balanced trade-off between reducing power consumption and\nminimizing GPU fragmentation."
                },
                "authors": [
                    {
                        "name": "Francesco Lettich"
                    },
                    {
                        "name": "Emanuele Carlini"
                    },
                    {
                        "name": "Franco Maria Nardini"
                    },
                    {
                        "name": "Raffaele Perego"
                    },
                    {
                        "name": "Salvatore Trani"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Trani"
                },
                "author": "Salvatore Trani",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17481v1",
                "updated": "2024-12-23T11:11:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:11:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "A Survey on Multi-Generative Agent System: Recent Advances and New\n  Frontiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multi-Generative Agent System: Recent Advances and New\n  Frontiers"
                },
                "summary": "Multi-generative agent systems (MGASs) have become a research hotspot since\nthe rise of large language models (LLMs). However, with the continuous influx\nof new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of MGAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of MGAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-generative agent systems (MGASs) have become a research hotspot since\nthe rise of large language models (LLMs). However, with the continuous influx\nof new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of MGAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of MGAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Shuaihang Chen"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15265v2",
                "updated": "2024-12-23T11:06:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    6,
                    56,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-17T03:03:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    3,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks."
                },
                "authors": [
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Boren Zheng"
                    },
                    {
                        "name": "Baihui Zheng"
                    },
                    {
                        "name": "Kerui Cao"
                    },
                    {
                        "name": "Huiyun Jing"
                    },
                    {
                        "name": "Jincheng Wei"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiangyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19157v3",
                "updated": "2024-12-23T11:03:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    3,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-27T13:21:33Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    13,
                    21,
                    33,
                    3,
                    179,
                    0
                ],
                "title": "How to build your latent Markov model -- the role of time and space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to build your latent Markov model -- the role of time and space"
                },
                "summary": "Statistical models that involve latent Markovian state processes have become\nimmensely popular tools for analysing time series and other sequential data.\nHowever, the plethora of model formulations, the inconsistent use of\nterminology, and the various inferential approaches and software packages can\nbe overwhelming to practitioners, especially when they are new to this area.\nWith this review-like paper, we thus aim to provide guidance for both\nstatisticians and practitioners working with latent Markov models by offering a\nunifying view on what otherwise are often considered separate model classes,\nfrom hidden Markov models over state-space models to Markov-modulated Poisson\nprocesses. In particular, we provide a roadmap for identifying a suitable\nlatent Markov model formulation given the data to be analysed. Furthermore, we\nemphasise that it is key to applied work with any of these model classes to\nunderstand how recursive techniques exploiting the models' dependence structure\ncan be used for inference. The R package LaMa adapts this unified view and\nprovides an easy-to-use framework for very fast (C++ based) numerical maximum\nlikelihood estimation of any of the models discussed in this paper, allowing\nusers to tailor a latent Markov model to their data using a Lego-type approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical models that involve latent Markovian state processes have become\nimmensely popular tools for analysing time series and other sequential data.\nHowever, the plethora of model formulations, the inconsistent use of\nterminology, and the various inferential approaches and software packages can\nbe overwhelming to practitioners, especially when they are new to this area.\nWith this review-like paper, we thus aim to provide guidance for both\nstatisticians and practitioners working with latent Markov models by offering a\nunifying view on what otherwise are often considered separate model classes,\nfrom hidden Markov models over state-space models to Markov-modulated Poisson\nprocesses. In particular, we provide a roadmap for identifying a suitable\nlatent Markov model formulation given the data to be analysed. Furthermore, we\nemphasise that it is key to applied work with any of these model classes to\nunderstand how recursive techniques exploiting the models' dependence structure\ncan be used for inference. The R package LaMa adapts this unified view and\nprovides an easy-to-use framework for very fast (C++ based) numerical maximum\nlikelihood estimation of any of the models discussed in this paper, allowing\nusers to tailor a latent Markov model to their data using a Lego-type approach."
                },
                "authors": [
                    {
                        "name": "Sina Mews"
                    },
                    {
                        "name": "Jan-Ole Koslik"
                    },
                    {
                        "name": "Roland Langrock"
                    }
                ],
                "author_detail": {
                    "name": "Roland Langrock"
                },
                "author": "Roland Langrock",
                "arxiv_comment": "52 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11909v2",
                "updated": "2024-12-23T11:00:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    0,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-16T15:54:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "Playground of Lognormal Seminumerical Simulations of~the~Lyman~$$\n  Forest: Thermal History of the Intergalactic Medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playground of Lognormal Seminumerical Simulations of~the~Lyman~$$\n  Forest: Thermal History of the Intergalactic Medium"
                },
                "summary": "This study aims to test a potential application of lognormal seminumerical\nsimulations to recover the thermal parameters and Jeans length. This could be\nsuitable for generating large number of synthetic spectra with various input\ndata and parameters, and thus ideal for interpreting the high-quality data\nobtained from QSO absorption spectra surveys. We use a seminumerical approach\nto simulate absorption spectra of quasars at redshifts $ 3 \\leq z \\leq 5$.\nThese synthetic spectra are compared with the 1D flux power spectra and using\nthe Markov Chain Monte Carlo analysis method we determine the temperature at\nmean density, slope of the temperature-density relation and Jeans length. Our\nbest-fit model is also compared with the evolution of the temperature of the\nintergalactic medium from various UVB models. We show that the lognormal\nsimulations can effectively recover thermal parameters and Jeans length.\nBesides, by comparing the synthetic flux power spectra with observations from\nBaryon Oscillation Spectroscopy Survey we found, that such an approach can be\nalso used for the cosmological parameter inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to test a potential application of lognormal seminumerical\nsimulations to recover the thermal parameters and Jeans length. This could be\nsuitable for generating large number of synthetic spectra with various input\ndata and parameters, and thus ideal for interpreting the high-quality data\nobtained from QSO absorption spectra surveys. We use a seminumerical approach\nto simulate absorption spectra of quasars at redshifts $ 3 \\leq z \\leq 5$.\nThese synthetic spectra are compared with the 1D flux power spectra and using\nthe Markov Chain Monte Carlo analysis method we determine the temperature at\nmean density, slope of the temperature-density relation and Jeans length. Our\nbest-fit model is also compared with the evolution of the temperature of the\nintergalactic medium from various UVB models. We show that the lognormal\nsimulations can effectively recover thermal parameters and Jeans length.\nBesides, by comparing the synthetic flux power spectra with observations from\nBaryon Oscillation Spectroscopy Survey we found, that such an approach can be\nalso used for the cosmological parameter inference."
                },
                "authors": [
                    {
                        "name": "Tomas Ondro"
                    },
                    {
                        "name": "Bhaskar Arya"
                    },
                    {
                        "name": "Rudolf Galis"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Galis"
                },
                "author": "Rudolf Galis",
                "arxiv_comment": "19 pages, 11 figures, submitted to JCAP. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v2",
                "updated": "2024-12-23T10:46:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    46,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "arxiv_comment": "In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15714v2",
                "updated": "2024-12-23T10:45:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    45,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-20T09:37:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLife: Automatic Life Journaling with Smartphones and LLMs"
                },
                "summary": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals."
                },
                "authors": [
                    {
                        "name": "Huatao Xu"
                    },
                    {
                        "name": "Panrong Tong"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12498v2",
                "updated": "2024-12-23T10:36:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    36,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-19T13:31:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus"
                },
                "summary": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH."
                },
                "authors": [
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Gaku Morio"
                    },
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Yasuhiro Sogawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhiro Sogawa"
                },
                "author": "Yasuhiro Sogawa",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v6",
                "updated": "2024-12-23T10:29:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    29,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17460v1",
                "updated": "2024-12-23T10:28:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    28,
                    10,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:28:10Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    28,
                    10,
                    0,
                    358,
                    0
                ],
                "title": "Quantum Gravity Signature in a Thermodynamic Observable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Gravity Signature in a Thermodynamic Observable"
                },
                "summary": "Proposed experiments for obtaining empirical evidence for a quantum\ndescription of gravity in a table-top setting focus on detecting quantum\ninformation signatures, such as entanglement or non-Gaussianity production, in\ngravitationally interacting quantum systems. Here, we explore an alternative\napproach where the quantization of gravity could be inferred through\nmeasurements of macroscopic, thermodynamical quantities, without the need for\naddressability of individual quantum systems. To demonstrate the idea, we take\nas a case study a gravitationally self-interacting Bose gas, and consider its\nheat capacity. We find a clear-cut distinction between the predictions of a\nclassical gravitational interaction and a quantum gravitational interaction in\nthe heat capacity of the Bose gas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proposed experiments for obtaining empirical evidence for a quantum\ndescription of gravity in a table-top setting focus on detecting quantum\ninformation signatures, such as entanglement or non-Gaussianity production, in\ngravitationally interacting quantum systems. Here, we explore an alternative\napproach where the quantization of gravity could be inferred through\nmeasurements of macroscopic, thermodynamical quantities, without the need for\naddressability of individual quantum systems. To demonstrate the idea, we take\nas a case study a gravitationally self-interacting Bose gas, and consider its\nheat capacity. We find a clear-cut distinction between the predictions of a\nclassical gravitational interaction and a quantum gravitational interaction in\nthe heat capacity of the Bose gas."
                },
                "authors": [
                    {
                        "name": "Thomas Strasser"
                    },
                    {
                        "name": "Marios Christodoulou"
                    },
                    {
                        "name": "Richard Howl"
                    },
                    {
                        "name": "Caslav Brukner"
                    }
                ],
                "author_detail": {
                    "name": "Caslav Brukner"
                },
                "author": "Caslav Brukner",
                "arxiv_comment": "4 pages, 4 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17455v1",
                "updated": "2024-12-23T10:21:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    21,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:21:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    21,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "Learning from Summarized Data: Gaussian Process Regression with Sample\n  Quasi-Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Summarized Data: Gaussian Process Regression with Sample\n  Quasi-Likelihood"
                },
                "summary": "Gaussian process regression is a powerful Bayesian nonlinear regression\nmethod. Recent research has enabled the capture of many types of observations\nusing non-Gaussian likelihoods. To deal with various tasks in spatial modeling,\nwe benefit from this development. Difficulties still arise when we can only\naccess summarized data consisting of representative features, summary\nstatistics, and data point counts. Such situations frequently occur primarily\ndue to concerns about confidentiality and management costs associated with\nspatial data. This study tackles learning and inference using only summarized\ndata within the framework of Gaussian process regression. To address this\nchallenge, we analyze the approximation errors in the marginal likelihood and\nposterior distribution that arise from utilizing representative features. We\nalso introduce the concept of sample quasi-likelihood, which facilitates\nlearning and inference using only summarized data. Non-Gaussian likelihoods\nsatisfying certain assumptions can be captured by specifying a variance\nfunction that characterizes a sample quasi-likelihood function. Theoretical and\nexperimental results demonstrate that the approximation performance is\ninfluenced by the granularity of summarized data relative to the length scale\nof covariance functions. Experiments on a real-world dataset highlight the\npracticality of our method for spatial modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process regression is a powerful Bayesian nonlinear regression\nmethod. Recent research has enabled the capture of many types of observations\nusing non-Gaussian likelihoods. To deal with various tasks in spatial modeling,\nwe benefit from this development. Difficulties still arise when we can only\naccess summarized data consisting of representative features, summary\nstatistics, and data point counts. Such situations frequently occur primarily\ndue to concerns about confidentiality and management costs associated with\nspatial data. This study tackles learning and inference using only summarized\ndata within the framework of Gaussian process regression. To address this\nchallenge, we analyze the approximation errors in the marginal likelihood and\nposterior distribution that arise from utilizing representative features. We\nalso introduce the concept of sample quasi-likelihood, which facilitates\nlearning and inference using only summarized data. Non-Gaussian likelihoods\nsatisfying certain assumptions can be captured by specifying a variance\nfunction that characterizes a sample quasi-likelihood function. Theoretical and\nexperimental results demonstrate that the approximation performance is\ninfluenced by the granularity of summarized data relative to the length scale\nof covariance functions. Experiments on a real-world dataset highlight the\npracticality of our method for spatial modeling."
                },
                "authors": [
                    {
                        "name": "Yuta Shikuri"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Shikuri"
                },
                "author": "Yuta Shikuri",
                "arxiv_comment": "19 pages, 4 figures, 5 tables, AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17449v1",
                "updated": "2024-12-23T10:14:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    14,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:14:32Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    14,
                    32,
                    0,
                    358,
                    0
                ],
                "title": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts"
                },
                "summary": "This study explores the use of Large language models to analyze therapist\nremarks in a psychotherapeutic setting. The paper focuses on the application of\nBERTopic, a machine learning-based topic modeling tool, to the dialogue of two\ndifferent groups of therapists (classical and modern), which makes it possible\nto identify and describe a set of topics that consistently emerge across these\ngroups. The paper describes in detail the chosen algorithm for BERTopic, which\nincluded creating a vector space from a corpus of therapist remarks, reducing\nits dimensionality, clustering the space, and creating and optimizing topic\nrepresentation. Along with the automatic topical modeling by the BERTopic, the\nresearch involved an expert assessment of the findings and manual topic\nstructure optimization. The topic modeling results highlighted the most common\nand stable topics in therapists speech, offering insights into how language\npatterns in therapy develop and remain stable across different therapeutic\nstyles. This work contributes to the growing field of machine learning in\npsychotherapy by demonstrating the potential of automated methods to improve\nboth the practice and training of therapists. The study highlights the value of\ntopic modeling as a tool for gaining a deeper understanding of therapeutic\ndialogue and offers new opportunities for improving therapeutic effectiveness\nand clinical supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of Large language models to analyze therapist\nremarks in a psychotherapeutic setting. The paper focuses on the application of\nBERTopic, a machine learning-based topic modeling tool, to the dialogue of two\ndifferent groups of therapists (classical and modern), which makes it possible\nto identify and describe a set of topics that consistently emerge across these\ngroups. The paper describes in detail the chosen algorithm for BERTopic, which\nincluded creating a vector space from a corpus of therapist remarks, reducing\nits dimensionality, clustering the space, and creating and optimizing topic\nrepresentation. Along with the automatic topical modeling by the BERTopic, the\nresearch involved an expert assessment of the findings and manual topic\nstructure optimization. The topic modeling results highlighted the most common\nand stable topics in therapists speech, offering insights into how language\npatterns in therapy develop and remain stable across different therapeutic\nstyles. This work contributes to the growing field of machine learning in\npsychotherapy by demonstrating the potential of automated methods to improve\nboth the practice and training of therapists. The study highlights the value of\ntopic modeling as a tool for gaining a deeper understanding of therapeutic\ndialogue and offers new opportunities for improving therapeutic effectiveness\nand clinical supervision."
                },
                "authors": [
                    {
                        "name": "Alexander Vanin"
                    },
                    {
                        "name": "Vadim Bolshev"
                    },
                    {
                        "name": "Anastasia Panfilova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Panfilova"
                },
                "author": "Anastasia Panfilova",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7, J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17429v1",
                "updated": "2024-12-23T09:47:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    47,
                    20,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:47:20Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    47,
                    20,
                    0,
                    358,
                    0
                ],
                "title": "Condor: A Code Discriminator Integrating General Semantics with Code\n  Details",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Condor: A Code Discriminator Integrating General Semantics with Code\n  Details"
                },
                "summary": "LLMs demonstrate significant potential across various software engineering\ntasks. However, they still face challenges in generating correct code on the\nfirst attempt when addressing complex requirements. Introducing a discriminator\nto select reliable outputs from multiple generated results is an effective way\nto enhance their reliability and stability. Currently, these discriminators\nfall into two categories: execution-based discriminators and\nnon-execution-based discriminators. Execution-based discriminators face\nflexibility challenges due to difficulties in obtaining test cases and security\nconcerns, while non-execution-based discriminators, although more flexible,\nstruggle to capture subtle differences in code details. To maintain flexibility\nwhile improving the model's ability to capture fine-grained code details, this\npaper proposes Condor. We first design contrastive learning to optimize the\ncode representations of the base model, enabling it to reflect differences in\ncode details. Then, we leverage intermediate data from the code modification\nprocess to further enrich the discriminator's training data, enhancing its\nability to discern code details. Experimental results indicate that on the\nsubtle code difference dataset (i.e., CodeNanoFix), Condor significantly\noutperforms other discriminators in discriminative performance: Condor (1.3B)\nimproves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%.\nIn discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise\nthe Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset\nfrom 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates\nstrong generalization capabilities on the MBPP and APPS datasets. For example,\nCondor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS\ndataset by 147.05%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs demonstrate significant potential across various software engineering\ntasks. However, they still face challenges in generating correct code on the\nfirst attempt when addressing complex requirements. Introducing a discriminator\nto select reliable outputs from multiple generated results is an effective way\nto enhance their reliability and stability. Currently, these discriminators\nfall into two categories: execution-based discriminators and\nnon-execution-based discriminators. Execution-based discriminators face\nflexibility challenges due to difficulties in obtaining test cases and security\nconcerns, while non-execution-based discriminators, although more flexible,\nstruggle to capture subtle differences in code details. To maintain flexibility\nwhile improving the model's ability to capture fine-grained code details, this\npaper proposes Condor. We first design contrastive learning to optimize the\ncode representations of the base model, enabling it to reflect differences in\ncode details. Then, we leverage intermediate data from the code modification\nprocess to further enrich the discriminator's training data, enhancing its\nability to discern code details. Experimental results indicate that on the\nsubtle code difference dataset (i.e., CodeNanoFix), Condor significantly\noutperforms other discriminators in discriminative performance: Condor (1.3B)\nimproves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%.\nIn discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise\nthe Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset\nfrom 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates\nstrong generalization capabilities on the MBPP and APPS datasets. For example,\nCondor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS\ndataset by 147.05%."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Zixiao Zhao"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17427v1",
                "updated": "2024-12-23T09:45:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    45,
                    3,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    45,
                    3,
                    0,
                    358,
                    0
                ],
                "title": "Measuring Contextual Informativeness in Child-Directed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Contextual Informativeness in Child-Directed Text"
                },
                "summary": "To address an important gap in creating children's stories for vocabulary\nenrichment, we investigate the automatic evaluation of how well stories convey\nthe semantics of target vocabulary words, a task with substantial implications\nfor generating educational content. We motivate this task, which we call\nmeasuring contextual informativeness in children's stories, and provide a\nformal task definition as well as a dataset for the task. We further propose a\nmethod for automating the task using a large language model (LLM). Our\nexperiments show that our approach reaches a Spearman correlation of 0.4983\nwith human judgments of informativeness, while the strongest baseline only\nobtains a correlation of 0.3534. An additional analysis shows that the\nLLM-based approach is able to generalize to measuring contextual\ninformativeness in adult-directed text, on which it also outperforms all\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address an important gap in creating children's stories for vocabulary\nenrichment, we investigate the automatic evaluation of how well stories convey\nthe semantics of target vocabulary words, a task with substantial implications\nfor generating educational content. We motivate this task, which we call\nmeasuring contextual informativeness in children's stories, and provide a\nformal task definition as well as a dataset for the task. We further propose a\nmethod for automating the task using a large language model (LLM). Our\nexperiments show that our approach reaches a Spearman correlation of 0.4983\nwith human judgments of informativeness, while the strongest baseline only\nobtains a correlation of 0.3534. An additional analysis shows that the\nLLM-based approach is able to generalize to measuring contextual\ninformativeness in adult-directed text, on which it also outperforms all\nbaselines."
                },
                "authors": [
                    {
                        "name": "Maria Valentini"
                    },
                    {
                        "name": "Ta Wright"
                    },
                    {
                        "name": "Ali Marashian"
                    },
                    {
                        "name": "Jennifer Weber"
                    },
                    {
                        "name": "Eliana Colunga"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "COLING 2025 main conference short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17415v1",
                "updated": "2024-12-23T09:26:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "VidCtx: Context-aware Video Question Answering with Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCtx: Context-aware Video Question Answering with Image Models"
                },
                "summary": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR."
                },
                "authors": [
                    {
                        "name": "Andreas Goulas"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17408v1",
                "updated": "2024-12-23T09:17:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    17,
                    6,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    17,
                    6,
                    0,
                    358,
                    0
                ],
                "title": "Just What You Desire: Constrained Timeline Summarization with\n  Self-Reflection for Enhanced Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just What You Desire: Constrained Timeline Summarization with\n  Self-Reflection for Enhanced Relevance"
                },
                "summary": "Given news articles about an entity, such as a public figure or organization,\ntimeline summarization (TLS) involves generating a timeline that summarizes the\nkey events about the entity. However, the TLS task is too underspecified, since\nwhat is of interest to each reader may vary, and hence there is not a single\nideal or optimal timeline. In this paper, we introduce a novel task, called\nConstrained Timeline Summarization (CTLS), where a timeline is generated in\nwhich all events in the timeline meet some constraint. An example of a\nconstrained timeline concerns the legal battles of Tiger Woods, where only\nevents related to his legal problems are selected to appear in the timeline. We\ncollected a new human-verified dataset of constrained timelines involving 47\nentities and 5 constraints per entity. We propose an approach that employs a\nlarge language model (LLM) to summarize news articles according to a specified\nconstraint and cluster them to identify key events to include in a constrained\ntimeline. In addition, we propose a novel self-reflection method during summary\ngeneration, demonstrating that this approach successfully leads to improved\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given news articles about an entity, such as a public figure or organization,\ntimeline summarization (TLS) involves generating a timeline that summarizes the\nkey events about the entity. However, the TLS task is too underspecified, since\nwhat is of interest to each reader may vary, and hence there is not a single\nideal or optimal timeline. In this paper, we introduce a novel task, called\nConstrained Timeline Summarization (CTLS), where a timeline is generated in\nwhich all events in the timeline meet some constraint. An example of a\nconstrained timeline concerns the legal battles of Tiger Woods, where only\nevents related to his legal problems are selected to appear in the timeline. We\ncollected a new human-verified dataset of constrained timelines involving 47\nentities and 5 constraints per entity. We propose an approach that employs a\nlarge language model (LLM) to summarize news articles according to a specified\nconstraint and cluster them to identify key events to include in a constrained\ntimeline. In addition, we propose a novel self-reflection method during summary\ngeneration, demonstrating that this approach successfully leads to improved\nperformance."
                },
                "authors": [
                    {
                        "name": "Muhammad Reza Qorib"
                    },
                    {
                        "name": "Qisheng Hu"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "arxiv_comment": "AAAI 2025 (with appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19835v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19835v3",
                "updated": "2024-12-23T09:16:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "published": "2024-03-28T21:08:14Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    21,
                    8,
                    14,
                    3,
                    88,
                    0
                ],
                "title": "Constrained least squares simplicial-simplicial regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained least squares simplicial-simplicial regression"
                },
                "summary": "Simplicial-simplicial regression refers to the regression setting where both\nthe responses and predictor variables lie within the simplex space, i.e. they\nare compositional. For this setting, constrained least squares, where the\nregression coefficients themselves lie within the simplex, is proposed. The\nmodel is transformation-free but the adoption of a power transformation is\nstraightforward, it can treat more than one compositional datasets as\npredictors and offers the possibility of weights among the simplicial\npredictors. Among the model's advantages are its ability to treat zeros in a\nnatural way and a highly computationally efficient algorithm to estimate its\ncoefficients. Resampling based hypothesis testing procedures are employed\nregarding inference, such as linear independence, and equality of the\nregression coefficients to some pre-specified values. The strategy behind the\nformulation of the new model is implemented is related to an existing\nmethodology, that is of the same spirit, showcasing how other similar models\ncan be employed as well. Finally, the performance of the proposed technique and\nits comparison to the existing methodology takes place using simulation studies\nand real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplicial-simplicial regression refers to the regression setting where both\nthe responses and predictor variables lie within the simplex space, i.e. they\nare compositional. For this setting, constrained least squares, where the\nregression coefficients themselves lie within the simplex, is proposed. The\nmodel is transformation-free but the adoption of a power transformation is\nstraightforward, it can treat more than one compositional datasets as\npredictors and offers the possibility of weights among the simplicial\npredictors. Among the model's advantages are its ability to treat zeros in a\nnatural way and a highly computationally efficient algorithm to estimate its\ncoefficients. Resampling based hypothesis testing procedures are employed\nregarding inference, such as linear independence, and equality of the\nregression coefficients to some pre-specified values. The strategy behind the\nformulation of the new model is implemented is related to an existing\nmethodology, that is of the same spirit, showcasing how other similar models\ncan be employed as well. Finally, the performance of the proposed technique and\nits comparison to the existing methodology takes place using simulation studies\nand real data examples."
                },
                "authors": [
                    {
                        "name": "Michail Tsagris"
                    }
                ],
                "author_detail": {
                    "name": "Michail Tsagris"
                },
                "author": "Michail Tsagris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19835v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19835v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03001v2",
                "updated": "2024-12-23T09:03:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    3,
                    2,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-06T07:19:51Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    19,
                    51,
                    1,
                    219,
                    0
                ],
                "title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning"
                },
                "summary": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yu Song"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jihong Hu"
                    },
                    {
                        "name": "Yen-Wei Chen"
                    },
                    {
                        "name": "Lanfen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lanfen Lin"
                },
                "author": "Lanfen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09498v2",
                "updated": "2024-12-23T08:59:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    59,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2024-03-14T15:40:13Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    15,
                    40,
                    13,
                    3,
                    74,
                    0
                ],
                "title": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News"
                },
                "summary": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_doi": "10.24963/ijcai.2024/873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.09498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IJCAI 2024 Oral",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18873v2",
                "updated": "2024-12-23T08:58:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    58,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-27T03:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    3,
                    57,
                    12,
                    3,
                    179,
                    0
                ],
                "title": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design"
                },
                "summary": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs."
                },
                "authors": [
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Xiaohan Gao"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Xiyuan Tang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "arxiv_comment": "8pages, 8figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17397v1",
                "updated": "2024-12-23T08:51:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    51,
                    48,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:51:48Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    51,
                    48,
                    0,
                    358,
                    0
                ],
                "title": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning"
                },
                "summary": "With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%)."
                },
                "authors": [
                    {
                        "name": "Huchen Jiang"
                    },
                    {
                        "name": "Yangyang Ma"
                    },
                    {
                        "name": "Chaofan Ding"
                    },
                    {
                        "name": "Kexin Luan"
                    },
                    {
                        "name": "Xinhan Di"
                    }
                ],
                "author_detail": {
                    "name": "Xinhan Di"
                },
                "author": "Xinhan Di",
                "arxiv_comment": "6 Pages,3 figures, accepted by AAAI 2025 Workshop NeurMAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17395v1",
                "updated": "2024-12-23T08:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models"
                },
                "summary": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to gather complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\nthe limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nlimits the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder which learns from expert battles\nto address these limitations. Specifically, we create an arena for current\nexpert code LLMs, where each model challenges and responds to others'\nchallenges, with evaluations conducted by uninvolved judge models. This\ncompetitive framework generates novel training data constructed from scratch,\nharnessing the strengths of all participants. Experimental results demonstrate\nthat WarriorCoder achieves competitive performance compared to previous\nmethods, even without relying on proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to gather complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\nthe limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nlimits the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder which learns from expert battles\nto address these limitations. Specifically, we create an arena for current\nexpert code LLMs, where each model challenges and responds to others'\nchallenges, with evaluations conducted by uninvolved judge models. This\ncompetitive framework generates novel training data constructed from scratch,\nharnessing the strengths of all participants. Experimental results demonstrate\nthat WarriorCoder achieves competitive performance compared to previous\nmethods, even without relying on proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17383v1",
                "updated": "2024-12-23T08:33:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    33,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:33:47Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    33,
                    47,
                    0,
                    358,
                    0
                ],
                "title": "Interweaving Memories of a Siamese Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interweaving Memories of a Siamese Large Language Model"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models\n(LLMs) by modifying or introducing a small number of parameters to enhance\nalignment with downstream tasks. However, they can result in catastrophic\nforgetting, where LLMs prioritize new knowledge at the expense of comprehensive\nworld knowledge. A promising approach to mitigate this issue is to recall prior\nmemories based on the original knowledge. To this end, we propose a\nmodel-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese\nLarge Language Model. Specifically, our siamese LLM is equipped with an\nexisting PEFT method. Given an incoming query, it generates two distinct\nmemories based on the pre-trained and fine-tuned parameters. IMSM then\nincorporates an interweaving mechanism that regulates the contributions of both\noriginal and enhanced memories when generating the next token. This framework\nis theoretically applicable to all open-source LLMs and existing PEFT methods.\nWe conduct extensive experiments across various benchmark datasets, evaluating\nthe performance of popular open-source LLMs using the proposed IMSM, in\ncomparison to both classical and leading PEFT methods. Our findings indicate\nthat IMSM maintains comparable time and space efficiency to backbone PEFT\nmethods while significantly improving performance and effectively mitigating\ncatastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models\n(LLMs) by modifying or introducing a small number of parameters to enhance\nalignment with downstream tasks. However, they can result in catastrophic\nforgetting, where LLMs prioritize new knowledge at the expense of comprehensive\nworld knowledge. A promising approach to mitigate this issue is to recall prior\nmemories based on the original knowledge. To this end, we propose a\nmodel-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese\nLarge Language Model. Specifically, our siamese LLM is equipped with an\nexisting PEFT method. Given an incoming query, it generates two distinct\nmemories based on the pre-trained and fine-tuned parameters. IMSM then\nincorporates an interweaving mechanism that regulates the contributions of both\noriginal and enhanced memories when generating the next token. This framework\nis theoretically applicable to all open-source LLMs and existing PEFT methods.\nWe conduct extensive experiments across various benchmark datasets, evaluating\nthe performance of popular open-source LLMs using the proposed IMSM, in\ncomparison to both classical and leading PEFT methods. Our findings indicate\nthat IMSM maintains comparable time and space efficiency to backbone PEFT\nmethods while significantly improving performance and effectively mitigating\ncatastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Zhikai Xue"
                    },
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Wei Lu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lu"
                },
                "author": "Wei Lu",
                "arxiv_comment": "Accepted by AAAI 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01446v2",
                "updated": "2024-12-23T08:29:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    29,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2023-10-01T12:28:36Z",
                "published_parsed": [
                    2023,
                    10,
                    1,
                    12,
                    28,
                    36,
                    6,
                    274,
                    0
                ],
                "title": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large\n  Language Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive ability in handling\nreasoning tasks. However, unlike humans who can instinctively adapt their\nproblem-solving strategies to the complexity of task, most LLM-based methods\nadopt a one-size-fits-all approach. These methods employ consistent models,\nsample sizes, prompting methods and levels of problem decomposition, regardless\nof the problem complexity. The inflexibility of these methods can bring\nunnecessary computational overhead or sub-optimal performance. To address this\nlimitation, we introduce an Adaptive-Solver (AS) framework tha dynamically\nadapts solving strategies to suit various problems, enabling the flexible\nallocation of test-time computational resources. The framework functions with\ntwo primary modules. The initial evaluation module assesses the reliability of\nthe current solution using answer consistency. If the solution is deemed\nunreliable, the subsequent adaptation module comes into play. Within this\nmodule, various types of adaptation strategies are employed collaboratively.\nThrough such dynamic and multi-faceted adaptations, our framework can help\nreduce computational consumption and improve performance. Experimental results\nfrom complex reasoning benchmarks reveal that our method can significantly\nreduce API costs (up to 85%) while maintaining original performance.\nAlternatively, it achieves up to 4.5% higher accuracy compared to the baselines\nat the same cost. The code and dataset are available at\nhttps://github.com/john1226966735/Adaptive-Solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive ability in handling\nreasoning tasks. However, unlike humans who can instinctively adapt their\nproblem-solving strategies to the complexity of task, most LLM-based methods\nadopt a one-size-fits-all approach. These methods employ consistent models,\nsample sizes, prompting methods and levels of problem decomposition, regardless\nof the problem complexity. The inflexibility of these methods can bring\nunnecessary computational overhead or sub-optimal performance. To address this\nlimitation, we introduce an Adaptive-Solver (AS) framework tha dynamically\nadapts solving strategies to suit various problems, enabling the flexible\nallocation of test-time computational resources. The framework functions with\ntwo primary modules. The initial evaluation module assesses the reliability of\nthe current solution using answer consistency. If the solution is deemed\nunreliable, the subsequent adaptation module comes into play. Within this\nmodule, various types of adaptation strategies are employed collaboratively.\nThrough such dynamic and multi-faceted adaptations, our framework can help\nreduce computational consumption and improve performance. Experimental results\nfrom complex reasoning benchmarks reveal that our method can significantly\nreduce API costs (up to 85%) while maintaining original performance.\nAlternatively, it achieves up to 4.5% higher accuracy compared to the baselines\nat the same cost. The code and dataset are available at\nhttps://github.com/john1226966735/Adaptive-Solver."
                },
                "authors": [
                    {
                        "name": "Jianpeng Zhou"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "arxiv_comment": "Accepted by Information Processing & Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00459v2",
                "updated": "2024-12-23T08:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    25,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-01T09:14:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    14,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques"
                },
                "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dekai Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15947v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15947v5",
                "updated": "2024-12-23T08:05:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    5,
                    14,
                    0,
                    358,
                    0
                ],
                "published": "2024-01-29T08:13:40Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    8,
                    13,
                    40,
                    0,
                    29,
                    0
                ],
                "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"
                },
                "summary": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA."
                },
                "authors": [
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Zhenyu Tang"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Junwu Zhang"
                    },
                    {
                        "name": "Yatian Pang"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Munan Ning"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "update author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15947v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15947v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17365v1",
                "updated": "2024-12-23T08:01:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    1,
                    24,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:01:24Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    1,
                    24,
                    0,
                    358,
                    0
                ],
                "title": "Boosting LLM via Learning from Data Iteratively and Selectively",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting LLM via Learning from Data Iteratively and Selectively"
                },
                "summary": "Datasets nowadays are generally constructed from multiple sources and using\ndifferent synthetic techniques, making data de-noising and de-duplication\ncrucial before being used for post-training. In this work, we propose to\nperform instruction tuning by iterative data selection (\\ApproachName{}). We\nmeasure the quality of a sample from complexity and diversity simultaneously.\nInstead of calculating the complexity score once for all before fine-tuning, we\nhighlight the importance of updating this model-specific score during\nfine-tuning to accurately accommodate the dynamic changes of the model. On the\nother hand, the diversity score is defined on top of the samples' responses\nunder the consideration of their informativeness. IterIT integrates the\nstrengths of both worlds by iteratively updating the complexity score for the\ntop-ranked samples and greedily selecting the ones with the highest\ncomplexity-diversity score. Experiments on multiple instruction-tuning data\ndemonstrate consistent improvements of IterIT over strong baselines. Moreover,\nour approach also generalizes well to domain-specific scenarios and different\nbackbone models. All resources will be available at\nhttps://github.com/JiaQiSJTU/IterIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datasets nowadays are generally constructed from multiple sources and using\ndifferent synthetic techniques, making data de-noising and de-duplication\ncrucial before being used for post-training. In this work, we propose to\nperform instruction tuning by iterative data selection (\\ApproachName{}). We\nmeasure the quality of a sample from complexity and diversity simultaneously.\nInstead of calculating the complexity score once for all before fine-tuning, we\nhighlight the importance of updating this model-specific score during\nfine-tuning to accurately accommodate the dynamic changes of the model. On the\nother hand, the diversity score is defined on top of the samples' responses\nunder the consideration of their informativeness. IterIT integrates the\nstrengths of both worlds by iteratively updating the complexity score for the\ntop-ranked samples and greedily selecting the ones with the highest\ncomplexity-diversity score. Experiments on multiple instruction-tuning data\ndemonstrate consistent improvements of IterIT over strong baselines. Moreover,\nour approach also generalizes well to domain-specific scenarios and different\nbackbone models. All resources will be available at\nhttps://github.com/JiaQiSJTU/IterIT."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Siyu Ren"
                    },
                    {
                        "name": "Ziheng Qin"
                    },
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17005v2",
                "updated": "2024-12-23T07:44:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    44,
                    13,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-30T04:37:52Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    37,
                    52,
                    4,
                    243,
                    0
                ],
                "title": "Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Camera Exposure Control for Visual Odometry via Deep\n  Reinforcement Learning"
                },
                "summary": "The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stability of visual odometry (VO) systems is undermined by degraded image\nquality, especially in environments with significant illumination changes. This\nstudy employs a deep reinforcement learning (DRL) framework to train agents for\nexposure control, aiming to enhance imaging performance in challenging\nconditions. A lightweight image simulator is developed to facilitate the\ntraining process, enabling the diversification of image exposure and sequence\ntrajectory. This setup enables completely offline training, eliminating the\nneed for direct interaction with camera hardware and the real environments.\nDifferent levels of reward functions are crafted to enhance the VO systems,\nequipping the DRL agents with varying intelligence. Extensive experiments have\nshown that our exposure control agents achieve superior efficiency-with an\naverage inference duration of 1.58 ms per frame on a CPU-and respond more\nquickly than traditional feedback control schemes. By choosing an appropriate\nreward function, agents acquire an intelligent understanding of motion trends\nand anticipate future illumination changes. This predictive capability allows\nVO systems to deliver more stable and precise odometry results. The codes and\ndatasets are available at https://github.com/ShuyangUni/drl_exposure_ctrl."
                },
                "authors": [
                    {
                        "name": "Shuyang Zhang"
                    },
                    {
                        "name": "Jinhao He"
                    },
                    {
                        "name": "Yilong Zhu"
                    },
                    {
                        "name": "Jin Wu"
                    },
                    {
                        "name": "Jie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yuan"
                },
                "author": "Jie Yuan",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.13486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.13486v2",
                "updated": "2024-12-23T07:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    29,
                    49,
                    0,
                    358,
                    0
                ],
                "published": "2022-08-29T10:40:58Z",
                "published_parsed": [
                    2022,
                    8,
                    29,
                    10,
                    40,
                    58,
                    0,
                    241,
                    0
                ],
                "title": "naab: A ready-to-use plug-and-play corpus for Farsi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "naab: A ready-to-use plug-and-play corpus for Farsi"
                },
                "summary": "The rise of large language models (LLMs) has transformed numerous natural\nlanguage processing (NLP) tasks, yet their performance in low and mid-resource\nlanguages, such as Farsi, still lags behind resource-rich languages like\nEnglish. To address this gap, we introduce naab, the largest publicly\navailable, cleaned, and ready-to-use Farsi textual corpus. naab consists of\n130GB of data, comprising over 250 million paragraphs and 15 billion words.\nNamed after the Farsi word NAAB (meaning \"pure\" or \"high-grade\"), this corpus\nis openly accessible via Hugging Face, offering researchers a valuable resource\nfor Farsi NLP tasks. In addition to naab, we provide naab-raw, an unprocessed\nversion of the dataset, along with a pre-processing toolkit that allows users\nto clean their custom corpora. These resources empower NLP researchers and\npractitioners, particularly those focusing on low-resource languages, to\nimprove the performance of LLMs in their respective domains and bridge the gap\nbetween resource-rich and resource-poor languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has transformed numerous natural\nlanguage processing (NLP) tasks, yet their performance in low and mid-resource\nlanguages, such as Farsi, still lags behind resource-rich languages like\nEnglish. To address this gap, we introduce naab, the largest publicly\navailable, cleaned, and ready-to-use Farsi textual corpus. naab consists of\n130GB of data, comprising over 250 million paragraphs and 15 billion words.\nNamed after the Farsi word NAAB (meaning \"pure\" or \"high-grade\"), this corpus\nis openly accessible via Hugging Face, offering researchers a valuable resource\nfor Farsi NLP tasks. In addition to naab, we provide naab-raw, an unprocessed\nversion of the dataset, along with a pre-processing toolkit that allows users\nto clean their custom corpora. These resources empower NLP researchers and\npractitioners, particularly those focusing on low-resource languages, to\nimprove the performance of LLMs in their respective domains and bridge the gap\nbetween resource-rich and resource-poor languages."
                },
                "authors": [
                    {
                        "name": "Sadra Sabouri"
                    },
                    {
                        "name": "Elnaz Rahmati"
                    },
                    {
                        "name": "Soroush Gooran"
                    },
                    {
                        "name": "Hossein Sameti"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Sameti"
                },
                "author": "Hossein Sameti",
                "arxiv_doi": "10.22034/jaiai.2024.480062.1016",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22034/jaiai.2024.480062.1016",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2208.13486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.13486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Artificial Intelligence, Applications and Innovations\n  (2024) 1-8",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17354v1",
                "updated": "2024-12-23T07:29:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    29,
                    14,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T07:29:14Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    29,
                    14,
                    0,
                    358,
                    0
                ],
                "title": "Bayesian penalized empirical likelihood and MCMC sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian penalized empirical likelihood and MCMC sampling"
                },
                "summary": "In this study, we introduce a novel methodological framework called Bayesian\nPenalized Empirical Likelihood (BPEL), designed to address the computational\nchallenges inherent in empirical likelihood (EL) approaches. Our approach has\ntwo primary objectives: (i) to enhance the inherent flexibility of EL in\naccommodating diverse model conditions, and (ii) to facilitate the use of\nwell-established Markov Chain Monte Carlo (MCMC) sampling schemes as a\nconvenient alternative to the complex optimization typically required for\nstatistical inference using EL. To achieve the first objective, we propose a\npenalized approach that regularizes the Lagrange multipliers, significantly\nreducing the dimensionality of the problem while accommodating a comprehensive\nset of model conditions. For the second objective, our study designs and\nthoroughly investigates two popular sampling schemes within the BPEL context.\nWe demonstrate that the BPEL framework is highly flexible and efficient,\nenhancing the adaptability and practicality of EL methods. Our study highlights\nthe practical advantages of using sampling techniques over traditional\noptimization methods for EL problems, showing rapid convergence to the global\noptima of posterior distributions and ensuring the effective resolution of\ncomplex statistical inference challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce a novel methodological framework called Bayesian\nPenalized Empirical Likelihood (BPEL), designed to address the computational\nchallenges inherent in empirical likelihood (EL) approaches. Our approach has\ntwo primary objectives: (i) to enhance the inherent flexibility of EL in\naccommodating diverse model conditions, and (ii) to facilitate the use of\nwell-established Markov Chain Monte Carlo (MCMC) sampling schemes as a\nconvenient alternative to the complex optimization typically required for\nstatistical inference using EL. To achieve the first objective, we propose a\npenalized approach that regularizes the Lagrange multipliers, significantly\nreducing the dimensionality of the problem while accommodating a comprehensive\nset of model conditions. For the second objective, our study designs and\nthoroughly investigates two popular sampling schemes within the BPEL context.\nWe demonstrate that the BPEL framework is highly flexible and efficient,\nenhancing the adaptability and practicality of EL methods. Our study highlights\nthe practical advantages of using sampling techniques over traditional\noptimization methods for EL problems, showing rapid convergence to the global\noptima of posterior distributions and ensuring the effective resolution of\ncomplex statistical inference challenges."
                },
                "authors": [
                    {
                        "name": "Jinyuan Chang"
                    },
                    {
                        "name": "Cheng Yong Tang"
                    },
                    {
                        "name": "Yuanzheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanzheng Zhu"
                },
                "author": "Yuanzheng Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17348v1",
                "updated": "2024-12-23T07:21:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    21,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T07:21:17Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    21,
                    17,
                    0,
                    358,
                    0
                ],
                "title": "ORIGAMI: A generative transformer architecture for predictions from\n  semi-structured data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORIGAMI: A generative transformer architecture for predictions from\n  semi-structured data"
                },
                "summary": "Despite the popularity and widespread use of semi-structured data formats\nsuch as JSON, end-to-end supervised learning applied directly to such data\nremains underexplored. We present ORIGAMI (Object RepresentatIon via Generative\nAutoregressive ModellIng), a transformer-based architecture that directly\nprocesses nested key/value pairs while preserving their hierarchical semantics.\nOur key technical contributions include: (1) a structure-preserving tokenizer,\n(2) a novel key/value position encoding scheme, and (3) a grammar-constrained\ntraining and inference framework that ensures valid outputs and accelerates\ntraining convergence. These enhancements enable efficient end-to-end modeling\nof semi-structured data. By reformulating classification as next-token\nprediction, ORIGAMI naturally handles both single-label and multi-label tasks\nwithout architectural modifications. Empirical evaluation across diverse\ndomains demonstrates ORIGAMI's effectiveness: On standard tabular benchmarks\nconverted to JSON, ORIGAMI remains competitive with classical and\nstate-of-the-art approaches. On native JSON datasets, we outperform baselines\non multi-label classification and specialized models such as convolutional and\ngraph neural networks on a code classification task. Through extensive ablation\nstudies, we validate the impact of each architectural component and establish\nORIGAMI as a robust framework for end-to-end learning on semi-structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the popularity and widespread use of semi-structured data formats\nsuch as JSON, end-to-end supervised learning applied directly to such data\nremains underexplored. We present ORIGAMI (Object RepresentatIon via Generative\nAutoregressive ModellIng), a transformer-based architecture that directly\nprocesses nested key/value pairs while preserving their hierarchical semantics.\nOur key technical contributions include: (1) a structure-preserving tokenizer,\n(2) a novel key/value position encoding scheme, and (3) a grammar-constrained\ntraining and inference framework that ensures valid outputs and accelerates\ntraining convergence. These enhancements enable efficient end-to-end modeling\nof semi-structured data. By reformulating classification as next-token\nprediction, ORIGAMI naturally handles both single-label and multi-label tasks\nwithout architectural modifications. Empirical evaluation across diverse\ndomains demonstrates ORIGAMI's effectiveness: On standard tabular benchmarks\nconverted to JSON, ORIGAMI remains competitive with classical and\nstate-of-the-art approaches. On native JSON datasets, we outperform baselines\non multi-label classification and specialized models such as convolutional and\ngraph neural networks on a code classification task. Through extensive ablation\nstudies, we validate the impact of each architectural component and establish\nORIGAMI as a robust framework for end-to-end learning on semi-structured data."
                },
                "authors": [
                    {
                        "name": "Thomas Rckstie"
                    },
                    {
                        "name": "Alana Huang"
                    },
                    {
                        "name": "Robin Vujanic"
                    }
                ],
                "author_detail": {
                    "name": "Robin Vujanic"
                },
                "author": "Robin Vujanic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07790v2",
                "updated": "2024-12-23T07:20:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    20,
                    3,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-12T06:50:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17341v1",
                "updated": "2024-12-23T07:12:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    12,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T07:12:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    12,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "HDTSA: An R package for high-dimensional time series analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDTSA: An R package for high-dimensional time series analysis"
                },
                "summary": "High-dimensional time series analysis has become increasingly important in\nfields such as finance, economics, and biology. The two primary tasks for\nhigh-dimensional time series analysis are modeling and statistical inference,\nwhich aim to capture the underlying dynamic structure and investigate valuable\ninformation in the data. This paper presents the HDTSA package for R, which\nprovides a general framework for analyzing high-dimensional time series data.\nThis package includes four dimension reduction methods for modeling: factor\nmodels, principal component analysis, CP-decomposition, and cointegration\nanalysis. It also implements two recently proposed white noise test and\nmartingale difference test in high-dimensional scenario for statistical\ninference. The methods provided in this package can help users to analyze\nhigh-dimensional time series data and make reliable predictions. To improve\ncomputational efficiency, the HDTSA package integrates C++ through the Rcpp\npackage. We illustrate the functions of the HDTSA package using simulated\nexamples and real-world applications from finance and economics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional time series analysis has become increasingly important in\nfields such as finance, economics, and biology. The two primary tasks for\nhigh-dimensional time series analysis are modeling and statistical inference,\nwhich aim to capture the underlying dynamic structure and investigate valuable\ninformation in the data. This paper presents the HDTSA package for R, which\nprovides a general framework for analyzing high-dimensional time series data.\nThis package includes four dimension reduction methods for modeling: factor\nmodels, principal component analysis, CP-decomposition, and cointegration\nanalysis. It also implements two recently proposed white noise test and\nmartingale difference test in high-dimensional scenario for statistical\ninference. The methods provided in this package can help users to analyze\nhigh-dimensional time series data and make reliable predictions. To improve\ncomputational efficiency, the HDTSA package integrates C++ through the Rcpp\npackage. We illustrate the functions of the HDTSA package using simulated\nexamples and real-world applications from finance and economics."
                },
                "authors": [
                    {
                        "name": "Jinyuan Chang"
                    },
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Qiwei Yao"
                    }
                ],
                "author_detail": {
                    "name": "Qiwei Yao"
                },
                "author": "Qiwei Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17335v1",
                "updated": "2024-12-23T07:01:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    1,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T07:01:29Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    1,
                    29,
                    0,
                    358,
                    0
                ],
                "title": "Hierarchical Dirichlet Process Mixture of Products of Multinomial\n  Distributions: Applications to Survey Data with Potentially Missing Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Dirichlet Process Mixture of Products of Multinomial\n  Distributions: Applications to Survey Data with Potentially Missing Values"
                },
                "summary": "In social science research, understanding latent structures in populations\nthrough survey data with categorical responses is a common and important task.\nTraditional methods like Factor Analysis and Latent Class Analysis have\nlimitations, particularly in handling categorical data and accommodating mixed\nmemberships in latent structures, respectively. Moreover, analyzing survey\nresponses with missing values using these methods is quite challenging. This\nstudy introduces a Hierarchical Dirichlet Process Mixture of Products of\nMultinomial Distributions (HDPMPM) model, which leverages the flexibility of\nnonparametric Bayesian methods to address these limitations. The HDPMPM model\nallows for multiple latent classes within individuals and supports a\npotentially infinite number of mixture components. Additionally, it\nincorporates missing data imputation directly into the model's Gibbs sampling\nprocess. By applying a truncated stick-breaking representation of the Dirichlet\nprocess, we can derive a Gibbs sampling scheme for posterior inference. An\napplication of the HDPMPM model to the 2016 American National Election Study\n(ANES) data demonstrates its effectiveness in identifying political profiles\nand handling missing data scenarios, including those that are missing at random\n(MAR) and missing completely at random (MCAR). The results show that the HDPMPM\nmodel successfully recovers dominant profiles and manages complex latent\nstructures in survey data, providing an alternative tool for social science\nresearchers in dealing with categorical data with missing values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In social science research, understanding latent structures in populations\nthrough survey data with categorical responses is a common and important task.\nTraditional methods like Factor Analysis and Latent Class Analysis have\nlimitations, particularly in handling categorical data and accommodating mixed\nmemberships in latent structures, respectively. Moreover, analyzing survey\nresponses with missing values using these methods is quite challenging. This\nstudy introduces a Hierarchical Dirichlet Process Mixture of Products of\nMultinomial Distributions (HDPMPM) model, which leverages the flexibility of\nnonparametric Bayesian methods to address these limitations. The HDPMPM model\nallows for multiple latent classes within individuals and supports a\npotentially infinite number of mixture components. Additionally, it\nincorporates missing data imputation directly into the model's Gibbs sampling\nprocess. By applying a truncated stick-breaking representation of the Dirichlet\nprocess, we can derive a Gibbs sampling scheme for posterior inference. An\napplication of the HDPMPM model to the 2016 American National Election Study\n(ANES) data demonstrates its effectiveness in identifying political profiles\nand handling missing data scenarios, including those that are missing at random\n(MAR) and missing completely at random (MCAR). The results show that the HDPMPM\nmodel successfully recovers dominant profiles and manages complex latent\nstructures in survey data, providing an alternative tool for social science\nresearchers in dealing with categorical data with missing values."
                },
                "authors": [
                    {
                        "name": "Chayut Wongkamthong"
                    }
                ],
                "author_detail": {
                    "name": "Chayut Wongkamthong"
                },
                "author": "Chayut Wongkamthong",
                "arxiv_comment": "This manuscript is currently undergoing the journal submission\n  process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17332v1",
                "updated": "2024-12-23T06:50:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    50,
                    4,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T06:50:04Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    50,
                    4,
                    0,
                    358,
                    0
                ],
                "title": "A Dual-Perspective Metaphor Detection Framework Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Perspective Metaphor Detection Framework Using Large Language\n  Models"
                },
                "summary": "Metaphor detection, a critical task in natural language processing, involves\nidentifying whether a particular word in a sentence is used metaphorically.\nTraditional approaches often rely on supervised learning models that implicitly\nencode semantic relationships based on metaphor theories. However, these\nmethods often suffer from a lack of transparency in their decision-making\nprocesses, which undermines the reliability of their predictions. Recent\nresearch indicates that LLMs (large language models) exhibit significant\npotential in metaphor detection. Nevertheless, their reasoning capabilities are\nconstrained by predefined knowledge graphs. To overcome these limitations, we\npropose DMD, a novel dual-perspective framework that harnesses both implicit\nand explicit applications of metaphor theories to guide LLMs in metaphor\ndetection and adopts a self-judgment mechanism to validate the responses from\nthe aforementioned forms of guidance. In comparison to previous methods, our\nframework offers more transparent reasoning processes and delivers more\nreliable predictions. Experimental results prove the effectiveness of DMD,\ndemonstrating state-of-the-art performance across widely-used datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor detection, a critical task in natural language processing, involves\nidentifying whether a particular word in a sentence is used metaphorically.\nTraditional approaches often rely on supervised learning models that implicitly\nencode semantic relationships based on metaphor theories. However, these\nmethods often suffer from a lack of transparency in their decision-making\nprocesses, which undermines the reliability of their predictions. Recent\nresearch indicates that LLMs (large language models) exhibit significant\npotential in metaphor detection. Nevertheless, their reasoning capabilities are\nconstrained by predefined knowledge graphs. To overcome these limitations, we\npropose DMD, a novel dual-perspective framework that harnesses both implicit\nand explicit applications of metaphor theories to guide LLMs in metaphor\ndetection and adopts a self-judgment mechanism to validate the responses from\nthe aforementioned forms of guidance. In comparison to previous methods, our\nframework offers more transparent reasoning processes and delivers more\nreliable predictions. Experimental results prove the effectiveness of DMD,\ndemonstrating state-of-the-art performance across widely-used datasets."
                },
                "authors": [
                    {
                        "name": "Yujie Lin"
                    },
                    {
                        "name": "Jingyao Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04344v3",
                "updated": "2024-12-23T06:46:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    46,
                    59,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-06T19:27:48Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    19,
                    27,
                    48,
                    1,
                    37,
                    0
                ],
                "title": "Does confidence calibration improve conformal prediction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does confidence calibration improve conformal prediction?"
                },
                "summary": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Previous works often employ temperature scaling to\ncalibrate classifiers, assuming that confidence calibration benefits conformal\nprediction. However, the specific impact of confidence calibration on conformal\nprediction remains underexplored. In this work, we make two key discoveries\nabout the impact of confidence calibration methods on adaptive conformal\nprediction. Firstly, we empirically show that current confidence calibration\nmethods (e.g., temperature scaling) typically lead to larger prediction sets in\nadaptive conformal prediction. Secondly, by investigating the role of\ntemperature value, we observe that high-confidence predictions can enhance the\nefficiency of adaptive conformal prediction. Theoretically, we prove that\npredictions with higher confidence result in smaller prediction sets on\nexpectation. This finding implies that the rescaling parameters in these\ncalibration methods, when optimized with cross-entropy loss, might counteract\nthe goal of generating efficient prediction sets. To address this issue, we\npropose Conformal Temperature Scaling (ConfTS), a variant of temperature\nscaling with a novel loss function designed to enhance the efficiency of\nprediction sets. This approach can be extended to optimize the parameters of\nother post-hoc methods of confidence calibration. Extensive experiments\ndemonstrate that our method improves existing adaptive conformal prediction\nmethods in classification tasks, especially with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Previous works often employ temperature scaling to\ncalibrate classifiers, assuming that confidence calibration benefits conformal\nprediction. However, the specific impact of confidence calibration on conformal\nprediction remains underexplored. In this work, we make two key discoveries\nabout the impact of confidence calibration methods on adaptive conformal\nprediction. Firstly, we empirically show that current confidence calibration\nmethods (e.g., temperature scaling) typically lead to larger prediction sets in\nadaptive conformal prediction. Secondly, by investigating the role of\ntemperature value, we observe that high-confidence predictions can enhance the\nefficiency of adaptive conformal prediction. Theoretically, we prove that\npredictions with higher confidence result in smaller prediction sets on\nexpectation. This finding implies that the rescaling parameters in these\ncalibration methods, when optimized with cross-entropy loss, might counteract\nthe goal of generating efficient prediction sets. To address this issue, we\npropose Conformal Temperature Scaling (ConfTS), a variant of temperature\nscaling with a novel loss function designed to enhance the efficiency of\nprediction sets. This approach can be extended to optimize the parameters of\nother post-hoc methods of confidence calibration. Extensive experiments\ndemonstrate that our method improves existing adaptive conformal prediction\nmethods in classification tasks, especially with LLMs."
                },
                "authors": [
                    {
                        "name": "Huajun Xi"
                    },
                    {
                        "name": "Jianguo Huang"
                    },
                    {
                        "name": "Kangdao Liu"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06608v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v4",
                "updated": "2024-12-23T18:38:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    38,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompting Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17767v1",
                "updated": "2024-12-23T18:26:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    26,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:26:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    26,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "ResearchTown: Simulator of Human Research Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchTown: Simulator of Human Research Community"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\nscientific domains, yet a fundamental question remains unanswered: Can we\nsimulate human research communities with LLMs? Addressing this question can\ndeepen our understanding of the processes behind idea brainstorming and inspire\nthe automatic discovery of novel scientific insights. In this work, we propose\nResearchTown, a multi-agent framework for research community simulation. Within\nthis framework, the human research community is simplified and modeled as an\nagent-data graph, where researchers and papers are represented as agent-type\nand data-type nodes, respectively, and connected based on their collaboration\nrelationships. We also introduce TextGNN, a text-based inference framework that\nmodels various research activities (e.g., paper reading, paper writing, and\nreview writing) as special forms of a unified message-passing process on the\nagent-data graph. To evaluate the quality of the research simulation, we\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\nscalable and objective assessment based on similarity. Our experiments reveal\nthree key findings: (1) ResearchTown can provide a realistic simulation of\ncollaborative research activities, including paper writing and review writing;\n(2) ResearchTown can maintain robust simulation with multiple researchers and\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\nthat potentially inspire novel research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable potential in\nscientific domains, yet a fundamental question remains unanswered: Can we\nsimulate human research communities with LLMs? Addressing this question can\ndeepen our understanding of the processes behind idea brainstorming and inspire\nthe automatic discovery of novel scientific insights. In this work, we propose\nResearchTown, a multi-agent framework for research community simulation. Within\nthis framework, the human research community is simplified and modeled as an\nagent-data graph, where researchers and papers are represented as agent-type\nand data-type nodes, respectively, and connected based on their collaboration\nrelationships. We also introduce TextGNN, a text-based inference framework that\nmodels various research activities (e.g., paper reading, paper writing, and\nreview writing) as special forms of a unified message-passing process on the\nagent-data graph. To evaluate the quality of the research simulation, we\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\nscalable and objective assessment based on similarity. Our experiments reveal\nthree key findings: (1) ResearchTown can provide a realistic simulation of\ncollaborative research activities, including paper writing and review writing;\n(2) ResearchTown can maintain robust simulation with multiple researchers and\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\nthat potentially inspire novel research directions."
                },
                "authors": [
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Zhaochen Hong"
                    },
                    {
                        "name": "Zirui Cheng"
                    },
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Keyang Xuan"
                    },
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17758v1",
                "updated": "2024-12-23T18:14:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    14,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:14:36Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    14,
                    36,
                    0,
                    358,
                    0
                ],
                "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging"
                },
                "summary": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\ndue to an evaluation setup that prevents direct comparison of answer choices\nrather than inherent complexity. Although some researchers have quietly shifted\nto a more appropriate scheme over the last year, the implications of this\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\nshow how similar evaluation practices falsely imply reasoning deficits in other\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\nto ensure that multiple-choice evaluations accurately reflect actual model\ncapabilities."
                },
                "authors": [
                    {
                        "name": "ukasz Borchmann"
                    }
                ],
                "author_detail": {
                    "name": "ukasz Borchmann"
                },
                "author": "ukasz Borchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v2",
                "updated": "2024-12-23T18:09:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    9,
                    34,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17754v1",
                "updated": "2024-12-23T18:07:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    7,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:07:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    7,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code\n  Line-Level Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code\n  Line-Level Feedback"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing and coding, yet they struggle with robustness and accuracy\nin complex function calls. To tackle these challenges, this paper introduces\nADC, an innovative approach that enhances LLMs' ability to follow function\nformats and match complex parameters. ADC utilizes a high-quality code\nfine-tuning dataset with line-level execution feedback, providing granular\nprocess supervision that fosters strong logical reasoning and adherence to\nfunction formats. It also employs an adversarial dataset generation process to\nimprove parameter matching. The staged training methodology capitalizes on both\nenriched code datasets and refined adversarial datasets, leading to marked\nimprovements in function calling capabilities on the Berkeley Function-Calling\nLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic\ncombination of process supervision, adversarial refinement, and incremental\nlearning, setting a new standard for LLM proficiency in complex function\ncalling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing and coding, yet they struggle with robustness and accuracy\nin complex function calls. To tackle these challenges, this paper introduces\nADC, an innovative approach that enhances LLMs' ability to follow function\nformats and match complex parameters. ADC utilizes a high-quality code\nfine-tuning dataset with line-level execution feedback, providing granular\nprocess supervision that fosters strong logical reasoning and adherence to\nfunction formats. It also employs an adversarial dataset generation process to\nimprove parameter matching. The staged training methodology capitalizes on both\nenriched code datasets and refined adversarial datasets, leading to marked\nimprovements in function calling capabilities on the Berkeley Function-Calling\nLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic\ncombination of process supervision, adversarial refinement, and incremental\nlearning, setting a new standard for LLM proficiency in complex function\ncalling."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Li Zhu"
                    },
                    {
                        "name": "Qianghuai Jia"
                    },
                    {
                        "name": "Feijun Jiang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Mengping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mengping Zhou"
                },
                "author": "Mengping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03752v3",
                "updated": "2024-12-23T17:57:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    57,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-05T17:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "Attention Heads of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Heads of Large Language Models: A Survey"
                },
                "summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain as black-box systems. Understanding the reasoning\nbottlenecks of LLMs has become a critical challenge, as these limitations are\ndeeply tied to their internal architecture. Among these, attention heads have\nemerged as a focal point for investigating the underlying mechanics of LLMs. In\nthis survey, we aim to demystify the internal reasoning processes of LLMs by\nsystematically exploring the roles and mechanisms of attention heads. We first\nintroduce a novel four-stage framework inspired by the human thought process:\nKnowledge Recalling, In-Context Identification, Latent Reasoning, and\nExpression Preparation. Using this framework, we comprehensively review\nexisting research to identify and categorize the functions of specific\nattention heads. Additionally, we analyze the experimental methodologies used\nto discover these special heads, dividing them into two categories:\nModeling-Free and Modeling-Required methods. We further summarize relevant\nevaluation methods and benchmarks. Finally, we discuss the limitations of\ncurrent research and propose several potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain as black-box systems. Understanding the reasoning\nbottlenecks of LLMs has become a critical challenge, as these limitations are\ndeeply tied to their internal architecture. Among these, attention heads have\nemerged as a focal point for investigating the underlying mechanics of LLMs. In\nthis survey, we aim to demystify the internal reasoning processes of LLMs by\nsystematically exploring the roles and mechanisms of attention heads. We first\nintroduce a novel four-stage framework inspired by the human thought process:\nKnowledge Recalling, In-Context Identification, Latent Reasoning, and\nExpression Preparation. Using this framework, we comprehensively review\nexisting research to identify and categorize the functions of specific\nattention heads. Additionally, we analyze the experimental methodologies used\nto discover these special heads, dividing them into two categories:\nModeling-Free and Modeling-Required methods. We further summarize relevant\nevaluation methods and benchmarks. Finally, we discuss the limitations of\ncurrent research and propose several potential future directions."
                },
                "authors": [
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "33 pages, 11 figures, 7 tables, 7 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17744v1",
                "updated": "2024-12-23T17:52:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    52,
                    10,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:52:10Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    52,
                    10,
                    0,
                    358,
                    0
                ],
                "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoTransBench: A Real-World Benchmark for Repository-Level Code\n  Translation"
                },
                "summary": "Repository-level code translation refers to translating an entire code\nrepository from one programming language to another while preserving the\nfunctionality of the source repository. Many benchmarks have been proposed to\nevaluate the performance of such code translators. However, previous benchmarks\nmostly provide fine-grained samples, focusing at either code snippet, function,\nor file-level code translation. Such benchmarks do not accurately reflect\nreal-world demands, where entire repositories often need to be translated,\ninvolving longer code length and more complex functionalities. To address this\ngap, we propose a new benchmark, named RepoTransBench, which is a real-world\nrepository-level code translation benchmark with an automatically executable\ntest suite. We conduct experiments on RepoTransBench to evaluate the\ntranslation performance of 11 advanced LLMs. We find that the Success@1 score\n(test success in one attempt) of the best-performing LLM is only 7.33%. To\nfurther explore the potential of LLMs for repository-level code translation, we\nprovide LLMs with error-related feedback to perform iterative debugging and\nobserve an average 7.09% improvement on Success@1. However, even with this\nimprovement, the Success@1 score of the best-performing LLM is only 21%, which\nmay not meet the need for reliable automatic repository-level code translation.\nFinally, we conduct a detailed error analysis and highlight current LLMs'\ndeficiencies in repository-level code translation, which could provide a\nreference for further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code translation refers to translating an entire code\nrepository from one programming language to another while preserving the\nfunctionality of the source repository. Many benchmarks have been proposed to\nevaluate the performance of such code translators. However, previous benchmarks\nmostly provide fine-grained samples, focusing at either code snippet, function,\nor file-level code translation. Such benchmarks do not accurately reflect\nreal-world demands, where entire repositories often need to be translated,\ninvolving longer code length and more complex functionalities. To address this\ngap, we propose a new benchmark, named RepoTransBench, which is a real-world\nrepository-level code translation benchmark with an automatically executable\ntest suite. We conduct experiments on RepoTransBench to evaluate the\ntranslation performance of 11 advanced LLMs. We find that the Success@1 score\n(test success in one attempt) of the best-performing LLM is only 7.33%. To\nfurther explore the potential of LLMs for repository-level code translation, we\nprovide LLMs with error-related feedback to perform iterative debugging and\nobserve an average 7.09% improvement on Success@1. However, even with this\nimprovement, the Success@1 score of the best-performing LLM is only 21%, which\nmay not meet the need for reliable automatic repository-level code translation.\nFinally, we conduct a detailed error analysis and highlight current LLMs'\ndeficiencies in repository-level code translation, which could provide a\nreference for further improvements."
                },
                "authors": [
                    {
                        "name": "Yanli Wang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Suiquan Wang"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "John Grundy"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Mingzhi Mao"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17743v1",
                "updated": "2024-12-23T17:47:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:47:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    47,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "YuLan-Mini: An Open Data-efficient Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YuLan-Mini: An Open Data-efficient Language Model"
                },
                "summary": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective pre-training of large language models (LLMs) has been challenging\ndue to the immense resource demands and the complexity of the technical\nprocesses involved. This paper presents a detailed technical report on\nYuLan-Mini, a highly capable base model with 2.42B parameters that achieves\ntop-tier performance among models of similar parameter scale. Our pre-training\napproach focuses on enhancing training efficacy through three key technical\ncontributions: an elaborate data pipeline combines data cleaning with data\nschedule strategies, a robust optimization method to mitigate training\ninstability, and an effective annealing approach that incorporates targeted\ndata selection and long context training. Remarkably, YuLan-Mini, trained on\n1.08T tokens, achieves performance comparable to industry-leading models that\nrequire significantly more data. To facilitate reproduction, we release the\nfull details of the data composition for each training phase. Project details\ncan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini."
                },
                "authors": [
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12135v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12135v3",
                "updated": "2024-12-23T17:46:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    46,
                    10,
                    0,
                    358,
                    0
                ],
                "published": "2024-04-18T12:35:39Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    12,
                    35,
                    39,
                    3,
                    109,
                    0
                ],
                "title": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause\n  analysis in micro-services architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause\n  analysis in micro-services architecture"
                },
                "summary": "Root cause analysis (RCA) in Micro-services architecture (MSA) with\nescalating complexity encounters complex challenges in maintaining system\nstability and efficiency due to fault propagation and circular dependencies\namong nodes. Diverse root cause analysis faults require multi-agents with\ndiverse expertise. To mitigate the hallucination problem of large language\nmodels (LLMs), we design blockchain-inspired voting to ensure the reliability\nof the analysis by using a decentralized decision-making process. To avoid\nnon-terminating loops led by common circular dependency in MSA, we objectively\nlimit steps and standardize task processing through Agent Workflow. We propose\na pioneering framework, multi-Agent Blockchain-inspired Collaboration for root\ncause analysis in micro-services architecture (mABC), where multiple agents\nbased on the powerful LLMs follow Agent Workflow and collaborate in\nblockchain-inspired voting. Specifically, seven specialized agents derived from\nAgent Workflow each provide valuable insights towards root cause analysis based\non their expertise and the intrinsic software knowledge of LLMs collaborating\nwithin a decentralized chain. Our experiments on the AIOps challenge dataset\nand a newly created Train-Ticket dataset demonstrate superior performance in\nidentifying root causes and generating effective resolutions. The ablation\nstudy further highlights Agent Workflow, multi-agent, and blockchain-inspired\nvoting is crucial for achieving optimal performance. mABC offers a\ncomprehensive automated root cause analysis and resolution in micro-services\narchitecture and significantly improves the IT Operation domain. The code and\ndataset are in https://github.com/zwpride/mABC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root cause analysis (RCA) in Micro-services architecture (MSA) with\nescalating complexity encounters complex challenges in maintaining system\nstability and efficiency due to fault propagation and circular dependencies\namong nodes. Diverse root cause analysis faults require multi-agents with\ndiverse expertise. To mitigate the hallucination problem of large language\nmodels (LLMs), we design blockchain-inspired voting to ensure the reliability\nof the analysis by using a decentralized decision-making process. To avoid\nnon-terminating loops led by common circular dependency in MSA, we objectively\nlimit steps and standardize task processing through Agent Workflow. We propose\na pioneering framework, multi-Agent Blockchain-inspired Collaboration for root\ncause analysis in micro-services architecture (mABC), where multiple agents\nbased on the powerful LLMs follow Agent Workflow and collaborate in\nblockchain-inspired voting. Specifically, seven specialized agents derived from\nAgent Workflow each provide valuable insights towards root cause analysis based\non their expertise and the intrinsic software knowledge of LLMs collaborating\nwithin a decentralized chain. Our experiments on the AIOps challenge dataset\nand a newly created Train-Ticket dataset demonstrate superior performance in\nidentifying root causes and generating effective resolutions. The ablation\nstudy further highlights Agent Workflow, multi-agent, and blockchain-inspired\nvoting is crucial for achieving optimal performance. mABC offers a\ncomprehensive automated root cause analysis and resolution in micro-services\narchitecture and significantly improves the IT Operation domain. The code and\ndataset are in https://github.com/zwpride/mABC."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhoujin Tian"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Chaoran Yan"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Tongliang Li"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Liangfan Zheng"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12135v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17741v1",
                "updated": "2024-12-23T17:44:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:44:05Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    44,
                    5,
                    0,
                    358,
                    0
                ],
                "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Attend: Try to Understand How <SEG> Token Works"
                },
                "summary": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specified model\n(\\eg, SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map,which\nreveals that what $\\texttt{<SEG>}$ token contributes to is the semantic\nsimilarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion.Also, extensive experiments have been conducted on the ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ."
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "https://github.com/rui-qian/READ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17729v1",
                "updated": "2024-12-23T17:19:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    19,
                    58,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:19:58Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    19,
                    58,
                    0,
                    358,
                    0
                ],
                "title": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding"
                },
                "summary": "Existing humor datasets and evaluations predominantly focus on English,\nleaving limited resources for culturally nuanced humor in non-English languages\nlike Chinese. To address this gap, we construct Chumor, the first Chinese humor\nexplanation dataset that exceeds the size of existing humor datasets. Chumor is\nsourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing\nintellectually challenging and culturally specific jokes. We test ten LLMs\nthrough direct and chain-of-thought prompting, revealing that Chumor poses\nsignificant challenges to existing LLMs, with their accuracy slightly above\nrandom and far below human. In addition, our analysis highlights that\nhuman-annotated humor explanations are significantly better than those\ngenerated by GPT-4o and ERNIE-4-turbo. We release Chumor at\nhttps://huggingface.co/datasets/dnaihao/Chumor, our project page is at\nhttps://dnaihao.github.io/Chumor-dataset/, our leaderboard is at\nhttps://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at\nhttps://github.com/dnaihao/Chumor-dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing humor datasets and evaluations predominantly focus on English,\nleaving limited resources for culturally nuanced humor in non-English languages\nlike Chinese. To address this gap, we construct Chumor, the first Chinese humor\nexplanation dataset that exceeds the size of existing humor datasets. Chumor is\nsourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing\nintellectually challenging and culturally specific jokes. We test ten LLMs\nthrough direct and chain-of-thought prompting, revealing that Chumor poses\nsignificant challenges to existing LLMs, with their accuracy slightly above\nrandom and far below human. In addition, our analysis highlights that\nhuman-annotated humor explanations are significantly better than those\ngenerated by GPT-4o and ERNIE-4-turbo. We release Chumor at\nhttps://huggingface.co/datasets/dnaihao/Chumor, our project page is at\nhttps://dnaihao.github.io/Chumor-dataset/, our leaderboard is at\nhttps://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at\nhttps://github.com/dnaihao/Chumor-dataset."
                },
                "authors": [
                    {
                        "name": "Ruiqi He"
                    },
                    {
                        "name": "Yushu He"
                    },
                    {
                        "name": "Longju Bai"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Zhenjie Sun"
                    },
                    {
                        "name": "Zenghao Tang"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Hanchen Xia"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Naihao Deng"
                    }
                ],
                "author_detail": {
                    "name": "Naihao Deng"
                },
                "author": "Naihao Deng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2406.12754",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17727v1",
                "updated": "2024-12-23T17:17:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    17,
                    50,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T17:17:50Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    17,
                    50,
                    0,
                    358,
                    0
                ],
                "title": "Knowledge Editing through Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing through Chain-of-Thought"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of natural language processing (NLP) tasks. However,\nkeeping these models up-to-date with evolving world knowledge remains a\nsignificant challenge due to the high costs of frequent retraining. To address\nthis challenge, knowledge editing techniques have emerged to update LLMs with\nnew information without rebuilding the model from scratch. Among these, the\nin-context editing paradigm stands out for its effectiveness in integrating new\nknowledge while preserving the model's original capabilities. Despite its\npotential, existing in-context knowledge editing methods are often\ntask-specific, focusing primarily on multi-hop QA tasks using structured\nknowledge triples. Moreover, their reliance on few-shot prompting for task\ndecomposition makes them unstable and less effective in generalizing across\ndiverse tasks.\n  In response to these limitations, we propose EditCoT, a novel knowledge\nediting framework that flexibly and efficiently updates LLMs across various\ntasks without retraining. EditCoT works by generating a chain-of-thought (CoT)\nfor a given input and then iteratively refining this CoT process using a CoT\neditor based on updated knowledge. We evaluate EditCoT across a diverse range\nof benchmarks, covering multiple languages and tasks. The results demonstrate\nthat our approach achieves state-of-the-art performance while offering superior\ngeneralization, effectiveness, and stability compared to existing methods,\nmarking a significant advancement in the field of knowledge updating. Code and\ndata are available at: https://github.com/bebr2/EditCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of natural language processing (NLP) tasks. However,\nkeeping these models up-to-date with evolving world knowledge remains a\nsignificant challenge due to the high costs of frequent retraining. To address\nthis challenge, knowledge editing techniques have emerged to update LLMs with\nnew information without rebuilding the model from scratch. Among these, the\nin-context editing paradigm stands out for its effectiveness in integrating new\nknowledge while preserving the model's original capabilities. Despite its\npotential, existing in-context knowledge editing methods are often\ntask-specific, focusing primarily on multi-hop QA tasks using structured\nknowledge triples. Moreover, their reliance on few-shot prompting for task\ndecomposition makes them unstable and less effective in generalizing across\ndiverse tasks.\n  In response to these limitations, we propose EditCoT, a novel knowledge\nediting framework that flexibly and efficiently updates LLMs across various\ntasks without retraining. EditCoT works by generating a chain-of-thought (CoT)\nfor a given input and then iteratively refining this CoT process using a CoT\neditor based on updated knowledge. We evaluate EditCoT across a diverse range\nof benchmarks, covering multiple languages and tasks. The results demonstrate\nthat our approach achieves state-of-the-art performance while offering superior\ngeneralization, effectiveness, and stability compared to existing methods,\nmarking a significant advancement in the field of knowledge updating. Code and\ndata are available at: https://github.com/bebr2/EditCoT."
                },
                "authors": [
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17690v1",
                "updated": "2024-12-23T16:16:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:16:30Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    16,
                    30,
                    0,
                    358,
                    0
                ],
                "title": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF\n  for Conversational QA over KGs with RAG"
                },
                "summary": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational question answering (ConvQA) is a convenient means of searching\nover RDF knowledge graphs (KGs), where a prevalent approach is to translate\nnatural language questions to SPARQL queries. However, SPARQL has certain\nshortcomings: (i) it is brittle for complex intents and conversational\nquestions, and (ii) it is not suitable for more abstract needs. Instead, we\npropose a novel two-pronged system where we fuse: (i) SQL-query results over a\ndatabase automatically derived from the KG, and (ii) text-search results over\nverbalizations of KG facts. Our pipeline supports iterative retrieval: when the\nresults of any branch are found to be unsatisfactory, the system can\nautomatically opt for further rounds. We put everything together in a retrieval\naugmented generation (RAG) setup, where an LLM generates a coherent response\nfrom accumulated search results. We demonstrate the superiority of our proposed\nsystem over several baselines on a knowledge graph of BMW automobiles."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at BTW 2025, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10571v3",
                "updated": "2024-12-23T16:12:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    12,
                    59,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-13T21:28:17Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    21,
                    28,
                    17,
                    4,
                    348,
                    0
                ],
                "title": "Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence Contextualization and Counterfactual Attribution for\n  Conversational QA over Heterogeneous Data with RAG Systems"
                },
                "summary": "Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation typically rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions: it has 300 hand-created conversational questions, each in\nEnglish and German, coupled with ground truth URLs, completed questions, and\nanswers from 215 public Confluence pages. These documents are typical of\nenterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE\non ConfQuestions show the viability of our ideas: contextualization improves\nRAG performance, and counterfactual explanations outperform standard\nattribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) works as a backbone for interacting with\nan enterprise's own data via Conversational Question Answering (ConvQA). In a\nRAG system, a retriever fetches passages from a collection in response to a\nquestion, which are then included in the prompt of a large language model (LLM)\nfor generating a natural language (NL) answer. However, several RAG systems\ntoday suffer from two shortcomings: (i) retrieved passages usually contain\ntheir raw text and lack appropriate document context, negatively impacting both\nretrieval and answering quality; and (ii) attribution strategies that explain\nanswer generation typically rely only on similarity between the answer and the\nretrieved passages, thereby only generating plausible but not causal\nexplanations. In this work, we demonstrate RAGONITE, a RAG system that remedies\nthe above concerns by: (i) contextualizing evidence with source metadata and\nsurrounding text; and (ii) computing counterfactual attribution, a causal\nexplanation approach where the contribution of an evidence to an answer is\ndetermined by the similarity of the original response to the answer obtained by\nremoving that evidence. To evaluate our proposals, we release a new benchmark\nConfQuestions: it has 300 hand-created conversational questions, each in\nEnglish and German, coupled with ground truth URLs, completed questions, and\nanswers from 215 public Confluence pages. These documents are typical of\nenterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE\non ConfQuestions show the viability of our ideas: contextualization improves\nRAG performance, and counterfactual explanations outperform standard\nattribution."
                },
                "authors": [
                    {
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "name": "Joel Schlotthauer"
                    },
                    {
                        "name": "Chris Hinze"
                    },
                    {
                        "name": "Andreas Foltyn"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Fabian Kuech"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Kuech"
                },
                "author": "Fabian Kuech",
                "arxiv_comment": "Accepted at WSDM 2025, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17686v1",
                "updated": "2024-12-23T16:11:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    27,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:27Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    27,
                    0,
                    358,
                    0
                ],
                "title": "Large Language Model Safety: A Holistic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Safety: A Holistic Survey"
                },
                "summary": "The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers."
                },
                "authors": [
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Zhigen Li"
                    },
                    {
                        "name": "Yongqi Leng"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Chuang Liu"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Zishan Guo"
                    },
                    {
                        "name": "Linhao Yu"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Bojian Jiang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "158 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16102v2",
                "updated": "2024-12-23T15:57:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    57,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-20T17:43:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    43,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Interleaved Speech-Text Language Models are Simple Streaming Text to\n  Speech Synthesizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved Speech-Text Language Models are Simple Streaming Text to\n  Speech Synthesizers"
                },
                "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for\nstreaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,\nIST-LM is directly trained on interleaved sequences of text and speech tokens\nwith a fixed ratio, eliminating the need for additional efforts in duration\nprediction and grapheme-to-phoneme alignment. The ratio of text chunk size to\nspeech chunk size is crucial for the performance of IST-LM. To explore this, we\nconducted a comprehensive series of statistical analyses on the training data\nand performed correlation analysis with the final performance, uncovering\nseveral key factors: 1) the distance between speech tokens and their\ncorresponding text tokens, 2) the number of future text tokens accessible to\neach speech token, and 3) the frequency of speech tokens precedes their\ncorresponding text tokens. Experimental results demonstrate how to achieve an\noptimal streaming TTS system without complicated engineering optimization,\nwhich has a limited gap with the non-streaming system. IST-LM is conceptually\nsimple and empirically powerful, paving the way for streaming TTS with minimal\noverhead while largely maintaining performance, showcasing broad prospects\ncoupled with real-time text stream from LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for\nstreaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches,\nIST-LM is directly trained on interleaved sequences of text and speech tokens\nwith a fixed ratio, eliminating the need for additional efforts in duration\nprediction and grapheme-to-phoneme alignment. The ratio of text chunk size to\nspeech chunk size is crucial for the performance of IST-LM. To explore this, we\nconducted a comprehensive series of statistical analyses on the training data\nand performed correlation analysis with the final performance, uncovering\nseveral key factors: 1) the distance between speech tokens and their\ncorresponding text tokens, 2) the number of future text tokens accessible to\neach speech token, and 3) the frequency of speech tokens precedes their\ncorresponding text tokens. Experimental results demonstrate how to achieve an\noptimal streaming TTS system without complicated engineering optimization,\nwhich has a limited gap with the non-streaming system. IST-LM is conceptually\nsimple and empirically powerful, paving the way for streaming TTS with minimal\noverhead while largely maintaining performance, showcasing broad prospects\ncoupled with real-time text stream from LLMs."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Ruiyang Xu"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "Submitted to ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17669v1",
                "updated": "2024-12-23T15:54:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    54,
                    15,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:54:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    54,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "Generating Completions for Fragmented Broca's Aphasic Sentences Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Completions for Fragmented Broca's Aphasic Sentences Using\n  Large Language Models"
                },
                "summary": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful\nand fragmented speech production with relatively good comprehension. Since\ntraditional aphasia treatment methods are often time-consuming,\nlabour-intensive, and do not reflect real-world conversations, applying natural\nlanguage processing based approaches such as Large Language Models (LLMs) could\npotentially contribute to improving existing treatment approaches. To address\nthis issue, we explore the use of sequence-to-sequence LLMs for completing\nfragmented Broca's aphasic sentences. We first generate synthetic Broca's\naphasic data using a rule-based system designed to mirror the linguistic\ncharacteristics of Broca's aphasic speech. Using this synthetic data, we then\nfine-tune four pre-trained LLMs on the task of completing fragmented sentences.\nWe evaluate our fine-tuned models on both synthetic and authentic Broca's\naphasic data. We demonstrate LLMs' capability for reconstructing fragmented\nsentences, with the models showing improved performance with longer input\nutterances. Our result highlights the LLMs' potential in advancing\ncommunication aids for individuals with Broca's aphasia and possibly other\nclinical populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful\nand fragmented speech production with relatively good comprehension. Since\ntraditional aphasia treatment methods are often time-consuming,\nlabour-intensive, and do not reflect real-world conversations, applying natural\nlanguage processing based approaches such as Large Language Models (LLMs) could\npotentially contribute to improving existing treatment approaches. To address\nthis issue, we explore the use of sequence-to-sequence LLMs for completing\nfragmented Broca's aphasic sentences. We first generate synthetic Broca's\naphasic data using a rule-based system designed to mirror the linguistic\ncharacteristics of Broca's aphasic speech. Using this synthetic data, we then\nfine-tune four pre-trained LLMs on the task of completing fragmented sentences.\nWe evaluate our fine-tuned models on both synthetic and authentic Broca's\naphasic data. We demonstrate LLMs' capability for reconstructing fragmented\nsentences, with the models showing improved performance with longer input\nutterances. Our result highlights the LLMs' potential in advancing\ncommunication aids for individuals with Broca's aphasia and possibly other\nclinical populations."
                },
                "authors": [
                    {
                        "name": "Sijbren van Vaals"
                    },
                    {
                        "name": "Yevgen Matusevych"
                    },
                    {
                        "name": "Frank Tsiwah"
                    }
                ],
                "author_detail": {
                    "name": "Frank Tsiwah"
                },
                "author": "Frank Tsiwah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11745v2",
                "updated": "2024-12-23T15:36:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    36,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-21T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "title": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing"
                },
                "summary": "Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Zhichao Duan"
                    },
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Rong Han"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyong Wang"
                },
                "author": "Jianyong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17651v1",
                "updated": "2024-12-23T15:29:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:29:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Detecting anxiety and depression in dialogues: a multi-label and\n  explainable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anxiety and depression in dialogues: a multi-label and\n  explainable approach"
                },
                "summary": "Anxiety and depression are the most common mental health issues worldwide,\naffecting a non-negligible part of the population. Accordingly, stakeholders,\nincluding governments' health systems, are developing new strategies to promote\nearly detection and prevention from a holistic perspective (i.e., addressing\nseveral disorders simultaneously). In this work, an entirely novel system for\nthe multi-label classification of anxiety and depression is proposed. The input\ndata consists of dialogues from user interactions with an assistant chatbot.\nAnother relevant contribution lies in using Large Language Models (LLMs) for\nfeature extraction, provided the complexity and variability of language. The\ncombination of LLMs, given their high capability for language understanding,\nand Machine Learning (ML) models, provided their contextual knowledge about the\nclassification problem thanks to the labeled data, constitute a promising\napproach towards mental health assessment. To promote the solution's\ntrustworthiness, reliability, and accountability, explainability descriptions\nof the model's decision are provided in a graphical dashboard. Experimental\nresults on a real dataset attain 90 % accuracy, improving those in the prior\nliterature. The ultimate objective is to contribute in an accessible and\nscalable way before formal treatment occurs in the healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anxiety and depression are the most common mental health issues worldwide,\naffecting a non-negligible part of the population. Accordingly, stakeholders,\nincluding governments' health systems, are developing new strategies to promote\nearly detection and prevention from a holistic perspective (i.e., addressing\nseveral disorders simultaneously). In this work, an entirely novel system for\nthe multi-label classification of anxiety and depression is proposed. The input\ndata consists of dialogues from user interactions with an assistant chatbot.\nAnother relevant contribution lies in using Large Language Models (LLMs) for\nfeature extraction, provided the complexity and variability of language. The\ncombination of LLMs, given their high capability for language understanding,\nand Machine Learning (ML) models, provided their contextual knowledge about the\nclassification problem thanks to the labeled data, constitute a promising\napproach towards mental health assessment. To promote the solution's\ntrustworthiness, reliability, and accountability, explainability descriptions\nof the model's decision are provided in a graphical dashboard. Experimental\nresults on a real dataset attain 90 % accuracy, improving those in the prior\nliterature. The ultimate objective is to contribute in an accessible and\nscalable way before formal treatment occurs in the healthcare systems."
                },
                "authors": [
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Silvia Garca-Mndez"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Garca-Mndez"
                },
                "author": "Silvia Garca-Mndez",
                "arxiv_journal_ref": "de Arriba-P\\'erez, F., Garc\\'ia-M\\'endez, S. (2024). Detecting\n  anxiety and depression in dialogues: a multi-label and explainable approach.\n  In Proceedings of the 3rd AIxIA Workshop on Artificial Intelligence For\n  Healthcare (pp. 257-271)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06540v2",
                "updated": "2024-12-23T15:29:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    29,
                    22,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-09T14:51:26Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families"
                },
                "summary": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications."
                },
                "authors": [
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Yuekai Sun"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17637v1",
                "updated": "2024-12-23T15:13:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    13,
                    56,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T15:13:56Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    15,
                    13,
                    56,
                    0,
                    358,
                    0
                ],
                "title": "SCBench: A Sports Commentary Benchmark for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A Sports Commentary Benchmark for Video LLMs"
                },
                "summary": "Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant advances have been made in Video Large Language Models\n(Video LLMs) in both academia and industry. However, methods to evaluate and\nbenchmark the performance of different Video LLMs, especially their\nfine-grained, temporal visual capabilities, remain very limited. On one hand,\ncurrent benchmarks use relatively simple videos (e.g., subtitled movie clips)\nwhere the model can understand the entire video by processing just a few\nframes. On the other hand, their datasets lack diversity in task format,\ncomprising only QA or multi-choice QA, which overlooks the models' capacity for\ngenerating in-depth and precise texts. Sports videos, which feature intricate\nvisual information, sequential events, and emotionally charged commentary,\npresent a critical challenge for Video LLMs, making sports commentary an ideal\nbenchmarking task. Inspired by these challenges, we propose a novel task:\nsports video commentary generation, developed $\\textbf{SCBench}$ for Video\nLLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a\nsix-dimensional metric specifically designed for our task, upon which we\npropose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a\ndataset consisting of 5,775 annotated video clips and ground-truth labels\ntailored to our metric. Based on SCBench, we conduct comprehensive evaluations\non multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought\nbaseline methods. Our results found that InternVL-Chat-2 achieves the best\nperformance with 5.44, surpassing the second-best by 1.04. Our work provides a\nfresh perspective for future research, aiming to enhance models' overall\ncapabilities in complex visual understanding tasks. Our dataset will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Kuangzhi Ge"
                    },
                    {
                        "name": "Lingjun Chen"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Liaoyuan Fan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Guanqun Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17626v1",
                "updated": "2024-12-23T14:58:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    58,
                    37,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:58:37Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    58,
                    37,
                    0,
                    358,
                    0
                ],
                "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study"
                },
                "summary": "Understanding training dynamics and feature evolution is crucial for the\nmechanistic interpretability of large language models (LLMs). Although sparse\nautoencoders (SAEs) have been used to identify features within LLMs, a clear\npicture of how these features evolve during training remains elusive. In this\nstudy, we: (1) introduce SAE-Track, a method to efficiently obtain a continual\nseries of SAEs; (2) formulate the process of feature formation and conduct a\nmechanistic analysis; and (3) analyze and visualize feature drift during\ntraining. Our work provides new insights into the dynamics of features in LLMs,\nenhancing our understanding of training mechanisms and feature evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding training dynamics and feature evolution is crucial for the\nmechanistic interpretability of large language models (LLMs). Although sparse\nautoencoders (SAEs) have been used to identify features within LLMs, a clear\npicture of how these features evolve during training remains elusive. In this\nstudy, we: (1) introduce SAE-Track, a method to efficiently obtain a continual\nseries of SAEs; (2) formulate the process of feature formation and conduct a\nmechanistic analysis; and (3) analyze and visualize feature drift during\ntraining. Our work provides new insights into the dynamics of features in LLMs,\nenhancing our understanding of training mechanisms and feature evolution."
                },
                "authors": [
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17618v1",
                "updated": "2024-12-23T14:43:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    43,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:43:41Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    43,
                    41,
                    0,
                    358,
                    0
                ],
                "title": "Dynamic safety cases for frontier AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic safety cases for frontier AI"
                },
                "summary": "Frontier artificial intelligence (AI) systems present both benefits and risks\nto society. Safety cases - structured arguments supported by evidence - are one\nway to help ensure the safe development and deployment of these systems. Yet\nthe evolving nature of AI capabilities, as well as changes in the operational\nenvironment and understanding of risk, necessitates mechanisms for continuously\nupdating these safety cases. Typically, in other sectors, safety cases are\nproduced pre-deployment and do not require frequent updates post-deployment,\nwhich can be a manual, costly process. This paper proposes a Dynamic Safety\nCase Management System (DSCMS) to support both the initial creation of a safety\ncase and its systematic, semi-automated revision over time. Drawing on methods\ndeveloped in the autonomous vehicles (AV) sector - state-of-the-art Checkable\nSafety Arguments (CSA) combined with Safety Performance Indicators (SPIs)\nrecommended by UL 4600, a DSCMS helps developers maintain alignment between\nsystem safety claims and the latest system state. We demonstrate this approach\non a safety case template for offensive cyber capabilities and suggest ways it\ncan be integrated into governance structures for safety-critical\ndecision-making. While the correctness of the initial safety argument remains\nparamount - particularly for high-severity risks - a DSCMS provides a framework\nfor adapting to new insights and strengthening incident response. We outline\nchallenges and further work towards development and implementation of this\napproach as part of continuous safety assurance of frontier AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier artificial intelligence (AI) systems present both benefits and risks\nto society. Safety cases - structured arguments supported by evidence - are one\nway to help ensure the safe development and deployment of these systems. Yet\nthe evolving nature of AI capabilities, as well as changes in the operational\nenvironment and understanding of risk, necessitates mechanisms for continuously\nupdating these safety cases. Typically, in other sectors, safety cases are\nproduced pre-deployment and do not require frequent updates post-deployment,\nwhich can be a manual, costly process. This paper proposes a Dynamic Safety\nCase Management System (DSCMS) to support both the initial creation of a safety\ncase and its systematic, semi-automated revision over time. Drawing on methods\ndeveloped in the autonomous vehicles (AV) sector - state-of-the-art Checkable\nSafety Arguments (CSA) combined with Safety Performance Indicators (SPIs)\nrecommended by UL 4600, a DSCMS helps developers maintain alignment between\nsystem safety claims and the latest system state. We demonstrate this approach\non a safety case template for offensive cyber capabilities and suggest ways it\ncan be integrated into governance structures for safety-critical\ndecision-making. While the correctness of the initial safety argument remains\nparamount - particularly for high-severity risks - a DSCMS provides a framework\nfor adapting to new insights and strengthening incident response. We outline\nchallenges and further work towards development and implementation of this\napproach as part of continuous safety assurance of frontier AI systems."
                },
                "authors": [
                    {
                        "name": "Carmen Crlan"
                    },
                    {
                        "name": "Francesca Gomez"
                    },
                    {
                        "name": "Yohan Mathew"
                    },
                    {
                        "name": "Ketana Krishna"
                    },
                    {
                        "name": "Ren King"
                    },
                    {
                        "name": "Peter Gebauer"
                    },
                    {
                        "name": "Ben R. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Ben R. Smith"
                },
                "author": "Ben R. Smith",
                "arxiv_comment": "75 pages, 41 tables/figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17614v1",
                "updated": "2024-12-23T14:36:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    36,
                    37,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    36,
                    37,
                    0,
                    358,
                    0
                ],
                "title": "Emerging Security Challenges of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Security Challenges of Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved record adoption in a short period\nof time across many different sectors including high importance areas such as\neducation [4] and healthcare [23]. LLMs are open-ended models trained on\ndiverse data without being tailored for specific downstream tasks, enabling\nbroad applicability across various domains. They are commonly used for text\ngeneration, but also widely used to assist with code generation [3], and even\nanalysis of security information, as Microsoft Security Copilot demonstrates\n[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial\nattacks [9]. So the concerns on the potential security implications of such\nwide scale adoption of LLMs have led to the creation of this working group on\nthe security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection\nand Defense - AI-Powered Threats and Responses\", the working group discussions\nfocused on the vulnerability of LLMs to adversarial attacks, rather than their\npotential use in generating malware or enabling cyberattacks. Although we note\nthe potential threat represented by the latter, the role of the LLMs in such\nuses is mostly as an accelerator for development, similar to what it is in\nbenign use. To make the analysis more specific, the working group employed\nChatGPT as a concrete example of an LLM and addressed the following points,\nwhich also form the structure of this report: 1. How do LLMs differ in\nvulnerabilities from traditional ML models? 2. What are the attack objectives\nin LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities\nof LLMs? 4. What is the supply chain in LLMs, how data flow in and out of\nsystems and what are the security implications? We conclude with an overview of\nopen challenges and outlook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved record adoption in a short period\nof time across many different sectors including high importance areas such as\neducation [4] and healthcare [23]. LLMs are open-ended models trained on\ndiverse data without being tailored for specific downstream tasks, enabling\nbroad applicability across various domains. They are commonly used for text\ngeneration, but also widely used to assist with code generation [3], and even\nanalysis of security information, as Microsoft Security Copilot demonstrates\n[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial\nattacks [9]. So the concerns on the potential security implications of such\nwide scale adoption of LLMs have led to the creation of this working group on\nthe security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection\nand Defense - AI-Powered Threats and Responses\", the working group discussions\nfocused on the vulnerability of LLMs to adversarial attacks, rather than their\npotential use in generating malware or enabling cyberattacks. Although we note\nthe potential threat represented by the latter, the role of the LLMs in such\nuses is mostly as an accelerator for development, similar to what it is in\nbenign use. To make the analysis more specific, the working group employed\nChatGPT as a concrete example of an LLM and addressed the following points,\nwhich also form the structure of this report: 1. How do LLMs differ in\nvulnerabilities from traditional ML models? 2. What are the attack objectives\nin LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities\nof LLMs? 4. What is the supply chain in LLMs, how data flow in and out of\nsystems and what are the security implications? We conclude with an overview of\nopen challenges and outlook."
                },
                "authors": [
                    {
                        "name": "Herve Debar"
                    },
                    {
                        "name": "Sven Dietrich"
                    },
                    {
                        "name": "Pavel Laskov"
                    },
                    {
                        "name": "Emil C. Lupu"
                    },
                    {
                        "name": "Eirini Ntoutsi"
                    }
                ],
                "author_detail": {
                    "name": "Eirini Ntoutsi"
                },
                "author": "Eirini Ntoutsi",
                "arxiv_comment": "A version of this appeared in the larger Dagstuhl seminar 23431\n  report (https://doi.org/10.4230/DagRep.13.10.90)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17610v1",
                "updated": "2024-12-23T14:29:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    29,
                    41,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:29:41Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    29,
                    41,
                    0,
                    358,
                    0
                ],
                "title": "Personalized Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Vision-Language Models"
                },
                "summary": "The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalization model has gained significant attention in image\ngeneration yet remains underexplored for large vision-language models (LVLMs).\nBeyond generic ones, with personalization, LVLMs handle interactive dialogues\nusing referential concepts (e.g., ``Mike and Susan are talking.'') instead of\nthe generic form (e.g., ``a boy and a girl are talking.''), making the\nconversation more customizable and referentially friendly. In addition, PLVM is\nequipped to continuously add new concepts during a dialogue without incurring\nadditional costs, which significantly enhances the practicality. PLVM proposes\nAligner, a pre-trained visual encoder to align referential concepts with the\nqueried images. During the dialogues, it extracts features of reference images\nwith these corresponding concepts and recognizes them in the queried image,\nenabling personalization. We note that the computational cost and parameter\ncount of the Aligner are negligible within the entire framework. With\ncomprehensive qualitative and quantitative analyses, we reveal the\neffectiveness and superiority of PLVM."
                },
                "authors": [
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Hoang Phan"
                    },
                    {
                        "name": "David Doermann"
                    },
                    {
                        "name": "Yunjie Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yunjie Tian"
                },
                "author": "Yunjie Tian",
                "arxiv_comment": "A simple way to personalize your LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17606v1",
                "updated": "2024-12-23T14:25:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    25,
                    33,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:25:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    25,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\n  Images"
                },
                "summary": "Building a large-scale figure QA dataset requires a considerable amount of\nwork, from gathering and selecting figures to extracting attributes like text,\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\nhave led to efforts to synthesize figures, most of these focus primarily on QA\ngeneration. Additionally, creating figures directly using LLMs often encounters\nissues such as code errors, similar-looking figures, and repetitive content in\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\nthe creation of chart figures with complete annotations of the visualized data\nand dense QA annotations without any manual annotation process. Our\nstage-by-stage pipeline makes it possible to create diverse topic and\nappearance figures efficiently while minimizing code errors. Our SBSFigures\ndemonstrate a strong pre-training effect, making it possible to achieve\nefficient training with a limited amount of real-world chart data starting from\nour pre-trained weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a large-scale figure QA dataset requires a considerable amount of\nwork, from gathering and selecting figures to extracting attributes like text,\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\nhave led to efforts to synthesize figures, most of these focus primarily on QA\ngeneration. Additionally, creating figures directly using LLMs often encounters\nissues such as code errors, similar-looking figures, and repetitive content in\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\nthe creation of chart figures with complete annotations of the visualized data\nand dense QA annotations without any manual annotation process. Our\nstage-by-stage pipeline makes it possible to create diverse topic and\nappearance figures efficiently while minimizing code errors. Our SBSFigures\ndemonstrate a strong pre-training effect, making it possible to achieve\nefficient training with a limited amount of real-world chart data starting from\nour pre-trained weights."
                },
                "authors": [
                    {
                        "name": "Risa Shinoda"
                    },
                    {
                        "name": "Kuniaki Saito"
                    },
                    {
                        "name": "Shohei Tanaka"
                    },
                    {
                        "name": "Tosho Hirasawa"
                    },
                    {
                        "name": "Yoshitaka Ushiku"
                    }
                ],
                "author_detail": {
                    "name": "Yoshitaka Ushiku"
                },
                "author": "Yoshitaka Ushiku",
                "arxiv_comment": "AAAI-25 Workshop on Document Understanding and Intelligence. Dataset\n  and code: https://github.com/omron-sinicx/SBSFigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17596v1",
                "updated": "2024-12-23T14:13:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea\n  Generation with Minimal Context"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin scientific tasks, existing evaluation frameworks primarily assess their\nperformance using rich contextual inputs, overlooking their ability to generate\nnovel ideas from minimal information. We introduce LiveIdeaBench, a\ncomprehensive benchmark that evaluates LLMs' scientific creativity and\ndivergent thinking capabilities using single-keyword prompts. Drawing from\nGuilford's creativity theory, our framework employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across four key dimensions:\noriginality, feasibility, fluency, and flexibility. Through extensive\nexperimentation with 20 leading models across 1,180 keywords spanning 18\nscientific domains, we reveal that scientific creative ability shows distinct\npatterns from general intelligence metrics. Notably, our results demonstrate\nthat models like QwQ-32B-preview achieve comparable creative performance to\ntop-tier models like o1-preview, despite significant gaps in their general\nintelligence scores. These findings highlight the importance of specialized\nevaluation frameworks for scientific creativity and suggest that the\ndevelopment of creative capabilities in LLMs may follow different trajectories\nthan traditional problem-solving abilities."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13945v2",
                "updated": "2024-12-23T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-20T02:25:07Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    25,
                    7,
                    3,
                    172,
                    0
                ],
                "title": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityBench: Evaluating the Capabilities of Large Language Models for\n  Urban Tasks"
                },
                "summary": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) with extensive general knowledge and\npowerful reasoning abilities have seen rapid development and widespread\napplication. A systematic and reliable evaluation of LLMs or vision-language\nmodel (VLMs) is a crucial step in applying and developing them for various\nfields. There have been some early explorations about the usability of LLMs for\nlimited urban tasks, but a systematic and scalable evaluation benchmark is\nstill lacking. The challenge in constructing a systematic evaluation benchmark\nfor urban research lies in the diversity of urban data, the complexity of\napplication scenarios and the highly dynamic nature of the urban environment.\nIn this paper, we design CityBench, an interactive simulator based evaluation\nplatform, as the first systematic benchmark for evaluating the capabilities of\nLLMs for diverse tasks in urban research. First, we build CityData to integrate\nthe diverse urban data and CitySimu to simulate fine-grained urban dynamics.\nBased on CityData and CitySimu, we design 8 representative urban tasks in 2\ncategories of perception-understanding and decision-making as the CityBench.\nWith extensive results from 30 well-known LLMs and VLMs in 13 cities around the\nworld, we find that advanced LLMs and VLMs can achieve competitive performance\nin diverse urban tasks requiring commonsense and semantic understanding\nabilities, e.g., understanding the human dynamics and semantic inference of\nurban images. Meanwhile, they fail to solve the challenging urban tasks\nrequiring professional knowledge and high-level reasoning abilities, e.g.,\ngeospatial prediction and traffic control task. These observations provide\nvaluable perspectives for utilizing and developing LLMs in the future. Codes\nare openly accessible via https://github.com/tsinghua-fib-lab/CityBench."
                },
                "authors": [
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Tianhui Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Junbo Yan"
                    },
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Siqi Guo"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "26 pages, https://github.com/tsinghua-fib-lab/CityBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17593v1",
                "updated": "2024-12-23T14:10:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "title": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation"
                },
                "summary": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation."
                },
                "authors": [
                    {
                        "name": "Chengbing Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04135v2",
                "updated": "2024-12-23T13:48:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    48,
                    55,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-07T09:04:52Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    4,
                    52,
                    1,
                    128,
                    0
                ],
                "title": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models"
                },
                "summary": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here"
                },
                "authors": [
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Jingyue Zhang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Boyue Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Alaa Khamis"
                    }
                ],
                "author_detail": {
                    "name": "Alaa Khamis"
                },
                "author": "Alaa Khamis",
                "arxiv_comment": "9 pages, 6 figures, 34 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17571v1",
                "updated": "2024-12-23T13:44:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    44,
                    29,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:44:29Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    44,
                    29,
                    0,
                    358,
                    0
                ],
                "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with\n  Transformer Attention for FPGA-based Particle Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with\n  Transformer Attention for FPGA-based Particle Physics"
                },
                "summary": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of\nSpiking Neural Networks (SNNs), Transformers, and high-performance computing\ntailored for particle physics, particularly in particle identification from\ndetector responses. Our approach leverages SNNs' intrinsic temporal dynamics\nand Transformers' robust attention mechanisms to enhance performance when\ndiscerning intricate particle interactions. At the heart of HPCNeuroNet lies\nthe integration of the sequential dynamism inherent in SNNs with the\ncontext-aware attention capabilities of Transformers, enabling the model to\nprecisely decode and interpret complex detector data. HPCNeuroNet is realized\nthrough the HLS4ML framework and optimized for deployment in FPGA environments.\nThe model accuracy and scalability are also enhanced by this architectural\nchoice. Benchmarked against machine learning models, HPCNeuroNet showcases\nbetter performance metrics, underlining its transformative potential in\nhigh-energy physics. We demonstrate that the combination of SNNs, Transformers,\nand FPGA-based high-performance computing in particle physics signifies a\nsignificant step forward and provides a strong foundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of\nSpiking Neural Networks (SNNs), Transformers, and high-performance computing\ntailored for particle physics, particularly in particle identification from\ndetector responses. Our approach leverages SNNs' intrinsic temporal dynamics\nand Transformers' robust attention mechanisms to enhance performance when\ndiscerning intricate particle interactions. At the heart of HPCNeuroNet lies\nthe integration of the sequential dynamism inherent in SNNs with the\ncontext-aware attention capabilities of Transformers, enabling the model to\nprecisely decode and interpret complex detector data. HPCNeuroNet is realized\nthrough the HLS4ML framework and optimized for deployment in FPGA environments.\nThe model accuracy and scalability are also enhanced by this architectural\nchoice. Benchmarked against machine learning models, HPCNeuroNet showcases\nbetter performance metrics, underlining its transformative potential in\nhigh-energy physics. We demonstrate that the combination of SNNs, Transformers,\nand FPGA-based high-performance computing in particle physics signifies a\nsignificant step forward and provides a strong foundation for future research."
                },
                "authors": [
                    {
                        "name": "Murat Isik"
                    },
                    {
                        "name": "Hiruna Vishwamith"
                    },
                    {
                        "name": "Jonathan Naoukin"
                    },
                    {
                        "name": "I. Can Dikmen"
                    }
                ],
                "author_detail": {
                    "name": "I. Can Dikmen"
                },
                "author": "I. Can Dikmen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17560v1",
                "updated": "2024-12-23T13:28:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:28:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference"
                },
                "summary": "With the rapid growth in the scale and complexity of large language models\n(LLMs), the costs of training and inference have risen substantially. Model\ncompression has emerged as a mainstream solution to reduce memory usage and\ncomputational overhead. This paper presents Group Quantization and Sparse\nAcceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. The proposed method consists of three key steps. First, GQSA\napplies group structured pruning to adhere to GPU-friendly sparse pattern\nconstraints. Second, a two-stage sparsity-aware training process is employed to\nmaximize performance retention after compression. Finally, the framework adopts\nthe Block Sparse Row (BSR) format to enable practical deployment and efficient\nexecution. Experimental results on the LLaMA model family show that GQSA\nachieves an excellent balance between model speed and accuracy. Furthermore, on\nthe latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\ncompression techniques significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the scale and complexity of large language models\n(LLMs), the costs of training and inference have risen substantially. Model\ncompression has emerged as a mainstream solution to reduce memory usage and\ncomputational overhead. This paper presents Group Quantization and Sparse\nAcceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. The proposed method consists of three key steps. First, GQSA\napplies group structured pruning to adhere to GPU-friendly sparse pattern\nconstraints. Second, a two-stage sparsity-aware training process is employed to\nmaximize performance retention after compression. Finally, the framework adopts\nthe Block Sparse Row (BSR) format to enable practical deployment and efficient\nexecution. Experimental results on the LLaMA model family show that GQSA\nachieves an excellent balance between model speed and accuracy. Furthermore, on\nthe latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM\ncompression techniques significantly."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    },
                    {
                        "name": "Lean Fu"
                    }
                ],
                "author_detail": {
                    "name": "Lean Fu"
                },
                "author": "Lean Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17558v1",
                "updated": "2024-12-23T13:26:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    26,
                    4,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:26:04Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    26,
                    4,
                    0,
                    358,
                    0
                ],
                "title": "A Survey of Query Optimization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Query Optimization in Large Language Models"
                },
                "summary": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the\nefficiency and quality of Large Language Models (LLMs) in understanding and\nanswering queries, especially complex ones in scenarios like\nRetrieval-Augmented Generation (RAG). Specifically, RAG mitigates the\nlimitations of LLMs by dynamically retrieving and leveraging up-to-date\nrelevant information, which provides a cost-effective solution to the challenge\nof LLMs producing plausible but potentially inaccurate responses. Recently, as\nRAG evolves and incorporates multiple components that influence its\nperformance, QO has emerged as a critical element, playing a pivotal role in\ndetermining the effectiveness of RAG's retrieval stage in accurately sourcing\nthe necessary multiple pieces of evidence to answer queries correctly. In this\npaper, we trace the evolution of QO techniques by summarizing and analyzing\nsignificant studies. Through an organized framework and categorization, we aim\nto consolidate existing QO techniques in RAG, elucidate their technological\nfoundations, and highlight their potential to enhance the versatility and\napplications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the\nefficiency and quality of Large Language Models (LLMs) in understanding and\nanswering queries, especially complex ones in scenarios like\nRetrieval-Augmented Generation (RAG). Specifically, RAG mitigates the\nlimitations of LLMs by dynamically retrieving and leveraging up-to-date\nrelevant information, which provides a cost-effective solution to the challenge\nof LLMs producing plausible but potentially inaccurate responses. Recently, as\nRAG evolves and incorporates multiple components that influence its\nperformance, QO has emerged as a critical element, playing a pivotal role in\ndetermining the effectiveness of RAG's retrieval stage in accurately sourcing\nthe necessary multiple pieces of evidence to answer queries correctly. In this\npaper, we trace the evolution of QO techniques by summarizing and analyzing\nsignificant studies. Through an organized framework and categorization, we aim\nto consolidate existing QO techniques in RAG, elucidate their technological\nfoundations, and highlight their potential to enhance the versatility and\napplications of LLMs."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Mao Zheng"
                },
                "author": "Mao Zheng",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17548v1",
                "updated": "2024-12-23T13:08:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    8,
                    48,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:08:48Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    8,
                    48,
                    0,
                    358,
                    0
                ],
                "title": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and\n  Multi-Domain Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and\n  Multi-Domain Testing"
                },
                "summary": "This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for\nArabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a\nsystem with only 4GB VRAM. We detail the process of adapting this large\nlanguage model to the Arabic domain, using diverse datasets including Bactrian,\nOpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom\ndata preprocessing, model configuration, and training optimization techniques\nsuch as gradient accumulation and mixed-precision training. We address specific\nchallenges in Arabic NLP, including morphological complexity, dialectal\nvariations, and diacritical mark handling. Experimental results over 10,000\ntraining steps show significant performance improvements, with the final loss\nconverging to 0.1083. We provide comprehensive analysis of GPU memory usage,\ntraining dynamics, and model evaluation across various Arabic language tasks,\nincluding text classification, question answering, and dialect identification.\nThe fine-tuned model demonstrates robustness to input perturbations and\nimproved handling of Arabic-specific linguistic phenomena. This research\ncontributes to multilingual AI by demonstrating a resource-efficient approach\nfor creating specialized language models, potentially democratizing access to\nadvanced NLP technologies for diverse linguistic communities. Our work paves\nthe way for future research in low-resource language adaptation and efficient\nfine-tuning of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for\nArabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a\nsystem with only 4GB VRAM. We detail the process of adapting this large\nlanguage model to the Arabic domain, using diverse datasets including Bactrian,\nOpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom\ndata preprocessing, model configuration, and training optimization techniques\nsuch as gradient accumulation and mixed-precision training. We address specific\nchallenges in Arabic NLP, including morphological complexity, dialectal\nvariations, and diacritical mark handling. Experimental results over 10,000\ntraining steps show significant performance improvements, with the final loss\nconverging to 0.1083. We provide comprehensive analysis of GPU memory usage,\ntraining dynamics, and model evaluation across various Arabic language tasks,\nincluding text classification, question answering, and dialect identification.\nThe fine-tuned model demonstrates robustness to input perturbations and\nimproved handling of Arabic-specific linguistic phenomena. This research\ncontributes to multilingual AI by demonstrating a resource-efficient approach\nfor creating specialized language models, potentially democratizing access to\nadvanced NLP technologies for diverse linguistic communities. Our work paves\nthe way for future research in low-resource language adaptation and efficient\nfine-tuning of large language models."
                },
                "authors": [
                    {
                        "name": "Prakash Aryan"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Aryan"
                },
                "author": "Prakash Aryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17544v1",
                "updated": "2024-12-23T13:05:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    51,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T13:05:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    5,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models"
                },
                "summary": "The emergence of Vision-Language Models (VLMs) is a significant advancement\nin integrating computer vision with Large Language Models (LLMs) to enhance\nmulti-modal machine learning capabilities. However, this progress has also made\nVLMs vulnerable to sophisticated adversarial attacks, raising concerns about\ntheir reliability. The objective of this paper is to assess the resilience of\nVLMs against jailbreak attacks that can compromise model safety compliance and\nresult in harmful outputs. To evaluate a VLM's ability to maintain its\nrobustness against adversarial input perturbations, we propose a novel metric\ncalled the \\textbf{Retention Score}. Retention Score is a multi-modal\nevaluation metric that includes Retention-I and Retention-T scores for\nquantifying jailbreak risks in visual and textual components of VLMs. Our\nprocess involves generating synthetic image-text pairs using a conditional\ndiffusion model. These pairs are then predicted for toxicity score by a VLM\nalongside a toxicity judgment classifier. By calculating the margin in toxicity\nscores, we can quantify the robustness of the VLM in an attack-agnostic manner.\nOur work has four main contributions. First, we prove that Retention Score can\nserve as a certified robustness metric. Second, we demonstrate that most VLMs\nwith visual components are less robust against jailbreak attacks than the\ncorresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find\nthat the security settings in Google Gemini significantly affect the score and\nrobustness. Moreover, the robustness of GPT4V is similar to the medium settings\nof Gemini. Finally, our approach offers a time-efficient alternative to\nexisting adversarial attack methods and provides consistent model robustness\nrankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Vision-Language Models (VLMs) is a significant advancement\nin integrating computer vision with Large Language Models (LLMs) to enhance\nmulti-modal machine learning capabilities. However, this progress has also made\nVLMs vulnerable to sophisticated adversarial attacks, raising concerns about\ntheir reliability. The objective of this paper is to assess the resilience of\nVLMs against jailbreak attacks that can compromise model safety compliance and\nresult in harmful outputs. To evaluate a VLM's ability to maintain its\nrobustness against adversarial input perturbations, we propose a novel metric\ncalled the \\textbf{Retention Score}. Retention Score is a multi-modal\nevaluation metric that includes Retention-I and Retention-T scores for\nquantifying jailbreak risks in visual and textual components of VLMs. Our\nprocess involves generating synthetic image-text pairs using a conditional\ndiffusion model. These pairs are then predicted for toxicity score by a VLM\nalongside a toxicity judgment classifier. By calculating the margin in toxicity\nscores, we can quantify the robustness of the VLM in an attack-agnostic manner.\nOur work has four main contributions. First, we prove that Retention Score can\nserve as a certified robustness metric. Second, we demonstrate that most VLMs\nwith visual components are less robust against jailbreak attacks than the\ncorresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find\nthat the security settings in Google Gemini significantly affect the score and\nrobustness. Moreover, the robustness of GPT4V is similar to the medium settings\nof Gemini. Finally, our approach offers a time-efficient alternative to\nexisting adversarial attack methods and provides consistent model robustness\nrankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA."
                },
                "authors": [
                    {
                        "name": "Zaitang Li"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "arxiv_comment": "14 pages, 8 figures, AAAI 2025",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v5",
                "updated": "2024-12-23T12:48:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    48,
                    43,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17522v1",
                "updated": "2024-12-23T12:44:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    44,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T12:44:54Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    44,
                    54,
                    0,
                    358,
                    0
                ],
                "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM\n  Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM\n  Jailbreak"
                },
                "summary": "Large Language Models (LLMs) are susceptible to generating harmful content\nwhen prompted with carefully crafted inputs, a vulnerability known as LLM\njailbreaking. As LLMs become more powerful, studying jailbreak methods is\ncritical to enhancing security and aligning models with human values.\nTraditionally, jailbreak techniques have relied on suffix addition or prompt\ntemplates, but these methods suffer from limited attack diversity. This paper\nintroduces DiffusionAttacker, an end-to-end generative approach for jailbreak\nrewriting inspired by diffusion models. Our method employs a\nsequence-to-sequence (seq2seq) text diffusion model as a generator,\nconditioning on the original prompt and guiding the denoising process with a\nnovel attack loss. Unlike previous approaches that use autoregressive LLMs to\ngenerate jailbreak prompts, which limit the modification of already generated\ntokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq\ndiffusion model, allowing more flexible token modifications. This approach\npreserves the semantic content of the original prompt while producing harmful\ncontent. Additionally, we leverage the Gumbel-Softmax technique to make the\nsampling process from the diffusion model's output distribution differentiable,\neliminating the need for iterative token search. Extensive experiments on\nAdvbench and Harmbench demonstrate that DiffusionAttacker outperforms previous\nmethods across various evaluation metrics, including attack success rate (ASR),\nfluency, and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to generating harmful content\nwhen prompted with carefully crafted inputs, a vulnerability known as LLM\njailbreaking. As LLMs become more powerful, studying jailbreak methods is\ncritical to enhancing security and aligning models with human values.\nTraditionally, jailbreak techniques have relied on suffix addition or prompt\ntemplates, but these methods suffer from limited attack diversity. This paper\nintroduces DiffusionAttacker, an end-to-end generative approach for jailbreak\nrewriting inspired by diffusion models. Our method employs a\nsequence-to-sequence (seq2seq) text diffusion model as a generator,\nconditioning on the original prompt and guiding the denoising process with a\nnovel attack loss. Unlike previous approaches that use autoregressive LLMs to\ngenerate jailbreak prompts, which limit the modification of already generated\ntokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq\ndiffusion model, allowing more flexible token modifications. This approach\npreserves the semantic content of the original prompt while producing harmful\ncontent. Additionally, we leverage the Gumbel-Softmax technique to make the\nsampling process from the diffusion model's output distribution differentiable,\neliminating the need for iterative token search. Extensive experiments on\nAdvbench and Harmbench demonstrate that DiffusionAttacker outperforms previous\nmethods across various evaluation metrics, including attack success rate (ASR),\nfluency, and diversity."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Junda Zhu"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "MinLie Huang"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21315v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21315v4",
                "updated": "2024-12-23T12:35:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    35,
                    12,
                    0,
                    358,
                    0
                ],
                "published": "2024-07-31T03:53:14Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    3,
                    53,
                    14,
                    2,
                    213,
                    0
                ],
                "title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances"
                },
                "summary": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%)."
                },
                "authors": [
                    {
                        "name": "Zehui Wu"
                    },
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Pengyuan Shi"
                    },
                    {
                        "name": "Kaan Donbekci"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21315v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21315v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14546v3",
                "updated": "2024-12-23T12:01:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    1,
                    28,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-20T17:55:04Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    55,
                    4,
                    3,
                    172,
                    0
                ],
                "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data"
                },
                "summary": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs."
                },
                "authors": [
                    {
                        "name": "Johannes Treutlein"
                    },
                    {
                        "name": "Dami Choi"
                    },
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Cem Anil"
                    },
                    {
                        "name": "Roger Grosse"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "Accepted at NeurIPS 2024. 10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v1",
                "updated": "2024-12-23T11:55:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to judge whether the translation in the current round is better\nthan the previous one or not. In this manner, we collect tens of thousands of\nlong-thought MT data, which is used to train our DRT-o1. The experimental\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to judge whether the translation in the current round is better\nthan the previous one or not. In this manner, we collect tens of thousands of\nlong-thought MT data, which is used to train our DRT-o1. The experimental\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1"
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17486v1",
                "updated": "2024-12-23T11:29:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    29,
                    44,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:29:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    29,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of\n  Large Language Models such as ChatGPT in Educational Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of\n  Large Language Models such as ChatGPT in Educational Settings"
                },
                "summary": "The rapid adoption of Generative AI (GenAI) based on Large Language Models\n(LLMs) such as ChatGPT has recently and profoundly impacted education, offering\ntransformative opportunities while raising significant concerns. In this study\nwe present the results of a survey that investigates how 395 students aged 13\nto 25 years old in France and Italy integrate LLMs into their educational\nroutines.\n  Key findings include the widespread use of these tools across all age groups\nand disciplines, with older students and male students demonstrating higher\nusage frequencies, particularly in scientific contexts. The results also show\ngender disparities, raising concerns about an emerging AI literacy and\ntechnological gender gap. Additionally, while most students utilise LLMs\nconstructively, the lack of systematic proofreading and critical evaluation\namong younger users suggests potential risks to cognitive skills development,\nincluding critical thinking and foundational knowledge. The survey results\nunderscore the need for educational institutions to adapt their curricula to\nintegrate AI tools effectively, promoting ethical use, critical thinking, and\nawareness of AI limitations and environmental costs. This paper provides\nactionable recommendations for fostering equitable and effective cohabitation\nof LLMs and education while addressing emerging challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Generative AI (GenAI) based on Large Language Models\n(LLMs) such as ChatGPT has recently and profoundly impacted education, offering\ntransformative opportunities while raising significant concerns. In this study\nwe present the results of a survey that investigates how 395 students aged 13\nto 25 years old in France and Italy integrate LLMs into their educational\nroutines.\n  Key findings include the widespread use of these tools across all age groups\nand disciplines, with older students and male students demonstrating higher\nusage frequencies, particularly in scientific contexts. The results also show\ngender disparities, raising concerns about an emerging AI literacy and\ntechnological gender gap. Additionally, while most students utilise LLMs\nconstructively, the lack of systematic proofreading and critical evaluation\namong younger users suggests potential risks to cognitive skills development,\nincluding critical thinking and foundational knowledge. The survey results\nunderscore the need for educational institutions to adapt their curricula to\nintegrate AI tools effectively, promoting ethical use, critical thinking, and\nawareness of AI limitations and environmental costs. This paper provides\nactionable recommendations for fostering equitable and effective cohabitation\nof LLMs and education while addressing emerging challenges."
                },
                "authors": [
                    {
                        "name": "Jrmie Sublime"
                    },
                    {
                        "name": "Ilaria Renna"
                    }
                ],
                "author_detail": {
                    "name": "Ilaria Renna"
                },
                "author": "Ilaria Renna",
                "arxiv_comment": "33 pages + references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17481v1",
                "updated": "2024-12-23T11:11:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T11:11:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "A Survey on Multi-Generative Agent System: Recent Advances and New\n  Frontiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multi-Generative Agent System: Recent Advances and New\n  Frontiers"
                },
                "summary": "Multi-generative agent systems (MGASs) have become a research hotspot since\nthe rise of large language models (LLMs). However, with the continuous influx\nof new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of MGAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of MGAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-generative agent systems (MGASs) have become a research hotspot since\nthe rise of large language models (LLMs). However, with the continuous influx\nof new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of MGAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of MGAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Shuaihang Chen"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15265v2",
                "updated": "2024-12-23T11:06:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    6,
                    56,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-17T03:03:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    3,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks."
                },
                "authors": [
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Boren Zheng"
                    },
                    {
                        "name": "Baihui Zheng"
                    },
                    {
                        "name": "Kerui Cao"
                    },
                    {
                        "name": "Huiyun Jing"
                    },
                    {
                        "name": "Jincheng Wei"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiangyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v2",
                "updated": "2024-12-23T10:46:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    46,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "arxiv_comment": "In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15714v2",
                "updated": "2024-12-23T10:45:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    45,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-20T09:37:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    37,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLife: Automatic Life Journaling with Smartphones and LLMs"
                },
                "summary": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel mobile sensing application - life journaling -\ndesigned to generate semantic descriptions of users' daily lives. We present\nAutoLife, an automatic life journaling system based on commercial smartphones.\nAutoLife only inputs low-cost sensor data (without photos or audio) from\nsmartphones and can automatically generate comprehensive life journals for\nusers. To achieve this, we first derive time, motion, and location contexts\nfrom multimodal sensor data, and harness the zero-shot capabilities of Large\nLanguage Models (LLMs), enriched with commonsense knowledge about human lives,\nto interpret diverse contexts and generate life journals. To manage the task\ncomplexity and long sensing duration, a multilayer framework is proposed, which\ndecomposes tasks and seamlessly integrates LLMs with other techniques for life\njournaling. This study establishes a real-life dataset as a benchmark and\nextensive experiment results demonstrate that AutoLife produces accurate and\nreliable life journals."
                },
                "authors": [
                    {
                        "name": "Huatao Xu"
                    },
                    {
                        "name": "Panrong Tong"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12498v2",
                "updated": "2024-12-23T10:36:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    36,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-19T13:31:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    31,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic\n  Corpus"
                },
                "summary": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of solving a wide range of tasks,\nyet they have struggled with reasoning. To address this, we propose\n$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\nreasoning capabilities by program-generated logical reasoning samples. We first\nestablish principles for designing high-quality samples by integrating symbolic\nlogic theory and previous empirical insights. Then, based on these principles,\nwe construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$\n($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT\non FLD$_{\\times2}$ substantially enhances the reasoning capabilities of\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\ncoding benchmarks, and 5 points on the benchmark suite BBH."
                },
                "authors": [
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Gaku Morio"
                    },
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Yasuhiro Sogawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhiro Sogawa"
                },
                "author": "Yasuhiro Sogawa",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v6",
                "updated": "2024-12-23T10:29:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    29,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17449v1",
                "updated": "2024-12-23T10:14:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    14,
                    32,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:14:32Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    14,
                    32,
                    0,
                    358,
                    0
                ],
                "title": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts"
                },
                "summary": "This study explores the use of Large language models to analyze therapist\nremarks in a psychotherapeutic setting. The paper focuses on the application of\nBERTopic, a machine learning-based topic modeling tool, to the dialogue of two\ndifferent groups of therapists (classical and modern), which makes it possible\nto identify and describe a set of topics that consistently emerge across these\ngroups. The paper describes in detail the chosen algorithm for BERTopic, which\nincluded creating a vector space from a corpus of therapist remarks, reducing\nits dimensionality, clustering the space, and creating and optimizing topic\nrepresentation. Along with the automatic topical modeling by the BERTopic, the\nresearch involved an expert assessment of the findings and manual topic\nstructure optimization. The topic modeling results highlighted the most common\nand stable topics in therapists speech, offering insights into how language\npatterns in therapy develop and remain stable across different therapeutic\nstyles. This work contributes to the growing field of machine learning in\npsychotherapy by demonstrating the potential of automated methods to improve\nboth the practice and training of therapists. The study highlights the value of\ntopic modeling as a tool for gaining a deeper understanding of therapeutic\ndialogue and offers new opportunities for improving therapeutic effectiveness\nand clinical supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of Large language models to analyze therapist\nremarks in a psychotherapeutic setting. The paper focuses on the application of\nBERTopic, a machine learning-based topic modeling tool, to the dialogue of two\ndifferent groups of therapists (classical and modern), which makes it possible\nto identify and describe a set of topics that consistently emerge across these\ngroups. The paper describes in detail the chosen algorithm for BERTopic, which\nincluded creating a vector space from a corpus of therapist remarks, reducing\nits dimensionality, clustering the space, and creating and optimizing topic\nrepresentation. Along with the automatic topical modeling by the BERTopic, the\nresearch involved an expert assessment of the findings and manual topic\nstructure optimization. The topic modeling results highlighted the most common\nand stable topics in therapists speech, offering insights into how language\npatterns in therapy develop and remain stable across different therapeutic\nstyles. This work contributes to the growing field of machine learning in\npsychotherapy by demonstrating the potential of automated methods to improve\nboth the practice and training of therapists. The study highlights the value of\ntopic modeling as a tool for gaining a deeper understanding of therapeutic\ndialogue and offers new opportunities for improving therapeutic effectiveness\nand clinical supervision."
                },
                "authors": [
                    {
                        "name": "Alexander Vanin"
                    },
                    {
                        "name": "Vadim Bolshev"
                    },
                    {
                        "name": "Anastasia Panfilova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Panfilova"
                },
                "author": "Anastasia Panfilova",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7, J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17429v1",
                "updated": "2024-12-23T09:47:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    47,
                    20,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:47:20Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    47,
                    20,
                    0,
                    358,
                    0
                ],
                "title": "Condor: A Code Discriminator Integrating General Semantics with Code\n  Details",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Condor: A Code Discriminator Integrating General Semantics with Code\n  Details"
                },
                "summary": "LLMs demonstrate significant potential across various software engineering\ntasks. However, they still face challenges in generating correct code on the\nfirst attempt when addressing complex requirements. Introducing a discriminator\nto select reliable outputs from multiple generated results is an effective way\nto enhance their reliability and stability. Currently, these discriminators\nfall into two categories: execution-based discriminators and\nnon-execution-based discriminators. Execution-based discriminators face\nflexibility challenges due to difficulties in obtaining test cases and security\nconcerns, while non-execution-based discriminators, although more flexible,\nstruggle to capture subtle differences in code details. To maintain flexibility\nwhile improving the model's ability to capture fine-grained code details, this\npaper proposes Condor. We first design contrastive learning to optimize the\ncode representations of the base model, enabling it to reflect differences in\ncode details. Then, we leverage intermediate data from the code modification\nprocess to further enrich the discriminator's training data, enhancing its\nability to discern code details. Experimental results indicate that on the\nsubtle code difference dataset (i.e., CodeNanoFix), Condor significantly\noutperforms other discriminators in discriminative performance: Condor (1.3B)\nimproves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%.\nIn discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise\nthe Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset\nfrom 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates\nstrong generalization capabilities on the MBPP and APPS datasets. For example,\nCondor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS\ndataset by 147.05%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs demonstrate significant potential across various software engineering\ntasks. However, they still face challenges in generating correct code on the\nfirst attempt when addressing complex requirements. Introducing a discriminator\nto select reliable outputs from multiple generated results is an effective way\nto enhance their reliability and stability. Currently, these discriminators\nfall into two categories: execution-based discriminators and\nnon-execution-based discriminators. Execution-based discriminators face\nflexibility challenges due to difficulties in obtaining test cases and security\nconcerns, while non-execution-based discriminators, although more flexible,\nstruggle to capture subtle differences in code details. To maintain flexibility\nwhile improving the model's ability to capture fine-grained code details, this\npaper proposes Condor. We first design contrastive learning to optimize the\ncode representations of the base model, enabling it to reflect differences in\ncode details. Then, we leverage intermediate data from the code modification\nprocess to further enrich the discriminator's training data, enhancing its\nability to discern code details. Experimental results indicate that on the\nsubtle code difference dataset (i.e., CodeNanoFix), Condor significantly\noutperforms other discriminators in discriminative performance: Condor (1.3B)\nimproves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%.\nIn discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise\nthe Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset\nfrom 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates\nstrong generalization capabilities on the MBPP and APPS datasets. For example,\nCondor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS\ndataset by 147.05%."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Zixiao Zhao"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Yanjie Jiang"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17427v1",
                "updated": "2024-12-23T09:45:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    45,
                    3,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    45,
                    3,
                    0,
                    358,
                    0
                ],
                "title": "Measuring Contextual Informativeness in Child-Directed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Contextual Informativeness in Child-Directed Text"
                },
                "summary": "To address an important gap in creating children's stories for vocabulary\nenrichment, we investigate the automatic evaluation of how well stories convey\nthe semantics of target vocabulary words, a task with substantial implications\nfor generating educational content. We motivate this task, which we call\nmeasuring contextual informativeness in children's stories, and provide a\nformal task definition as well as a dataset for the task. We further propose a\nmethod for automating the task using a large language model (LLM). Our\nexperiments show that our approach reaches a Spearman correlation of 0.4983\nwith human judgments of informativeness, while the strongest baseline only\nobtains a correlation of 0.3534. An additional analysis shows that the\nLLM-based approach is able to generalize to measuring contextual\ninformativeness in adult-directed text, on which it also outperforms all\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address an important gap in creating children's stories for vocabulary\nenrichment, we investigate the automatic evaluation of how well stories convey\nthe semantics of target vocabulary words, a task with substantial implications\nfor generating educational content. We motivate this task, which we call\nmeasuring contextual informativeness in children's stories, and provide a\nformal task definition as well as a dataset for the task. We further propose a\nmethod for automating the task using a large language model (LLM). Our\nexperiments show that our approach reaches a Spearman correlation of 0.4983\nwith human judgments of informativeness, while the strongest baseline only\nobtains a correlation of 0.3534. An additional analysis shows that the\nLLM-based approach is able to generalize to measuring contextual\ninformativeness in adult-directed text, on which it also outperforms all\nbaselines."
                },
                "authors": [
                    {
                        "name": "Maria Valentini"
                    },
                    {
                        "name": "Ta Wright"
                    },
                    {
                        "name": "Ali Marashian"
                    },
                    {
                        "name": "Jennifer Weber"
                    },
                    {
                        "name": "Eliana Colunga"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "arxiv_comment": "COLING 2025 main conference short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17417v1",
                "updated": "2024-12-23T09:29:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    29,
                    40,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:29:40Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    29,
                    40,
                    0,
                    358,
                    0
                ],
                "title": "Multimodal Preference Data Synthetic Alignment with Reward Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Preference Data Synthetic Alignment with Reward Model"
                },
                "summary": "Multimodal large language models (MLLMs) have significantly advanced tasks\nlike caption generation and visual question answering by integrating visual and\ntextual data. However, they sometimes produce misleading or hallucinate content\ndue to discrepancies between their pre-training data and real user prompts.\nExisting approaches using Direct Preference Optimization (DPO) in\nvision-language tasks often rely on strong models like GPT-4 or CLIP to\ndetermine positive and negative responses. Here, we propose a new framework in\ngenerating synthetic data using a reward model as a proxy of human preference\nfor effective multimodal alignment with DPO training. The resulting DPO dataset\nranges from 2K to 9K image-text pairs, was evaluated on LLaVA-v1.5-7B, where\nour approach demonstrated substantial improvements in both the trustworthiness\nand reasoning capabilities of the base model across multiple hallucination and\nvision-language benchmark. The experiment results indicate that integrating\nselected synthetic data, such as from generative and rewards models can\neffectively reduce reliance on human-annotated data while enhancing MLLMs'\nalignment capability, offering a scalable solution for safer deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have significantly advanced tasks\nlike caption generation and visual question answering by integrating visual and\ntextual data. However, they sometimes produce misleading or hallucinate content\ndue to discrepancies between their pre-training data and real user prompts.\nExisting approaches using Direct Preference Optimization (DPO) in\nvision-language tasks often rely on strong models like GPT-4 or CLIP to\ndetermine positive and negative responses. Here, we propose a new framework in\ngenerating synthetic data using a reward model as a proxy of human preference\nfor effective multimodal alignment with DPO training. The resulting DPO dataset\nranges from 2K to 9K image-text pairs, was evaluated on LLaVA-v1.5-7B, where\nour approach demonstrated substantial improvements in both the trustworthiness\nand reasoning capabilities of the base model across multiple hallucination and\nvision-language benchmark. The experiment results indicate that integrating\nselected synthetic data, such as from generative and rewards models can\neffectively reduce reliance on human-annotated data while enhancing MLLMs'\nalignment capability, offering a scalable solution for safer deployment."
                },
                "authors": [
                    {
                        "name": "Robert Wijaya"
                    },
                    {
                        "name": "Ngoc-Bao Nguyen"
                    },
                    {
                        "name": "Ngai-Man Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Ngai-Man Cheung"
                },
                "author": "Ngai-Man Cheung",
                "arxiv_comment": "Project Page: https://pds-dpo.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17415v1",
                "updated": "2024-12-23T09:26:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "VidCtx: Context-aware Video Question Answering with Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCtx: Context-aware Video Question Answering with Image Models"
                },
                "summary": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR."
                },
                "authors": [
                    {
                        "name": "Andreas Goulas"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17408v1",
                "updated": "2024-12-23T09:17:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    17,
                    6,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T09:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    17,
                    6,
                    0,
                    358,
                    0
                ],
                "title": "Just What You Desire: Constrained Timeline Summarization with\n  Self-Reflection for Enhanced Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just What You Desire: Constrained Timeline Summarization with\n  Self-Reflection for Enhanced Relevance"
                },
                "summary": "Given news articles about an entity, such as a public figure or organization,\ntimeline summarization (TLS) involves generating a timeline that summarizes the\nkey events about the entity. However, the TLS task is too underspecified, since\nwhat is of interest to each reader may vary, and hence there is not a single\nideal or optimal timeline. In this paper, we introduce a novel task, called\nConstrained Timeline Summarization (CTLS), where a timeline is generated in\nwhich all events in the timeline meet some constraint. An example of a\nconstrained timeline concerns the legal battles of Tiger Woods, where only\nevents related to his legal problems are selected to appear in the timeline. We\ncollected a new human-verified dataset of constrained timelines involving 47\nentities and 5 constraints per entity. We propose an approach that employs a\nlarge language model (LLM) to summarize news articles according to a specified\nconstraint and cluster them to identify key events to include in a constrained\ntimeline. In addition, we propose a novel self-reflection method during summary\ngeneration, demonstrating that this approach successfully leads to improved\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given news articles about an entity, such as a public figure or organization,\ntimeline summarization (TLS) involves generating a timeline that summarizes the\nkey events about the entity. However, the TLS task is too underspecified, since\nwhat is of interest to each reader may vary, and hence there is not a single\nideal or optimal timeline. In this paper, we introduce a novel task, called\nConstrained Timeline Summarization (CTLS), where a timeline is generated in\nwhich all events in the timeline meet some constraint. An example of a\nconstrained timeline concerns the legal battles of Tiger Woods, where only\nevents related to his legal problems are selected to appear in the timeline. We\ncollected a new human-verified dataset of constrained timelines involving 47\nentities and 5 constraints per entity. We propose an approach that employs a\nlarge language model (LLM) to summarize news articles according to a specified\nconstraint and cluster them to identify key events to include in a constrained\ntimeline. In addition, we propose a novel self-reflection method during summary\ngeneration, demonstrating that this approach successfully leads to improved\nperformance."
                },
                "authors": [
                    {
                        "name": "Muhammad Reza Qorib"
                    },
                    {
                        "name": "Qisheng Hu"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "arxiv_comment": "AAAI 2025 (with appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03001v2",
                "updated": "2024-12-23T09:03:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    3,
                    2,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-06T07:19:51Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    19,
                    51,
                    1,
                    219,
                    0
                ],
                "title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning"
                },
                "summary": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yu Song"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jihong Hu"
                    },
                    {
                        "name": "Yen-Wei Chen"
                    },
                    {
                        "name": "Lanfen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lanfen Lin"
                },
                "author": "Lanfen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09498v2",
                "updated": "2024-12-23T08:59:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    59,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2024-03-14T15:40:13Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    15,
                    40,
                    13,
                    3,
                    74,
                    0
                ],
                "title": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News"
                },
                "summary": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_doi": "10.24963/ijcai.2024/873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.09498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IJCAI 2024 Oral",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18873v2",
                "updated": "2024-12-23T08:58:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    58,
                    9,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-27T03:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    3,
                    57,
                    12,
                    3,
                    179,
                    0
                ],
                "title": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design"
                },
                "summary": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs."
                },
                "authors": [
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Xiaohan Gao"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Xiyuan Tang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "arxiv_comment": "8pages, 8figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17397v1",
                "updated": "2024-12-23T08:51:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    51,
                    48,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:51:48Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    51,
                    48,
                    0,
                    358,
                    0
                ],
                "title": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search\n  Boosted Reasoning via Iterative Preference Learning"
                },
                "summary": "With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With current state-of-the-art approaches aimed at enhancing the reasoning\ncapabilities of Large Language Models(LLMs) through iterative preference\nlearning inspired by AlphaZero, we propose to further enhance the step-wise\nreasoning capabilities through intrinsic self-correction to some extent. Our\nwork leverages step-wise preference learning to enhance self-verification via\nreinforcement learning. We initially conduct our work through a two-stage\ntraining procedure. At the first stage, the self-correction reasoning ability\nof an LLM is enhanced through its own predictions, relying entirely on\nself-generated data within the intrinsic self-correction to some extent. At the\nsecond stage, the baseline step-wise preference learning is leveraged via the\napplication of the enhanced self-correct policy achieved at the first stage. In\nthe evaluation of arithmetic reasoning tasks, our approach outperforms\nOpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in\naccuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct,\nMistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%)\nand 38.06%(+2.28%)."
                },
                "authors": [
                    {
                        "name": "Huchen Jiang"
                    },
                    {
                        "name": "Yangyang Ma"
                    },
                    {
                        "name": "Chaofan Ding"
                    },
                    {
                        "name": "Kexin Luan"
                    },
                    {
                        "name": "Xinhan Di"
                    }
                ],
                "author_detail": {
                    "name": "Xinhan Di"
                },
                "author": "Xinhan Di",
                "arxiv_comment": "6 Pages,3 figures, accepted by AAAI 2025 Workshop NeurMAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17395v1",
                "updated": "2024-12-23T08:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models"
                },
                "summary": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to gather complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\nthe limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nlimits the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder which learns from expert battles\nto address these limitations. Specifically, we create an arena for current\nexpert code LLMs, where each model challenges and responds to others'\nchallenges, with evaluations conducted by uninvolved judge models. This\ncompetitive framework generates novel training data constructed from scratch,\nharnessing the strengths of all participants. Experimental results demonstrate\nthat WarriorCoder achieves competitive performance compared to previous\nmethods, even without relying on proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to gather complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\nthe limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nlimits the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder which learns from expert battles\nto address these limitations. Specifically, we create an arena for current\nexpert code LLMs, where each model challenges and responds to others'\nchallenges, with evaluations conducted by uninvolved judge models. This\ncompetitive framework generates novel training data constructed from scratch,\nharnessing the strengths of all participants. Experimental results demonstrate\nthat WarriorCoder achieves competitive performance compared to previous\nmethods, even without relying on proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17383v1",
                "updated": "2024-12-23T08:33:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    33,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:33:47Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    33,
                    47,
                    0,
                    358,
                    0
                ],
                "title": "Interweaving Memories of a Siamese Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interweaving Memories of a Siamese Large Language Model"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models\n(LLMs) by modifying or introducing a small number of parameters to enhance\nalignment with downstream tasks. However, they can result in catastrophic\nforgetting, where LLMs prioritize new knowledge at the expense of comprehensive\nworld knowledge. A promising approach to mitigate this issue is to recall prior\nmemories based on the original knowledge. To this end, we propose a\nmodel-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese\nLarge Language Model. Specifically, our siamese LLM is equipped with an\nexisting PEFT method. Given an incoming query, it generates two distinct\nmemories based on the pre-trained and fine-tuned parameters. IMSM then\nincorporates an interweaving mechanism that regulates the contributions of both\noriginal and enhanced memories when generating the next token. This framework\nis theoretically applicable to all open-source LLMs and existing PEFT methods.\nWe conduct extensive experiments across various benchmark datasets, evaluating\nthe performance of popular open-source LLMs using the proposed IMSM, in\ncomparison to both classical and leading PEFT methods. Our findings indicate\nthat IMSM maintains comparable time and space efficiency to backbone PEFT\nmethods while significantly improving performance and effectively mitigating\ncatastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models\n(LLMs) by modifying or introducing a small number of parameters to enhance\nalignment with downstream tasks. However, they can result in catastrophic\nforgetting, where LLMs prioritize new knowledge at the expense of comprehensive\nworld knowledge. A promising approach to mitigate this issue is to recall prior\nmemories based on the original knowledge. To this end, we propose a\nmodel-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese\nLarge Language Model. Specifically, our siamese LLM is equipped with an\nexisting PEFT method. Given an incoming query, it generates two distinct\nmemories based on the pre-trained and fine-tuned parameters. IMSM then\nincorporates an interweaving mechanism that regulates the contributions of both\noriginal and enhanced memories when generating the next token. This framework\nis theoretically applicable to all open-source LLMs and existing PEFT methods.\nWe conduct extensive experiments across various benchmark datasets, evaluating\nthe performance of popular open-source LLMs using the proposed IMSM, in\ncomparison to both classical and leading PEFT methods. Our findings indicate\nthat IMSM maintains comparable time and space efficiency to backbone PEFT\nmethods while significantly improving performance and effectively mitigating\ncatastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Zhikai Xue"
                    },
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Wei Lu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lu"
                },
                "author": "Wei Lu",
                "arxiv_comment": "Accepted by AAAI 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01446v2",
                "updated": "2024-12-23T08:29:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    29,
                    47,
                    0,
                    358,
                    0
                ],
                "published": "2023-10-01T12:28:36Z",
                "published_parsed": [
                    2023,
                    10,
                    1,
                    12,
                    28,
                    36,
                    6,
                    274,
                    0
                ],
                "title": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large\n  Language Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive ability in handling\nreasoning tasks. However, unlike humans who can instinctively adapt their\nproblem-solving strategies to the complexity of task, most LLM-based methods\nadopt a one-size-fits-all approach. These methods employ consistent models,\nsample sizes, prompting methods and levels of problem decomposition, regardless\nof the problem complexity. The inflexibility of these methods can bring\nunnecessary computational overhead or sub-optimal performance. To address this\nlimitation, we introduce an Adaptive-Solver (AS) framework tha dynamically\nadapts solving strategies to suit various problems, enabling the flexible\nallocation of test-time computational resources. The framework functions with\ntwo primary modules. The initial evaluation module assesses the reliability of\nthe current solution using answer consistency. If the solution is deemed\nunreliable, the subsequent adaptation module comes into play. Within this\nmodule, various types of adaptation strategies are employed collaboratively.\nThrough such dynamic and multi-faceted adaptations, our framework can help\nreduce computational consumption and improve performance. Experimental results\nfrom complex reasoning benchmarks reveal that our method can significantly\nreduce API costs (up to 85%) while maintaining original performance.\nAlternatively, it achieves up to 4.5% higher accuracy compared to the baselines\nat the same cost. The code and dataset are available at\nhttps://github.com/john1226966735/Adaptive-Solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive ability in handling\nreasoning tasks. However, unlike humans who can instinctively adapt their\nproblem-solving strategies to the complexity of task, most LLM-based methods\nadopt a one-size-fits-all approach. These methods employ consistent models,\nsample sizes, prompting methods and levels of problem decomposition, regardless\nof the problem complexity. The inflexibility of these methods can bring\nunnecessary computational overhead or sub-optimal performance. To address this\nlimitation, we introduce an Adaptive-Solver (AS) framework tha dynamically\nadapts solving strategies to suit various problems, enabling the flexible\nallocation of test-time computational resources. The framework functions with\ntwo primary modules. The initial evaluation module assesses the reliability of\nthe current solution using answer consistency. If the solution is deemed\nunreliable, the subsequent adaptation module comes into play. Within this\nmodule, various types of adaptation strategies are employed collaboratively.\nThrough such dynamic and multi-faceted adaptations, our framework can help\nreduce computational consumption and improve performance. Experimental results\nfrom complex reasoning benchmarks reveal that our method can significantly\nreduce API costs (up to 85%) while maintaining original performance.\nAlternatively, it achieves up to 4.5% higher accuracy compared to the baselines\nat the same cost. The code and dataset are available at\nhttps://github.com/john1226966735/Adaptive-Solver."
                },
                "authors": [
                    {
                        "name": "Jianpeng Zhou"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "arxiv_comment": "Accepted by Information Processing & Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00459v2",
                "updated": "2024-12-23T08:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    25,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-11-01T09:14:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    14,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques"
                },
                "summary": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of technology, large language models (LLMs) have\nachieved remarkable performance across various natural language processing\n(NLP) tasks, powering LLM-integrated applications like Microsoft Copilot.\nHowever, as LLMs continue to evolve, new vulnerabilities, especially prompt\ninjection attacks arise. These attacks trick LLMs into deviating from the\noriginal input instructions and executing the attacker's instructions injected\nin data content, such as retrieved results. Recent attack methods leverage\nLLMs' instruction-following abilities and their inabilities to distinguish\ninstructions injected in the data content, and achieve a high attack success\nrate (ASR). When comparing the attack and defense methods, we interestingly\nfind that they share similar design goals, of inducing the model to ignore\nunwanted instructions and instead to execute wanted instructions. Therefore, we\nraise an intuitive question: Could these attack techniques be utilized for\ndefensive purposes? In this paper, we invert the intention of prompt injection\nmethods to develop novel defense methods based on previous training-free attack\nmethods, by repeating the attack process but with the original input\ninstruction rather than the injected instruction. Our comprehensive experiments\ndemonstrate that our defense techniques outperform existing training-free\ndefense approaches, achieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dekai Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15947v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15947v5",
                "updated": "2024-12-23T08:05:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    5,
                    14,
                    0,
                    358,
                    0
                ],
                "published": "2024-01-29T08:13:40Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    8,
                    13,
                    40,
                    0,
                    29,
                    0
                ],
                "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"
                },
                "summary": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA."
                },
                "authors": [
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Zhenyu Tang"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Junwu Zhang"
                    },
                    {
                        "name": "Yatian Pang"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Munan Ning"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "update author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15947v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15947v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17365v1",
                "updated": "2024-12-23T08:01:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    1,
                    24,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T08:01:24Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    1,
                    24,
                    0,
                    358,
                    0
                ],
                "title": "Boosting LLM via Learning from Data Iteratively and Selectively",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting LLM via Learning from Data Iteratively and Selectively"
                },
                "summary": "Datasets nowadays are generally constructed from multiple sources and using\ndifferent synthetic techniques, making data de-noising and de-duplication\ncrucial before being used for post-training. In this work, we propose to\nperform instruction tuning by iterative data selection (\\ApproachName{}). We\nmeasure the quality of a sample from complexity and diversity simultaneously.\nInstead of calculating the complexity score once for all before fine-tuning, we\nhighlight the importance of updating this model-specific score during\nfine-tuning to accurately accommodate the dynamic changes of the model. On the\nother hand, the diversity score is defined on top of the samples' responses\nunder the consideration of their informativeness. IterIT integrates the\nstrengths of both worlds by iteratively updating the complexity score for the\ntop-ranked samples and greedily selecting the ones with the highest\ncomplexity-diversity score. Experiments on multiple instruction-tuning data\ndemonstrate consistent improvements of IterIT over strong baselines. Moreover,\nour approach also generalizes well to domain-specific scenarios and different\nbackbone models. All resources will be available at\nhttps://github.com/JiaQiSJTU/IterIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datasets nowadays are generally constructed from multiple sources and using\ndifferent synthetic techniques, making data de-noising and de-duplication\ncrucial before being used for post-training. In this work, we propose to\nperform instruction tuning by iterative data selection (\\ApproachName{}). We\nmeasure the quality of a sample from complexity and diversity simultaneously.\nInstead of calculating the complexity score once for all before fine-tuning, we\nhighlight the importance of updating this model-specific score during\nfine-tuning to accurately accommodate the dynamic changes of the model. On the\nother hand, the diversity score is defined on top of the samples' responses\nunder the consideration of their informativeness. IterIT integrates the\nstrengths of both worlds by iteratively updating the complexity score for the\ntop-ranked samples and greedily selecting the ones with the highest\ncomplexity-diversity score. Experiments on multiple instruction-tuning data\ndemonstrate consistent improvements of IterIT over strong baselines. Moreover,\nour approach also generalizes well to domain-specific scenarios and different\nbackbone models. All resources will be available at\nhttps://github.com/JiaQiSJTU/IterIT."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Siyu Ren"
                    },
                    {
                        "name": "Ziheng Qin"
                    },
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.13486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.13486v2",
                "updated": "2024-12-23T07:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    29,
                    49,
                    0,
                    358,
                    0
                ],
                "published": "2022-08-29T10:40:58Z",
                "published_parsed": [
                    2022,
                    8,
                    29,
                    10,
                    40,
                    58,
                    0,
                    241,
                    0
                ],
                "title": "naab: A ready-to-use plug-and-play corpus for Farsi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "naab: A ready-to-use plug-and-play corpus for Farsi"
                },
                "summary": "The rise of large language models (LLMs) has transformed numerous natural\nlanguage processing (NLP) tasks, yet their performance in low and mid-resource\nlanguages, such as Farsi, still lags behind resource-rich languages like\nEnglish. To address this gap, we introduce naab, the largest publicly\navailable, cleaned, and ready-to-use Farsi textual corpus. naab consists of\n130GB of data, comprising over 250 million paragraphs and 15 billion words.\nNamed after the Farsi word NAAB (meaning \"pure\" or \"high-grade\"), this corpus\nis openly accessible via Hugging Face, offering researchers a valuable resource\nfor Farsi NLP tasks. In addition to naab, we provide naab-raw, an unprocessed\nversion of the dataset, along with a pre-processing toolkit that allows users\nto clean their custom corpora. These resources empower NLP researchers and\npractitioners, particularly those focusing on low-resource languages, to\nimprove the performance of LLMs in their respective domains and bridge the gap\nbetween resource-rich and resource-poor languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has transformed numerous natural\nlanguage processing (NLP) tasks, yet their performance in low and mid-resource\nlanguages, such as Farsi, still lags behind resource-rich languages like\nEnglish. To address this gap, we introduce naab, the largest publicly\navailable, cleaned, and ready-to-use Farsi textual corpus. naab consists of\n130GB of data, comprising over 250 million paragraphs and 15 billion words.\nNamed after the Farsi word NAAB (meaning \"pure\" or \"high-grade\"), this corpus\nis openly accessible via Hugging Face, offering researchers a valuable resource\nfor Farsi NLP tasks. In addition to naab, we provide naab-raw, an unprocessed\nversion of the dataset, along with a pre-processing toolkit that allows users\nto clean their custom corpora. These resources empower NLP researchers and\npractitioners, particularly those focusing on low-resource languages, to\nimprove the performance of LLMs in their respective domains and bridge the gap\nbetween resource-rich and resource-poor languages."
                },
                "authors": [
                    {
                        "name": "Sadra Sabouri"
                    },
                    {
                        "name": "Elnaz Rahmati"
                    },
                    {
                        "name": "Soroush Gooran"
                    },
                    {
                        "name": "Hossein Sameti"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Sameti"
                },
                "author": "Hossein Sameti",
                "arxiv_doi": "10.22034/jaiai.2024.480062.1016",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22034/jaiai.2024.480062.1016",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2208.13486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.13486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Artificial Intelligence, Applications and Innovations\n  (2024) 1-8",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07790v2",
                "updated": "2024-12-23T07:20:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    20,
                    3,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-12T06:50:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17332v1",
                "updated": "2024-12-23T06:50:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    50,
                    4,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T06:50:04Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    50,
                    4,
                    0,
                    358,
                    0
                ],
                "title": "A Dual-Perspective Metaphor Detection Framework Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Perspective Metaphor Detection Framework Using Large Language\n  Models"
                },
                "summary": "Metaphor detection, a critical task in natural language processing, involves\nidentifying whether a particular word in a sentence is used metaphorically.\nTraditional approaches often rely on supervised learning models that implicitly\nencode semantic relationships based on metaphor theories. However, these\nmethods often suffer from a lack of transparency in their decision-making\nprocesses, which undermines the reliability of their predictions. Recent\nresearch indicates that LLMs (large language models) exhibit significant\npotential in metaphor detection. Nevertheless, their reasoning capabilities are\nconstrained by predefined knowledge graphs. To overcome these limitations, we\npropose DMD, a novel dual-perspective framework that harnesses both implicit\nand explicit applications of metaphor theories to guide LLMs in metaphor\ndetection and adopts a self-judgment mechanism to validate the responses from\nthe aforementioned forms of guidance. In comparison to previous methods, our\nframework offers more transparent reasoning processes and delivers more\nreliable predictions. Experimental results prove the effectiveness of DMD,\ndemonstrating state-of-the-art performance across widely-used datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor detection, a critical task in natural language processing, involves\nidentifying whether a particular word in a sentence is used metaphorically.\nTraditional approaches often rely on supervised learning models that implicitly\nencode semantic relationships based on metaphor theories. However, these\nmethods often suffer from a lack of transparency in their decision-making\nprocesses, which undermines the reliability of their predictions. Recent\nresearch indicates that LLMs (large language models) exhibit significant\npotential in metaphor detection. Nevertheless, their reasoning capabilities are\nconstrained by predefined knowledge graphs. To overcome these limitations, we\npropose DMD, a novel dual-perspective framework that harnesses both implicit\nand explicit applications of metaphor theories to guide LLMs in metaphor\ndetection and adopts a self-judgment mechanism to validate the responses from\nthe aforementioned forms of guidance. In comparison to previous methods, our\nframework offers more transparent reasoning processes and delivers more\nreliable predictions. Experimental results prove the effectiveness of DMD,\ndemonstrating state-of-the-art performance across widely-used datasets."
                },
                "authors": [
                    {
                        "name": "Yujie Lin"
                    },
                    {
                        "name": "Jingyao Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Ante Wang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04344v3",
                "updated": "2024-12-23T06:46:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    46,
                    59,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-06T19:27:48Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    19,
                    27,
                    48,
                    1,
                    37,
                    0
                ],
                "title": "Does confidence calibration improve conformal prediction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does confidence calibration improve conformal prediction?"
                },
                "summary": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Previous works often employ temperature scaling to\ncalibrate classifiers, assuming that confidence calibration benefits conformal\nprediction. However, the specific impact of confidence calibration on conformal\nprediction remains underexplored. In this work, we make two key discoveries\nabout the impact of confidence calibration methods on adaptive conformal\nprediction. Firstly, we empirically show that current confidence calibration\nmethods (e.g., temperature scaling) typically lead to larger prediction sets in\nadaptive conformal prediction. Secondly, by investigating the role of\ntemperature value, we observe that high-confidence predictions can enhance the\nefficiency of adaptive conformal prediction. Theoretically, we prove that\npredictions with higher confidence result in smaller prediction sets on\nexpectation. This finding implies that the rescaling parameters in these\ncalibration methods, when optimized with cross-entropy loss, might counteract\nthe goal of generating efficient prediction sets. To address this issue, we\npropose Conformal Temperature Scaling (ConfTS), a variant of temperature\nscaling with a novel loss function designed to enhance the efficiency of\nprediction sets. This approach can be extended to optimize the parameters of\nother post-hoc methods of confidence calibration. Extensive experiments\ndemonstrate that our method improves existing adaptive conformal prediction\nmethods in classification tasks, especially with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction is an emerging technique for uncertainty quantification\nthat constructs prediction sets guaranteed to contain the true label with a\npredefined probability. Previous works often employ temperature scaling to\ncalibrate classifiers, assuming that confidence calibration benefits conformal\nprediction. However, the specific impact of confidence calibration on conformal\nprediction remains underexplored. In this work, we make two key discoveries\nabout the impact of confidence calibration methods on adaptive conformal\nprediction. Firstly, we empirically show that current confidence calibration\nmethods (e.g., temperature scaling) typically lead to larger prediction sets in\nadaptive conformal prediction. Secondly, by investigating the role of\ntemperature value, we observe that high-confidence predictions can enhance the\nefficiency of adaptive conformal prediction. Theoretically, we prove that\npredictions with higher confidence result in smaller prediction sets on\nexpectation. This finding implies that the rescaling parameters in these\ncalibration methods, when optimized with cross-entropy loss, might counteract\nthe goal of generating efficient prediction sets. To address this issue, we\npropose Conformal Temperature Scaling (ConfTS), a variant of temperature\nscaling with a novel loss function designed to enhance the efficiency of\nprediction sets. This approach can be extended to optimize the parameters of\nother post-hoc methods of confidence calibration. Extensive experiments\ndemonstrate that our method improves existing adaptive conformal prediction\nmethods in classification tasks, especially with LLMs."
                },
                "authors": [
                    {
                        "name": "Huajun Xi"
                    },
                    {
                        "name": "Jianguo Huang"
                    },
                    {
                        "name": "Kangdao Liu"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17325v1",
                "updated": "2024-12-23T06:34:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    34,
                    23,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T06:34:23Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    34,
                    23,
                    0,
                    358,
                    0
                ],
                "title": "Feature Based Methods Domain Adaptation for Object Detection: A Review\n  Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Based Methods Domain Adaptation for Object Detection: A Review\n  Paper"
                },
                "summary": "Domain adaptation, a pivotal branch of transfer learning, aims to enhance the\nperformance of machine learning models when deployed in target domains with\ndistinct data distributions. This is particularly critical for object detection\ntasks, where domain shifts (caused by factors such as lighting conditions,\nviewing angles, and environmental variations) can lead to significant\nperformance degradation. This review delves into advanced methodologies for\ndomain adaptation, including adversarial learning, discrepancy-based,\nmulti-domain, teacher-student, ensemble, and VLM techniques, emphasizing their\nefficacy in reducing domain gaps and enhancing model robustness. Feature-based\nmethods have emerged as powerful tools for addressing these challenges by\nharmonizing feature representations across domains. These techniques, such as\nFeature Alignment, Feature Augmentation/Reconstruction, and Feature\nTransformation, are employed alongside or as integral parts of other domain\nadaptation strategies to minimize domain gaps and improve model performance.\nSpecial attention is given to strategies that minimize the reliance on\nextensive labeled data and using unlabeled data, particularly in scenarios\ninvolving synthetic-to-real domain shifts. Applications in fields such as\nautonomous driving and medical imaging are explored, showcasing the potential\nof these methods to ensure reliable object detection in diverse and complex\nsettings. By providing a thorough analysis of state-of-the-art techniques,\nchallenges, and future directions, this work offers a valuable reference for\nresearchers striving to develop resilient and adaptable object detection\nframeworks, advancing the seamless deployment of artificial intelligence in\ndynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain adaptation, a pivotal branch of transfer learning, aims to enhance the\nperformance of machine learning models when deployed in target domains with\ndistinct data distributions. This is particularly critical for object detection\ntasks, where domain shifts (caused by factors such as lighting conditions,\nviewing angles, and environmental variations) can lead to significant\nperformance degradation. This review delves into advanced methodologies for\ndomain adaptation, including adversarial learning, discrepancy-based,\nmulti-domain, teacher-student, ensemble, and VLM techniques, emphasizing their\nefficacy in reducing domain gaps and enhancing model robustness. Feature-based\nmethods have emerged as powerful tools for addressing these challenges by\nharmonizing feature representations across domains. These techniques, such as\nFeature Alignment, Feature Augmentation/Reconstruction, and Feature\nTransformation, are employed alongside or as integral parts of other domain\nadaptation strategies to minimize domain gaps and improve model performance.\nSpecial attention is given to strategies that minimize the reliance on\nextensive labeled data and using unlabeled data, particularly in scenarios\ninvolving synthetic-to-real domain shifts. Applications in fields such as\nautonomous driving and medical imaging are explored, showcasing the potential\nof these methods to ensure reliable object detection in diverse and complex\nsettings. By providing a thorough analysis of state-of-the-art techniques,\nchallenges, and future directions, this work offers a valuable reference for\nresearchers striving to develop resilient and adaptable object detection\nframeworks, advancing the seamless deployment of artificial intelligence in\ndynamic environments."
                },
                "authors": [
                    {
                        "name": "Helia Mohamadi"
                    },
                    {
                        "name": "Mohammad Ali Keyvanrad"
                    },
                    {
                        "name": "Mohammad Reza Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reza Mohammadi"
                },
                "author": "Mohammad Reza Mohammadi",
                "arxiv_comment": "46 pages, 13 figures, It will be submitted to a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17321v1",
                "updated": "2024-12-23T06:29:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    29,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T06:29:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    29,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Assessing Human Editing Effort on LLM-Generated Texts via\n  Compression-Based Edit Distance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Human Editing Effort on LLM-Generated Texts via\n  Compression-Based Edit Distance"
                },
                "summary": "Assessing the extent of human edits on texts generated by Large Language\nModels (LLMs) is crucial to understanding the human-AI interactions and\nimproving the quality of automated text generation systems. Existing edit\ndistance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to\naccurately measure the effort required for post-editing, especially when edits\ninvolve substantial modifications, such as block operations. In this paper, we\nintroduce a novel compression-based edit distance metric grounded in the\nLempel-Ziv-77 algorithm, designed to quantify the amount of post-editing\napplied to LLM-generated texts. Our method leverages the properties of text\ncompression to measure the informational difference between the original and\nedited texts. Through experiments on real-world human edits datasets, we\ndemonstrate that our proposed metric is highly correlated with actual edit time\nand effort. We also show that LLMs exhibit an implicit understanding of editing\nspeed, that aligns well with our metric. Furthermore, we compare our metric\nwith existing ones, highlighting its advantages in capturing complex edits with\nlinear computational efficiency. Our code and data are available at:\nhttps://github.com/NDV-tiime/CompressionDistance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the extent of human edits on texts generated by Large Language\nModels (LLMs) is crucial to understanding the human-AI interactions and\nimproving the quality of automated text generation systems. Existing edit\ndistance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to\naccurately measure the effort required for post-editing, especially when edits\ninvolve substantial modifications, such as block operations. In this paper, we\nintroduce a novel compression-based edit distance metric grounded in the\nLempel-Ziv-77 algorithm, designed to quantify the amount of post-editing\napplied to LLM-generated texts. Our method leverages the properties of text\ncompression to measure the informational difference between the original and\nedited texts. Through experiments on real-world human edits datasets, we\ndemonstrate that our proposed metric is highly correlated with actual edit time\nand effort. We also show that LLMs exhibit an implicit understanding of editing\nspeed, that aligns well with our metric. Furthermore, we compare our metric\nwith existing ones, highlighting its advantages in capturing complex edits with\nlinear computational efficiency. Our code and data are available at:\nhttps://github.com/NDV-tiime/CompressionDistance"
                },
                "authors": [
                    {
                        "name": "Nicolas Devatine"
                    },
                    {
                        "name": "Louis Abraham"
                    }
                ],
                "author_detail": {
                    "name": "Louis Abraham"
                },
                "author": "Louis Abraham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17315v1",
                "updated": "2024-12-23T06:17:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    17,
                    11,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T06:17:11Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    17,
                    11,
                    0,
                    358,
                    0
                ],
                "title": "CodeV: Issue Resolving with Visual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeV: Issue Resolving with Visual Data"
                },
                "summary": "Large Language Models (LLMs) have advanced rapidly in recent years, with\ntheir applications in software engineering expanding to more complex\nrepository-level tasks. GitHub issue resolving is a key challenge among these\ntasks. While recent approaches have made progress on this task, they focus on\ntextual data within issues, neglecting visual data. However, this visual data\nis crucial for resolving issues as it conveys additional knowledge that text\nalone cannot. We propose CodeV, the first approach to leveraging visual data to\nenhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by\nfollowing a two-phase process: data processing and patch generation. To\nevaluate CodeV, we construct a benchmark for visual issue resolving, namely\nVisual SWE-bench. Through extensive experiments, we demonstrate the\neffectiveness of CodeV, as well as provide valuable insights into leveraging\nvisual data to resolve GitHub issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have advanced rapidly in recent years, with\ntheir applications in software engineering expanding to more complex\nrepository-level tasks. GitHub issue resolving is a key challenge among these\ntasks. While recent approaches have made progress on this task, they focus on\ntextual data within issues, neglecting visual data. However, this visual data\nis crucial for resolving issues as it conveys additional knowledge that text\nalone cannot. We propose CodeV, the first approach to leveraging visual data to\nenhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by\nfollowing a two-phase process: data processing and patch generation. To\nevaluate CodeV, we construct a benchmark for visual issue resolving, namely\nVisual SWE-bench. Through extensive experiments, we demonstrate the\neffectiveness of CodeV, as well as provide valuable insights into leveraging\nvisual data to resolve GitHub issues."
                },
                "authors": [
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Quanshun Yang"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yongshun Gong"
                    },
                    {
                        "name": "Pengjie Huang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Guangtai Liang"
                    },
                    {
                        "name": "Lizhen Cui"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "https://github.com/luolin101/CodeV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15673v2",
                "updated": "2024-12-23T06:03:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    6,
                    3,
                    31,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-21T22:29:40Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    22,
                    29,
                    40,
                    4,
                    173,
                    0
                ],
                "title": "Large Language Models have Intrinsic Self-Correction Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have Intrinsic Self-Correction Ability"
                },
                "summary": "Large language models (LLMs) have attracted significant attention for their\nexceptional abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention for their\nexceptional abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential."
                },
                "authors": [
                    {
                        "name": "Dancheng Liu"
                    },
                    {
                        "name": "Amir Nassereldine"
                    },
                    {
                        "name": "Ziming Yang"
                    },
                    {
                        "name": "Chenhui Xu"
                    },
                    {
                        "name": "Yuting Hu"
                    },
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Utkarsh Kumar"
                    },
                    {
                        "name": "Changjae Lee"
                    },
                    {
                        "name": "Ruiyang Qin"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Jinjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Jinjun Xiong"
                },
                "author": "Jinjun Xiong",
                "arxiv_comment": "in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17304v1",
                "updated": "2024-12-23T05:52:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    52,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T05:52:17Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    52,
                    17,
                    0,
                    358,
                    0
                ],
                "title": "On the Feasibility of Vision-Language Models for Time-Series\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Feasibility of Vision-Language Models for Time-Series\n  Classification"
                },
                "summary": "We build upon time-series classification by leveraging the capabilities of\nVision Language Models (VLMs). We find that VLMs produce competitive results\nafter two or less epochs of fine-tuning. We develop a novel approach that\nincorporates graphical data representations as images in conjunction with\nnumerical data. This approach is rooted in the hypothesis that graphical\nrepresentations can provide additional contextual information that numerical\ndata alone may not capture. Additionally, providing a graphical representation\ncan circumvent issues such as limited context length faced by LLMs. To further\nadvance this work, we implemented a scalable end-to-end pipeline for training\non different scenarios, allowing us to isolate the most effective strategies\nfor transferring learning capabilities from LLMs to Time Series Classification\n(TSC) tasks. Our approach works with univariate and multivariate time-series\ndata. In addition, we conduct extensive and practical experiments to show how\nthis approach works for time-series classification and generative labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We build upon time-series classification by leveraging the capabilities of\nVision Language Models (VLMs). We find that VLMs produce competitive results\nafter two or less epochs of fine-tuning. We develop a novel approach that\nincorporates graphical data representations as images in conjunction with\nnumerical data. This approach is rooted in the hypothesis that graphical\nrepresentations can provide additional contextual information that numerical\ndata alone may not capture. Additionally, providing a graphical representation\ncan circumvent issues such as limited context length faced by LLMs. To further\nadvance this work, we implemented a scalable end-to-end pipeline for training\non different scenarios, allowing us to isolate the most effective strategies\nfor transferring learning capabilities from LLMs to Time Series Classification\n(TSC) tasks. Our approach works with univariate and multivariate time-series\ndata. In addition, we conduct extensive and practical experiments to show how\nthis approach works for time-series classification and generative labels."
                },
                "authors": [
                    {
                        "name": "Vinay Prithyani"
                    },
                    {
                        "name": "Mohsin Mohammed"
                    },
                    {
                        "name": "Richa Gadgil"
                    },
                    {
                        "name": "Ricardo Buitrago"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02632v2",
                "updated": "2024-12-23T05:44:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    44,
                    30,
                    0,
                    358,
                    0
                ],
                "published": "2024-08-05T16:55:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    16,
                    55,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models. Our code and datasets are released\nat https://SEAS-LLM.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models. Our code and datasets are released\nat https://SEAS-LLM.github.io/."
                },
                "authors": [
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Rumei Li"
                    },
                    {
                        "name": "Shiyang Liu"
                    },
                    {
                        "name": "Guogang Liao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02746v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02746v5",
                "updated": "2024-12-23T05:43:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    43,
                    1,
                    0,
                    358,
                    0
                ],
                "published": "2024-06-04T20:02:52Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    20,
                    2,
                    52,
                    1,
                    156,
                    0
                ],
                "title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) gain substantial reasoning and decision-making\ncapabilities from thought structures. However, existing methods such as Tree of\nThought and Retrieval Augmented Thoughts often fall short in complex tasks due\nto the limitations of insufficient local retrieval of factual knowledge and\ninadequate global selection of strategies. These limitations make it\nchallenging for these methods to balance factual accuracy and comprehensive\nlogical optimization effectively. To address these limitations, we introduce\nthe Retrieval Augmented Thought Tree (RATT), a novel thought structure that\nconsiders both overall logical soundness and factual correctness at each step\nof the thinking process. Specifically, at every point of a thought branch, RATT\nperforms planning and lookahead to explore and evaluate multiple potential\nreasoning steps, and integrate the fact-checking ability of Retrieval-Augmented\nGeneration (RAG) with LLM's ability to assess overall strategy. Through this\ncombination of factual knowledge and strategic feasibility, the RATT adjusts\nand integrates the thought tree structure to search for the most promising\nbranches within the search space. This thought structure significantly enhances\nthe model's coherence in logical inference and efficiency in decision-making,\nand thus increases the limit of the capacity of LLM to generate reliable\ninferences and decisions based on thought structures. A broad range of\nexperiments on different types of tasks showcases that the RATT structure\nsignificantly outperforms existing methods in factual correctness and logical\ncoherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) gain substantial reasoning and decision-making\ncapabilities from thought structures. However, existing methods such as Tree of\nThought and Retrieval Augmented Thoughts often fall short in complex tasks due\nto the limitations of insufficient local retrieval of factual knowledge and\ninadequate global selection of strategies. These limitations make it\nchallenging for these methods to balance factual accuracy and comprehensive\nlogical optimization effectively. To address these limitations, we introduce\nthe Retrieval Augmented Thought Tree (RATT), a novel thought structure that\nconsiders both overall logical soundness and factual correctness at each step\nof the thinking process. Specifically, at every point of a thought branch, RATT\nperforms planning and lookahead to explore and evaluate multiple potential\nreasoning steps, and integrate the fact-checking ability of Retrieval-Augmented\nGeneration (RAG) with LLM's ability to assess overall strategy. Through this\ncombination of factual knowledge and strategic feasibility, the RATT adjusts\nand integrates the thought tree structure to search for the most promising\nbranches within the search space. This thought structure significantly enhances\nthe model's coherence in logical inference and efficiency in decision-making,\nand thus increases the limit of the capacity of LLM to generate reliable\ninferences and decisions based on thought structures. A broad range of\nexperiments on different types of tasks showcases that the RATT structure\nsignificantly outperforms existing methods in factual correctness and logical\ncoherence."
                },
                "authors": [
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Weijieying Ren"
                    },
                    {
                        "name": "Lu Jiang"
                    },
                    {
                        "name": "Dongjie Wang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02746v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02746v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17298v1",
                "updated": "2024-12-23T05:41:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    41,
                    1,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T05:41:01Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    41,
                    1,
                    0,
                    358,
                    0
                ],
                "title": "Prompting in the Wild: An Empirical Study of Prompt Evolution in\n  Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting in the Wild: An Empirical Study of Prompt Evolution in\n  Software Repositories"
                },
                "summary": "The adoption of Large Language Models (LLMs) is reshaping software\ndevelopment as developers integrate these LLMs into their applications. In such\napplications, prompts serve as the primary means of interacting with LLMs.\nDespite the widespread use of LLM-integrated applications, there is limited\nunderstanding of how developers manage and evolve prompts. This study presents\nthe first empirical analysis of prompt evolution in LLM-integrated software\ndevelopment. We analyzed 1,262 prompt changes across 243 GitHub repositories to\ninvestigate the patterns and frequencies of prompt changes, their relationship\nwith code changes, documentation practices, and their impact on system\nbehavior. Our findings show that developers primarily evolve prompts through\nadditions and modifications, with most changes occurring during feature\ndevelopment. We identified key challenges in prompt engineering: only 21.9\\% of\nprompt changes are documented in commit messages, changes can introduce logical\ninconsistencies, and misalignment often occurs between prompt changes and LLM\nresponses. These insights emphasize the need for specialized testing\nframeworks, automated validation tools, and improved documentation practices to\nenhance the reliability of LLM-integrated applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) is reshaping software\ndevelopment as developers integrate these LLMs into their applications. In such\napplications, prompts serve as the primary means of interacting with LLMs.\nDespite the widespread use of LLM-integrated applications, there is limited\nunderstanding of how developers manage and evolve prompts. This study presents\nthe first empirical analysis of prompt evolution in LLM-integrated software\ndevelopment. We analyzed 1,262 prompt changes across 243 GitHub repositories to\ninvestigate the patterns and frequencies of prompt changes, their relationship\nwith code changes, documentation practices, and their impact on system\nbehavior. Our findings show that developers primarily evolve prompts through\nadditions and modifications, with most changes occurring during feature\ndevelopment. We identified key challenges in prompt engineering: only 21.9\\% of\nprompt changes are documented in commit messages, changes can introduce logical\ninconsistencies, and misalignment often occurs between prompt changes and LLM\nresponses. These insights emphasize the need for specialized testing\nframeworks, automated validation tools, and improved documentation practices to\nenhance the reliability of LLM-integrated applications."
                },
                "authors": [
                    {
                        "name": "Mahan Tafreshipour"
                    },
                    {
                        "name": "Aaron Imani"
                    },
                    {
                        "name": "Eric Huang"
                    },
                    {
                        "name": "Eduardo Almeida"
                    },
                    {
                        "name": "Thomas Zimmermann"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10713v2",
                "updated": "2024-12-23T05:29:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    29,
                    17,
                    0,
                    358,
                    0
                ],
                "published": "2024-09-16T20:33:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    20,
                    33,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "\"The Data Says Otherwise\"-Towards Automated Fact-checking and\n  Communication of Data Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"The Data Says Otherwise\"-Towards Automated Fact-checking and\n  Communication of Data Claims"
                },
                "summary": "Fact-checking data claims requires data evidence retrieval and analysis,\nwhich can become tedious and intractable when done manually. This work presents\nAletheia, an automated fact-checking prototype designed to facilitate data\nclaims verification and enhance data evidence communication. For verification,\nwe utilize a pre-trained LLM to parse the semantics for evidence retrieval. To\neffectively communicate the data evidence, we design representations in two\nforms: data tables and visualizations, tailored to various data fact types.\nAdditionally, we design interactions that showcase a real-world application of\nthese techniques. We evaluate the performance of two core NLP tasks with a\ncurated dataset comprising 400 data claims and compare the two representation\nforms regarding viewers' assessment time, confidence, and preference via a user\nstudy with 20 participants. The evaluation offers insights into the feasibility\nand bottlenecks of using LLMs for data fact-checking tasks, potential\nadvantages and disadvantages of using visualizations over data tables, and\ndesign recommendations for presenting data evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking data claims requires data evidence retrieval and analysis,\nwhich can become tedious and intractable when done manually. This work presents\nAletheia, an automated fact-checking prototype designed to facilitate data\nclaims verification and enhance data evidence communication. For verification,\nwe utilize a pre-trained LLM to parse the semantics for evidence retrieval. To\neffectively communicate the data evidence, we design representations in two\nforms: data tables and visualizations, tailored to various data fact types.\nAdditionally, we design interactions that showcase a real-world application of\nthese techniques. We evaluate the performance of two core NLP tasks with a\ncurated dataset comprising 400 data claims and compare the two representation\nforms regarding viewers' assessment time, confidence, and preference via a user\nstudy with 20 participants. The evaluation offers insights into the feasibility\nand bottlenecks of using LLMs for data fact-checking tasks, potential\nadvantages and disadvantages of using visualizations over data tables, and\ndesign recommendations for presenting data evidence."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Shunan Guo"
                    },
                    {
                        "name": "Jane Hoffswell"
                    },
                    {
                        "name": "Victor S. Bursztyn"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "John Stasko"
                    }
                ],
                "author_detail": {
                    "name": "John Stasko"
                },
                "author": "John Stasko",
                "arxiv_doi": "10.1145/3654777.3676359",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654777.3676359",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 13 figures, UIST 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.7.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17292v1",
                "updated": "2024-12-23T05:24:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    24,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T05:24:26Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    24,
                    26,
                    0,
                    358,
                    0
                ],
                "title": "AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues"
                },
                "summary": "In human communication, both verbal and non-verbal cues play a crucial role\nin conveying emotions, intentions, and meaning beyond words alone. These\nnon-linguistic information, such as facial expressions, eye contact, voice\ntone, and pitch, are fundamental elements of effective interactions, enriching\nconversations by adding emotional and contextual depth. Recognizing the\nimportance of non-linguistic content in communication, we present AV-EmoDialog,\na dialogue system designed to exploit verbal and non-verbal information from\nusers' audio-visual inputs to generate more responsive and empathetic\ninteractions. AV-EmoDialog systematically exploits the emotional cues in\naudio-visual dialogues; extracting speech content and emotional tones from\nspeech, analyzing fine-grained facial expressions from visuals, and integrating\nthese cues to generate emotionally aware responses in an end-to-end manner.\nThrough extensive experiments, we validate that the proposed AV-EmoDialog\noutperforms existing multimodal LLMs in generating not only emotionally\nappropriate but also contextually appropriate responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human communication, both verbal and non-verbal cues play a crucial role\nin conveying emotions, intentions, and meaning beyond words alone. These\nnon-linguistic information, such as facial expressions, eye contact, voice\ntone, and pitch, are fundamental elements of effective interactions, enriching\nconversations by adding emotional and contextual depth. Recognizing the\nimportance of non-linguistic content in communication, we present AV-EmoDialog,\na dialogue system designed to exploit verbal and non-verbal information from\nusers' audio-visual inputs to generate more responsive and empathetic\ninteractions. AV-EmoDialog systematically exploits the emotional cues in\naudio-visual dialogues; extracting speech content and emotional tones from\nspeech, analyzing fine-grained facial expressions from visuals, and integrating\nthese cues to generate emotionally aware responses in an end-to-end manner.\nThrough extensive experiments, we validate that the proposed AV-EmoDialog\noutperforms existing multimodal LLMs in generating not only emotionally\nappropriate but also contextually appropriate responses."
                },
                "authors": [
                    {
                        "name": "Se Jin Park"
                    },
                    {
                        "name": "Yeonju Kim"
                    },
                    {
                        "name": "Hyeongseop Rha"
                    },
                    {
                        "name": "Bella Godiva"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17288v1",
                "updated": "2024-12-23T05:20:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    20,
                    1,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T05:20:01Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    20,
                    1,
                    0,
                    358,
                    0
                ],
                "title": "Multi-Modal Grounded Planning and Efficient Replanning For Learning\n  Embodied Agents with A Few Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Grounded Planning and Efficient Replanning For Learning\n  Embodied Agents with A Few Examples"
                },
                "summary": "Learning a perception and reasoning module for robotic assistants to plan\nsteps to perform complex tasks based on natural language instructions often\nrequires large free-form language annotations, especially for short high-level\ninstructions. To reduce the cost of annotation, large language models (LLMs)\nare used as a planner with few data. However, when elaborating the steps, even\nthe state-of-the-art planner that uses LLMs mostly relies on linguistic common\nsense, often neglecting the status of the environment at command reception,\nresulting in inappropriate plans. To generate plans grounded in the\nenvironment, we propose FLARE (Few-shot Language with environmental Adaptive\nReplanning Embodied agent), which improves task planning using both language\ncommand and environmental perception. As language instructions often contain\nambiguities or incorrect expressions, we additionally propose to correct the\nmistakes using visual cues from the agent. The proposed scheme allows us to use\na few language pairs thanks to the visual cues and outperforms state-of-the-art\napproaches. Our code is available at https://github.com/snumprlab/flare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a perception and reasoning module for robotic assistants to plan\nsteps to perform complex tasks based on natural language instructions often\nrequires large free-form language annotations, especially for short high-level\ninstructions. To reduce the cost of annotation, large language models (LLMs)\nare used as a planner with few data. However, when elaborating the steps, even\nthe state-of-the-art planner that uses LLMs mostly relies on linguistic common\nsense, often neglecting the status of the environment at command reception,\nresulting in inappropriate plans. To generate plans grounded in the\nenvironment, we propose FLARE (Few-shot Language with environmental Adaptive\nReplanning Embodied agent), which improves task planning using both language\ncommand and environmental perception. As language instructions often contain\nambiguities or incorrect expressions, we additionally propose to correct the\nmistakes using visual cues from the agent. The proposed scheme allows us to use\na few language pairs thanks to the visual cues and outperforms state-of-the-art\napproaches. Our code is available at https://github.com/snumprlab/flare."
                },
                "authors": [
                    {
                        "name": "Taewoong Kim"
                    },
                    {
                        "name": "Byeonghwi Kim"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "arxiv_comment": "AAAI 2025 (Project page: https://twoongg.github.io/projects/flare/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17287v1",
                "updated": "2024-12-23T05:12:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    12,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T05:12:54Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    12,
                    54,
                    0,
                    358,
                    0
                ],
                "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4AD: A Platform for Algorithm Design with Large Language Model"
                },
                "summary": "We introduce LLM4AD, a unified Python platform for algorithm design (AD) with\nlarge language models (LLMs). LLM4AD is a generic framework with modularized\nblocks for search methods, algorithm design tasks, and LLM interface. The\nplatform integrates numerous key methods and supports a wide range of algorithm\ndesign tasks across various domains including optimization, machine learning,\nand scientific discovery. We have also designed a unified evaluation sandbox to\nensure a secure and robust assessment of algorithms. Additionally, we have\ncompiled a comprehensive suite of support resources, including tutorials,\nexamples, a user manual, online resources, and a dedicated graphical user\ninterface (GUI) to enhance the usage of LLM4AD. We believe this platform will\nserve as a valuable tool for fostering future development in the merging\nresearch direction of LLM-assisted algorithm design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLM4AD, a unified Python platform for algorithm design (AD) with\nlarge language models (LLMs). LLM4AD is a generic framework with modularized\nblocks for search methods, algorithm design tasks, and LLM interface. The\nplatform integrates numerous key methods and supports a wide range of algorithm\ndesign tasks across various domains including optimization, machine learning,\nand scientific discovery. We have also designed a unified evaluation sandbox to\nensure a secure and robust assessment of algorithms. Additionally, we have\ncompiled a comprehensive suite of support resources, including tutorials,\nexamples, a user manual, online resources, and a dedicated graphical user\ninterface (GUI) to enhance the usage of LLM4AD. We believe this platform will\nserve as a valuable tool for fostering future development in the merging\nresearch direction of LLM-assisted algorithm design."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18580v2",
                "updated": "2024-12-23T05:04:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    4,
                    49,
                    0,
                    358,
                    0
                ],
                "published": "2023-11-30T14:18:47Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    14,
                    18,
                    47,
                    3,
                    334,
                    0
                ],
                "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\n  Factuality, Fairness, Toxicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\n  Factuality, Fairness, Toxicity"
                },
                "summary": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research."
                },
                "authors": [
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Tianyun Liu"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "Accepted by KDD workshop on Evaluation and Trustworthiness of\n  Generative AI Models",
                "arxiv_journal_ref": "https://genai-evaluation-kdd2024.github.io/genai-evalution-kdd2024/assets/papers/GenAI_Evaluation_KDD2024_paper_5.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17270v1",
                "updated": "2024-12-23T04:34:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    4,
                    34,
                    55,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T04:34:55Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    4,
                    34,
                    55,
                    0,
                    358,
                    0
                ],
                "title": "AsymLLIC: Asymmetric Lightweight Learned Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymLLIC: Asymmetric Lightweight Learned Image Compression"
                },
                "summary": "Learned image compression (LIC) methods often employ symmetrical encoder and\ndecoder architectures, evitably increasing decoding time. However, practical\nscenarios demand an asymmetric design, where the decoder requires low\ncomplexity to cater to diverse low-end devices, while the encoder can\naccommodate higher complexity to improve coding performance. In this paper, we\npropose an asymmetric lightweight learned image compression (AsymLLIC)\narchitecture with a novel training scheme, enabling the gradual substitution of\ncomplex decoding modules with simpler ones. Building upon this approach, we\nconduct a comprehensive comparison of different decoder network structures to\nstrike a better trade-off between complexity and compression performance.\nExperiment results validate the efficiency of our proposed method, which not\nonly achieves comparable performance to VVC but also offers a lightweight\ndecoder with only 51.47 GMACs computation and 19.65M parameters. Furthermore,\nthis design methodology can be easily applied to any LIC models, enabling the\npractical deployment of LIC techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned image compression (LIC) methods often employ symmetrical encoder and\ndecoder architectures, evitably increasing decoding time. However, practical\nscenarios demand an asymmetric design, where the decoder requires low\ncomplexity to cater to diverse low-end devices, while the encoder can\naccommodate higher complexity to improve coding performance. In this paper, we\npropose an asymmetric lightweight learned image compression (AsymLLIC)\narchitecture with a novel training scheme, enabling the gradual substitution of\ncomplex decoding modules with simpler ones. Building upon this approach, we\nconduct a comprehensive comparison of different decoder network structures to\nstrike a better trade-off between complexity and compression performance.\nExperiment results validate the efficiency of our proposed method, which not\nonly achieves comparable performance to VVC but also offers a lightweight\ndecoder with only 51.47 GMACs computation and 19.65M parameters. Furthermore,\nthis design methodology can be easily applied to any LIC models, enabling the\npractical deployment of LIC techniques."
                },
                "authors": [
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Zhengxue Cheng"
                    },
                    {
                        "name": "Donghui Feng"
                    },
                    {
                        "name": "Guo Lu"
                    },
                    {
                        "name": "Li Song"
                    },
                    {
                        "name": "Wenjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Zhang"
                },
                "author": "Wenjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17259v1",
                "updated": "2024-12-23T04:02:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    4,
                    2,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T04:02:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    4,
                    2,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LegalAgentBench: Evaluating LLM Agents in Legal Domain"
                },
                "summary": "With the increasing intelligence and autonomy of LLM agents, their potential\napplications in the legal domain are becoming increasingly apparent. However,\nexisting general-domain benchmarks cannot fully capture the complexity and\nsubtle nuances of real-world judicial cognition and decision-making. Therefore,\nwe propose LegalAgentBench, a comprehensive benchmark specifically designed to\nevaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17\ncorpora from real-world legal scenarios and provides 37 tools for interacting\nwith external knowledge. We designed a scalable task construction framework and\ncarefully annotated 300 tasks. These tasks span various types, including\nmulti-hop reasoning and writing, and range across different difficulty levels,\neffectively reflecting the complexity of real-world legal scenarios. Moreover,\nbeyond evaluating final success, LegalAgentBench incorporates keyword analysis\nduring intermediate processes to calculate progress rates, enabling more\nfine-grained evaluation. We evaluated eight popular LLMs, highlighting the\nstrengths, limitations, and potential areas for improvement of existing models\nand methods. LegalAgentBench sets a new benchmark for the practical application\nof LLMs in the legal domain, with its code and data available at\n\\url{https://github.com/CSHaitao/LegalAgentBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing intelligence and autonomy of LLM agents, their potential\napplications in the legal domain are becoming increasingly apparent. However,\nexisting general-domain benchmarks cannot fully capture the complexity and\nsubtle nuances of real-world judicial cognition and decision-making. Therefore,\nwe propose LegalAgentBench, a comprehensive benchmark specifically designed to\nevaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17\ncorpora from real-world legal scenarios and provides 37 tools for interacting\nwith external knowledge. We designed a scalable task construction framework and\ncarefully annotated 300 tasks. These tasks span various types, including\nmulti-hop reasoning and writing, and range across different difficulty levels,\neffectively reflecting the complexity of real-world legal scenarios. Moreover,\nbeyond evaluating final success, LegalAgentBench incorporates keyword analysis\nduring intermediate processes to calculate progress rates, enabling more\nfine-grained evaluation. We evaluated eight popular LLMs, highlighting the\nstrengths, limitations, and potential areas for improvement of existing models\nand methods. LegalAgentBench sets a new benchmark for the practical application\nof LLMs in the legal domain, with its code and data available at\n\\url{https://github.com/CSHaitao/LegalAgentBench}."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Youfeng Liu"
                    },
                    {
                        "name": "Kai Lin"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Guozhi Yuan"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Wuyue Wang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17255v1",
                "updated": "2024-12-23T03:57:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    57,
                    45,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:57:45Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    57,
                    45,
                    0,
                    358,
                    0
                ],
                "title": "Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation:\n  A Multimodal Generative AI Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation:\n  A Multimodal Generative AI Approach"
                },
                "summary": "Emojis have become ubiquitous in online communication, serving as a universal\nmedium to convey emotions and decorative elements. Their widespread use\ntranscends language and cultural barriers, enhancing understanding and\nfostering more inclusive interactions. While existing work gained valuable\ninsight into emojis understanding, exploring emojis' capability to serve as a\nuniversal sentiment indicator leveraging large language models (LLMs) has not\nbeen thoroughly examined. Our study aims to investigate the capacity of emojis\nto serve as reliable sentiment markers through LLMs across languages and\ncultures. We leveraged the multimodal capabilities of ChatGPT to explore the\nsentiments of various representations of emojis and evaluated how well\nemoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset\ncollected from 32 countries. Our analysis reveals that the accuracy of\nLLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant\npotential to serve as a universal sentiment marker. We also found a consistent\ntrend that the accuracy of sentiment conveyed by emojis increased as the number\nof emojis grew in text. The results reinforce the potential of emojis to serve\nas global sentiment indicators, offering insight into fields such as\ncross-lingual and cross-cultural sentiment analysis on social media platforms.\nCode: https://github.com/ResponsibleAILab/emoji-universal-sentiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis have become ubiquitous in online communication, serving as a universal\nmedium to convey emotions and decorative elements. Their widespread use\ntranscends language and cultural barriers, enhancing understanding and\nfostering more inclusive interactions. While existing work gained valuable\ninsight into emojis understanding, exploring emojis' capability to serve as a\nuniversal sentiment indicator leveraging large language models (LLMs) has not\nbeen thoroughly examined. Our study aims to investigate the capacity of emojis\nto serve as reliable sentiment markers through LLMs across languages and\ncultures. We leveraged the multimodal capabilities of ChatGPT to explore the\nsentiments of various representations of emojis and evaluated how well\nemoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset\ncollected from 32 countries. Our analysis reveals that the accuracy of\nLLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant\npotential to serve as a universal sentiment marker. We also found a consistent\ntrend that the accuracy of sentiment conveyed by emojis increased as the number\nof emojis grew in text. The results reinforce the potential of emojis to serve\nas global sentiment indicators, offering insight into fields such as\ncross-lingual and cross-cultural sentiment analysis on social media platforms.\nCode: https://github.com/ResponsibleAILab/emoji-universal-sentiment."
                },
                "authors": [
                    {
                        "name": "Rafid Ishrak Jahan"
                    },
                    {
                        "name": "Heng Fan"
                    },
                    {
                        "name": "Haihua Chen"
                    },
                    {
                        "name": "Yunhe Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Feng"
                },
                "author": "Yunhe Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17250v1",
                "updated": "2024-12-23T03:49:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    49,
                    0,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:49:00Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    49,
                    0,
                    0,
                    358,
                    0
                ],
                "title": "SyNeg: LLM-Driven Synthetic Hard-Negatives for Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyNeg: LLM-Driven Synthetic Hard-Negatives for Dense Retrieval"
                },
                "summary": "The performance of Dense retrieval (DR) is significantly influenced by the\nquality of negative sampling. Traditional DR methods primarily depend on naive\nnegative sampling techniques or on mining hard negatives through external\nretriever and meticulously crafted strategies. However, naive negative sampling\noften fails to adequately capture the accurate boundaries between positive and\nnegative samples, whereas existing hard negative sampling methods are prone to\nfalse negatives, resulting in performance degradation and training instability.\nRecent advancements in large language models (LLMs) offer an innovative\nsolution to these challenges by generating contextually rich and diverse\nnegative samples. In this work, we present a framework that harnesses LLMs to\nsynthesize high-quality hard negative samples. We first devise a\n\\textit{multi-attribute self-reflection prompting strategy} to direct LLMs in\nhard negative sample generation. Then, we implement a \\textit{hybrid sampling\nstrategy} that integrates these synthetic negatives with traditionally\nretrieved negatives, thereby stabilizing the training process and improving\nretrieval performance. Extensive experiments on five benchmark datasets\ndemonstrate the efficacy of our approach, and code is also publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Dense retrieval (DR) is significantly influenced by the\nquality of negative sampling. Traditional DR methods primarily depend on naive\nnegative sampling techniques or on mining hard negatives through external\nretriever and meticulously crafted strategies. However, naive negative sampling\noften fails to adequately capture the accurate boundaries between positive and\nnegative samples, whereas existing hard negative sampling methods are prone to\nfalse negatives, resulting in performance degradation and training instability.\nRecent advancements in large language models (LLMs) offer an innovative\nsolution to these challenges by generating contextually rich and diverse\nnegative samples. In this work, we present a framework that harnesses LLMs to\nsynthesize high-quality hard negative samples. We first devise a\n\\textit{multi-attribute self-reflection prompting strategy} to direct LLMs in\nhard negative sample generation. Then, we implement a \\textit{hybrid sampling\nstrategy} that integrates these synthetic negatives with traditionally\nretrieved negatives, thereby stabilizing the training process and improving\nretrieval performance. Extensive experiments on five benchmark datasets\ndemonstrate the efficacy of our approach, and code is also publicly available."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17249v1",
                "updated": "2024-12-23T03:47:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    47,
                    54,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:47:54Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    47,
                    54,
                    0,
                    358,
                    0
                ],
                "title": "EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models\n  through Ensemble Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models\n  through Ensemble Modeling"
                },
                "summary": "With the widespread application of large language models (LLM), concerns\nabout the privacy leakage of model training data have increasingly become a\nfocus. Membership Inference Attacks (MIAs) have emerged as a critical tool for\nevaluating the privacy risks associated with these models. Although existing\nattack methods, such as LOSS, Reference-based, min-k, and zlib, perform well in\ncertain scenarios, their effectiveness on large pre-trained language models\noften approaches random guessing, particularly in the context of large-scale\ndatasets and single-epoch training. To address this issue, this paper proposes\na novel ensemble attack method that integrates several existing MIAs techniques\n(LOSS, Reference-based, min-k, zlib) into an XGBoost-based model to enhance\noverall attack performance (EM-MIAs). Experimental results demonstrate that the\nensemble model significantly improves both AUC-ROC and accuracy compared to\nindividual attack methods across various large language models and datasets.\nThis indicates that by combining the strengths of different methods, we can\nmore effectively identify members of the model's training data, thereby\nproviding a more robust tool for evaluating the privacy risks of LLM. This\nstudy offers new directions for further research in the field of LLM privacy\nprotection and underscores the necessity of developing more powerful privacy\nauditing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large language models (LLM), concerns\nabout the privacy leakage of model training data have increasingly become a\nfocus. Membership Inference Attacks (MIAs) have emerged as a critical tool for\nevaluating the privacy risks associated with these models. Although existing\nattack methods, such as LOSS, Reference-based, min-k, and zlib, perform well in\ncertain scenarios, their effectiveness on large pre-trained language models\noften approaches random guessing, particularly in the context of large-scale\ndatasets and single-epoch training. To address this issue, this paper proposes\na novel ensemble attack method that integrates several existing MIAs techniques\n(LOSS, Reference-based, min-k, zlib) into an XGBoost-based model to enhance\noverall attack performance (EM-MIAs). Experimental results demonstrate that the\nensemble model significantly improves both AUC-ROC and accuracy compared to\nindividual attack methods across various large language models and datasets.\nThis indicates that by combining the strengths of different methods, we can\nmore effectively identify members of the model's training data, thereby\nproviding a more robust tool for evaluating the privacy risks of LLM. This\nstudy offers new directions for further research in the field of LLM privacy\nprotection and underscores the necessity of developing more powerful privacy\nauditing methods."
                },
                "authors": [
                    {
                        "name": "Zichen Song"
                    },
                    {
                        "name": "Sitan Huang"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Kang"
                },
                "author": "Zhongfeng Kang",
                "arxiv_comment": "Accepted by ICASSP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17242v1",
                "updated": "2024-12-23T03:30:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    30,
                    34,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:30:34Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    30,
                    34,
                    0,
                    358,
                    0
                ],
                "title": "On the Generalization Ability of Machine-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization Ability of Machine-Generated Text Detectors"
                },
                "summary": "The rise of large language models (LLMs) has raised concerns about\nmachine-generated text (MGT), including ethical and practical issues like\nplagiarism and misinformation. Building a robust and highly generalizable MGT\ndetection system has become increasingly important. This work investigates the\ngeneralization capabilities of MGT detectors in three aspects: First, we\nconstruct MGTAcademic, a large-scale dataset focused on academic writing,\nfeaturing human-written texts (HWTs) and MGTs across STEM, Humanities, and\nSocial Sciences, paired with an extensible code framework for efficient\nbenchmarking. Second, we investigate the transferability of detectors across\ndomains and LLMs, leveraging fine-grained datasets to reveal insights into\ndomain transferring and implementing few-shot techniques to improve the\nperformance by roughly 13.2%. Third, we introduce a novel attribution task\nwhere models must adapt to new classes over time without (or with very limited)\naccess to prior training data and benchmark detectors. We implement several\nadapting techniques to improve the performance by roughly 10% and highlight the\ninherent complexity of the task. Our findings provide insights into the\ngeneralization ability of MGT detectors across diverse scenarios and lay the\nfoundation for building robust, adaptive detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has raised concerns about\nmachine-generated text (MGT), including ethical and practical issues like\nplagiarism and misinformation. Building a robust and highly generalizable MGT\ndetection system has become increasingly important. This work investigates the\ngeneralization capabilities of MGT detectors in three aspects: First, we\nconstruct MGTAcademic, a large-scale dataset focused on academic writing,\nfeaturing human-written texts (HWTs) and MGTs across STEM, Humanities, and\nSocial Sciences, paired with an extensible code framework for efficient\nbenchmarking. Second, we investigate the transferability of detectors across\ndomains and LLMs, leveraging fine-grained datasets to reveal insights into\ndomain transferring and implementing few-shot techniques to improve the\nperformance by roughly 13.2%. Third, we introduce a novel attribution task\nwhere models must adapt to new classes over time without (or with very limited)\naccess to prior training data and benchmark detectors. We implement several\nadapting techniques to improve the performance by roughly 10% and highlight the\ninherent complexity of the task. Our findings provide insights into the\ngeneralization ability of MGT detectors across diverse scenarios and lay the\nfoundation for building robust, adaptive detection systems."
                },
                "authors": [
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Zhiyuan Zhong"
                    },
                    {
                        "name": "Yifan Liao"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Jiaheng Wei"
                    },
                    {
                        "name": "Qingyuan Gong"
                    },
                    {
                        "name": "Fenghua Tong"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11764v3",
                "updated": "2024-12-23T03:27:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    27,
                    7,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-16T13:31:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    31,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study"
                },
                "summary": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines. The policy derived by\nSimpleFlight consistently excels across both smooth polynominal trajectories\nand challenging infeasible zigzag trajectories on small thrust-to-weight\nquadrotors. In contrast, baseline methods struggle with high-speed or\ninfeasible trajectories. To support further research and reproducibility, we\nintegrate SimpleFlight into a GPU-based simulator Omnidrones and provide\nopen-source access to the code and model checkpoints. We hope SimpleFlight will\noffer valuable insights for advancing RL-based quadrotor control. For more\ndetails, visit our project website at\nhttps://sites.google.com/view/simpleflight/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines. The policy derived by\nSimpleFlight consistently excels across both smooth polynominal trajectories\nand challenging infeasible zigzag trajectories on small thrust-to-weight\nquadrotors. In contrast, baseline methods struggle with high-speed or\ninfeasible trajectories. To support further research and reproducibility, we\nintegrate SimpleFlight into a GPU-based simulator Omnidrones and provide\nopen-source access to the code and model checkpoints. We hope SimpleFlight will\noffer valuable insights for advancing RL-based quadrotor control. For more\ndetails, visit our project website at\nhttps://sites.google.com/view/simpleflight/."
                },
                "authors": [
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yuqing Xie"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yinuo Chen"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Shilong Ji"
                    },
                    {
                        "name": "Mo Mu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first two authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15236v2",
                "updated": "2024-12-23T02:44:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    44,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-12T05:27:43Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    5,
                    27,
                    43,
                    3,
                    347,
                    0
                ],
                "title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model"
                },
                "summary": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional domains such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,\nwhich leverages a comprehensive approach integrating continuous pre-training\n(CPT), supervised fine-tuning (SFT), and reinforcement learning with human\nfeedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and\nBoost CPT, effectively bridges the gap between general and domain-specific\ndata, facilitating a smooth transition from pre-training to fine-tuning and\nenhancing domain knowledge progressively. We also introduce DataRater, a model\ndesigned to assess data quality during CPT, ensuring that the training data is\nboth accurate and relevant. For SFT, we develope a large and diverse bilingual\ndataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,\nwhich is crucial to improving the model's ability to handle more complex\ndialogues. The combination of high-quality data sources and innovative\ntechniques significantly improves CareBot's performance across a range of\nmedical applications. Our rigorous evaluations on Chinese and English\nbenchmarks confirm CareBot's effectiveness in medical consultation and\neducation. These advancements not only address current limitations in medical\nLLMs but also set a new standard for developing effective and reliable\nopen-source models in the medical domain. We will open-source the datasets and\nmodels later, contributing valuable resources to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional domains such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,\nwhich leverages a comprehensive approach integrating continuous pre-training\n(CPT), supervised fine-tuning (SFT), and reinforcement learning with human\nfeedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and\nBoost CPT, effectively bridges the gap between general and domain-specific\ndata, facilitating a smooth transition from pre-training to fine-tuning and\nenhancing domain knowledge progressively. We also introduce DataRater, a model\ndesigned to assess data quality during CPT, ensuring that the training data is\nboth accurate and relevant. For SFT, we develope a large and diverse bilingual\ndataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,\nwhich is crucial to improving the model's ability to handle more complex\ndialogues. The combination of high-quality data sources and innovative\ntechniques significantly improves CareBot's performance across a range of\nmedical applications. Our rigorous evaluations on Chinese and English\nbenchmarks confirm CareBot's effectiveness in medical consultation and\neducation. These advancements not only address current limitations in medical\nLLMs but also set a new standard for developing effective and reliable\nopen-source models in the medical domain. We will open-source the datasets and\nmodels later, contributing valuable resources to the research community."
                },
                "authors": [
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Xiaofeng Shi"
                    },
                    {
                        "name": "Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Hua Zhou"
                },
                "author": "Hua Zhou",
                "arxiv_comment": "Accept by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17222v1",
                "updated": "2024-12-23T02:32:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    32,
                    40,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T02:32:40Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    32,
                    40,
                    0,
                    358,
                    0
                ],
                "title": "Energy-Efficient RIS-Aided Cell-Free Massive MIMO Systems: Application,\n  Opportunities, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient RIS-Aided Cell-Free Massive MIMO Systems: Application,\n  Opportunities, and Challenges"
                },
                "summary": "Reconfigurable intelligent surfaces (RIS)-assisted cell-free massive\nmultiple-input multiple-output (CF mMIMO) systems have emerged as a promising\ntechnology for sixth-generation communication systems. These systems capitalize\non RIS to minimize power consumption, thereby achieving consistent performance\nand enhancing communication quality through the establishment and shaping of\nauxiliary signal propagation pathways between access points (APs) and users.\nHowever, integrating RIS into existing CF mMIMO infrastructures presents\nseveral technical challenges. This study delves into the signal transmission\nscheme and deployment architecture of RIS-aided CF mMIMO systems, addressing\ninherent challenges such as interference induced by RIS and the increased\ncomplexity in beam alignment. Furthermore, we address the complexities arising\nfrom the joint optimization of the reflection phase of RIS and beamforming\ntechnology at the APs, intending to fully exploit the reflection capabilities\nof RISs and beamforming technology to maximize the energy efficiency (EE) of\nthe system. To overcome these challenges, we propose cooperation communication\nto suppress RIS-induced interference, beam tracking, and joint optimization to\nimprove system EE. We also present specific examples of cooperative\ncommunication under the constraint of electromagnetic interference and the beam\ntracking of a mobile system. Additionally, we emphasize important research\ndirections for RIS-aided CF mMIMO systems, aiming to inspire future\ninvestigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RIS)-assisted cell-free massive\nmultiple-input multiple-output (CF mMIMO) systems have emerged as a promising\ntechnology for sixth-generation communication systems. These systems capitalize\non RIS to minimize power consumption, thereby achieving consistent performance\nand enhancing communication quality through the establishment and shaping of\nauxiliary signal propagation pathways between access points (APs) and users.\nHowever, integrating RIS into existing CF mMIMO infrastructures presents\nseveral technical challenges. This study delves into the signal transmission\nscheme and deployment architecture of RIS-aided CF mMIMO systems, addressing\ninherent challenges such as interference induced by RIS and the increased\ncomplexity in beam alignment. Furthermore, we address the complexities arising\nfrom the joint optimization of the reflection phase of RIS and beamforming\ntechnology at the APs, intending to fully exploit the reflection capabilities\nof RISs and beamforming technology to maximize the energy efficiency (EE) of\nthe system. To overcome these challenges, we propose cooperation communication\nto suppress RIS-induced interference, beam tracking, and joint optimization to\nimprove system EE. We also present specific examples of cooperative\ncommunication under the constraint of electromagnetic interference and the beam\ntracking of a mobile system. Additionally, we emphasize important research\ndirections for RIS-aided CF mMIMO systems, aiming to inspire future\ninvestigations."
                },
                "authors": [
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Enyu Shi"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Derrick Wing Kwan Ng"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17198v1",
                "updated": "2024-12-23T00:25:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    25,
                    52,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:25:52Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    25,
                    52,
                    0,
                    358,
                    0
                ],
                "title": "Exponential Tethers for Accelerated Space Elevator Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exponential Tethers for Accelerated Space Elevator Deployment"
                },
                "summary": "An exponential space elevator is a space elevator with a tether cross-section\nthat varies exponentially with altitude. With such an elevator it is possible\nto reel in tether material at one end of the elevator while reeling out at the\nother end, without changing the overall taper profile. I show how to use this\nproperty to build up or clone a space elevator much more efficiently than with\nstandard climber-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An exponential space elevator is a space elevator with a tether cross-section\nthat varies exponentially with altitude. With such an elevator it is possible\nto reel in tether material at one end of the elevator while reeling out at the\nother end, without changing the overall taper profile. I show how to use this\nproperty to build up or clone a space elevator much more efficiently than with\nstandard climber-based methods."
                },
                "authors": [
                    {
                        "name": "Blaise Gassend"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Gassend"
                },
                "author": "Blaise Gassend",
                "arxiv_comment": "This paper was first published in Proc. of the 3rd International\n  Space Elevator Conference, June 2004, reprinted on arXiv by permission of\n  Bradley Edwards former director at ISR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17189v1",
                "updated": "2024-12-22T23:31:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T23:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "title": "Better Think with Tables: Leveraging Tables to Enhance Large Language\n  Model Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Think with Tables: Leveraging Tables to Enhance Large Language\n  Model Comprehension"
                },
                "summary": "Despite the recent advancement of Large Langauge Models (LLMs), they struggle\nwith complex queries often involving multiple conditions, common in real-world\nscenarios. We propose Thinking with Tables, a technique that assists LLMs to\nleverage tables for intermediate thinking aligning with human cognitive\nbehavior. By introducing a pre-instruction that triggers an LLM to organize\ninformation in tables, our approach achieves a 40.29\\% average relative\nperformance increase, higher robustness, and show generalizability to different\nrequests, conditions, or scenarios. We additionally show the influence of data\nstructuredness for the model by comparing results from four distinct\nstructuring levels that we introduce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advancement of Large Langauge Models (LLMs), they struggle\nwith complex queries often involving multiple conditions, common in real-world\nscenarios. We propose Thinking with Tables, a technique that assists LLMs to\nleverage tables for intermediate thinking aligning with human cognitive\nbehavior. By introducing a pre-instruction that triggers an LLM to organize\ninformation in tables, our approach achieves a 40.29\\% average relative\nperformance increase, higher robustness, and show generalizability to different\nrequests, conditions, or scenarios. We additionally show the influence of data\nstructuredness for the model by comparing results from four distinct\nstructuring levels that we introduce."
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Geon Heo"
                    },
                    {
                        "name": "Seungjun Oh"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17171v1",
                "updated": "2024-12-22T21:56:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    21,
                    56,
                    15,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T21:56:15Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    21,
                    56,
                    15,
                    6,
                    357,
                    0
                ],
                "title": "Enhancing Item Tokenization for Generative Recommendation through\n  Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Item Tokenization for Generative Recommendation through\n  Self-Improvement"
                },
                "summary": "Generative recommendation systems, driven by large language models (LLMs),\npresent an innovative approach to predicting user preferences by modeling items\nas token sequences and generating recommendations in a generative manner. A\ncritical challenge in this approach is the effective tokenization of items,\nensuring that they are represented in a form compatible with LLMs. Current item\ntokenization methods include using text descriptions, numerical strings, or\nsequences of discrete tokens. While text-based representations integrate\nseamlessly with LLM tokenization, they are often too lengthy, leading to\ninefficiencies and complicating accurate generation. Numerical strings, while\nconcise, lack semantic depth and fail to capture meaningful item relationships.\nTokenizing items as sequences of newly defined tokens has gained traction, but\nit often requires external models or algorithms for token assignment. These\nexternal processes may not align with the LLM's internal pretrained\ntokenization schema, leading to inconsistencies and reduced model performance.\nTo address these limitations, we propose a self-improving item tokenization\nmethod that allows the LLM to refine its own item tokenizations during training\nprocess. Our approach starts with item tokenizations generated by any external\nmodel and periodically adjusts these tokenizations based on the LLM's learned\npatterns. Such alignment process ensures consistency between the tokenization\nand the LLM's internal understanding of the items, leading to more accurate\nrecommendations. Furthermore, our method is simple to implement and can be\nintegrated as a plug-and-play enhancement into existing generative\nrecommendation systems. Experimental results on multiple datasets and using\nvarious initial tokenization strategies demonstrate the effectiveness of our\nmethod, with an average improvement of 8\\% in recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation systems, driven by large language models (LLMs),\npresent an innovative approach to predicting user preferences by modeling items\nas token sequences and generating recommendations in a generative manner. A\ncritical challenge in this approach is the effective tokenization of items,\nensuring that they are represented in a form compatible with LLMs. Current item\ntokenization methods include using text descriptions, numerical strings, or\nsequences of discrete tokens. While text-based representations integrate\nseamlessly with LLM tokenization, they are often too lengthy, leading to\ninefficiencies and complicating accurate generation. Numerical strings, while\nconcise, lack semantic depth and fail to capture meaningful item relationships.\nTokenizing items as sequences of newly defined tokens has gained traction, but\nit often requires external models or algorithms for token assignment. These\nexternal processes may not align with the LLM's internal pretrained\ntokenization schema, leading to inconsistencies and reduced model performance.\nTo address these limitations, we propose a self-improving item tokenization\nmethod that allows the LLM to refine its own item tokenizations during training\nprocess. Our approach starts with item tokenizations generated by any external\nmodel and periodically adjusts these tokenizations based on the LLM's learned\npatterns. Such alignment process ensures consistency between the tokenization\nand the LLM's internal understanding of the items, leading to more accurate\nrecommendations. Furthermore, our method is simple to implement and can be\nintegrated as a plug-and-play enhancement into existing generative\nrecommendation systems. Experimental results on multiple datasets and using\nvarious initial tokenization strategies demonstrate the effectiveness of our\nmethod, with an average improvement of 8\\% in recommendation performance."
                },
                "authors": [
                    {
                        "name": "Runjin Chen"
                    },
                    {
                        "name": "Mingxuan Ju"
                    },
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Stanley Cai"
                    },
                    {
                        "name": "Xiaopeng Wu"
                    },
                    {
                        "name": "Leonardo Neves"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Neil Shah"
                    },
                    {
                        "name": "Tong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhao"
                },
                "author": "Tong Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17156v1",
                "updated": "2024-12-22T20:45:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    45,
                    15,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T20:45:15Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    45,
                    15,
                    6,
                    357,
                    0
                ],
                "title": "LLM-based relevance assessment still can't replace human relevance\n  assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based relevance assessment still can't replace human relevance\n  assessment"
                },
                "summary": "The use of large language models (LLMs) for relevance assessment in\ninformation retrieval has gained significant attention, with recent studies\nsuggesting that LLM-based judgments provide comparable evaluations to human\njudgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim\nthat LLM-based relevance assessments, such as those generated by the UMBRELA\nsystem, can fully replace traditional human relevance assessments in TREC-style\nevaluations. This paper critically examines this claim, highlighting practical\nand theoretical limitations that undermine the validity of this conclusion.\nFirst, we question whether the evidence provided by Upadhyay et al. really\nsupports their claim, particularly if a test collection is used asa benchmark\nfor future improvements. Second, through a submission deliberately intended to\ndo so, we demonstrate the ease with which automatic evaluation metrics can be\nsubverted, showing that systems designed to exploit these evaluations can\nachieve artificially high scores. Theoretical challenges -- such as the\ninherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and\nthe potential degradation of future LLM performance -- must be addressed before\nLLM-based relevance assessments can be considered a viable replacement for\nhuman judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for relevance assessment in\ninformation retrieval has gained significant attention, with recent studies\nsuggesting that LLM-based judgments provide comparable evaluations to human\njudgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim\nthat LLM-based relevance assessments, such as those generated by the UMBRELA\nsystem, can fully replace traditional human relevance assessments in TREC-style\nevaluations. This paper critically examines this claim, highlighting practical\nand theoretical limitations that undermine the validity of this conclusion.\nFirst, we question whether the evidence provided by Upadhyay et al. really\nsupports their claim, particularly if a test collection is used asa benchmark\nfor future improvements. Second, through a submission deliberately intended to\ndo so, we demonstrate the ease with which automatic evaluation metrics can be\nsubverted, showing that systems designed to exploit these evaluations can\nachieve artificially high scores. Theoretical challenges -- such as the\ninherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and\nthe potential degradation of future LLM performance -- must be addressed before\nLLM-based relevance assessments can be considered a viable replacement for\nhuman judgments."
                },
                "authors": [
                    {
                        "name": "Charles L. A. Clarke"
                    },
                    {
                        "name": "Laura Dietz"
                    }
                ],
                "author_detail": {
                    "name": "Laura Dietz"
                },
                "author": "Laura Dietz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17153v1",
                "updated": "2024-12-22T20:21:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    21,
                    54,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T20:21:54Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    21,
                    54,
                    6,
                    357,
                    0
                ],
                "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\n  with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\n  with Flow Matching"
                },
                "summary": "Autoregressive (AR) models have achieved state-of-the-art performance in text\nand image generation but suffer from slow generation due to the token-by-token\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\ngenerate outputs in just one or two steps? If successful, this would\nsignificantly advance the development and deployment of AR models. We notice\nthat existing works that try to speed up AR generation by generating multiple\ntokens at once fundamentally cannot capture the output distribution due to the\nconditional dependencies between tokens, limiting their effectiveness for\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\nuses flow matching to create a deterministic mapping from Gaussian distribution\nto the output distribution of the pre-trained AR model. We then train a network\nto distill this mapping, enabling few-step generation. DD doesn't need the\ntraining data of the original AR model, making it more practical.We evaluate DD\non state-of-the-art image AR models and present promising results on\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\ngeneration (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\n217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In\nboth cases, baseline methods completely fail with FID>100. DD also excels on\ntext-to-image generation, reducing the generation from 256 steps to 2 for\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\ndemonstrate the possibility of one-step generation for image AR models, DD\nchallenges the prevailing notion that AR models are inherently slow, and opens\nup new opportunities for efficient AR generation. The project website is at\nhttps://imagination-research.github.io/distilled-decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have achieved state-of-the-art performance in text\nand image generation but suffer from slow generation due to the token-by-token\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\ngenerate outputs in just one or two steps? If successful, this would\nsignificantly advance the development and deployment of AR models. We notice\nthat existing works that try to speed up AR generation by generating multiple\ntokens at once fundamentally cannot capture the output distribution due to the\nconditional dependencies between tokens, limiting their effectiveness for\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\nuses flow matching to create a deterministic mapping from Gaussian distribution\nto the output distribution of the pre-trained AR model. We then train a network\nto distill this mapping, enabling few-step generation. DD doesn't need the\ntraining data of the original AR model, making it more practical.We evaluate DD\non state-of-the-art image AR models and present promising results on\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\ngeneration (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\n217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In\nboth cases, baseline methods completely fail with FID>100. DD also excels on\ntext-to-image generation, reducing the generation from 256 steps to 2 for\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\ndemonstrate the possibility of one-step generation for image AR models, DD\nchallenges the prevailing notion that AR models are inherently slow, and opens\nup new opportunities for efficient AR generation. The project website is at\nhttps://imagination-research.github.io/distilled-decoding."
                },
                "authors": [
                    {
                        "name": "Enshu Liu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zinan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zinan Lin"
                },
                "author": "Zinan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17149v1",
                "updated": "2024-12-22T20:08:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    8,
                    4,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T20:08:04Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    8,
                    4,
                    6,
                    357,
                    0
                ],
                "title": "A Multi-AI Agent System for Autonomous Optimization of Agentic AI\n  Solutions via Iterative Refinement and LLM-Driven Feedback Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-AI Agent System for Autonomous Optimization of Agentic AI\n  Solutions via Iterative Refinement and LLM-Driven Feedback Loops"
                },
                "summary": "Agentic AI systems use specialized agents to handle tasks within complex\nworkflows, enabling automation and efficiency. However, optimizing these\nsystems often requires labor-intensive, manual adjustments to refine roles,\ntasks, and interactions. This paper introduces a framework for autonomously\noptimizing Agentic AI solutions across industries, such as NLP-driven\nenterprise applications. The system employs agents for Refinement, Execution,\nEvaluation, Modification, and Documentation, leveraging iterative feedback\nloops powered by an LLM (Llama 3.2-3B). The framework achieves optimal\nperformance without human input by autonomously generating and testing\nhypotheses to improve system configurations. This approach enhances scalability\nand adaptability, offering a robust solution for real-world applications in\ndynamic environments. Case studies across diverse domains illustrate the\ntransformative impact of this framework, showcasing significant improvements in\noutput quality, relevance, and actionability. All data for these case studies,\nincluding original and evolved agent codes, along with their outputs, are here:\nhttps://anonymous.4open.science/r/evolver-1D11/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems use specialized agents to handle tasks within complex\nworkflows, enabling automation and efficiency. However, optimizing these\nsystems often requires labor-intensive, manual adjustments to refine roles,\ntasks, and interactions. This paper introduces a framework for autonomously\noptimizing Agentic AI solutions across industries, such as NLP-driven\nenterprise applications. The system employs agents for Refinement, Execution,\nEvaluation, Modification, and Documentation, leveraging iterative feedback\nloops powered by an LLM (Llama 3.2-3B). The framework achieves optimal\nperformance without human input by autonomously generating and testing\nhypotheses to improve system configurations. This approach enhances scalability\nand adaptability, offering a robust solution for real-world applications in\ndynamic environments. Case studies across diverse domains illustrate the\ntransformative impact of this framework, showcasing significant improvements in\noutput quality, relevance, and actionability. All data for these case studies,\nincluding original and evolved agent codes, along with their outputs, are here:\nhttps://anonymous.4open.science/r/evolver-1D11/"
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17146v1",
                "updated": "2024-12-22T20:03:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    3,
                    35,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T20:03:35Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    20,
                    3,
                    35,
                    6,
                    357,
                    0
                ],
                "title": "LLM Agent for Fire Dynamics Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent for Fire Dynamics Simulations"
                },
                "summary": "Significant advances have been achieved in leveraging foundation models, such\nas large language models (LLMs), to accelerate complex scientific workflows. In\nthis work we introduce FoamPilot, a proof-of-concept LLM agent designed to\nenhance the usability of FireFOAM, a specialized solver for fire dynamics and\nfire suppression simulations built using OpenFOAM, a popular open-source\ntoolbox for computational fluid dynamics (CFD). FoamPilot provides three core\nfunctionalities: code insight, case configuration and simulation evaluation.\nCode insight is an alternative to traditional keyword searching leveraging\nretrieval-augmented generation (RAG) and aims to enable efficient navigation\nand summarization of the FireFOAM source code for developers and experienced\nusers. For case configuration, the agent interprets user requests in natural\nlanguage and aims to modify existing simulation setups accordingly to support\nintermediate users. FoamPilot's job execution functionality seeks to manage the\nsubmission and execution of simulations in high-performance computing (HPC)\nenvironments and provide preliminary analysis of simulation results to support\nless experienced users. Promising results were achieved for each functionality,\nparticularly for simple tasks, and opportunities were identified for\nsignificant further improvement for more complex tasks. The integration of\nthese functionalities into a single LLM agent is a step aimed at accelerating\nthe simulation workflow for engineers and scientists employing FireFOAM for\ncomplex simulations critical for improving fire safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advances have been achieved in leveraging foundation models, such\nas large language models (LLMs), to accelerate complex scientific workflows. In\nthis work we introduce FoamPilot, a proof-of-concept LLM agent designed to\nenhance the usability of FireFOAM, a specialized solver for fire dynamics and\nfire suppression simulations built using OpenFOAM, a popular open-source\ntoolbox for computational fluid dynamics (CFD). FoamPilot provides three core\nfunctionalities: code insight, case configuration and simulation evaluation.\nCode insight is an alternative to traditional keyword searching leveraging\nretrieval-augmented generation (RAG) and aims to enable efficient navigation\nand summarization of the FireFOAM source code for developers and experienced\nusers. For case configuration, the agent interprets user requests in natural\nlanguage and aims to modify existing simulation setups accordingly to support\nintermediate users. FoamPilot's job execution functionality seeks to manage the\nsubmission and execution of simulations in high-performance computing (HPC)\nenvironments and provide preliminary analysis of simulation results to support\nless experienced users. Promising results were achieved for each functionality,\nparticularly for simple tasks, and opportunities were identified for\nsignificant further improvement for more complex tasks. The integration of\nthese functionalities into a single LLM agent is a step aimed at accelerating\nthe simulation workflow for engineers and scientists employing FireFOAM for\ncomplex simulations critical for improving fire safety."
                },
                "authors": [
                    {
                        "name": "Leidong Xu"
                    },
                    {
                        "name": "Danyal Mohaddes"
                    },
                    {
                        "name": "Yi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wang"
                },
                "author": "Yi Wang",
                "arxiv_comment": "NeurIPS 2024 Foundation Models for Science Workshop (38th Conference\n  on Neural Information Processing Systems). 12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17131v1",
                "updated": "2024-12-22T18:38:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    18,
                    38,
                    24,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T18:38:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    18,
                    38,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "Hate Speech Detection and Target Identification in Devanagari Languages\n  via Parameter Efficient Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate Speech Detection and Target Identification in Devanagari Languages\n  via Parameter Efficient Fine-Tuning of LLMs"
                },
                "summary": "The detection of hate speech has become increasingly important in combating\nonline hostility and its real-world consequences. Despite recent advancements,\nthere is limited research addressing hate speech detection in\nDevanagari-scripted languages, where resources and tools are scarce. While\nlarge language models (LLMs) have shown promise in language-related tasks,\ntraditional fine-tuning approaches are often infeasible given the size of the\nmodels. In this paper, we propose a Parameter Efficient Fine tuning (PEFT)\nbased solution for hate speech detection and target identification. We evaluate\nmultiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which\ncontains annotated instances in 2 languages - Hindi and Nepali. The results\ndemonstrate the efficacy of our approach in handling Devanagari-scripted\ncontent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of hate speech has become increasingly important in combating\nonline hostility and its real-world consequences. Despite recent advancements,\nthere is limited research addressing hate speech detection in\nDevanagari-scripted languages, where resources and tools are scarce. While\nlarge language models (LLMs) have shown promise in language-related tasks,\ntraditional fine-tuning approaches are often infeasible given the size of the\nmodels. In this paper, we propose a Parameter Efficient Fine tuning (PEFT)\nbased solution for hate speech detection and target identification. We evaluate\nmultiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which\ncontains annotated instances in 2 languages - Hindi and Nepali. The results\ndemonstrate the efficacy of our approach in handling Devanagari-scripted\ncontent."
                },
                "authors": [
                    {
                        "name": "Rushendra Sidibomma"
                    },
                    {
                        "name": "Pransh Patwa"
                    },
                    {
                        "name": "Parth Patwa"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Amitava Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitava Das"
                },
                "author": "Amitava Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17128v1",
                "updated": "2024-12-22T18:34:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    18,
                    34,
                    10,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T18:34:10Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    18,
                    34,
                    10,
                    6,
                    357,
                    0
                ],
                "title": "Lies, Damned Lies, and Distributional Language Statistics: Persuasion\n  and Deception with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lies, Damned Lies, and Distributional Language Statistics: Persuasion\n  and Deception with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can generate content that is as persuasive as\nhuman-written text and appear capable of selectively producing deceptive\noutputs. These capabilities raise concerns about potential misuse and\nunintended consequences as these systems become more widely deployed. This\nreview synthesizes recent empirical work examining LLMs' capacity and\nproclivity for persuasion and deception, analyzes theoretical risks that could\narise from these capabilities, and evaluates proposed mitigations. While\ncurrent persuasive effects are relatively small, various mechanisms could\nincrease their impact, including fine-tuning, multimodality, and social\nfactors. We outline key open questions for future research, including how\npersuasive AI systems might become, whether truth enjoys an inherent advantage\nover falsehoods, and how effective different mitigation strategies may be in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate content that is as persuasive as\nhuman-written text and appear capable of selectively producing deceptive\noutputs. These capabilities raise concerns about potential misuse and\nunintended consequences as these systems become more widely deployed. This\nreview synthesizes recent empirical work examining LLMs' capacity and\nproclivity for persuasion and deception, analyzes theoretical risks that could\narise from these capabilities, and evaluates proposed mitigations. While\ncurrent persuasive effects are relatively small, various mechanisms could\nincrease their impact, including fine-tuning, multimodality, and social\nfactors. We outline key open questions for future research, including how\npersuasive AI systems might become, whether truth enjoys an inherent advantage\nover falsehoods, and how effective different mitigation strategies may be in\npractice."
                },
                "authors": [
                    {
                        "name": "Cameron R. Jones"
                    },
                    {
                        "name": "Benjamin K. Bergen"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin K. Bergen"
                },
                "author": "Benjamin K. Bergen",
                "arxiv_comment": "37 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.0; I.2.7; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]