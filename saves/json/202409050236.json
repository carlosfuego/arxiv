[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Beno√Æt Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.15444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15444v2",
                "updated": "2024-09-03T17:48:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    48,
                    55,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-30T18:07:13Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    7,
                    13,
                    3,
                    151,
                    0
                ],
                "title": "Investigating the Robustness of LLMs on Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of LLMs on Math Word Problems"
                },
                "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%."
                },
                "authors": [
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Kevin Scaria"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Swaroop Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Mishra"
                },
                "author": "Swaroop Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06385v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06385v3",
                "updated": "2024-09-03T16:36:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    36,
                    6,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-10T15:44:22Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    15,
                    44,
                    22,
                    0,
                    162,
                    0
                ],
                "title": "Low-Rank Quantization-Aware Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Quantization-Aware Training for LLMs"
                },
                "summary": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT"
                },
                "authors": [
                    {
                        "name": "Yelysei Bondarenko"
                    },
                    {
                        "name": "Riccardo Del Chiaro"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06385v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06385v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16465v2",
                "updated": "2024-09-03T15:45:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    45,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-29T11:54:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors"
                },
                "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions."
                },
                "authors": [
                    {
                        "name": "Szeyi Chan"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Mirjana Prpa"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15126v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15126v3",
                "updated": "2024-09-03T15:30:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    30,
                    27,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-27T15:07:27Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "title": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides"
                },
                "summary": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems."
                },
                "authors": [
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15126v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15126v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06425v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06425v6",
                "updated": "2024-09-03T15:07:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    7,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-12T18:04:59Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    4,
                    59,
                    0,
                    225,
                    0
                ],
                "title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Learning in a Nonlinear Multiscale State-Space Model"
                },
                "summary": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Nayely V√©lez-Cruz"
                    },
                    {
                        "name": "Manfred D. Laubichler"
                    }
                ],
                "author_detail": {
                    "name": "Manfred D. Laubichler"
                },
                "author": "Manfred D. Laubichler",
                "arxiv_comment": "Corrected a typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06425v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06425v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v3",
                "updated": "2024-09-03T14:53:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    53,
                    34,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders √òland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Gy√∂rgy Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19477v2",
                "updated": "2024-09-03T14:24:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    24,
                    49,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-29T18:59:31Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    59,
                    31,
                    3,
                    60,
                    0
                ],
                "title": "Learning a Generalized Physical Face Model From Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a Generalized Physical Face Model From Data"
                },
                "summary": "Physically-based simulation is a powerful approach for 3D facial animation as\nthe resulting deformations are governed by physical constraints, allowing to\neasily resolve self-collisions, respond to external forces and perform\nrealistic anatomy edits. Today's methods are data-driven, where the actuations\nfor finite elements are inferred from captured skin geometry. Unfortunately,\nthese approaches have not been widely adopted due to the complexity of\ninitializing the material space and learning the deformation model for each\ncharacter separately, which often requires a skilled artist followed by lengthy\nnetwork training. In this work, we aim to make physics-based facial animation\nmore accessible by proposing a generalized physical face model that we learn\nfrom a large 3D face dataset. Once trained, our model can be quickly fit to any\nunseen identity and produce a ready-to-animate physical face model\nautomatically. Fitting is as easy as providing a single 3D face scan, or even a\nsingle face image. After fitting, we offer intuitive animation controls, as\nwell as the ability to retarget animations across characters. All the while,\nthe resulting animations allow for physical effects like collision avoidance,\ngravity, paralysis, bone reshaping and more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physically-based simulation is a powerful approach for 3D facial animation as\nthe resulting deformations are governed by physical constraints, allowing to\neasily resolve self-collisions, respond to external forces and perform\nrealistic anatomy edits. Today's methods are data-driven, where the actuations\nfor finite elements are inferred from captured skin geometry. Unfortunately,\nthese approaches have not been widely adopted due to the complexity of\ninitializing the material space and learning the deformation model for each\ncharacter separately, which often requires a skilled artist followed by lengthy\nnetwork training. In this work, we aim to make physics-based facial animation\nmore accessible by proposing a generalized physical face model that we learn\nfrom a large 3D face dataset. Once trained, our model can be quickly fit to any\nunseen identity and produce a ready-to-animate physical face model\nautomatically. Fitting is as easy as providing a single 3D face scan, or even a\nsingle face image. After fitting, we offer intuitive animation controls, as\nwell as the ability to retarget animations across characters. All the while,\nthe resulting animations allow for physical effects like collision avoidance,\ngravity, paralysis, bone reshaping and more."
                },
                "authors": [
                    {
                        "name": "Lingchen Yang"
                    },
                    {
                        "name": "Gaspard Zoss"
                    },
                    {
                        "name": "Prashanth Chandran"
                    },
                    {
                        "name": "Markus Gross"
                    },
                    {
                        "name": "Barbara Solenthaler"
                    },
                    {
                        "name": "Eftychios Sifakis"
                    },
                    {
                        "name": "Derek Bradley"
                    }
                ],
                "author_detail": {
                    "name": "Derek Bradley"
                },
                "author": "Derek Bradley",
                "arxiv_doi": "10.1145/3658189",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658189",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.19477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00267v3",
                "updated": "2024-09-03T14:01:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    1,
                    54,
                    1,
                    247,
                    0
                ],
                "published": "2023-09-01T05:53:33Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    5,
                    53,
                    33,
                    4,
                    244,
                    0
                ],
                "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with\n  AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with\n  AI Feedback"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but gathering\nhigh-quality preference labels is expensive. RL from AI Feedback (RLAIF),\nintroduced in Bai et al., offers a promising alternative that trains the reward\nmodel (RM) on preferences generated by an off-the-shelf LLM. Across the tasks\nof summarization, helpful dialogue generation, and harmless dialogue\ngeneration, we show that RLAIF achieves comparable performance to RLHF.\nFurthermore, we take a step towards \"self-improvement\" by demonstrating that\nRLAIF can outperform a supervised fine-tuned baseline even when the AI labeler\nis the same size as the policy, or even the exact same checkpoint as the\ninitial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that\ncircumvents RM training by obtaining rewards directly from an off-the-shelf LLM\nduring RL, which achieves superior performance to canonical RLAIF. Our results\nsuggest that RLAIF can achieve performance on-par with using human feedback,\noffering a potential solution to the scalability limitations of RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but gathering\nhigh-quality preference labels is expensive. RL from AI Feedback (RLAIF),\nintroduced in Bai et al., offers a promising alternative that trains the reward\nmodel (RM) on preferences generated by an off-the-shelf LLM. Across the tasks\nof summarization, helpful dialogue generation, and harmless dialogue\ngeneration, we show that RLAIF achieves comparable performance to RLHF.\nFurthermore, we take a step towards \"self-improvement\" by demonstrating that\nRLAIF can outperform a supervised fine-tuned baseline even when the AI labeler\nis the same size as the policy, or even the exact same checkpoint as the\ninitial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that\ncircumvents RM training by obtaining rewards directly from an off-the-shelf LLM\nduring RL, which achieves superior performance to canonical RLAIF. Our results\nsuggest that RLAIF can achieve performance on-par with using human feedback,\noffering a potential solution to the scalability limitations of RLHF."
                },
                "authors": [
                    {
                        "name": "Harrison Lee"
                    },
                    {
                        "name": "Samrat Phatale"
                    },
                    {
                        "name": "Hassan Mansoor"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Kellie Lu"
                    },
                    {
                        "name": "Colton Bishop"
                    },
                    {
                        "name": "Ethan Hall"
                    },
                    {
                        "name": "Victor Carbune"
                    },
                    {
                        "name": "Abhinav Rastogi"
                    },
                    {
                        "name": "Sushant Prakash"
                    }
                ],
                "author_detail": {
                    "name": "Sushant Prakash"
                },
                "author": "Sushant Prakash",
                "arxiv_comment": "Presented at ICML 2024",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning, PMLR 235:26874-26901, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04444v2",
                "updated": "2024-09-03T13:58:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    58,
                    15,
                    1,
                    247,
                    0
                ],
                "published": "2023-12-07T17:08:35Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    17,
                    8,
                    35,
                    3,
                    341,
                    0
                ],
                "title": "Parameter Inference for Hypo-Elliptic Diffusions under a Weak Design\n  Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Inference for Hypo-Elliptic Diffusions under a Weak Design\n  Condition"
                },
                "summary": "We address the problem of parameter estimation for degenerate diffusion\nprocesses defined via the solution of Stochastic Differential Equations (SDEs)\nwith diffusion matrix that is not full-rank. For this class of hypo-elliptic\ndiffusions recent works have proposed contrast estimators that are\nasymptotically normal, provided that the step-size in-between observations\n$\\Delta=\\Delta_n$ and their total number $n$ satisfy $n \\to \\infty$, $n\n\\Delta_n \\to \\infty$, $\\Delta_n \\to 0$, and additionally $\\Delta_n = o\n(n^{-1/2})$. This latter restriction places a requirement for a so-called\n`rapidly increasing experimental design'. In this paper, we overcome this\nlimitation and develop a general contrast estimator satisfying asymptotic\nnormality under the weaker design condition $\\Delta_n = o(n^{-1/p})$ for\ngeneral $p \\ge 2$. Such a result has been obtained for elliptic SDEs in the\nliterature, but its derivation in a hypo-elliptic setting is highly\nnon-trivial. We provide numerical results to illustrate the advantages of the\ndeveloped theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of parameter estimation for degenerate diffusion\nprocesses defined via the solution of Stochastic Differential Equations (SDEs)\nwith diffusion matrix that is not full-rank. For this class of hypo-elliptic\ndiffusions recent works have proposed contrast estimators that are\nasymptotically normal, provided that the step-size in-between observations\n$\\Delta=\\Delta_n$ and their total number $n$ satisfy $n \\to \\infty$, $n\n\\Delta_n \\to \\infty$, $\\Delta_n \\to 0$, and additionally $\\Delta_n = o\n(n^{-1/2})$. This latter restriction places a requirement for a so-called\n`rapidly increasing experimental design'. In this paper, we overcome this\nlimitation and develop a general contrast estimator satisfying asymptotic\nnormality under the weaker design condition $\\Delta_n = o(n^{-1/p})$ for\ngeneral $p \\ge 2$. Such a result has been obtained for elliptic SDEs in the\nliterature, but its derivation in a hypo-elliptic setting is highly\nnon-trivial. We provide numerical results to illustrate the advantages of the\ndeveloped theory."
                },
                "authors": [
                    {
                        "name": "Yuga Iguchi"
                    },
                    {
                        "name": "Alexandros Beskos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Beskos"
                },
                "author": "Alexandros Beskos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07918v4",
                "updated": "2024-09-03T13:52:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    52,
                    24,
                    1,
                    247,
                    0
                ],
                "published": "2023-09-14T17:59:49Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    17,
                    59,
                    49,
                    3,
                    257,
                    0
                ],
                "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts"
                },
                "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI ."
                },
                "authors": [
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "A unified Human-Scene Interaction framework that supports versatile\n  interactions through language commands.Project URL:\n  https://xizaoqu.github.io/unihsi/ . Code:\n  https://github.com/OpenRobotLab/UniHSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12587v2",
                "updated": "2024-09-03T13:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    36,
                    22,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-18T13:18:32Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    18,
                    32,
                    1,
                    170,
                    0
                ],
                "title": "Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt\n  Guidance"
                },
                "summary": "There are many excellent solutions in image restoration.However, most methods\nrequire on training separate models to restore images with different types of\ndegradation.Although existing all-in-one models effectively address multiple\ntypes of degradation simultaneously, their performance in real-world scenarios\nis still constrained by the task confusion problem.In this work, we attempt to\naddress this issue by introducing \\textbf{Restorer}, a novel Transformer-based\nall-in-one image restoration model.To effectively address the complex\ndegradation present in real-world images, we propose All-Axis Attention (AAA),\na mechanism that simultaneously models long-range dependencies across both\nspatial and channel dimensions, capturing potential correlations along all\naxes.Additionally, we introduce textual prompts in Restorer to incorporate\nexplicit task priors, enabling the removal of specific degradation types based\non user instructions. By iterating over these prompts, Restorer can handle\ncomposite degradation in real-world scenarios without requiring additional\ntraining.Based on these designs, Restorer with one set of parameters\ndemonstrates state-of-the-art performance in multiple image restoration tasks\ncompared to existing all-in-one and even single-task models.Additionally,\nRestorer is efficient during inference, suggesting the potential in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are many excellent solutions in image restoration.However, most methods\nrequire on training separate models to restore images with different types of\ndegradation.Although existing all-in-one models effectively address multiple\ntypes of degradation simultaneously, their performance in real-world scenarios\nis still constrained by the task confusion problem.In this work, we attempt to\naddress this issue by introducing \\textbf{Restorer}, a novel Transformer-based\nall-in-one image restoration model.To effectively address the complex\ndegradation present in real-world images, we propose All-Axis Attention (AAA),\na mechanism that simultaneously models long-range dependencies across both\nspatial and channel dimensions, capturing potential correlations along all\naxes.Additionally, we introduce textual prompts in Restorer to incorporate\nexplicit task priors, enabling the removal of specific degradation types based\non user instructions. By iterating over these prompts, Restorer can handle\ncomposite degradation in real-world scenarios without requiring additional\ntraining.Based on these designs, Restorer with one set of parameters\ndemonstrates state-of-the-art performance in multiple image restoration tasks\ncompared to existing all-in-one and even single-task models.Additionally,\nRestorer is efficient during inference, suggesting the potential in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jiawei Mao"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Xuesong Yin"
                    },
                    {
                        "name": "Yuanqi Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yuanqi Chang"
                },
                "author": "Yuanqi Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12458v2",
                "updated": "2024-09-03T13:21:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    21,
                    53,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-22T14:55:25Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    55,
                    25,
                    3,
                    235,
                    0
                ],
                "title": "Non-perturbative Resolution of Strong Coupling Singularities in 4d N=1\n  Heterotic/M-theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-perturbative Resolution of Strong Coupling Singularities in 4d N=1\n  Heterotic/M-theory"
                },
                "summary": "We investigate the interior of the moduli space of four-dimensional\n$\\mathcal{N}=1$ theories of gravity arising from compactifications of the\n$E_8\\times E_8$ heterotic string on Calabi-Yau threefolds. By studying the\nthreshold corrections to the coupling of the heterotic gauge groups, we infer\nthe existence of a strong coupling singularity for one of the perturbative\nheterotic gauge groups, which effectively yields an additional finite distance\nboundary of the classical scalar field space. In heterotic M-theory, this\nboundary maps to a domain wall solution for which the gauge coupling and the\nwarp factor on one of the Horava-Witten 9-branes diverge, thus highlighting the\ngravitational origin of the classical strong coupling singularity. The\ndivergence of the warp factor is, however, regulated once non-perturbative\neffects are taken into account, as we demonstrate by studying the instanton\ncorrections to the 5d BPS domain wall equations. This regularization implies\nthat the classical strong coupling boundary of the scalar field 4d\n$\\mathcal{N}=1$ heterotic/M-theory is resolved, indicating that, at the quantum\nlevel, the field space can be extended beyond this classical boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the interior of the moduli space of four-dimensional\n$\\mathcal{N}=1$ theories of gravity arising from compactifications of the\n$E_8\\times E_8$ heterotic string on Calabi-Yau threefolds. By studying the\nthreshold corrections to the coupling of the heterotic gauge groups, we infer\nthe existence of a strong coupling singularity for one of the perturbative\nheterotic gauge groups, which effectively yields an additional finite distance\nboundary of the classical scalar field space. In heterotic M-theory, this\nboundary maps to a domain wall solution for which the gauge coupling and the\nwarp factor on one of the Horava-Witten 9-branes diverge, thus highlighting the\ngravitational origin of the classical strong coupling singularity. The\ndivergence of the warp factor is, however, regulated once non-perturbative\neffects are taken into account, as we demonstrate by studying the instanton\ncorrections to the 5d BPS domain wall equations. This regularization implies\nthat the classical strong coupling boundary of the scalar field 4d\n$\\mathcal{N}=1$ heterotic/M-theory is resolved, indicating that, at the quantum\nlevel, the field space can be extended beyond this classical boundary."
                },
                "authors": [
                    {
                        "name": "Mirjam Cvetiƒç"
                    },
                    {
                        "name": "Max Wiesner"
                    }
                ],
                "author_detail": {
                    "name": "Max Wiesner"
                },
                "author": "Max Wiesner",
                "arxiv_comment": "v2: fixed errors and updated discussion in section 4; 35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02937v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02937v5",
                "updated": "2024-09-03T11:32:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    11,
                    32,
                    50,
                    1,
                    247,
                    0
                ],
                "published": "2024-04-03T07:14:15Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    14,
                    15,
                    2,
                    94,
                    0
                ],
                "title": "Towards Explainable Traffic Flow Prediction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Traffic Flow Prediction with Large Language Models"
                },
                "summary": "Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows."
                },
                "authors": [
                    {
                        "name": "Xusen Guo"
                    },
                    {
                        "name": "Qiming Zhang"
                    },
                    {
                        "name": "Junyue Jiang"
                    },
                    {
                        "name": "Mingxing Peng"
                    },
                    {
                        "name": "Meixin Zhu"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang"
                },
                "arxiv_affiliation": "Frank",
                "author": "Yang",
                "arxiv_comment": "31pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02937v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02937v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02031v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02031v8",
                "updated": "2024-09-03T10:19:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    19,
                    52,
                    1,
                    247,
                    0
                ],
                "published": "2023-10-03T13:17:35Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    13,
                    17,
                    35,
                    1,
                    276,
                    0
                ],
                "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OceanGPT: A Large Language Model for Ocean Science Tasks"
                },
                "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology."
                },
                "authors": [
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yida Xue"
                    },
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Daxiong Ji"
                    },
                    {
                        "name": "Guozhou Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "ACL2024. Project Website: http://oceangpt.zjukg.cn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02031v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02031v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12501v3",
                "updated": "2024-09-03T10:12:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    12,
                    34,
                    1,
                    247,
                    0
                ],
                "published": "2024-04-18T20:43:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    20,
                    43,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation"
                },
                "summary": "Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications. The code and pre-trained models\nare publicly available at https://github.com/Lavreniuk/SPIdepth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications. The code and pre-trained models\nare publicly available at https://github.com/Lavreniuk/SPIdepth."
                },
                "authors": [
                    {
                        "name": "Mykola Lavreniuk"
                    }
                ],
                "author_detail": {
                    "name": "Mykola Lavreniuk"
                },
                "author": "Mykola Lavreniuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19047v2",
                "updated": "2024-09-03T09:25:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    25,
                    46,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-29T12:44:41Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    44,
                    41,
                    2,
                    150,
                    0
                ],
                "title": "Statistical Context Detection for Deep Lifelong Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Context Detection for Deep Lifelong Reinforcement Learning"
                },
                "summary": "Context detection involves labeling segments of an online stream of data as\nbelonging to different tasks. Task labels are used in lifelong learning\nalgorithms to perform consolidation or other procedures that prevent\ncatastrophic forgetting. Inferring task labels from online experiences remains\na challenging problem. Most approaches assume finite and low-dimension\nobservation spaces or a preliminary training phase during which task labels are\nlearned. Moreover, changes in the transition or reward functions can be\ndetected only in combination with a policy, and therefore are more difficult to\ndetect than changes in the input distribution. This paper presents an approach\nto learning both policies and labels in an online deep reinforcement learning\nsetting. The key idea is to use distance metrics, obtained via optimal\ntransport methods, i.e., Wasserstein distance, on suitable latent action-reward\nspaces to measure distances between sets of data points from past and current\nstreams. Such distances can then be used for statistical tests based on an\nadapted Kolmogorov-Smirnov calculation to assign labels to sequences of\nexperiences. A rollback procedure is introduced to learn multiple policies by\nensuring that only the appropriate data is used to train the corresponding\npolicy. The combination of task detection and policy deployment allows for the\noptimization of lifelong reinforcement learning agents without an oracle that\nprovides task labels. The approach is tested using two benchmarks and the\nresults show promising performance when compared with related context detection\nalgorithms. The results suggest that optimal transport statistical methods\nprovide an explainable and justifiable procedure for online context detection\nand reward optimization in lifelong reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context detection involves labeling segments of an online stream of data as\nbelonging to different tasks. Task labels are used in lifelong learning\nalgorithms to perform consolidation or other procedures that prevent\ncatastrophic forgetting. Inferring task labels from online experiences remains\na challenging problem. Most approaches assume finite and low-dimension\nobservation spaces or a preliminary training phase during which task labels are\nlearned. Moreover, changes in the transition or reward functions can be\ndetected only in combination with a policy, and therefore are more difficult to\ndetect than changes in the input distribution. This paper presents an approach\nto learning both policies and labels in an online deep reinforcement learning\nsetting. The key idea is to use distance metrics, obtained via optimal\ntransport methods, i.e., Wasserstein distance, on suitable latent action-reward\nspaces to measure distances between sets of data points from past and current\nstreams. Such distances can then be used for statistical tests based on an\nadapted Kolmogorov-Smirnov calculation to assign labels to sequences of\nexperiences. A rollback procedure is introduced to learn multiple policies by\nensuring that only the appropriate data is used to train the corresponding\npolicy. The combination of task detection and policy deployment allows for the\noptimization of lifelong reinforcement learning agents without an oracle that\nprovides task labels. The approach is tested using two benchmarks and the\nresults show promising performance when compared with related context detection\nalgorithms. The results suggest that optimal transport statistical methods\nprovide an explainable and justifiable procedure for online context detection\nand reward optimization in lifelong reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Jeffery Dick"
                    },
                    {
                        "name": "Saptarshi Nath"
                    },
                    {
                        "name": "Christos Peridis"
                    },
                    {
                        "name": "Eseoghene Benjamin"
                    },
                    {
                        "name": "Soheil Kolouri"
                    },
                    {
                        "name": "Andrea Soltoggio"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Soltoggio"
                },
                "author": "Andrea Soltoggio",
                "arxiv_comment": "10 pages excluding references and bibliography. Accepted at CoLLAs\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08538v2",
                "updated": "2024-09-03T09:00:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    0,
                    37,
                    1,
                    247,
                    0
                ],
                "published": "2024-07-11T14:26:33Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    14,
                    26,
                    33,
                    3,
                    193,
                    0
                ],
                "title": "Phenomenological model of gravitational self-force enhanced tides in\n  inspiralling binary neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phenomenological model of gravitational self-force enhanced tides in\n  inspiralling binary neutron stars"
                },
                "summary": "Gravitational waves from inspiralling binary neutron stars provide unique\naccess to ultra-dense nuclear matter and offer the ability to constrain the\ncurrently unknown neutron star equation-of-state through tidal measurements.\nThis, however, requires the availability of accurate and efficient tidal\nwaveform models. In this paper we present PhenomGSF, a new phenomenological\ntidal phase model for the inspiral of neutron stars in the frequency-domain,\nwhich captures the gravitational self-force informed tidal contributions of the\ntime-domain effective-one-body model TEOBResumS. PhenomGSF is highly faithful\nand computationally efficient, and by choosing a modular approach, it can be\nused in conjunction with any frequency-domain binary black hole waveform model\nto generate the complete phase for a binary neutron star inspiral. PhenomGSF is\nvalid for neutron star binaries with unequal masses and mass ratios between 1\nand 3, and dimensionless tidal deformabilities up to 5000. Furthermore,\nPhenomGSF does not assume universal relations or parameterised\nequations-of-state, hence allowing for exotic matter analyses and beyond\nstandard model physics investigations. We demonstrate the efficacy and accuracy\nof our model through comparisons against TEOBResumS, numerical relativity\nwaveforms and full Bayesian inference, including a reanalysis of the binary\nneutron star observation GW170817.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from inspiralling binary neutron stars provide unique\naccess to ultra-dense nuclear matter and offer the ability to constrain the\ncurrently unknown neutron star equation-of-state through tidal measurements.\nThis, however, requires the availability of accurate and efficient tidal\nwaveform models. In this paper we present PhenomGSF, a new phenomenological\ntidal phase model for the inspiral of neutron stars in the frequency-domain,\nwhich captures the gravitational self-force informed tidal contributions of the\ntime-domain effective-one-body model TEOBResumS. PhenomGSF is highly faithful\nand computationally efficient, and by choosing a modular approach, it can be\nused in conjunction with any frequency-domain binary black hole waveform model\nto generate the complete phase for a binary neutron star inspiral. PhenomGSF is\nvalid for neutron star binaries with unequal masses and mass ratios between 1\nand 3, and dimensionless tidal deformabilities up to 5000. Furthermore,\nPhenomGSF does not assume universal relations or parameterised\nequations-of-state, hence allowing for exotic matter analyses and beyond\nstandard model physics investigations. We demonstrate the efficacy and accuracy\nof our model through comparisons against TEOBResumS, numerical relativity\nwaveforms and full Bayesian inference, including a reanalysis of the binary\nneutron star observation GW170817."
                },
                "authors": [
                    {
                        "name": "Natalie Williams"
                    },
                    {
                        "name": "Patricia Schmidt"
                    },
                    {
                        "name": "Geraint Pratten"
                    }
                ],
                "author_detail": {
                    "name": "Geraint Pratten"
                },
                "author": "Geraint Pratten",
                "arxiv_comment": "26 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01306v3",
                "updated": "2024-09-03T07:41:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    41,
                    51,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-02T10:53:36Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    10,
                    53,
                    36,
                    4,
                    33,
                    0
                ],
                "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTO: Model Alignment as Prospect Theoretic Optimization"
                },
                "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration."
                },
                "authors": [
                    {
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Douwe Kiela"
                    }
                ],
                "author_detail": {
                    "name": "Douwe Kiela"
                },
                "author": "Douwe Kiela",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18542v2",
                "updated": "2024-09-03T07:34:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    34,
                    54,
                    1,
                    247,
                    0
                ],
                "published": "2023-10-28T00:15:10Z",
                "published_parsed": [
                    2023,
                    10,
                    28,
                    0,
                    15,
                    10,
                    5,
                    301,
                    0
                ],
                "title": "End-to-end Feature Selection Approach for Learning Skinny Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Feature Selection Approach for Learning Skinny Trees"
                },
                "summary": "We propose a new optimization-based approach for feature selection in tree\nensembles, an important problem in statistics and machine learning. Popular\ntree ensemble toolkits e.g., Gradient Boosted Trees and Random Forests support\nfeature selection post-training based on feature importance scores, while very\npopular, they are known to have drawbacks. We propose Skinny Trees: an\nend-to-end toolkit for feature selection in tree ensembles where we train a\ntree ensemble while controlling the number of selected features. Our\noptimization-based approach learns an ensemble of differentiable trees, and\nsimultaneously performs feature selection using a grouped $\\ell_0$-regularizer.\nWe use first-order methods for optimization and present convergence guarantees\nfor our approach. We use a dense-to-sparse regularization scheduling scheme\nthat can lead to more expressive and sparser tree ensembles. On 15 synthetic\nand real-world datasets, Skinny Trees can achieve $1.5\\!\\times\\!\n-~620~\\!\\times\\!$ feature compression rates, leading up to $10\\times$ faster\ninference over dense trees, without any loss in performance. Skinny Trees lead\nto superior feature selection than many existing toolkits e.g., in terms of AUC\nperformance for 25\\% feature budget, Skinny Trees outperforms LightGBM by\n$10.2\\%$ (up to $37.7\\%$), and Random Forests by $3\\%$ (up to $12.5\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new optimization-based approach for feature selection in tree\nensembles, an important problem in statistics and machine learning. Popular\ntree ensemble toolkits e.g., Gradient Boosted Trees and Random Forests support\nfeature selection post-training based on feature importance scores, while very\npopular, they are known to have drawbacks. We propose Skinny Trees: an\nend-to-end toolkit for feature selection in tree ensembles where we train a\ntree ensemble while controlling the number of selected features. Our\noptimization-based approach learns an ensemble of differentiable trees, and\nsimultaneously performs feature selection using a grouped $\\ell_0$-regularizer.\nWe use first-order methods for optimization and present convergence guarantees\nfor our approach. We use a dense-to-sparse regularization scheduling scheme\nthat can lead to more expressive and sparser tree ensembles. On 15 synthetic\nand real-world datasets, Skinny Trees can achieve $1.5\\!\\times\\!\n-~620~\\!\\times\\!$ feature compression rates, leading up to $10\\times$ faster\ninference over dense trees, without any loss in performance. Skinny Trees lead\nto superior feature selection than many existing toolkits e.g., in terms of AUC\nperformance for 25\\% feature budget, Skinny Trees outperforms LightGBM by\n$10.2\\%$ (up to $37.7\\%$), and Random Forests by $3\\%$ (up to $12.5\\%$)."
                },
                "authors": [
                    {
                        "name": "Shibal Ibrahim"
                    },
                    {
                        "name": "Kayhan Behdin"
                    },
                    {
                        "name": "Rahul Mazumder"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mazumder"
                },
                "author": "Rahul Mazumder",
                "arxiv_comment": "Accepted in AISTATS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01252v3",
                "updated": "2024-09-03T07:07:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    7,
                    59,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-03T12:10:26Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    12,
                    10,
                    26,
                    0,
                    155,
                    0
                ],
                "title": "Towards Scalable Automated Alignment of LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Automated Alignment of LLMs: A Survey"
                },
                "summary": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment."
                },
                "authors": [
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Xinyu Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Mengjie Ren"
                    },
                    {
                        "name": "Hao Xiang"
                    },
                    {
                        "name": "Peilin Liu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Yu"
                },
                "author": "Bowen Yu",
                "arxiv_comment": "Paper List: https://github.com/cascip/awesome-auto-alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11336v2",
                "updated": "2024-09-03T06:36:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    6,
                    36,
                    24,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-17T08:54:23Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    8,
                    54,
                    23,
                    0,
                    169,
                    0
                ],
                "title": "A General Framework for Load Forecasting based on Pre-trained Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Load Forecasting based on Pre-trained Large\n  Language Model"
                },
                "summary": "Accurate load forecasting is crucial for maintaining the power balance\nbetween generators and consumers,particularly with the increasing integration\nof renewable energy sources, which introduce significant intermittent\nvolatility. With the advancement of data-driven methods, machine learning and\ndeep learning models have become the predominant approaches for load\nforecasting tasks. In recent years, pre-trained large language models (LLMs)\nhave achieved significant progress, demonstrating superior performance across\nvarious fields. This paper proposes a load forecasting method based on LLMs,\noffering not only precise predictive capabilities but also broad and flexible\napplicability. Additionally, a data modeling method is introduced to\neffectively transform load sequence data into natural language suitable for LLM\ntraining. Furthermore, a data enhancement strategy is designed to mitigate the\nimpact of LLM hallucinations on forecasting results. The effectiveness of the\nproposed method is validated using two real-world datasets. Compared to\nexisting methods, our approach demonstrates state-of-the-art performance across\nall validation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate load forecasting is crucial for maintaining the power balance\nbetween generators and consumers,particularly with the increasing integration\nof renewable energy sources, which introduce significant intermittent\nvolatility. With the advancement of data-driven methods, machine learning and\ndeep learning models have become the predominant approaches for load\nforecasting tasks. In recent years, pre-trained large language models (LLMs)\nhave achieved significant progress, demonstrating superior performance across\nvarious fields. This paper proposes a load forecasting method based on LLMs,\noffering not only precise predictive capabilities but also broad and flexible\napplicability. Additionally, a data modeling method is introduced to\neffectively transform load sequence data into natural language suitable for LLM\ntraining. Furthermore, a data enhancement strategy is designed to mitigate the\nimpact of LLM hallucinations on forecasting results. The effectiveness of the\nproposed method is validated using two real-world datasets. Compared to\nexisting methods, our approach demonstrates state-of-the-art performance across\nall validation metrics."
                },
                "authors": [
                    {
                        "name": "Mingyang Gao"
                    },
                    {
                        "name": "Suyang Zhou"
                    },
                    {
                        "name": "Wei Gu"
                    },
                    {
                        "name": "Zhi Wu"
                    },
                    {
                        "name": "Haiquan Liu"
                    },
                    {
                        "name": "Aihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aihua Zhou"
                },
                "author": "Aihua Zhou",
                "arxiv_comment": "11 pages, 3 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11169v4",
                "updated": "2024-09-03T05:51:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    5,
                    51,
                    40,
                    1,
                    247,
                    0
                ],
                "published": "2024-03-17T10:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    10,
                    59,
                    9,
                    6,
                    77,
                    0
                ],
                "title": "Correcting misinformation on social media with a large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting misinformation on social media with a large language model"
                },
                "summary": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale."
                },
                "authors": [
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Ashish Sharma"
                    },
                    {
                        "name": "Amy X. Zhang"
                    },
                    {
                        "name": "Tim Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Tim Althoff"
                },
                "author": "Tim Althoff",
                "arxiv_comment": "50 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01481v2",
                "updated": "2024-09-03T05:47:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    5,
                    47,
                    42,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-02T17:13:40Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    17,
                    13,
                    40,
                    3,
                    123,
                    0
                ],
                "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to a thousand GPUs for training the largest open-source LLMs such as\nNemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized\nand scalable implementations for major paradigms of model alignment such as:\nReinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to a thousand GPUs for training the largest open-source LLMs such as\nNemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized\nand scalable implementations for major paradigms of model alignment such as:\nReinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner"
                },
                "authors": [
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Shengyang Sun"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Markel Sanz Ausin"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "arxiv_comment": "16 pages, 4 figures, Accepted to COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02850v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02850v6",
                "updated": "2024-09-03T03:49:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    49,
                    16,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-05T08:43:07Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    8,
                    43,
                    7,
                    6,
                    126,
                    0
                ],
                "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems"
                },
                "summary": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record."
                },
                "authors": [
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Anwar PP Abdul Majeed"
                    },
                    {
                        "name": "Pascal Lefevre"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Lefevre"
                },
                "author": "Pascal Lefevre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02850v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02850v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09600v2",
                "updated": "2024-09-03T03:45:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    45,
                    21,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-18T21:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning"
                },
                "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}"
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Gautam Bhattacharya"
                    },
                    {
                        "name": "Pratik Joshi"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00648v2",
                "updated": "2024-09-03T03:40:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    40,
                    8,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-01T17:24:42Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    17,
                    24,
                    42,
                    2,
                    122,
                    0
                ],
                "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination\n  Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination\n  Detection in Large Language Models"
                },
                "summary": "Large language models (LLMs) have transformed the landscape of language\nprocessing, yet struggle with significant challenges in terms of security,\nprivacy, and the generation of seemingly coherent but factually inaccurate\noutputs, commonly referred to as hallucinations. Among these challenges, one\nparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs\ngenerate content that directly contradicts established facts. Tackling FCH\nposes a formidable task due to two primary obstacles: Firstly, automating the\nconstruction and updating of benchmark datasets is challenging, as current\nmethods rely on static benchmarks that don't cover the diverse range of FCH\nscenarios. Secondly, validating LLM outputs' reasoning process is inherently\ncomplex, especially with intricate logical relations involved.\n  In addressing these obstacles, we propose an innovative approach leveraging\nlogic programming to enhance metamorphic testing for detecting Fact-Conflicting\nHallucinations (FCH). Our method gathers data from sources like Wikipedia,\nexpands it with logical reasoning to create diverse test cases, assesses LLMs\nthrough structured prompts, and validates their coherence using semantic-aware\nassessment mechanisms. Our method generates test cases and detects\nhallucinations across six different LLMs spanning nine domains, revealing\nhallucination rates ranging from 24.7% to 59.8%. Key observations indicate that\nLLMs encounter challenges, particularly with temporal concepts, handling\nout-of-distribution knowledge, and exhibiting deficiencies in logical reasoning\ncapabilities. The outcomes underscore the efficacy of logic-based test cases\ngenerated by our tool in both triggering and identifying hallucinations. These\nfindings underscore the imperative for ongoing collaborative endeavors within\nthe community to detect and address LLM hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed the landscape of language\nprocessing, yet struggle with significant challenges in terms of security,\nprivacy, and the generation of seemingly coherent but factually inaccurate\noutputs, commonly referred to as hallucinations. Among these challenges, one\nparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs\ngenerate content that directly contradicts established facts. Tackling FCH\nposes a formidable task due to two primary obstacles: Firstly, automating the\nconstruction and updating of benchmark datasets is challenging, as current\nmethods rely on static benchmarks that don't cover the diverse range of FCH\nscenarios. Secondly, validating LLM outputs' reasoning process is inherently\ncomplex, especially with intricate logical relations involved.\n  In addressing these obstacles, we propose an innovative approach leveraging\nlogic programming to enhance metamorphic testing for detecting Fact-Conflicting\nHallucinations (FCH). Our method gathers data from sources like Wikipedia,\nexpands it with logical reasoning to create diverse test cases, assesses LLMs\nthrough structured prompts, and validates their coherence using semantic-aware\nassessment mechanisms. Our method generates test cases and detects\nhallucinations across six different LLMs spanning nine domains, revealing\nhallucination rates ranging from 24.7% to 59.8%. Key observations indicate that\nLLMs encounter challenges, particularly with temporal concepts, handling\nout-of-distribution knowledge, and exhibiting deficiencies in logical reasoning\ncapabilities. The outcomes underscore the efficacy of logic-based test cases\ngenerated by our tool in both triggering and identifying hallucinations. These\nfindings underscore the imperative for ongoing collaborative endeavors within\nthe community to detect and address LLM hallucinations."
                },
                "authors": [
                    {
                        "name": "Ningke Li"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "29 pages, 11 figures, 4 tables, to appear in OOPSLA'24 (Vol.8,\n  No.OOPSLA2, Article 336)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00976v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00976v3",
                "updated": "2024-09-03T02:35:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    2,
                    35,
                    52,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-01T19:47:31Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    19,
                    47,
                    31,
                    3,
                    32,
                    0
                ],
                "title": "Investigating Recurrent Transformers with Dynamic Halt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Recurrent Transformers with Dynamic Halt"
                },
                "summary": "In this paper, we comprehensively study the inductive biases of two major\napproaches to augmenting Transformers with a recurrent mechanism: (1) the\napproach of incorporating a depth-wise recurrence similar to Universal\nTransformers; and (2) the approach of incorporating a chunk-wise temporal\nrecurrence like Temporal Latent Bottleneck. Furthermore, we propose and\ninvestigate novel ways to extend and combine the above methods - for example,\nwe propose a global mean-based dynamic halting mechanism for Universal\nTransformers and an augmentation of Temporal Latent Bottleneck with elements\nfrom Universal Transformer. We compare the models and probe their inductive\nbiases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop\nlanguage modeling, ListOps, and Logical Inference. The code is released in:\nhttps://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we comprehensively study the inductive biases of two major\napproaches to augmenting Transformers with a recurrent mechanism: (1) the\napproach of incorporating a depth-wise recurrence similar to Universal\nTransformers; and (2) the approach of incorporating a chunk-wise temporal\nrecurrence like Temporal Latent Bottleneck. Furthermore, we propose and\ninvestigate novel ways to extend and combine the above methods - for example,\nwe propose a global mean-based dynamic halting mechanism for Universal\nTransformers and an augmentation of Temporal Latent Bottleneck with elements\nfrom Universal Transformer. We compare the models and probe their inductive\nbiases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop\nlanguage modeling, ListOps, and Logical Inference. The code is released in:\nhttps://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main"
                },
                "authors": [
                    {
                        "name": "Jishnu Ray Chowdhury"
                    },
                    {
                        "name": "Cornelia Caragea"
                    }
                ],
                "author_detail": {
                    "name": "Cornelia Caragea"
                },
                "author": "Cornelia Caragea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00976v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00976v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06576v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06576v4",
                "updated": "2024-09-03T02:11:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    2,
                    11,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-04T04:17:40Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    4,
                    17,
                    40,
                    1,
                    156,
                    0
                ],
                "title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step"
                },
                "summary": "Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. Language model systems often enable LLMs to generate\ncode for arithmetic operations to achieve accurate calculations. However, this\napproach compromises speed and security, and fine-tuning risks the language\nmodel losing prior capabilities. We propose a framework that enables exact\narithmetic in a single autoregressive step, providing faster, more secure, and\nmore interpretable LLM systems with arithmetic capabilities. We use the hidden\nstates of a LLM to control a symbolic architecture that performs arithmetic.\nOur implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama)\nachieves 100\\% accuracy on single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nwith and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o\nwith and without a code interpreter on average across a range of mathematical\nproblem solving benchmarks, demonstrating that OccamLLMs can excel in\narithmetic tasks, even surpassing much larger models. We will make our code\npublic shortly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. Language model systems often enable LLMs to generate\ncode for arithmetic operations to achieve accurate calculations. However, this\napproach compromises speed and security, and fine-tuning risks the language\nmodel losing prior capabilities. We propose a framework that enables exact\narithmetic in a single autoregressive step, providing faster, more secure, and\nmore interpretable LLM systems with arithmetic capabilities. We use the hidden\nstates of a LLM to control a symbolic architecture that performs arithmetic.\nOur implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama)\nachieves 100\\% accuracy on single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nwith and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o\nwith and without a code interpreter on average across a range of mathematical\nproblem solving benchmarks, demonstrating that OccamLLMs can excel in\narithmetic tasks, even surpassing much larger models. We will make our code\npublic shortly."
                },
                "authors": [
                    {
                        "name": "Owen Dugan"
                    },
                    {
                        "name": "Donato Manuel Jimenez Beneto"
                    },
                    {
                        "name": "Charlotte Loh"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Marin Soljaƒçiƒá"
                    }
                ],
                "author_detail": {
                    "name": "Marin Soljaƒçiƒá"
                },
                "author": "Marin Soljaƒçiƒá",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06576v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06576v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v3",
                "updated": "2024-09-03T01:14:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    1,
                    14,
                    30,
                    1,
                    247,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates decision-making capabilities of LLMs through the lens of\nGame Theory. We focus specifically on games that support the simultaneous\nparticipation of more than two agents. We introduce GAMA($\\gamma$)-Bench, which\nevaluates LLMs' Gaming Ability in Multi-Agent environments. $\\gamma$-Bench\nincludes eight classical multi-agent games and a scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. Leveraging $\\gamma$-Bench,\nwe investigate LLMs' robustness, generalizability, and strategies for\nenhancement. Results reveal that while GPT-3.5 shows satisfying robustness, its\ngeneralizability is relatively limited. However, its performance can be\nimproved through approaches such as Chain-of-Thought. Additionally, we evaluate\ntwelve versions from six models, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1,\nMixtral, and Qwen-2. We find that Gemini-1.5-Pro outperforms other models with\na score of $63.8$ out of $100$, followed by LLaMA-3.1-70B and GPT-4 with scores\nof $60.9$ and $60.5$, respectively. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates decision-making capabilities of LLMs through the lens of\nGame Theory. We focus specifically on games that support the simultaneous\nparticipation of more than two agents. We introduce GAMA($\\gamma$)-Bench, which\nevaluates LLMs' Gaming Ability in Multi-Agent environments. $\\gamma$-Bench\nincludes eight classical multi-agent games and a scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. Leveraging $\\gamma$-Bench,\nwe investigate LLMs' robustness, generalizability, and strategies for\nenhancement. Results reveal that while GPT-3.5 shows satisfying robustness, its\ngeneralizability is relatively limited. However, its performance can be\nimproved through approaches such as Chain-of-Thought. Additionally, we evaluate\ntwelve versions from six models, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1,\nMixtral, and Qwen-2. We find that Gemini-1.5-Pro outperforms other models with\na score of $63.8$ out of $100$, followed by LLaMA-3.1-70B and GPT-4 with scores\nof $60.9$ and $60.5$, respectively. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "11 pages of main text. 20 pages of appendices. 12 figures, 9 tables.\n  Added models: Gemini-1.5-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15092v2",
                "updated": "2024-09-02T22:40:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    22,
                    40,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-23T22:38:58Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    22,
                    38,
                    58,
                    3,
                    144,
                    0
                ],
                "title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissociation of Faithful and Unfaithful Reasoning in LLMs"
                },
                "summary": "Large language models (LLMs) often improve their performance in downstream\ntasks when they generate Chain of Thought reasoning text before producing an\nanswer. We investigate how LLMs recover from errors in Chain of Thought.\nThrough analysis of error recovery behaviors, we find evidence for\nunfaithfulness in Chain of Thought, which occurs when models arrive at the\ncorrect answer despite invalid reasoning text. We identify factors that shift\nLLM recovery behavior: LLMs recover more frequently from obvious errors and in\ncontexts that provide more evidence for the correct answer. Critically, these\nfactors have divergent effects on faithful and unfaithful recoveries. Our\nresults indicate that there are distinct mechanisms driving faithful and\nunfaithful error recoveries. Selective targeting of these mechanisms may be\nable to drive down the rate of unfaithful reasoning and improve model\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often improve their performance in downstream\ntasks when they generate Chain of Thought reasoning text before producing an\nanswer. We investigate how LLMs recover from errors in Chain of Thought.\nThrough analysis of error recovery behaviors, we find evidence for\nunfaithfulness in Chain of Thought, which occurs when models arrive at the\ncorrect answer despite invalid reasoning text. We identify factors that shift\nLLM recovery behavior: LLMs recover more frequently from obvious errors and in\ncontexts that provide more evidence for the correct answer. Critically, these\nfactors have divergent effects on faithful and unfaithful recoveries. Our\nresults indicate that there are distinct mechanisms driving faithful and\nunfaithful error recoveries. Selective targeting of these mechanisms may be\nable to drive down the rate of unfaithful reasoning and improve model\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Evelyn Yee"
                    },
                    {
                        "name": "Alice Li"
                    },
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Yeon Ho Jung"
                    },
                    {
                        "name": "Ramamohan Paturi"
                    },
                    {
                        "name": "Leon Bergen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Bergen"
                },
                "author": "Leon Bergen",
                "arxiv_comment": "code published at\n  https://github.com/CoTErrorRecovery/CoTErrorRecovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07981v2",
                "updated": "2024-09-02T21:29:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    21,
                    29,
                    4,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-11T17:57:32Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    32,
                    3,
                    102,
                    0
                ],
                "title": "Manipulating Large Language Models to Increase Product Visibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulating Large Language Models to Increase Product Visibility"
                },
                "summary": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer."
                },
                "authors": [
                    {
                        "name": "Aounon Kumar"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    }
                ],
                "author_detail": {
                    "name": "Himabindu Lakkaraju"
                },
                "author": "Himabindu Lakkaraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17937v2",
                "updated": "2024-09-02T20:58:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    58,
                    43,
                    0,
                    246,
                    0
                ],
                "published": "2024-03-26T17:59:58Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    17,
                    59,
                    58,
                    1,
                    86,
                    0
                ],
                "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory"
                },
                "summary": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Syed Talal Wasim"
                    },
                    {
                        "name": "Martin Danelljan"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.11686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.11686v3",
                "updated": "2024-09-02T20:57:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    57,
                    56,
                    0,
                    246,
                    0
                ],
                "published": "2022-07-24T08:08:32Z",
                "published_parsed": [
                    2022,
                    7,
                    24,
                    8,
                    8,
                    32,
                    6,
                    205,
                    0
                ],
                "title": "Inference for linear functionals of high-dimensional longitudinal\n  proteomics data using generalized estimating equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for linear functionals of high-dimensional longitudinal\n  proteomics data using generalized estimating equations"
                },
                "summary": "Regression analysis of correlated data, where multiple correlated responses\nare recorded on the same unit, is ubiquitous in many scientific areas. With the\nadvent of new technologies, in particular high-throughput omics profiling\nassays, such correlated data increasingly consist of large number of variables\ncompared with the available sample size. Motivated by recent longitudinal\nproteomics studies of COVID-19, we propose a novel inference procedure for\nlinear functionals of high-dimensional regression coefficients in generalized\nestimating equations, which are widely used to analyze correlated data. Our\nestimator for this more general inferential target, obtained via constructing\nprojected estimating equations, is shown to be asymptotically normally\ndistributed under mild regularity conditions. We also introduce a data-driven\ncross-validation procedure to select the tuning parameter for estimating the\nprojection direction, which is not addressed in the existing procedures. We\nillustrate the utility of the proposed procedure in providing confidence\nintervals for associations of individual proteins and severe COVID risk scores\nobtained based on high-dimensional proteomics data, and demonstrate its robust\nfinite-sample performance, especially in estimation bias and confidence\ninterval coverage, via extensive simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression analysis of correlated data, where multiple correlated responses\nare recorded on the same unit, is ubiquitous in many scientific areas. With the\nadvent of new technologies, in particular high-throughput omics profiling\nassays, such correlated data increasingly consist of large number of variables\ncompared with the available sample size. Motivated by recent longitudinal\nproteomics studies of COVID-19, we propose a novel inference procedure for\nlinear functionals of high-dimensional regression coefficients in generalized\nestimating equations, which are widely used to analyze correlated data. Our\nestimator for this more general inferential target, obtained via constructing\nprojected estimating equations, is shown to be asymptotically normally\ndistributed under mild regularity conditions. We also introduce a data-driven\ncross-validation procedure to select the tuning parameter for estimating the\nprojection direction, which is not addressed in the existing procedures. We\nillustrate the utility of the proposed procedure in providing confidence\nintervals for associations of individual proteins and severe COVID risk scores\nobtained based on high-dimensional proteomics data, and demonstrate its robust\nfinite-sample performance, especially in estimation bias and confidence\ninterval coverage, via extensive simulations."
                },
                "authors": [
                    {
                        "name": "Lu Xia"
                    },
                    {
                        "name": "Ali Shojaie"
                    }
                ],
                "author_detail": {
                    "name": "Ali Shojaie"
                },
                "author": "Ali Shojaie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.11686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.11686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07529v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07529v3",
                "updated": "2024-09-02T20:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    42,
                    8,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-11T17:55:25Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    55,
                    25,
                    1,
                    163,
                    0
                ],
                "title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via\n  Quadratic Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via\n  Quadratic Approximation"
                },
                "summary": "Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages."
                },
                "authors": [
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Zhiqi Bu"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Yoshua Bengio"
                    }
                ],
                "author_detail": {
                    "name": "Yoshua Bengio"
                },
                "author": "Yoshua Bengio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07529v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07529v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v2",
                "updated": "2024-09-02T20:26:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    26,
                    30,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Hanyang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Hanyang Zhong"
                },
                "author": "Hanyang Zhong",
                "arxiv_comment": "This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11927v3",
                "updated": "2024-09-02T20:26:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    26,
                    26,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-17T10:45:22Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    10,
                    45,
                    22,
                    0,
                    169,
                    0
                ],
                "title": "On the Impacts of Contexts on Repository-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impacts of Contexts on Repository-Level Code Generation"
                },
                "summary": "CodeLLMs have gained widespread adoption for code generation tasks, yet their\ncapacity to handle repository-level code generation with complex contextual\ndependencies remains underexplored. Our work underscores the critical\nimportance of leveraging repository-level contexts to generate executable and\nfunctionally correct code. We present \\textbf{\\methodnamews}, a novel benchmark\ndesigned to evaluate repository-level code generation, with a focus on three\nkey aspects: executability, functional correctness through comprehensive test\ncase generation, and accurate utilization of cross-file contexts. Our study\nexamines a controlled scenario where developers specify essential code\ndependencies (contexts), challenging models to integrate them effectively.\nAdditionally, we introduce an instruction-tuned dataset that enhances CodeLLMs'\nability to leverage dependencies, along with a new metric, \\textit{Dependency\nInvocation Rate (DIR)}, to quantify context utilization. Experimental results\nreveal that while pretrained LLMs demonstrate superior performance in terms of\ncorrectness, instruction-tuned models excel in context utilization and\ndebugging capabilities. \\methodnamews offers a comprehensive evaluation\nframework for assessing code functionality and alignment with developer intent,\nthereby advancing the development of more reliable CodeLLMs for real-world\napplications. The dataset and source code are available\nat~\\url{https://github.com/FSoft-AI4Code/RepoExec}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeLLMs have gained widespread adoption for code generation tasks, yet their\ncapacity to handle repository-level code generation with complex contextual\ndependencies remains underexplored. Our work underscores the critical\nimportance of leveraging repository-level contexts to generate executable and\nfunctionally correct code. We present \\textbf{\\methodnamews}, a novel benchmark\ndesigned to evaluate repository-level code generation, with a focus on three\nkey aspects: executability, functional correctness through comprehensive test\ncase generation, and accurate utilization of cross-file contexts. Our study\nexamines a controlled scenario where developers specify essential code\ndependencies (contexts), challenging models to integrate them effectively.\nAdditionally, we introduce an instruction-tuned dataset that enhances CodeLLMs'\nability to leverage dependencies, along with a new metric, \\textit{Dependency\nInvocation Rate (DIR)}, to quantify context utilization. Experimental results\nreveal that while pretrained LLMs demonstrate superior performance in terms of\ncorrectness, instruction-tuned models excel in context utilization and\ndebugging capabilities. \\methodnamews offers a comprehensive evaluation\nframework for assessing code functionality and alignment with developer intent,\nthereby advancing the development of more reliable CodeLLMs for real-world\napplications. The dataset and source code are available\nat~\\url{https://github.com/FSoft-AI4Code/RepoExec}."
                },
                "authors": [
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Dung Manh Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15077v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15077v4",
                "updated": "2024-09-02T20:25:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    25,
                    36,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-23T21:56:12Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    21,
                    56,
                    12,
                    3,
                    144,
                    0
                ],
                "title": "Eliciting Informative Text Evaluations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Informative Text Evaluations with Large Language Models"
                },
                "summary": "Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Shengwei Xu"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuqing Kong"
                    },
                    {
                        "name": "Grant Schoenebeck"
                    }
                ],
                "author_detail": {
                    "name": "Grant Schoenebeck"
                },
                "author": "Grant Schoenebeck",
                "arxiv_comment": "Accepted by the Twenty-Fifth ACM Conference on Economics and\n  Computation (EC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15077v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15077v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12967v2",
                "updated": "2024-09-02T19:19:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    19,
                    19,
                    59,
                    0,
                    246,
                    0
                ],
                "published": "2024-01-23T18:52:05Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    18,
                    52,
                    5,
                    1,
                    23,
                    0
                ],
                "title": "Measure transport with kernel mean embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measure transport with kernel mean embeddings"
                },
                "summary": "Kalman filters constitute a scalable and robust methodology for approximate\nBayesian inference, matching first and second order moments of the target\nposterior. To improve the accuracy in nonlinear and non-Gaussian settings, we\nextend this principle to include more or different characteristics, based on\nkernel mean embeddings (KMEs) of probability measures into reproducing kernel\nHilbert spaces. Focusing on the continuous-time setting, we develop a family of\ninteracting particle systems (termed $\\textit{KME-dynamics}$) that bridge\nbetween prior and posterior, and that include the Kalman-Bucy filter as a\nspecial case. KME-dynamics does not require the score of the target, but rather\nestimates the score implicitly and intrinsically, and we develop links to\nscore-based generative modeling and importance reweighting. A variant of\nKME-dynamics has recently been derived from an optimal transport and Fisher-Rao\ngradient flow perspective by Maurais and Marzouk, and we expose further\nconnections to (kernelised) diffusion maps, leading to a variational\nformulation of regression type. Finally, we conduct numerical experiments on\ntoy examples and the Lorenz 63 and 96 models, comparing our results against the\nensemble Kalman filter and the mapping particle filter (Pulido and van Leeuwen,\n2019, J. Comput. Phys.). Our experiments show particular promise for a hybrid\nmodification (called Kalman-adjusted KME-dynamics).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalman filters constitute a scalable and robust methodology for approximate\nBayesian inference, matching first and second order moments of the target\nposterior. To improve the accuracy in nonlinear and non-Gaussian settings, we\nextend this principle to include more or different characteristics, based on\nkernel mean embeddings (KMEs) of probability measures into reproducing kernel\nHilbert spaces. Focusing on the continuous-time setting, we develop a family of\ninteracting particle systems (termed $\\textit{KME-dynamics}$) that bridge\nbetween prior and posterior, and that include the Kalman-Bucy filter as a\nspecial case. KME-dynamics does not require the score of the target, but rather\nestimates the score implicitly and intrinsically, and we develop links to\nscore-based generative modeling and importance reweighting. A variant of\nKME-dynamics has recently been derived from an optimal transport and Fisher-Rao\ngradient flow perspective by Maurais and Marzouk, and we expose further\nconnections to (kernelised) diffusion maps, leading to a variational\nformulation of regression type. Finally, we conduct numerical experiments on\ntoy examples and the Lorenz 63 and 96 models, comparing our results against the\nensemble Kalman filter and the mapping particle filter (Pulido and van Leeuwen,\n2019, J. Comput. Phys.). Our experiments show particular promise for a hybrid\nmodification (called Kalman-adjusted KME-dynamics)."
                },
                "authors": [
                    {
                        "name": "L. Wang"
                    },
                    {
                        "name": "N. N√ºsken"
                    }
                ],
                "author_detail": {
                    "name": "N. N√ºsken"
                },
                "author": "N. N√ºsken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09147v3",
                "updated": "2024-09-02T19:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    19,
                    1,
                    44,
                    0,
                    246,
                    0
                ],
                "published": "2024-02-14T12:56:58Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    12,
                    56,
                    58,
                    2,
                    45,
                    0
                ],
                "title": "Into the Unknown: Self-Learning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Into the Unknown: Self-Learning Large Language Models"
                },
                "summary": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. We introduce a concept called Point in the Unknown\n(PiU) to identify atomic knowledge unknown to a model, along with four methods\nfor automatic PiUs identification, facilitating the creation of a self-learning\nloop that focuses exclusively on the absorption of currently unknown knowledge\ninto the model. Additionally, we developed evaluation metrics to gauge an LLM's\nself-learning capability. Our experiments revealed that LLMs with at least 3B\nparameters that have undergone some instruction training would be able to\nperform self-learning well. We further proved the effectiveness of\nself-learning by comparing the performance of a model that has undergone\nself-learning to a model that has not. Our self-learning concept allows more\nefficient LLM updates and opens new perspectives for LLM knowledge exchange.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. We introduce a concept called Point in the Unknown\n(PiU) to identify atomic knowledge unknown to a model, along with four methods\nfor automatic PiUs identification, facilitating the creation of a self-learning\nloop that focuses exclusively on the absorption of currently unknown knowledge\ninto the model. Additionally, we developed evaluation metrics to gauge an LLM's\nself-learning capability. Our experiments revealed that LLMs with at least 3B\nparameters that have undergone some instruction training would be able to\nperform self-learning well. We further proved the effectiveness of\nself-learning by comparing the performance of a model that has undergone\nself-learning to a model that has not. Our self-learning concept allows more\nefficient LLM updates and opens new perspectives for LLM knowledge exchange."
                },
                "authors": [
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Jan Koco≈Ñ"
                    },
                    {
                        "name": "Przemys≈Çaw Kazienko"
                    }
                ],
                "author_detail": {
                    "name": "Przemys≈Çaw Kazienko"
                },
                "author": "Przemys≈Çaw Kazienko",
                "arxiv_comment": "10 pages, 3 figures, 3 tables, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13423v3",
                "updated": "2024-09-02T18:02:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    18,
                    2,
                    3,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-24T01:33:28Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    1,
                    33,
                    28,
                    5,
                    237,
                    0
                ],
                "title": "Training-free Long Video Generation with Chain of Diffusion Model\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Long Video Generation with Chain of Diffusion Model\n  Experts"
                },
                "summary": "Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yichao Cao"
                    },
                    {
                        "name": "Xiu Su"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shan You"
                    },
                    {
                        "name": "Mingkai Zheng"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21009v2",
                "updated": "2024-09-02T18:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    18,
                    1,
                    44,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-30T17:55:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    55,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "AI-Assisted Generation of Difficult Math Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Generation of Difficult Math Questions"
                },
                "summary": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills."
                },
                "authors": [
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00872v2",
                "updated": "2024-09-02T17:41:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    41,
                    24,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-01T18:46:05Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    46,
                    5,
                    3,
                    214,
                    0
                ],
                "title": "Online Detection of Anomalies in Temporal Knowledge Graphs with\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Detection of Anomalies in Temporal Knowledge Graphs with\n  Interpretability"
                },
                "summary": "Temporal knowledge graphs (TKGs) are valuable resources for capturing\nevolving relationships among entities, yet they are often plagued by noise,\nnecessitating robust anomaly detection mechanisms. Existing dynamic graph\nanomaly detection approaches struggle to capture the rich semantics introduced\nby node and edge categories within TKGs, while TKG embedding methods lack\ninterpretability, undermining the credibility of anomaly detection. Moreover,\nthese methods falter in adapting to pattern changes and semantic drifts\nresulting from knowledge updates. To tackle these challenges, we introduce\nAnoT, an efficient TKG summarization method tailored for interpretable online\nanomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule\ngraph, enabling flexible inference of complex patterns in TKGs. When new\nknowledge emerges, AnoT maps it onto a node in the rule graph and traverses the\nrule graph recursively to derive the anomaly score of the knowledge. The\ntraversal yields reachable nodes that furnish interpretable evidence for the\nvalidity or the anomalous of the new knowledge. Overall, AnoT embodies a\ndetector-updater-monitor architecture, encompassing a detector for offline TKG\nsummarization and online scoring, an updater for real-time rule graph updates\nbased on emerging knowledge, and a monitor for estimating the approximation\nerror of the rule graph. Experimental results on four real-world datasets\ndemonstrate that AnoT surpasses existing methods significantly in terms of\naccuracy and interoperability. All of the raw datasets and the implementation\nof AnoT are provided in https://github.com/zjs123/ANoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal knowledge graphs (TKGs) are valuable resources for capturing\nevolving relationships among entities, yet they are often plagued by noise,\nnecessitating robust anomaly detection mechanisms. Existing dynamic graph\nanomaly detection approaches struggle to capture the rich semantics introduced\nby node and edge categories within TKGs, while TKG embedding methods lack\ninterpretability, undermining the credibility of anomaly detection. Moreover,\nthese methods falter in adapting to pattern changes and semantic drifts\nresulting from knowledge updates. To tackle these challenges, we introduce\nAnoT, an efficient TKG summarization method tailored for interpretable online\nanomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule\ngraph, enabling flexible inference of complex patterns in TKGs. When new\nknowledge emerges, AnoT maps it onto a node in the rule graph and traverses the\nrule graph recursively to derive the anomaly score of the knowledge. The\ntraversal yields reachable nodes that furnish interpretable evidence for the\nvalidity or the anomalous of the new knowledge. Overall, AnoT embodies a\ndetector-updater-monitor architecture, encompassing a detector for offline TKG\nsummarization and online scoring, an updater for real-time rule graph updates\nbased on emerging knowledge, and a monitor for estimating the approximation\nerror of the rule graph. Experimental results on four real-world datasets\ndemonstrate that AnoT surpasses existing methods significantly in terms of\naccuracy and interoperability. All of the raw datasets and the implementation\nof AnoT are provided in https://github.com/zjs123/ANoT."
                },
                "authors": [
                    {
                        "name": "Jiasheng Zhang"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Jie Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jie Shao"
                },
                "author": "Jie Shao",
                "arxiv_comment": "26 pages, 10 figures. Accepted by SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04268v3",
                "updated": "2024-09-02T17:13:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    13,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-05T05:45:34Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    5,
                    45,
                    34,
                    4,
                    187,
                    0
                ],
                "title": "NeuFair: Neural Network Fairness Repair with Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuFair: Neural Network Fairness Repair with Dropout"
                },
                "summary": "This paper investigates neuron dropout as a post-processing bias mitigation\nfor deep neural networks (DNNs). Neural-driven software solutions are\nincreasingly applied in socially critical domains with significant fairness\nimplications. While neural networks are exceptionally good at finding\nstatistical patterns from data, they may encode and amplify existing biases\nfrom the historical data. Existing bias mitigation algorithms often require\nmodifying the input dataset or the learning algorithms. We posit that the\nprevalent dropout methods that prevent over-fitting during training by randomly\ndropping neurons may be an effective and less intrusive approach to improve the\nfairness of pre-trained DNNs. However, finding the ideal set of neurons to drop\nis a combinatorial problem. We propose NeuFair, a family of post-processing\nrandomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts\nduring inference after training. Our randomized search is guided by an\nobjective to minimize discrimination while maintaining the model's utility. We\nshow that our design of randomized algorithms is effective and efficient in\nimproving fairness (up to 69%) with minimal or no model performance\ndegradation. We provide intuitive explanations of these phenomena and carefully\nexamine the influence of various hyperparameters of search algorithms on the\nresults. Finally, we empirically and conceptually compare NeuFair to different\nstate-of-the-art bias mitigators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates neuron dropout as a post-processing bias mitigation\nfor deep neural networks (DNNs). Neural-driven software solutions are\nincreasingly applied in socially critical domains with significant fairness\nimplications. While neural networks are exceptionally good at finding\nstatistical patterns from data, they may encode and amplify existing biases\nfrom the historical data. Existing bias mitigation algorithms often require\nmodifying the input dataset or the learning algorithms. We posit that the\nprevalent dropout methods that prevent over-fitting during training by randomly\ndropping neurons may be an effective and less intrusive approach to improve the\nfairness of pre-trained DNNs. However, finding the ideal set of neurons to drop\nis a combinatorial problem. We propose NeuFair, a family of post-processing\nrandomized algorithms that mitigate unfairness in pre-trained DNNs via dropouts\nduring inference after training. Our randomized search is guided by an\nobjective to minimize discrimination while maintaining the model's utility. We\nshow that our design of randomized algorithms is effective and efficient in\nimproving fairness (up to 69%) with minimal or no model performance\ndegradation. We provide intuitive explanations of these phenomena and carefully\nexamine the influence of various hyperparameters of search algorithms on the\nresults. Finally, we empirically and conceptually compare NeuFair to different\nstate-of-the-art bias mitigators."
                },
                "authors": [
                    {
                        "name": "Vishnu Asutosh Dasu"
                    },
                    {
                        "name": "Ashish Kumar"
                    },
                    {
                        "name": "Saeid Tizpaz-Niari"
                    },
                    {
                        "name": "Gang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Gang Tan"
                },
                "author": "Gang Tan",
                "arxiv_comment": "Paper accepted at ACM ISSTA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12288v3",
                "updated": "2024-09-02T17:12:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    12,
                    48,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-18T05:49:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    5,
                    49,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "An Investigation of Neuron Activation as a Unified Lens to Explain\n  Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Neuron Activation as a Unified Lens to Explain\n  Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs"
                },
                "summary": "Large language models (LLMs) have shown strong arithmetic reasoning\ncapabilities when prompted with Chain-of-Thought (CoT) prompts. However, we\nhave only a limited understanding of how they are processed by LLMs. To\ndemystify it, prior work has primarily focused on ablating different components\nin the CoT prompt and empirically observing their resulting LLM performance\nchange. Yet, the reason why these components are important to LLM reasoning is\nnot explored. To fill this gap, in this work, we investigate ``neuron\nactivation'' as a lens to provide a unified explanation to observations made by\nprior work. Specifically, we look into neurons within the feed-forward layers\nof LLMs that may have activated their arithmetic reasoning capabilities, using\nLlama2 as an example. To facilitate this investigation, we also propose an\napproach based on GPT-4 to automatically identify neurons that imply arithmetic\nreasoning. Our analyses revealed that the activation of reasoning neurons in\nthe feed-forward layers of an LLM can explain the importance of various\ncomponents in a CoT prompt, and future research can extend it for a more\ncomplete understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong arithmetic reasoning\ncapabilities when prompted with Chain-of-Thought (CoT) prompts. However, we\nhave only a limited understanding of how they are processed by LLMs. To\ndemystify it, prior work has primarily focused on ablating different components\nin the CoT prompt and empirically observing their resulting LLM performance\nchange. Yet, the reason why these components are important to LLM reasoning is\nnot explored. To fill this gap, in this work, we investigate ``neuron\nactivation'' as a lens to provide a unified explanation to observations made by\nprior work. Specifically, we look into neurons within the feed-forward layers\nof LLMs that may have activated their arithmetic reasoning capabilities, using\nLlama2 as an example. To facilitate this investigation, we also propose an\napproach based on GPT-4 to automatically identify neurons that imply arithmetic\nreasoning. Our analyses revealed that the activation of reasoning neurons in\nthe feed-forward layers of an LLM can explain the importance of various\ncomponents in a CoT prompt, and future research can extend it for a more\ncomplete understanding."
                },
                "authors": [
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "arxiv_comment": "9 pages, 1 figure, to be published in ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.04915v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.04915v4",
                "updated": "2024-09-02T17:08:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    8,
                    55,
                    0,
                    246,
                    0
                ],
                "published": "2022-11-09T14:36:39Z",
                "published_parsed": [
                    2022,
                    11,
                    9,
                    14,
                    36,
                    39,
                    2,
                    313,
                    0
                ],
                "title": "Inferring Mobility of Care Travel Behavior From Transit\n  Origin-Destination Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Mobility of Care Travel Behavior From Transit\n  Origin-Destination Data"
                },
                "summary": "There are substantial differences in travel behavior by gender on public\ntransit. Studies have concluded that these differences are largely attributable\nto household responsibilities typically falling disproportionately on women,\nleading to women being more likely to utilize transit for purposes referred to\nby the umbrella concept of \"mobility of care\". In contrast to past studies that\nhave quantified the impact of gender using survey and qualitative data, we\npropose a novel data-driven workflow utilizing a combination of previously\ndeveloped origin, destination, and transfer inference (ODX) based on individual\ntransit fare card transactions, name-based gender inference, and geospatial\nanalysis as a framework to identify mobility of care trip making. We apply this\nframework to data from the Washington Metropolitan Area Transit Authority\n(WMATA). Analyzing data from millions of journeys conducted in the first\nquarter of 2019, the results of this study show that our proposed workflow can\nidentify mobility of care travel behavior, detecting times and places of\ninterest where the share of women travelers in an equally-sampled subset (on\nbasis of inferred gender) of transit users is 10% - 15% higher than that of\nmen. The workflow presented in this study provides a blueprint for combining\ntransit origin-destination data, inferred customer demographics, and geospatial\nanalyses enabling public transit agencies to assess, at the fare card level,\nthe gendered impacts of different policy and operational decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are substantial differences in travel behavior by gender on public\ntransit. Studies have concluded that these differences are largely attributable\nto household responsibilities typically falling disproportionately on women,\nleading to women being more likely to utilize transit for purposes referred to\nby the umbrella concept of \"mobility of care\". In contrast to past studies that\nhave quantified the impact of gender using survey and qualitative data, we\npropose a novel data-driven workflow utilizing a combination of previously\ndeveloped origin, destination, and transfer inference (ODX) based on individual\ntransit fare card transactions, name-based gender inference, and geospatial\nanalysis as a framework to identify mobility of care trip making. We apply this\nframework to data from the Washington Metropolitan Area Transit Authority\n(WMATA). Analyzing data from millions of journeys conducted in the first\nquarter of 2019, the results of this study show that our proposed workflow can\nidentify mobility of care travel behavior, detecting times and places of\ninterest where the share of women travelers in an equally-sampled subset (on\nbasis of inferred gender) of transit users is 10% - 15% higher than that of\nmen. The workflow presented in this study provides a blueprint for combining\ntransit origin-destination data, inferred customer demographics, and geospatial\nanalyses enabling public transit agencies to assess, at the fare card level,\nthe gendered impacts of different policy and operational decisions."
                },
                "authors": [
                    {
                        "name": "Awad Abdelhalim"
                    },
                    {
                        "name": "Daniela Shuman"
                    },
                    {
                        "name": "Anson F Stewart"
                    },
                    {
                        "name": "Kayleigh B Campbell"
                    },
                    {
                        "name": "Mira Patel"
                    },
                    {
                        "name": "Ines Sanchez de Madariaga"
                    },
                    {
                        "name": "Jinhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Zhao"
                },
                "author": "Jinhua Zhao",
                "arxiv_comment": "Final revised version for journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.04915v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.04915v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10108v2",
                "updated": "2024-09-02T17:00:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    0,
                    21,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-15T06:30:55Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    6,
                    30,
                    55,
                    4,
                    349,
                    0
                ],
                "title": "Privacy-Aware Document Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Aware Document Visual Question Answering"
                },
                "summary": "Document Visual Question Answering (DocVQA) has quickly grown into a central\ntask of document understanding. But despite the fact that documents contain\nsensitive or copyrighted information, none of the current DocVQA methods offers\nstrong privacy guarantees. In this work, we explore privacy in the domain of\nDocVQA for the first time, highlighting privacy issues in state of the art\nmulti-modal LLM models used for DocVQA, and explore possible solutions.\nSpecifically, we focus on invoice processing as a realistic document\nunderstanding scenario, and propose a large scale DocVQA dataset comprising\ninvoice documents and associated questions and answers. We employ a federated\nlearning scheme, that reflects the real-life distribution of documents in\ndifferent businesses, and we explore the use case where the data of the invoice\nprovider is the sensitive information to be protected. We demonstrate that\nnon-private models tend to memorise, a behaviour that can lead to exposing\nprivate information. We then evaluate baseline training schemes employing\nfederated learning and differential privacy in this multi-modal scenario, where\nthe sensitive information might be exposed through either or both of the two\ninput modalities: vision (document image) or language (OCR tokens). Finally, we\ndesign attacks exploiting the memorisation effect of the model, and demonstrate\ntheir effectiveness in probing a representative DocVQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (DocVQA) has quickly grown into a central\ntask of document understanding. But despite the fact that documents contain\nsensitive or copyrighted information, none of the current DocVQA methods offers\nstrong privacy guarantees. In this work, we explore privacy in the domain of\nDocVQA for the first time, highlighting privacy issues in state of the art\nmulti-modal LLM models used for DocVQA, and explore possible solutions.\nSpecifically, we focus on invoice processing as a realistic document\nunderstanding scenario, and propose a large scale DocVQA dataset comprising\ninvoice documents and associated questions and answers. We employ a federated\nlearning scheme, that reflects the real-life distribution of documents in\ndifferent businesses, and we explore the use case where the data of the invoice\nprovider is the sensitive information to be protected. We demonstrate that\nnon-private models tend to memorise, a behaviour that can lead to exposing\nprivate information. We then evaluate baseline training schemes employing\nfederated learning and differential privacy in this multi-modal scenario, where\nthe sensitive information might be exposed through either or both of the two\ninput modalities: vision (document image) or language (OCR tokens). Finally, we\ndesign attacks exploiting the memorisation effect of the model, and demonstrate\ntheir effectiveness in probing a representative DocVQA models."
                },
                "authors": [
                    {
                        "name": "Rub√®n Tito"
                    },
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Marlon Tobaben"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mohamed Ali Souibgui"
                    },
                    {
                        "name": "Kangsoo Jung"
                    },
                    {
                        "name": "Joonas J√§lk√∂"
                    },
                    {
                        "name": "Vincent Poulain D'Andecy"
                    },
                    {
                        "name": "Aurelie Joseph"
                    },
                    {
                        "name": "Lei Kang"
                    },
                    {
                        "name": "Ernest Valveny"
                    },
                    {
                        "name": "Antti Honkela"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Dimosthenis Karatzas"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Karatzas"
                },
                "author": "Dimosthenis Karatzas",
                "arxiv_comment": "35 pages, 12 figures, accepted for publication at the 18th\n  International Conference on Document Analysis and Recognition, ICDAR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16160v2",
                "updated": "2024-09-02T16:33:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    33,
                    29,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-24T19:30:18Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    19,
                    30,
                    18,
                    2,
                    115,
                    0
                ],
                "title": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive generalization\ncapabilities on specific tasks with human-written instruction data. However,\nthe limited quantity, diversity, and professional expertise of such instruction\ndata raise concerns about the performance of LLMs in psychotherapy tasks when\nprovided with domain-specific instructions. To address this, we firstly propose\nDomain-Specific Assistant Instructions based on AlexanderStreet therapy, and\nsecondly, we use an adaption fine-tuning method and retrieval augmented\ngeneration method to improve pre-trained LLMs. Through quantitative evaluation\nof linguistic quality using automatic and human evaluation, we observe that\npre-trained LLMs on Psychotherapy Assistant Instructions outperform\nstate-of-the-art LLMs response baselines. Our Assistant-Instruction approach\noffers a half-annotation method to align pre-trained LLMs with instructions and\nprovide pre-trained LLMs with more psychotherapy knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive generalization\ncapabilities on specific tasks with human-written instruction data. However,\nthe limited quantity, diversity, and professional expertise of such instruction\ndata raise concerns about the performance of LLMs in psychotherapy tasks when\nprovided with domain-specific instructions. To address this, we firstly propose\nDomain-Specific Assistant Instructions based on AlexanderStreet therapy, and\nsecondly, we use an adaption fine-tuning method and retrieval augmented\ngeneration method to improve pre-trained LLMs. Through quantitative evaluation\nof linguistic quality using automatic and human evaluation, we observe that\npre-trained LLMs on Psychotherapy Assistant Instructions outperform\nstate-of-the-art LLMs response baselines. Our Assistant-Instruction approach\noffers a half-annotation method to align pre-trained LLMs with instructions and\nprovide pre-trained LLMs with more psychotherapy knowledge."
                },
                "authors": [
                    {
                        "name": "Cheng Kang"
                    },
                    {
                        "name": "Daniel Novak"
                    },
                    {
                        "name": "Katerina Urbanova"
                    },
                    {
                        "name": "Yuqing Cheng"
                    },
                    {
                        "name": "Yong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Hu"
                },
                "author": "Yong Hu",
                "arxiv_doi": "10.1109/ICASSPW62465.2024.10626529",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSPW62465.2024.10626529",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.16160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ICASSP 2024 EIHRC Workshop",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09493v2",
                "updated": "2024-09-02T16:19:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    19,
                    25,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-18T14:16:55Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    14,
                    16,
                    55,
                    6,
                    231,
                    0
                ],
                "title": "Ancestral Reinforcement Learning: Unifying Zeroth-Order Optimization and\n  Genetic Algorithms for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancestral Reinforcement Learning: Unifying Zeroth-Order Optimization and\n  Genetic Algorithms for Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) offers a fundamental framework for discovering\noptimal action strategies through interactions within unknown environments.\nRecent advancement have shown that the performance and applicability of RL can\nsignificantly be enhanced by exploiting a population of agents in various ways.\nZeroth-Order Optimization (ZOO) leverages an agent population to estimate the\ngradient of the objective function, enabling robust policy refinement even in\nnon-differentiable scenarios. As another application, Genetic Algorithms (GA)\nboosts the exploration of policy landscapes by mutational generation of policy\ndiversity in an agent population and its refinement by selection. A natural\nquestion is whether we can have the best of two worlds that the agent\npopulation can have. In this work, we propose Ancestral Reinforcement Learning\n(ARL), which synergistically combines the robust gradient estimation of ZOO\nwith the exploratory power of GA. The key idea in ARL is that each agent within\na population infers gradient by exploiting the history of its ancestors, i.e.,\nthe ancestor population in the past, while maintaining the diversity of\npolicies in the current population as in GA. We also theoretically reveal that\nthe populational search in ARL implicitly induces the KL-regularization of the\nobjective function, resulting in the enhanced exploration. Our results extend\nthe applicability of populational algorithms for RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) offers a fundamental framework for discovering\noptimal action strategies through interactions within unknown environments.\nRecent advancement have shown that the performance and applicability of RL can\nsignificantly be enhanced by exploiting a population of agents in various ways.\nZeroth-Order Optimization (ZOO) leverages an agent population to estimate the\ngradient of the objective function, enabling robust policy refinement even in\nnon-differentiable scenarios. As another application, Genetic Algorithms (GA)\nboosts the exploration of policy landscapes by mutational generation of policy\ndiversity in an agent population and its refinement by selection. A natural\nquestion is whether we can have the best of two worlds that the agent\npopulation can have. In this work, we propose Ancestral Reinforcement Learning\n(ARL), which synergistically combines the robust gradient estimation of ZOO\nwith the exploratory power of GA. The key idea in ARL is that each agent within\na population infers gradient by exploiting the history of its ancestors, i.e.,\nthe ancestor population in the past, while maintaining the diversity of\npolicies in the current population as in GA. We also theoretically reveal that\nthe populational search in ARL implicitly induces the KL-regularization of the\nobjective function, resulting in the enhanced exploration. Our results extend\nthe applicability of populational algorithms for RL."
                },
                "authors": [
                    {
                        "name": "So Nakashima"
                    },
                    {
                        "name": "Tetsuya J. Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuya J. Kobayashi"
                },
                "author": "Tetsuya J. Kobayashi",
                "arxiv_comment": "16pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05873v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05873v7",
                "updated": "2024-09-02T15:46:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    46,
                    13,
                    0,
                    246,
                    0
                ],
                "published": "2023-10-09T17:13:10Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    17,
                    13,
                    10,
                    0,
                    282,
                    0
                ],
                "title": "Implicit Concept Removal of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Concept Removal of Diffusion Models"
                },
                "summary": "Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Zhili Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    },
                    {
                        "name": "James Kwok"
                    }
                ],
                "author_detail": {
                    "name": "James Kwok"
                },
                "author": "James Kwok",
                "arxiv_comment": "Accepted by ECCV2024. Project Page:\n  https://kaichen1998.github.io/projects/geom-erasing/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05873v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05873v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13152v2",
                "updated": "2024-09-02T15:42:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    42,
                    3,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-19T02:00:51Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    0,
                    51,
                    2,
                    171,
                    0
                ],
                "title": "Analyzing Diversity in Healthcare LLM Research: A Scientometric\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Diversity in Healthcare LLM Research: A Scientometric\n  Perspective"
                },
                "summary": "The deployment of large language models (LLMs) in healthcare has demonstrated\nsubstantial potential for enhancing clinical decision-making, administrative\nefficiency, and patient outcomes. However, the underrepresentation of diverse\ngroups in the development and application of these models can perpetuate\nbiases, leading to inequitable healthcare delivery. This paper presents a\ncomprehensive scientometric analysis of LLM research for healthcare, including\ndata from January 1, 2021, to July 1, 2024. By analyzing metadata from PubMed\nand Dimensions, including author affiliations, countries, and funding sources,\nwe assess the diversity of contributors to LLM research. Our findings highlight\nsignificant gender and geographic disparities, with a predominance of male\nauthors and contributions primarily from high-income countries (HICs). We\nintroduce a novel journal diversity index based on Gini diversity to measure\nthe inclusiveness of scientific publications. Our results underscore the\nnecessity for greater representation in order to ensure the equitable\napplication of LLMs in healthcare. We propose actionable strategies to enhance\ndiversity and inclusivity in artificial intelligence research, with the\nultimate goal of fostering a more inclusive and equitable future in healthcare\ninnovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) in healthcare has demonstrated\nsubstantial potential for enhancing clinical decision-making, administrative\nefficiency, and patient outcomes. However, the underrepresentation of diverse\ngroups in the development and application of these models can perpetuate\nbiases, leading to inequitable healthcare delivery. This paper presents a\ncomprehensive scientometric analysis of LLM research for healthcare, including\ndata from January 1, 2021, to July 1, 2024. By analyzing metadata from PubMed\nand Dimensions, including author affiliations, countries, and funding sources,\nwe assess the diversity of contributors to LLM research. Our findings highlight\nsignificant gender and geographic disparities, with a predominance of male\nauthors and contributions primarily from high-income countries (HICs). We\nintroduce a novel journal diversity index based on Gini diversity to measure\nthe inclusiveness of scientific publications. Our results underscore the\nnecessity for greater representation in order to ensure the equitable\napplication of LLMs in healthcare. We propose actionable strategies to enhance\ndiversity and inclusivity in artificial intelligence research, with the\nultimate goal of fostering a more inclusive and equitable future in healthcare\ninnovation."
                },
                "authors": [
                    {
                        "name": "David Restrepo"
                    },
                    {
                        "name": "Chenwei Wu"
                    },
                    {
                        "name": "Constanza V√°squez-Venegas"
                    },
                    {
                        "name": "Jo√£o Matos"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Luis Filipe Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Luis Filipe Nakayama"
                },
                "author": "Luis Filipe Nakayama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06875v2",
                "updated": "2024-09-02T15:19:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    19,
                    38,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-09T14:04:08Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    4,
                    8,
                    1,
                    191,
                    0
                ],
                "title": "Extending the blended generalized extreme value distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the blended generalized extreme value distribution"
                },
                "summary": "The generalized extreme value (GEV) distribution is commonly employed to help\nestimate the likelihood of extreme events in many geophysical and other\napplication areas. The recently proposed blended generalized extreme value\n(bGEV) distribution modifies the GEV with positive shape parameter to avoid a\nhard lower bound that complicates fitting and inference. Here, the bGEV is\nextended to the GEV with negative shape parameter, avoiding a hard upper bound\nthat is unrealistic in many applications. This extended bGEV is shown to\nimprove on the GEV for forecasting heat and sea level extremes based on past\ndata. Software implementing this bGEV and applying it to the example\ntemperature and sea level data is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalized extreme value (GEV) distribution is commonly employed to help\nestimate the likelihood of extreme events in many geophysical and other\napplication areas. The recently proposed blended generalized extreme value\n(bGEV) distribution modifies the GEV with positive shape parameter to avoid a\nhard lower bound that complicates fitting and inference. Here, the bGEV is\nextended to the GEV with negative shape parameter, avoiding a hard upper bound\nthat is unrealistic in many applications. This extended bGEV is shown to\nimprove on the GEV for forecasting heat and sea level extremes based on past\ndata. Software implementing this bGEV and applying it to the example\ntemperature and sea level data is provided."
                },
                "authors": [
                    {
                        "name": "Nir Y. Krakauer"
                    }
                ],
                "author_detail": {
                    "name": "Nir Y. Krakauer"
                },
                "author": "Nir Y. Krakauer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15102v3",
                "updated": "2024-09-02T15:13:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    13,
                    26,
                    0,
                    246,
                    0
                ],
                "published": "2024-03-22T10:41:25Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    10,
                    41,
                    25,
                    4,
                    82,
                    0
                ],
                "title": "Driving from Vision through Differentiable Optimal Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving from Vision through Differentiable Optimal Control"
                },
                "summary": "This paper proposes DriViDOC: a framework for Driving from Vision through\nDifferentiable Optimal Control, and its application to learn autonomous driving\ncontrollers from human demonstrations. DriViDOC combines the automatic\ninference of relevant features from camera frames with the properties of\nnonlinear model predictive control (NMPC), such as constraint satisfaction. Our\napproach leverages the differentiability of parametric NMPC, allowing for\nend-to-end learning of the driving model from images to control. The model is\ntrained on an offline dataset comprising various human demonstrations collected\non a motion-base driving simulator. During online testing, the model\ndemonstrates successful imitation of different driving styles, and the\ninterpreted NMPC parameters provide insights into the achievement of specific\ndriving behaviors. Our experimental results show that DriViDOC outperforms\nother methods involving NMPC and neural networks, exhibiting an average\nimprovement of 20% in imitation scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes DriViDOC: a framework for Driving from Vision through\nDifferentiable Optimal Control, and its application to learn autonomous driving\ncontrollers from human demonstrations. DriViDOC combines the automatic\ninference of relevant features from camera frames with the properties of\nnonlinear model predictive control (NMPC), such as constraint satisfaction. Our\napproach leverages the differentiability of parametric NMPC, allowing for\nend-to-end learning of the driving model from images to control. The model is\ntrained on an offline dataset comprising various human demonstrations collected\non a motion-base driving simulator. During online testing, the model\ndemonstrates successful imitation of different driving styles, and the\ninterpreted NMPC parameters provide insights into the achievement of specific\ndriving behaviors. Our experimental results show that DriViDOC outperforms\nother methods involving NMPC and neural networks, exhibiting an average\nimprovement of 20% in imitation scores."
                },
                "authors": [
                    {
                        "name": "Flavia Sofia Acerbo"
                    },
                    {
                        "name": "Jan Swevers"
                    },
                    {
                        "name": "Tinne Tuytelaars"
                    },
                    {
                        "name": "Tong Duy Son"
                    }
                ],
                "author_detail": {
                    "name": "Tong Duy Son"
                },
                "author": "Tong Duy Son",
                "arxiv_comment": "This work has been accepted for publication in the Proceedings of the\n  2024 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2024). Accompanying video available at: https://youtu.be/ENHhphpbPLs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v4",
                "updated": "2024-09-02T15:08:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    8,
                    32,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Evolving Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Shize Wei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12801v2",
                "updated": "2024-09-02T12:37:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    37,
                    27,
                    0,
                    246,
                    0
                ],
                "published": "2024-01-11T00:45:33Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    0,
                    45,
                    33,
                    3,
                    11,
                    0
                ],
                "title": "Deep Learning-based Target-To-User Association in Integrated Sensing and\n  Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Target-To-User Association in Integrated Sensing and\n  Communication Systems"
                },
                "summary": "In Integrated Sensing and Communication (ISAC) systems, matching the radar\ntargets with communication user equipments (UEs) is functional to several\ncommunication tasks, such as proactive handover and beam prediction. In this\npaper, we consider a radar-assisted communication system where a base station\n(BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a\ndouble aim: (i) associate vehicular radar targets to vehicular equipments (VEs)\nin the communication beamspace and (ii) predict the beamforming vector for each\nVE from radar data. The proposed target-to-user (T2U) association consists of\ntwo stages. First, vehicular radar targets are detected from range-angle\nimages, and, for each, a beamforming vector is estimated. Then, the inferred\nper-target beamforming vectors are matched with the ones utilized at the BS for\ncommunication to perform target-to-user (T2U) association. Joint multi-target\ndetection and beam inference is obtained by modifying the you only look once\n(YOLO) model, which is trained over simulated range-angle radar images.\nSimulation results over different urban vehicular mobility scenarios show that\nthe proposed T2U method provides a probability of correct association that\nincreases with the size of the BS antenna array, highlighting the respective\nincrease of the separability of the VEs in the beamspace. Moreover, we show\nthat the modified YOLO architecture can effectively perform both beam\nprediction and radar target detection, with similar performance in mean average\nprecision on the latter over different antenna array sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Integrated Sensing and Communication (ISAC) systems, matching the radar\ntargets with communication user equipments (UEs) is functional to several\ncommunication tasks, such as proactive handover and beam prediction. In this\npaper, we consider a radar-assisted communication system where a base station\n(BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a\ndouble aim: (i) associate vehicular radar targets to vehicular equipments (VEs)\nin the communication beamspace and (ii) predict the beamforming vector for each\nVE from radar data. The proposed target-to-user (T2U) association consists of\ntwo stages. First, vehicular radar targets are detected from range-angle\nimages, and, for each, a beamforming vector is estimated. Then, the inferred\nper-target beamforming vectors are matched with the ones utilized at the BS for\ncommunication to perform target-to-user (T2U) association. Joint multi-target\ndetection and beam inference is obtained by modifying the you only look once\n(YOLO) model, which is trained over simulated range-angle radar images.\nSimulation results over different urban vehicular mobility scenarios show that\nthe proposed T2U method provides a probability of correct association that\nincreases with the size of the BS antenna array, highlighting the respective\nincrease of the separability of the VEs in the beamspace. Moreover, we show\nthat the modified YOLO architecture can effectively perform both beam\nprediction and radar target detection, with similar performance in mean average\nprecision on the latter over different antenna array sizes."
                },
                "authors": [
                    {
                        "name": "Lorenzo Cazzella"
                    },
                    {
                        "name": "Marouan Mizmizi"
                    },
                    {
                        "name": "Dario Tagliaferri"
                    },
                    {
                        "name": "Damiano Badini"
                    },
                    {
                        "name": "Matteo Matteucci"
                    },
                    {
                        "name": "Umberto Spagnolini"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Spagnolini"
                },
                "author": "Umberto Spagnolini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12537v3",
                "updated": "2024-09-02T12:36:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    36,
                    6,
                    0,
                    246,
                    0
                ],
                "published": "2023-10-19T07:39:00Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    7,
                    39,
                    0,
                    3,
                    292,
                    0
                ],
                "title": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction"
                },
                "summary": "In order to facilitate features such as faceted product search and product\ncomparison, e-commerce platforms require accurately structured product data,\nincluding precise attribute/value pairs. Vendors often times provide\nunstructured product descriptions consisting only of an offer title and a\ntextual description. Consequently, extracting attribute values from titles and\ndescriptions is vital for e-commerce platforms. State-of-the-art attribute\nvalue extraction methods based on pre-trained language models, such as BERT,\nface two drawbacks (i) the methods require significant amounts of task-specific\ntraining data and (ii) the fine-tuned models have problems with generalising to\nunseen attribute values that were not part of the training data. This paper\nexplores the potential of using large language models as a more training\ndata-efficient and more robust alternative to existing AVE methods. We propose\nprompt templates for describing the target attributes of the extraction to the\nLLM, covering both zero-shot and few-shot scenarios. In the zero-shot scenario,\ntextual and JSON-based target schema representations of the attributes are\ncompared. In the few-shot scenario, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. We\nevaluate the prompt templates in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs which can be run locally. We compare the\nperformance of the LLMs to the PLM-based methods SU-OpenTag, AVEQA, and MAVEQA.\nThe highest average F1-score of 86% was achieved by GPT-4. Llama-3-70B performs\nonly 3% worse than GPT-4, making it a competitive open-source alternative.\nGiven the same training data, this prompt/GPT-4 combination outperforms the\nbest PLM baseline by an average of 6% F1-score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to facilitate features such as faceted product search and product\ncomparison, e-commerce platforms require accurately structured product data,\nincluding precise attribute/value pairs. Vendors often times provide\nunstructured product descriptions consisting only of an offer title and a\ntextual description. Consequently, extracting attribute values from titles and\ndescriptions is vital for e-commerce platforms. State-of-the-art attribute\nvalue extraction methods based on pre-trained language models, such as BERT,\nface two drawbacks (i) the methods require significant amounts of task-specific\ntraining data and (ii) the fine-tuned models have problems with generalising to\nunseen attribute values that were not part of the training data. This paper\nexplores the potential of using large language models as a more training\ndata-efficient and more robust alternative to existing AVE methods. We propose\nprompt templates for describing the target attributes of the extraction to the\nLLM, covering both zero-shot and few-shot scenarios. In the zero-shot scenario,\ntextual and JSON-based target schema representations of the attributes are\ncompared. In the few-shot scenario, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. We\nevaluate the prompt templates in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs which can be run locally. We compare the\nperformance of the LLMs to the PLM-based methods SU-OpenTag, AVEQA, and MAVEQA.\nThe highest average F1-score of 86% was achieved by GPT-4. Llama-3-70B performs\nonly 3% worse than GPT-4, making it a competitive open-source alternative.\nGiven the same training data, this prompt/GPT-4 combination outperforms the\nbest PLM baseline by an average of 6% F1-score."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Roee Shraga"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16802v3",
                "updated": "2024-09-04T02:57:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    57,
                    15,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-29T11:53:09Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    53,
                    9,
                    3,
                    242,
                    0
                ],
                "title": "Auto-resolving atomic structure at van der Waal interfaces using a\n  generative model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-resolving atomic structure at van der Waal interfaces using a\n  generative model"
                },
                "summary": "Unveiling atomic structures is significant for the relationship construction\nbetween microscopic configurations and macroscopic properties of materials.\nHowever, we still lack a rapid, accurate, and robust approach to automatically\nresolve complex patterns in atomic-resolution microscopy. Here, we present a\nTrident strategy-enhanced disentangled representation learning method (a\ngenerative model), which utilizes a few unlabeled experimental images with\nabundant low-cost simulated images to generate a large corpus of annotated\nsimulation data that closely resembles experimental conditions, realizing\nsimultaneous achievement of high quality and large volumes of the training\ndataset. A structural inference model is then trained via a residual neural\nnetwork which can directly deduce the interlayer slip and rotation of\ndiversified and complicated stacking patterns at van der Waals (vdWs)\ninterfaces with picometer-scale accuracy across various materials (ReS2, ReSe2,\nand MoS2) with different layer numbers (bilayer and trilayers) and demonstrates\nrobustness to defects, imaging quality, and surface contaminations. The\nframework can also identify pattern transition interfaces, quantify subtle\nmotif variations, and discriminate moir\\'e patterns that are undistinguishable\nin frequency domains. The high-throughput processing ability of our method\nhelps discover a novel vdW epitaxy where various thermodynamically favorable\nslip stackings can coexist, demonstrating the machine learning contribution to\nthe new knowledge emergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling atomic structures is significant for the relationship construction\nbetween microscopic configurations and macroscopic properties of materials.\nHowever, we still lack a rapid, accurate, and robust approach to automatically\nresolve complex patterns in atomic-resolution microscopy. Here, we present a\nTrident strategy-enhanced disentangled representation learning method (a\ngenerative model), which utilizes a few unlabeled experimental images with\nabundant low-cost simulated images to generate a large corpus of annotated\nsimulation data that closely resembles experimental conditions, realizing\nsimultaneous achievement of high quality and large volumes of the training\ndataset. A structural inference model is then trained via a residual neural\nnetwork which can directly deduce the interlayer slip and rotation of\ndiversified and complicated stacking patterns at van der Waals (vdWs)\ninterfaces with picometer-scale accuracy across various materials (ReS2, ReSe2,\nand MoS2) with different layer numbers (bilayer and trilayers) and demonstrates\nrobustness to defects, imaging quality, and surface contaminations. The\nframework can also identify pattern transition interfaces, quantify subtle\nmotif variations, and discriminate moir\\'e patterns that are undistinguishable\nin frequency domains. The high-throughput processing ability of our method\nhelps discover a novel vdW epitaxy where various thermodynamically favorable\nslip stackings can coexist, demonstrating the machine learning contribution to\nthe new knowledge emergence."
                },
                "authors": [
                    {
                        "name": "Wenqiang Huang"
                    },
                    {
                        "name": "Yuchen Jin"
                    },
                    {
                        "name": "Zhemin Li"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Zheng Luo"
                    },
                    {
                        "name": "Shen Zhou"
                    },
                    {
                        "name": "Jinguo Lin"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Jun Cheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Fangping Ouyang"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Shanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Wang"
                },
                "author": "Shanshan Wang",
                "arxiv_comment": "25 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05141v3",
                "updated": "2024-09-02T10:55:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    55,
                    30,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-09T15:53:55Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}."
                },
                "authors": [
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Chengwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07376v2",
                "updated": "2024-09-02T10:46:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    46,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-12T15:44:28Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    15,
                    44,
                    28,
                    1,
                    346,
                    0
                ],
                "title": "Impact of higher harmonics of gravitational radiation on the population\n  inference of binary black holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of higher harmonics of gravitational radiation on the population\n  inference of binary black holes"
                },
                "summary": "Templates modeling just the dominant mode of gravitational radiation are\ngenerally sufficient for the unbiased parameter inference of near-equal-mass\ncompact binary mergers. However, neglecting the subdominant modes can bias the\ninference if the binary is significantly asymmetric, very massive, or has\nmisaligned spins. In this work, we explore if neglecting these subdominant\nmodes in the parameter estimation of non-spinning binary black hole mergers can\nbias the inference of their population-level properties such as mass and merger\nredshift distributions. Assuming the design sensitivity of advanced LIGO-Virgo\ndetector network, we find that neglecting subdominant modes will not cause a\nsignificant bias in the population inference, although including them will\nprovide more precise estimates. This is primarily due to the fact that\nasymmetric binaries are expected to be rarer in our detected sample, due to\ntheir intrinsic rareness and the observational selection effects. The increased\nprecision in the measurement of the maximum black hole mass can help in better\nconstraining the upper mass gap in the mass spectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Templates modeling just the dominant mode of gravitational radiation are\ngenerally sufficient for the unbiased parameter inference of near-equal-mass\ncompact binary mergers. However, neglecting the subdominant modes can bias the\ninference if the binary is significantly asymmetric, very massive, or has\nmisaligned spins. In this work, we explore if neglecting these subdominant\nmodes in the parameter estimation of non-spinning binary black hole mergers can\nbias the inference of their population-level properties such as mass and merger\nredshift distributions. Assuming the design sensitivity of advanced LIGO-Virgo\ndetector network, we find that neglecting subdominant modes will not cause a\nsignificant bias in the population inference, although including them will\nprovide more precise estimates. This is primarily due to the fact that\nasymmetric binaries are expected to be rarer in our detected sample, due to\ntheir intrinsic rareness and the observational selection effects. The increased\nprecision in the measurement of the maximum black hole mass can help in better\nconstraining the upper mass gap in the mass spectrum."
                },
                "authors": [
                    {
                        "name": "Mukesh Kumar Singh"
                    },
                    {
                        "name": "Shasvath J Kapadia"
                    },
                    {
                        "name": "Aditya Vijaykumar"
                    },
                    {
                        "name": "Parameswaran Ajith"
                    }
                ],
                "author_detail": {
                    "name": "Parameswaran Ajith"
                },
                "author": "Parameswaran Ajith",
                "arxiv_doi": "10.3847/1538-4357/ad499b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad499b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.07376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 7 figures",
                "arxiv_journal_ref": "The Astrophysical Journal, Volume 971, Number 1 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17161v2",
                "updated": "2024-09-02T10:35:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    35,
                    57,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-30T10:04:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    4,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Leveraging Blockchain and ANFIS for Optimal Supply Chain Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Blockchain and ANFIS for Optimal Supply Chain Management"
                },
                "summary": "The supply chain is a critical segment of the product manufacturing cycle,\ncontinuously influenced by risky, uncertain, and undesirable events. Optimizing\nflexibility in the supply chain presents a complex, multi-objective, and\nnonlinear programming challenge. In the poultry supply chain, the development\nof mass customization capabilities has led manufacturing companies to\nincreasingly focus on offering tailored and customized services for individual\nproducts. To safeguard against data tampering and ensure the integrity of setup\ncosts and overall profitability, a multi-signature decentralized finance (DeFi)\nprotocol, integrated with the IoT on a blockchain platform, is proposed.\nManaging the poultry supply chain involves uncertainties that may not account\nfor parameters such as delivery time to retailers, reorder time, and the number\nof requested products. To address these challenges, this study employs an\nadaptive neuro-fuzzy inference system (ANFIS), combining neural networks with\nfuzzy logic to compensate for the lack of data training in parameter\nidentification. Through MATLAB simulations, the study investigates the average\nshop delivery duration, the reorder time, and the number of products per order.\nBy implementing the proposed technique, the average delivery time decreases\nfrom 40 to 37 minutes, the reorder time decreases from five to four days, and\nthe quantity of items requested per order grows from six to eleven.\nAdditionally, the ANFIS model enhances overall supply chain performance by\nreducing transaction times by 15\\% compared to conventional systems, thereby\nimproving real-time responsiveness and boosting transparency in supply chain\noperations, effectively resolving operational issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The supply chain is a critical segment of the product manufacturing cycle,\ncontinuously influenced by risky, uncertain, and undesirable events. Optimizing\nflexibility in the supply chain presents a complex, multi-objective, and\nnonlinear programming challenge. In the poultry supply chain, the development\nof mass customization capabilities has led manufacturing companies to\nincreasingly focus on offering tailored and customized services for individual\nproducts. To safeguard against data tampering and ensure the integrity of setup\ncosts and overall profitability, a multi-signature decentralized finance (DeFi)\nprotocol, integrated with the IoT on a blockchain platform, is proposed.\nManaging the poultry supply chain involves uncertainties that may not account\nfor parameters such as delivery time to retailers, reorder time, and the number\nof requested products. To address these challenges, this study employs an\nadaptive neuro-fuzzy inference system (ANFIS), combining neural networks with\nfuzzy logic to compensate for the lack of data training in parameter\nidentification. Through MATLAB simulations, the study investigates the average\nshop delivery duration, the reorder time, and the number of products per order.\nBy implementing the proposed technique, the average delivery time decreases\nfrom 40 to 37 minutes, the reorder time decreases from five to four days, and\nthe quantity of items requested per order grows from six to eleven.\nAdditionally, the ANFIS model enhances overall supply chain performance by\nreducing transaction times by 15\\% compared to conventional systems, thereby\nimproving real-time responsiveness and boosting transparency in supply chain\noperations, effectively resolving operational issues."
                },
                "authors": [
                    {
                        "name": "Amirfarhad Farhadi"
                    },
                    {
                        "name": "Homayoun Safarpour Motealegh Mahalegi"
                    },
                    {
                        "name": "Abolfazl Pourrezaeian Firouzabad"
                    },
                    {
                        "name": "Azadeh Zamanifar"
                    },
                    {
                        "name": "Majid Sorouri"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sorouri"
                },
                "author": "Majid Sorouri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.03531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.03531v2",
                "updated": "2024-09-02T10:24:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    24,
                    46,
                    0,
                    246,
                    0
                ],
                "published": "2023-02-07T15:23:52Z",
                "published_parsed": [
                    2023,
                    2,
                    7,
                    15,
                    23,
                    52,
                    1,
                    38,
                    0
                ],
                "title": "Structured Generative Models for Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Generative Models for Scene Understanding"
                },
                "summary": "This position paper argues for the use of \\emph{structured generative models}\n(SGMs) for the understanding of static scenes. This requires the reconstruction\nof a 3D scene from an input image (or a set of multi-view images), whereby the\ncontents of the image(s) are causally explained in terms of models of\ninstantiated objects, each with their own type, shape, appearance and pose,\nalong with global variables like scene lighting and camera parameters. This\napproach also requires scene models which account for the co-occurrences and\ninter-relationships of objects in a scene. The SGM approach has the merits that\nit is compositional and generative, which lead to interpretability and\neditability. \\\\\\\\ To pursue the SGM agenda, we need models for objects and\nscenes, and approaches to carry out inference. We first review models for\nobjects, which include ``things'' (object categories that have a well defined\nshape), and ``stuff'' (categories which have amorphous spatial extent). We then\nmove on to review \\emph{scene models} which describe the inter-relationships of\nobjects. Perhaps the most challenging problem for SGMs is \\emph{inference} of\nthe objects, lighting and camera parameters, and scene inter-relationships from\ninput consisting of a single or multiple images. We conclude with a discussion\nof issues that need addressing to advance the SGM agenda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues for the use of \\emph{structured generative models}\n(SGMs) for the understanding of static scenes. This requires the reconstruction\nof a 3D scene from an input image (or a set of multi-view images), whereby the\ncontents of the image(s) are causally explained in terms of models of\ninstantiated objects, each with their own type, shape, appearance and pose,\nalong with global variables like scene lighting and camera parameters. This\napproach also requires scene models which account for the co-occurrences and\ninter-relationships of objects in a scene. The SGM approach has the merits that\nit is compositional and generative, which lead to interpretability and\neditability. \\\\\\\\ To pursue the SGM agenda, we need models for objects and\nscenes, and approaches to carry out inference. We first review models for\nobjects, which include ``things'' (object categories that have a well defined\nshape), and ``stuff'' (categories which have amorphous spatial extent). We then\nmove on to review \\emph{scene models} which describe the inter-relationships of\nobjects. Perhaps the most challenging problem for SGMs is \\emph{inference} of\nthe objects, lighting and camera parameters, and scene inter-relationships from\ninput consisting of a single or multiple images. We conclude with a discussion\nof issues that need addressing to advance the SGM agenda."
                },
                "authors": [
                    {
                        "name": "Christopher K. I. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Christopher K. I. Williams"
                },
                "author": "Christopher K. I. Williams",
                "arxiv_comment": "32 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.03531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.03531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02091v2",
                "updated": "2024-09-02T10:02:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    2,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-04T18:06:41Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    6,
                    41,
                    0,
                    338,
                    0
                ],
                "title": "Physics simulation capabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics simulation capabilities of LLMs"
                },
                "summary": "[Abridged abstract] Large Language Models (LLMs) can solve some\nundergraduate-level to graduate-level physics textbook problems and are\nproficient at coding. Combining these two capabilities could one day enable AI\nsystems to simulate and predict the physical world.\n  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to\nresearch-level computational physics problems. We condition LLM generation on\nthe use of well-documented and widely-used packages to elicit coding\ncapabilities in the physics and astrophysics domains. We contribute $\\sim 50$\noriginal and challenging problems in celestial mechanics (with REBOUND),\nstellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear\ndynamics (with SciPy). Since our problems do not admit unique solutions, we\nevaluate LLM performance on several soft metrics: counts of lines that contain\ndifferent types of errors (coding, physics, necessity and sufficiency) as well\nas a more \"educational\" Pass-Fail metric focused on capturing the salient\nphysical ingredients of the problem at hand.\n  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,\nalthough about 40\\% of the solutions could plausibly get a passing grade. About\n$70-90 \\%$ of the code lines produced are necessary, sufficient and correct\n(coding \\& physics). Physics and coding errors are the most common, with some\nunnecessary or insufficient lines. We observe significant variations across\nproblem class and difficulty. We identify several failure modes of GPT4 in the\ncomputational physics domain.\n  Our reconnaissance work provides a snapshot of current computational\ncapabilities in classical physics and points to obvious improvement targets if\nAI systems are ever to reach a basic level of autonomy in physics simulation\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Abridged abstract] Large Language Models (LLMs) can solve some\nundergraduate-level to graduate-level physics textbook problems and are\nproficient at coding. Combining these two capabilities could one day enable AI\nsystems to simulate and predict the physical world.\n  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to\nresearch-level computational physics problems. We condition LLM generation on\nthe use of well-documented and widely-used packages to elicit coding\ncapabilities in the physics and astrophysics domains. We contribute $\\sim 50$\noriginal and challenging problems in celestial mechanics (with REBOUND),\nstellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear\ndynamics (with SciPy). Since our problems do not admit unique solutions, we\nevaluate LLM performance on several soft metrics: counts of lines that contain\ndifferent types of errors (coding, physics, necessity and sufficiency) as well\nas a more \"educational\" Pass-Fail metric focused on capturing the salient\nphysical ingredients of the problem at hand.\n  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,\nalthough about 40\\% of the solutions could plausibly get a passing grade. About\n$70-90 \\%$ of the code lines produced are necessary, sufficient and correct\n(coding \\& physics). Physics and coding errors are the most common, with some\nunnecessary or insufficient lines. We observe significant variations across\nproblem class and difficulty. We identify several failure modes of GPT4 in the\ncomputational physics domain.\n  Our reconnaissance work provides a snapshot of current computational\ncapabilities in classical physics and points to obvious improvement targets if\nAI systems are ever to reach a basic level of autonomy in physics simulation\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Mohamad Ali-Dib"
                    },
                    {
                        "name": "Kristen Menou"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Menou"
                },
                "author": "Kristen Menou",
                "arxiv_comment": "Accepted for publication in Physica Scripta. Abridged abstract. 15\n  pages + appendix, 1 figure. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03816v2",
                "updated": "2024-09-02T09:48:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    48,
                    18,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-06T07:40:00Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    7,
                    40,
                    0,
                    3,
                    158,
                    0
                ],
                "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"
                },
                "summary": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16069v2",
                "updated": "2024-09-02T09:13:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    13,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-23T10:36:35Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    10,
                    36,
                    35,
                    6,
                    175,
                    0
                ],
                "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by fine-tuning only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by fine-tuning only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem"
                },
                "authors": [
                    {
                        "name": "Junyi Zhu"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Matthew B. Blaschko"
                    }
                ],
                "author_detail": {
                    "name": "Matthew B. Blaschko"
                },
                "author": "Matthew B. Blaschko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05982v2",
                "updated": "2024-09-02T08:48:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    48,
                    12,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-12T08:17:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    17,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "Exploring and Learning Structure: Active Inference Approach in\n  Navigational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Learning Structure: Active Inference Approach in\n  Navigational Agents"
                },
                "summary": "Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments."
                },
                "authors": [
                    {
                        "name": "Daria de Tinguy"
                    },
                    {
                        "name": "Tim Verbelen"
                    },
                    {
                        "name": "Bart Dhoedt"
                    }
                ],
                "author_detail": {
                    "name": "Bart Dhoedt"
                },
                "author": "Bart Dhoedt",
                "arxiv_comment": "IWAI workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08928v3",
                "updated": "2024-09-02T08:42:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    42,
                    31,
                    0,
                    246,
                    0
                ],
                "published": "2024-03-13T19:36:03Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    19,
                    36,
                    3,
                    2,
                    73,
                    0
                ],
                "title": "Neuromorphic force-control in an industrial task: validating energy and\n  latency benefits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic force-control in an industrial task: validating energy and\n  latency benefits"
                },
                "summary": "As robots become smarter and more ubiquitous, optimizing the power\nconsumption of intelligent compute becomes imperative towards ensuring the\nsustainability of technological advancements. Neuromorphic computing hardware\nmakes use of biologically inspired neural architectures to achieve energy and\nlatency improvements compared to conventional von Neumann computing\narchitecture. Applying these benefits to robots has been demonstrated in\nseveral works in the field of neurorobotics, typically on relatively simple\ncontrol tasks. Here, we introduce an example of neuromorphic computing applied\nto the real-world industrial task of object insertion. We trained a spiking\nneural network (SNN) to perform force-torque feedback control using a\nreinforcement learning approach in simulation. We then ported the SNN to the\nIntel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At\ninference time we show latency competitive with current CPU/GPU architectures,\nand one order of magnitude less energy usage in comparison to state-of-the-art\nlow-energy edge-hardware. We offer this example as a proof of concept\nimplementation of a neuromoprhic controller in real-world robotic setting,\nhighlighting the benefits of neuromorphic hardware for the development of\nintelligent controllers for robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots become smarter and more ubiquitous, optimizing the power\nconsumption of intelligent compute becomes imperative towards ensuring the\nsustainability of technological advancements. Neuromorphic computing hardware\nmakes use of biologically inspired neural architectures to achieve energy and\nlatency improvements compared to conventional von Neumann computing\narchitecture. Applying these benefits to robots has been demonstrated in\nseveral works in the field of neurorobotics, typically on relatively simple\ncontrol tasks. Here, we introduce an example of neuromorphic computing applied\nto the real-world industrial task of object insertion. We trained a spiking\nneural network (SNN) to perform force-torque feedback control using a\nreinforcement learning approach in simulation. We then ported the SNN to the\nIntel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At\ninference time we show latency competitive with current CPU/GPU architectures,\nand one order of magnitude less energy usage in comparison to state-of-the-art\nlow-energy edge-hardware. We offer this example as a proof of concept\nimplementation of a neuromoprhic controller in real-world robotic setting,\nhighlighting the benefits of neuromorphic hardware for the development of\nintelligent controllers for robots."
                },
                "authors": [
                    {
                        "name": "Camilo Amaya"
                    },
                    {
                        "name": "Evan Eames"
                    },
                    {
                        "name": "Gintautas Palinauskas"
                    },
                    {
                        "name": "Alexander Perzylo"
                    },
                    {
                        "name": "Yulia Sandamirskaya"
                    },
                    {
                        "name": "Axel von Arnim"
                    }
                ],
                "author_detail": {
                    "name": "Axel von Arnim"
                },
                "author": "Axel von Arnim",
                "arxiv_comment": "Accepted at IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07179v2",
                "updated": "2024-09-02T08:00:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    0,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-11T11:41:41Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    11,
                    41,
                    41,
                    1,
                    163,
                    0
                ],
                "title": "Comprehensive Study of $k$-essence Model: Dynamical System Analysis and\n  Observational Constraints from Latest Type Ia Supernova and BAO Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Study of $k$-essence Model: Dynamical System Analysis and\n  Observational Constraints from Latest Type Ia Supernova and BAO Observations"
                },
                "summary": "We constrain the parameters of the $k$-essence scalar field model with\ninverse square and exponential potentials using data sets including\nPantheon+SHOES and the Dark Energy Survey (DES) of Type Ia supernovae, Baryon\nAcoustic Oscillation (BAO) data from SDSS and DESI surveys, and direct\nmeasurements of the Hubble parameter and redshift obtained from the\ndifferential age method (CC). We also provide a brief perspective on the\ndynamical evolution of both models and derive stability constraints on the\nmodel parameters, which are then used to set appropriate priors. We adopt a\nBayesian inference procedure to estimate the model parameters that best fit the\ndata. A comprehensive analysis in light of observational data shows that the\n$k$-essence model fits well across all data combinations. However, according to\nthe BIC criterion, the $\\Lambda$CDM model provides a slightly better fit\ncompared to the $k$-essence model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We constrain the parameters of the $k$-essence scalar field model with\ninverse square and exponential potentials using data sets including\nPantheon+SHOES and the Dark Energy Survey (DES) of Type Ia supernovae, Baryon\nAcoustic Oscillation (BAO) data from SDSS and DESI surveys, and direct\nmeasurements of the Hubble parameter and redshift obtained from the\ndifferential age method (CC). We also provide a brief perspective on the\ndynamical evolution of both models and derive stability constraints on the\nmodel parameters, which are then used to set appropriate priors. We adopt a\nBayesian inference procedure to estimate the model parameters that best fit the\ndata. A comprehensive analysis in light of observational data shows that the\n$k$-essence model fits well across all data combinations. However, according to\nthe BIC criterion, the $\\Lambda$CDM model provides a slightly better fit\ncompared to the $k$-essence model."
                },
                "authors": [
                    {
                        "name": "Saddam Hussain"
                    },
                    {
                        "name": "Sarath Nelleri"
                    },
                    {
                        "name": "Kaushik Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Bhattacharya"
                },
                "author": "Kaushik Bhattacharya",
                "arxiv_comment": "26 pages, 11 figures, 5 tables, Included recent BAO(DESI+SDSS) data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09067v3",
                "updated": "2024-09-02T07:26:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    7,
                    26,
                    46,
                    0,
                    246,
                    0
                ],
                "published": "2023-08-17T15:54:38Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    15,
                    54,
                    38,
                    3,
                    229,
                    0
                ],
                "title": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text"
                },
                "summary": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs."
                },
                "authors": [
                    {
                        "name": "Alberto Mu√±oz-Ortiz"
                    },
                    {
                        "name": "Carlos G√≥mez-Rodr√≠guez"
                    },
                    {
                        "name": "David Vilares"
                    }
                ],
                "author_detail": {
                    "name": "David Vilares"
                },
                "author": "David Vilares",
                "arxiv_doi": "10.1007/s10462-024-10903-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-024-10903-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at Artificial Intelligence Review vol. 57, 265",
                "arxiv_journal_ref": "Artificial Intelligence Review 57, 265 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16237v2",
                "updated": "2024-09-02T07:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    7,
                    25,
                    21,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-23T07:22:25Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    7,
                    22,
                    25,
                    1,
                    205,
                    0
                ],
                "title": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection"
                },
                "summary": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Fan Cui"
                    },
                    {
                        "name": "Chenyang Yin"
                    },
                    {
                        "name": "Kexing Zhou"
                    },
                    {
                        "name": "Youwei Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Qiang Xu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Demin Song"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Yun"
                    },
                    {
                        "name": "Liang"
                    }
                ],
                "author_detail": {
                    "name": "Liang"
                },
                "arxiv_affiliation": "Eric",
                "author": "Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03963v3",
                "updated": "2024-09-02T06:51:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    51,
                    36,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-07T02:49:59Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    2,
                    49,
                    59,
                    1,
                    128,
                    0
                ],
                "title": "ERATTA: Extreme RAG for Table To Answers with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERATTA: Extreme RAG for Table To Answers with Large Language Models"
                },
                "summary": "Large language models (LLMs) with retrieval augmented-generation (RAG) have\nbeen the optimal choice for scalable generative AI solutions in the recent\npast. Although RAG implemented with AI agents (agentic-RAG) has been recently\npopularized, its suffers from unstable cost and unreliable performances for\nEnterprise-level data-practices. Most existing use-cases that incorporate RAG\nwith LLMs have been either generic or extremely domain specific, thereby\nquestioning the scalability and generalizability of RAG-LLM approaches. In this\nwork, we propose a unique LLM-based system where multiple LLMs can be invoked\nto enable data authentication, user-query routing, data-retrieval and custom\nprompting for question-answering capabilities from Enterprise-data tables. The\nsource tables here are highly fluctuating and large in size and the proposed\nframework enables structured responses in under 10 seconds per query.\nAdditionally, we propose a five metric scoring module that detects and reports\nhallucinations in the LLM responses. Our proposed system and scoring metrics\nachieve >90% confidence scores across hundreds of user queries in the\nsustainability, financial health and social media domains. Extensions to the\nproposed extreme RAG architectures can enable heterogeneous source querying\nusing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with retrieval augmented-generation (RAG) have\nbeen the optimal choice for scalable generative AI solutions in the recent\npast. Although RAG implemented with AI agents (agentic-RAG) has been recently\npopularized, its suffers from unstable cost and unreliable performances for\nEnterprise-level data-practices. Most existing use-cases that incorporate RAG\nwith LLMs have been either generic or extremely domain specific, thereby\nquestioning the scalability and generalizability of RAG-LLM approaches. In this\nwork, we propose a unique LLM-based system where multiple LLMs can be invoked\nto enable data authentication, user-query routing, data-retrieval and custom\nprompting for question-answering capabilities from Enterprise-data tables. The\nsource tables here are highly fluctuating and large in size and the proposed\nframework enables structured responses in under 10 seconds per query.\nAdditionally, we propose a five metric scoring module that detects and reports\nhallucinations in the LLM responses. Our proposed system and scoring metrics\nachieve >90% confidence scores across hundreds of user queries in the\nsustainability, financial health and social media domains. Extensions to the\nproposed extreme RAG architectures can enable heterogeneous source querying\nusing LLMs."
                },
                "authors": [
                    {
                        "name": "Sohini Roychowdhury"
                    },
                    {
                        "name": "Marko Krema"
                    },
                    {
                        "name": "Anvar Mahammad"
                    },
                    {
                        "name": "Brian Moore"
                    },
                    {
                        "name": "Arijit Mukherjee"
                    },
                    {
                        "name": "Punit Prakashchandra"
                    }
                ],
                "author_detail": {
                    "name": "Punit Prakashchandra"
                },
                "author": "Punit Prakashchandra",
                "arxiv_comment": "5 pages, 4 tables, IEEE Big Data, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07510v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07510v5",
                "updated": "2024-09-02T06:27:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    27,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-13T07:10:53Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    7,
                    10,
                    53,
                    0,
                    134,
                    0
                ],
                "title": "PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator"
                },
                "summary": "We present Piecewise Rectified Flow (PeRFlow), a flow-based method for\naccelerating diffusion models. PeRFlow divides the sampling process of\ngenerative flows into several time windows and straightens the trajectories in\neach interval via the reflow operation, thereby approaching piecewise linear\nflows. PeRFlow achieves superior performance in a few-step generation.\nMoreover, through dedicated parameterizations, the PeRFlow models inherit\nknowledge from the pretrained diffusion models. Thus, the training converges\nfast and the obtained models show advantageous transfer ability, serving as\nuniversal plug-and-play accelerators that are compatible with various workflows\nbased on the pre-trained diffusion models. Codes for training and inference are\npublicly released. https://github.com/magic-research/piecewise-rectified-flow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Piecewise Rectified Flow (PeRFlow), a flow-based method for\naccelerating diffusion models. PeRFlow divides the sampling process of\ngenerative flows into several time windows and straightens the trajectories in\neach interval via the reflow operation, thereby approaching piecewise linear\nflows. PeRFlow achieves superior performance in a few-step generation.\nMoreover, through dedicated parameterizations, the PeRFlow models inherit\nknowledge from the pretrained diffusion models. Thus, the training converges\nfast and the obtained models show advantageous transfer ability, serving as\nuniversal plug-and-play accelerators that are compatible with various workflows\nbased on the pre-trained diffusion models. Codes for training and inference are\npublicly released. https://github.com/magic-research/piecewise-rectified-flow"
                },
                "authors": [
                    {
                        "name": "Hanshu Yan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Jiachun Pan"
                    },
                    {
                        "name": "Jun Hao Liew"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Jiashi Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jiashi Feng"
                },
                "author": "Jiashi Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07510v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07510v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05191v2",
                "updated": "2024-09-02T06:24:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    24,
                    32,
                    0,
                    246,
                    0
                ],
                "published": "2023-10-08T15:00:04Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    15,
                    0,
                    4,
                    6,
                    281,
                    0
                ],
                "title": "LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of\n  Student-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of\n  Student-LLM Interaction"
                },
                "summary": "In the context of English as a Foreign Language (EFL) writing education,\nLLM-as-a-tutor can assist students by providing real-time feedback on their\nessays. However, challenges arise in assessing LLM-as-a-tutor due to differing\nstandards between educational and general use cases. To bridge this gap, we\nintegrate pedagogical principles to assess student-LLM interaction. First, we\nexplore how LLMs can function as English tutors, providing effective essay\nfeedback tailored to students. Second, we propose three metrics to evaluate\nLLM-as-a-tutor specifically designed for EFL writing education, emphasizing\npedagogical aspects. In this process, EFL experts evaluate the feedback from\nLLM-as-a-tutor regarding quality and characteristics. On the other hand, EFL\nlearners assess their learning outcomes from interaction with LLM-as-a-tutor.\nThis approach lays the groundwork for developing LLMs-as-a-tutor tailored to\nthe needs of EFL learners, advancing the effectiveness of writing education in\nthis context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of English as a Foreign Language (EFL) writing education,\nLLM-as-a-tutor can assist students by providing real-time feedback on their\nessays. However, challenges arise in assessing LLM-as-a-tutor due to differing\nstandards between educational and general use cases. To bridge this gap, we\nintegrate pedagogical principles to assess student-LLM interaction. First, we\nexplore how LLMs can function as English tutors, providing effective essay\nfeedback tailored to students. Second, we propose three metrics to evaluate\nLLM-as-a-tutor specifically designed for EFL writing education, emphasizing\npedagogical aspects. In this process, EFL experts evaluate the feedback from\nLLM-as-a-tutor regarding quality and characteristics. On the other hand, EFL\nlearners assess their learning outcomes from interaction with LLM-as-a-tutor.\nThis approach lays the groundwork for developing LLMs-as-a-tutor tailored to\nthe needs of EFL learners, advancing the effectiveness of writing education in\nthis context."
                },
                "authors": [
                    {
                        "name": "Jieun Han"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Minsun Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Yoonsu Kim"
                    },
                    {
                        "name": "Tak Yeon Lee"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Juho Kim"
                    },
                    {
                        "name": "So-Yeon Ahn"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14033v2",
                "updated": "2024-09-02T05:55:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    55,
                    6,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-26T05:55:48Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents"
                },
                "summary": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations."
                },
                "authors": [
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06764v3",
                "updated": "2024-09-02T05:48:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    48,
                    54,
                    0,
                    246,
                    0
                ],
                "published": "2024-03-11T14:35:32Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    14,
                    35,
                    32,
                    0,
                    71,
                    0
                ],
                "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models"
                },
                "summary": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "Accepted to ECCV 2024 (Oral), code is released at\n  https://github.com/pkunlp-icler/FastV,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10311v2",
                "updated": "2024-09-02T03:37:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    3,
                    37,
                    35,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-14T06:47:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    6,
                    47,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large\n  Language Models"
                },
                "summary": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench."
                },
                "authors": [
                    {
                        "name": "Wenjing Zhang"
                    },
                    {
                        "name": "Xuejiao Lei"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Meijuan An"
                    },
                    {
                        "name": "Bikun Yang"
                    },
                    {
                        "name": "KaiKai Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04818v2",
                "updated": "2024-09-02T02:44:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    44,
                    11,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-08T05:36:52Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    5,
                    36,
                    52,
                    2,
                    129,
                    0
                ],
                "title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation"
                },
                "summary": "Evaluating the quality of free-text explanations is a multifaceted,\nsubjective, and labor-intensive task. Large language models (LLMs) present an\nappealing alternative due to their potential for consistency, scalability, and\ncost-efficiency. In this work, we present ACORN, a new dataset of 3,500\nfree-text explanations and aspect-wise quality ratings, and use it to evaluate\nhow LLMs rate explanations. We observed that larger models outputted labels\nthat maintained or increased the inter-annotator agreement, suggesting that\nthey are within the expected variance between human raters. However, their\ncorrelation with majority-voted human ratings varied across different quality\naspects, indicating that they are not a complete replacement. In turn, using\nLLMs as a supplement to a smaller group of human raters in some cases improved\nthe correlation with the original majority labels. However, the effect was\nlimited to cases where human raters were scarce, and an additional human rater\nhad a more pronounced effect in all cases. Overall, we recommend against using\nLLMs as a complete replacement for human raters but encourage using them in\nconfigurations that end with targeted human involvement. Data available here:\nhttps://github.com/a-brassard/ACORN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of free-text explanations is a multifaceted,\nsubjective, and labor-intensive task. Large language models (LLMs) present an\nappealing alternative due to their potential for consistency, scalability, and\ncost-efficiency. In this work, we present ACORN, a new dataset of 3,500\nfree-text explanations and aspect-wise quality ratings, and use it to evaluate\nhow LLMs rate explanations. We observed that larger models outputted labels\nthat maintained or increased the inter-annotator agreement, suggesting that\nthey are within the expected variance between human raters. However, their\ncorrelation with majority-voted human ratings varied across different quality\naspects, indicating that they are not a complete replacement. In turn, using\nLLMs as a supplement to a smaller group of human raters in some cases improved\nthe correlation with the original majority labels. However, the effect was\nlimited to cases where human raters were scarce, and an additional human rater\nhad a more pronounced effect in all cases. Overall, we recommend against using\nLLMs as a complete replacement for human raters but encourage using them in\nconfigurations that end with targeted human involvement. Data available here:\nhttps://github.com/a-brassard/ACORN"
                },
                "authors": [
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "arxiv_comment": "18 pages, 7 figures, accepted to COLM 2024. Data available here:\n  https://github.com/a-brassard/ACORN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15879v2",
                "updated": "2024-09-02T02:30:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    30,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-28T15:50:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Persuasion Games using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Games using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase)."
                },
                "authors": [
                    {
                        "name": "Ganesh Prasath Ramani"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Santhosh V"
                    },
                    {
                        "name": "Yash Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Yash Bhatia"
                },
                "author": "Yash Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.10349v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.10349v7",
                "updated": "2024-09-02T01:13:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    1,
                    13,
                    6,
                    0,
                    246,
                    0
                ],
                "published": "2022-08-19T04:36:21Z",
                "published_parsed": [
                    2022,
                    8,
                    19,
                    4,
                    36,
                    21,
                    4,
                    231,
                    0
                ],
                "title": "Limiting free energy per particle for Ising Model by approximating its\n  functional integral",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limiting free energy per particle for Ising Model by approximating its\n  functional integral"
                },
                "summary": "There have been a lot of methods aimed at studying the limiting free energy\nper particle (LFEPP) for 3D Ising model in absence of an external magnetic\nfield. These methods are elegant, but most of them are complicated and often\nrequire specialized knowledge and special skills. Here we approximate the LFEPP\nfor Ising model from its corresponding functional integral. The resulting Ising\nLFEPP, which includes an integral of a special function, is exact\nasymptotically. For 1D and 2D Ising model, this LFEPPs in two limiting cases\nare consistent or formally consistent with those well-known, respectively.\nBased on these consistences, we infer the LFEPPs for 3D model in these special\ncases. Additionally, with this LFEPP, we derive a further approximate one for\n3D Ising model, from which we infer the critical temperature $z_c\\approx\n0.21\\sim 0.22$. Furthermore, we suggest similar LFEPPs for 1D-3D Ising models\nwith an external magnetic field, although they are too complicated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been a lot of methods aimed at studying the limiting free energy\nper particle (LFEPP) for 3D Ising model in absence of an external magnetic\nfield. These methods are elegant, but most of them are complicated and often\nrequire specialized knowledge and special skills. Here we approximate the LFEPP\nfor Ising model from its corresponding functional integral. The resulting Ising\nLFEPP, which includes an integral of a special function, is exact\nasymptotically. For 1D and 2D Ising model, this LFEPPs in two limiting cases\nare consistent or formally consistent with those well-known, respectively.\nBased on these consistences, we infer the LFEPPs for 3D model in these special\ncases. Additionally, with this LFEPP, we derive a further approximate one for\n3D Ising model, from which we infer the critical temperature $z_c\\approx\n0.21\\sim 0.22$. Furthermore, we suggest similar LFEPPs for 1D-3D Ising models\nwith an external magnetic field, although they are too complicated."
                },
                "authors": [
                    {
                        "name": "Rong Qiang Wei"
                    }
                ],
                "author_detail": {
                    "name": "Rong Qiang Wei"
                },
                "author": "Rong Qiang Wei",
                "arxiv_comment": "38 pages; 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.10349v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.10349v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12606v2",
                "updated": "2024-09-02T00:52:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    52,
                    1,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-08T05:04:13Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    5,
                    4,
                    13,
                    3,
                    221,
                    0
                ],
                "title": "Towards Non-invasive and Personalized Management of Breast Cancer\n  Patients from Multiparametric MRI via A Large Mixture-of-Modality-Experts\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Non-invasive and Personalized Management of Breast Cancer\n  Patients from Multiparametric MRI via A Large Mixture-of-Modality-Experts\n  Model"
                },
                "summary": "Breast magnetic resonance imaging (MRI) is the imaging technique with the\nhighest sensitivity for detecting breast cancer and is routinely used for women\nat high risk. Despite the comprehensive multiparametric protocol of breast MRI,\nexisting artificial intelligence-based studies predominantly rely on single\nsequences and have limited validation. Here we report a large\nmixture-of-modality-experts model (MOME) that integrates multiparametric MRI\ninformation within a unified structure, offering a noninvasive method for\npersonalized breast cancer management. We have curated the largest\nmultiparametric breast MRI dataset, involving 5,205 patients from three\nhospitals in the north, southeast, and southwest of China, for the development\nand extensive evaluation of our model. MOME demonstrated accurate and robust\nidentification of breast cancer. It achieved comparable performance for\nmalignancy recognition to that of four senior radiologists and significantly\noutperformed a junior radiologist, with 0.913 AUROC, 0.948 AUPRC, 0.905 F1\nscore, and 0.723 MCC. Our findings suggest that MOME could reduce the need for\nbiopsies in BI-RADS 4 patients with a ratio of 7.3%, classify triple-negative\nbreast cancer with an AUROC of 0.709, and predict pathological complete\nresponse to neoadjuvant chemotherapy with an AUROC of 0.694. The model further\nsupports scalable and interpretable inference, adapting to missing modalities\nand providing decision explanations by highlighting lesions and measuring\nmodality contributions. MOME exemplifies a discriminative, robust, scalable,\nand interpretable multimodal model, paving the way for noninvasive,\npersonalized management of breast cancer patients based on multiparametric\nbreast imaging data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast magnetic resonance imaging (MRI) is the imaging technique with the\nhighest sensitivity for detecting breast cancer and is routinely used for women\nat high risk. Despite the comprehensive multiparametric protocol of breast MRI,\nexisting artificial intelligence-based studies predominantly rely on single\nsequences and have limited validation. Here we report a large\nmixture-of-modality-experts model (MOME) that integrates multiparametric MRI\ninformation within a unified structure, offering a noninvasive method for\npersonalized breast cancer management. We have curated the largest\nmultiparametric breast MRI dataset, involving 5,205 patients from three\nhospitals in the north, southeast, and southwest of China, for the development\nand extensive evaluation of our model. MOME demonstrated accurate and robust\nidentification of breast cancer. It achieved comparable performance for\nmalignancy recognition to that of four senior radiologists and significantly\noutperformed a junior radiologist, with 0.913 AUROC, 0.948 AUPRC, 0.905 F1\nscore, and 0.723 MCC. Our findings suggest that MOME could reduce the need for\nbiopsies in BI-RADS 4 patients with a ratio of 7.3%, classify triple-negative\nbreast cancer with an AUROC of 0.709, and predict pathological complete\nresponse to neoadjuvant chemotherapy with an AUROC of 0.694. The model further\nsupports scalable and interpretable inference, adapting to missing modalities\nand providing decision explanations by highlighting lesions and measuring\nmodality contributions. MOME exemplifies a discriminative, robust, scalable,\nand interpretable multimodal model, paving the way for noninvasive,\npersonalized management of breast cancer patients based on multiparametric\nbreast imaging data."
                },
                "authors": [
                    {
                        "name": "Luyang Luo"
                    },
                    {
                        "name": "Mingxiang Wu"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Qiong Wang"
                    },
                    {
                        "name": "Varut Vardhanabhuti"
                    },
                    {
                        "name": "Winnie CW Chu"
                    },
                    {
                        "name": "Zhenhui Li"
                    },
                    {
                        "name": "Juan Zhou"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "27 pages, 8 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07445v2",
                "updated": "2024-09-02T00:46:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    46,
                    10,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-10T07:56:19Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    7,
                    56,
                    19,
                    2,
                    192,
                    0
                ],
                "title": "Network inference from oscillatory signals based on circle map",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network inference from oscillatory signals based on circle map"
                },
                "summary": "Synchronization is ubiquitous in nature, which is mathematically described by\ncoupled oscillators. Synchronization strongly depends on the interaction\nnetwork, and the network plays a crucial role in controlling the dynamics. To\nunderstand and control synchronization dynamics in the real world, it is\nessential to identify the network from the observed data. While previous\nstudies have developed the methods for inferring the network of asynchronous\nsystems, it remains challenging to infer the network of well-synchronized\noscillators. In this study, we develop a method for non-invasively inferring\nthe network of synchronized and desynchronized oscillators. This method is\nbased on the circle map, which describes the phase change in an oscillatory\ncycle. Our method discards a large part of data used for inference, which may\nseem counterintuitive. However, the effectiveness of the method is supported by\nthe phase reduction theory, a well-established theory for analyzing weakly\ncoupled oscillators. We verify the proposed method by applying it to simulated\ndata of the limit-cycle oscillators. This study provides an important step\ntowards understanding synchronization in real-world systems from a network\nperspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronization is ubiquitous in nature, which is mathematically described by\ncoupled oscillators. Synchronization strongly depends on the interaction\nnetwork, and the network plays a crucial role in controlling the dynamics. To\nunderstand and control synchronization dynamics in the real world, it is\nessential to identify the network from the observed data. While previous\nstudies have developed the methods for inferring the network of asynchronous\nsystems, it remains challenging to infer the network of well-synchronized\noscillators. In this study, we develop a method for non-invasively inferring\nthe network of synchronized and desynchronized oscillators. This method is\nbased on the circle map, which describes the phase change in an oscillatory\ncycle. Our method discards a large part of data used for inference, which may\nseem counterintuitive. However, the effectiveness of the method is supported by\nthe phase reduction theory, a well-established theory for analyzing weakly\ncoupled oscillators. We verify the proposed method by applying it to simulated\ndata of the limit-cycle oscillators. This study provides an important step\ntowards understanding synchronization in real-world systems from a network\nperspective."
                },
                "authors": [
                    {
                        "name": "Akari Matsuki"
                    },
                    {
                        "name": "Hiroshi Kori"
                    },
                    {
                        "name": "Ryota Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Ryota Kobayashi"
                },
                "author": "Ryota Kobayashi",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.AO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16528v3",
                "updated": "2024-09-02T00:29:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    29,
                    12,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-25T11:41:24Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    11,
                    41,
                    24,
                    3,
                    116,
                    0
                ],
                "title": "Generalized Posterior Calibration via Sequential Monte Carlo Sampler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Posterior Calibration via Sequential Monte Carlo Sampler"
                },
                "summary": "As the amount and complexity of available data increases, the need for robust\nstatistical learning becomes more pressing. To enhance resilience against model\nmisspecification, the generalized posterior inference method adjusts the\nlikelihood term by exponentiating it with a learning rate, thereby fine-tuning\nthe dispersion of the posterior distribution. This study proposes a\ncomputationally efficient strategy for selecting an appropriate learning rate.\nThe proposed approach builds upon the generalized posterior calibration (GPC)\nalgorithm, which is designed to select a learning rate that ensures nominal\nfrequentist coverage. This algorithm, which evaluates the coverage probability\nusing bootstrap samples, has high computational costs because of the repeated\nposterior simulations needed for bootstrap samples. To address this limitation,\nthe study proposes an algorithm that combines elements of the GPC algorithm\nwith the sequential Monte Carlo (SMC) sampler. By leveraging the similarity\nbetween the learning rate in generalized posterior inference and the inverse\ntemperature in SMC sampling, the proposed algorithm efficiently calibrates the\nposterior distribution with a reduced computational cost. For demonstration,\nthe proposed algorithm was applied to several statistical learning models and\nshown to be significantly faster than the original GPC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the amount and complexity of available data increases, the need for robust\nstatistical learning becomes more pressing. To enhance resilience against model\nmisspecification, the generalized posterior inference method adjusts the\nlikelihood term by exponentiating it with a learning rate, thereby fine-tuning\nthe dispersion of the posterior distribution. This study proposes a\ncomputationally efficient strategy for selecting an appropriate learning rate.\nThe proposed approach builds upon the generalized posterior calibration (GPC)\nalgorithm, which is designed to select a learning rate that ensures nominal\nfrequentist coverage. This algorithm, which evaluates the coverage probability\nusing bootstrap samples, has high computational costs because of the repeated\nposterior simulations needed for bootstrap samples. To address this limitation,\nthe study proposes an algorithm that combines elements of the GPC algorithm\nwith the sequential Monte Carlo (SMC) sampler. By leveraging the similarity\nbetween the learning rate in generalized posterior inference and the inverse\ntemperature in SMC sampling, the proposed algorithm efficiently calibrates the\nposterior distribution with a reduced computational cost. For demonstration,\nthe proposed algorithm was applied to several statistical learning models and\nshown to be significantly faster than the original GPC."
                },
                "authors": [
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2024 6th Asia\n  Conference on Machine Learning and Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08414v2",
                "updated": "2024-09-01T22:58:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    22,
                    58,
                    51,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-12T16:58:41Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    16,
                    58,
                    41,
                    2,
                    164,
                    0
                ],
                "title": "Discovering Preference Optimization Algorithms with and for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Preference Optimization Algorithms with and for Large\n  Language Models"
                },
                "summary": "Offline preference optimization is a key method for enhancing and controlling\nthe quality of Large Language Model (LLM) outputs. Typically, preference\noptimization is approached as an offline supervised learning task using\nmanually-crafted convex loss functions. While these methods are based on\ntheoretical insights, they are inherently constrained by human creativity, so\nthe large search space of possible loss functions remains under explored. We\naddress this by performing LLM-driven objective discovery to automatically\ndiscover new state-of-the-art preference optimization algorithms without\n(expert) human intervention. Specifically, we iteratively prompt an LLM to\npropose and implement new preference optimization loss functions based on\npreviously-evaluated performance metrics. This process leads to the discovery\nof previously-unknown and performant preference optimization algorithms. The\nbest performing of these we call Discovered Preference Optimization (DiscoPOP),\na novel algorithm that adaptively blends logistic and exponential losses.\nExperiments demonstrate the state-of-the-art performance of DiscoPOP and its\nsuccessful transfer to held-out tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline preference optimization is a key method for enhancing and controlling\nthe quality of Large Language Model (LLM) outputs. Typically, preference\noptimization is approached as an offline supervised learning task using\nmanually-crafted convex loss functions. While these methods are based on\ntheoretical insights, they are inherently constrained by human creativity, so\nthe large search space of possible loss functions remains under explored. We\naddress this by performing LLM-driven objective discovery to automatically\ndiscover new state-of-the-art preference optimization algorithms without\n(expert) human intervention. Specifically, we iteratively prompt an LLM to\npropose and implement new preference optimization loss functions based on\npreviously-evaluated performance metrics. This process leads to the discovery\nof previously-unknown and performant preference optimization algorithms. The\nbest performing of these we call Discovered Preference Optimization (DiscoPOP),\na novel algorithm that adaptively blends logistic and exponential losses.\nExperiments demonstrate the state-of-the-art performance of DiscoPOP and its\nsuccessful transfer to held-out tasks."
                },
                "authors": [
                    {
                        "name": "Chris Lu"
                    },
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Alex J. Chan"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "Robert Tjarko Lange"
                    }
                ],
                "author_detail": {
                    "name": "Robert Tjarko Lange"
                },
                "author": "Robert Tjarko Lange",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01663v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01663v4",
                "updated": "2024-09-01T22:02:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    22,
                    2,
                    32,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-02T06:07:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    6,
                    7,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models"
                },
                "summary": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "JingSong Yang"
                    }
                ],
                "author_detail": {
                    "name": "JingSong Yang"
                },
                "author": "JingSong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01663v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01663v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16767v3",
                "updated": "2024-09-01T19:54:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    54,
                    56,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-25T17:20:45Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    20,
                    45,
                    3,
                    116,
                    0
                ],
                "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
                },
                "summary": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "name": "Wenhao Zhan"
                    },
                    {
                        "name": "Owen Oertell"
                    },
                    {
                        "name": "Gokul Swamy"
                    },
                    {
                        "name": "Kiant√© Brantley"
                    },
                    {
                        "name": "Thorsten Joachims"
                    },
                    {
                        "name": "J. Andrew Bagnell"
                    },
                    {
                        "name": "Jason D. Lee"
                    },
                    {
                        "name": "Wen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wen Sun"
                },
                "author": "Wen Sun",
                "arxiv_comment": "New experimental results on general chat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06573v2",
                "updated": "2024-09-01T19:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    38,
                    2,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-03T18:15:56Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    15,
                    56,
                    0,
                    155,
                    0
                ],
                "title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering"
                },
                "summary": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings."
                },
                "authors": [
                    {
                        "name": "Robert Osazuwa Ness"
                    },
                    {
                        "name": "Katie Matton"
                    },
                    {
                        "name": "Hayden Helm"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Junaid Bajwa"
                    },
                    {
                        "name": "Carey E. Priebe"
                    },
                    {
                        "name": "Eric Horvitz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Horvitz"
                },
                "author": "Eric Horvitz",
                "arxiv_comment": "9 pages, 3 figures, 2 algorithms, appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04668v2",
                "updated": "2024-09-01T19:00:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    0,
                    25,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-07T01:50:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    1,
                    50,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "Forecasting Live Chat Intent from Browsing History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Live Chat Intent from Browsing History"
                },
                "summary": "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage."
                },
                "authors": [
                    {
                        "name": "Se-eun Yoon"
                    },
                    {
                        "name": "Ahmad Bin Rabiah"
                    },
                    {
                        "name": "Zaid Alibadi"
                    },
                    {
                        "name": "Surya Kallumadi"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18629v2",
                "updated": "2024-09-01T18:49:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    18,
                    49,
                    10,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-26T09:40:30Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    9,
                    40,
                    30,
                    4,
                    208,
                    0
                ],
                "title": "CardioLab: Laboratory Values Estimation from Electrocardiogram Features\n  -- An Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CardioLab: Laboratory Values Estimation from Electrocardiogram Features\n  -- An Exploratory Study"
                },
                "summary": "Introduction: Laboratory value represents a cornerstone of medical\ndiagnostics, but suffers from slow turnaround times, and high costs and only\nprovides information about a single point in time. The continuous estimation of\nlaboratory values from non-invasive data such as electrocardiogram (ECG) would\ntherefore mark a significant frontier in healthcare monitoring. Despite its\ntransformative potential, this domain remains relatively underexplored within\nthe medical community.\n  Methods: In this preliminary study, we used a publicly available dataset\n(MIMIC-IV-ECG) to investigate the feasibility of inferring laboratory values\nfrom ECG features and patient demographics using tree-based models (XGBoost).\nWe define the prediction task as a binary prediction problem of predicting\nwhether the lab value falls into low or high abnormalities. The model\nperformance can then be assessed using AUROC.\n  Results: Our findings demonstrate promising results in the estimation of\nlaboratory values related to different organ systems based on a small yet\ncomprehensive set of features. While further research and validation are\nwarranted to fully assess the clinical utility and generalizability of\nECG-based estimation in healthcare monitoring, our findings lay the groundwork\nfor future investigations into approaches to laboratory value estimation using\nECG data. Such advancements hold promise for revolutionizing predictive\nhealthcare applications, offering faster, non-invasive, and more affordable\nmeans of patient monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Laboratory value represents a cornerstone of medical\ndiagnostics, but suffers from slow turnaround times, and high costs and only\nprovides information about a single point in time. The continuous estimation of\nlaboratory values from non-invasive data such as electrocardiogram (ECG) would\ntherefore mark a significant frontier in healthcare monitoring. Despite its\ntransformative potential, this domain remains relatively underexplored within\nthe medical community.\n  Methods: In this preliminary study, we used a publicly available dataset\n(MIMIC-IV-ECG) to investigate the feasibility of inferring laboratory values\nfrom ECG features and patient demographics using tree-based models (XGBoost).\nWe define the prediction task as a binary prediction problem of predicting\nwhether the lab value falls into low or high abnormalities. The model\nperformance can then be assessed using AUROC.\n  Results: Our findings demonstrate promising results in the estimation of\nlaboratory values related to different organ systems based on a small yet\ncomprehensive set of features. While further research and validation are\nwarranted to fully assess the clinical utility and generalizability of\nECG-based estimation in healthcare monitoring, our findings lay the groundwork\nfor future investigations into approaches to laboratory value estimation using\nECG data. Such advancements hold promise for revolutionizing predictive\nhealthcare applications, offering faster, non-invasive, and more affordable\nmeans of patient monitoring."
                },
                "authors": [
                    {
                        "name": "Juan Miguel Lopez Alcaraz"
                    },
                    {
                        "name": "Nils Strodthoff"
                    }
                ],
                "author_detail": {
                    "name": "Nils Strodthoff"
                },
                "author": "Nils Strodthoff",
                "arxiv_comment": "4 pages, (updated dataset features set description version) code\n  under https://github.com/AI4HealthUOL/CardioLab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15319v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15319v3",
                "updated": "2024-09-01T17:21:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    17,
                    21,
                    18,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-21T17:23:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    23,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"
                },
                "summary": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15319v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15319v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10237v3",
                "updated": "2024-09-01T16:39:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    16,
                    39,
                    31,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-16T02:35:17Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    2,
                    35,
                    17,
                    1,
                    107,
                    0
                ],
                "title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models"
                },
                "summary": "Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method."
                },
                "authors": [
                    {
                        "name": "Songtao Jiang"
                    },
                    {
                        "name": "Tuo Zheng"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08193v2",
                "updated": "2024-09-01T16:37:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    16,
                    37,
                    4,
                    6,
                    245,
                    0
                ],
                "published": "2023-05-14T16:06:52Z",
                "published_parsed": [
                    2023,
                    5,
                    14,
                    16,
                    6,
                    52,
                    6,
                    134,
                    0
                ],
                "title": "Finite sample inference in nonlinear regression estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite sample inference in nonlinear regression estimation"
                },
                "summary": "Nonlinear regression problem is one of the most popular and important\nstatistical tasks. The first methods like least squares estimation go back to\nGauss and Legendre. Recent models and developments in statistics and machine\nlearning like Deep Neuronal Networks or Bayesian methods for nonlinear PDE\nstimulate new research in this direction which has to address the important\nissues and challenges of modern statistical inference such as huge complexity\nand parameter dimension of the model, limited samples size, lack of convexity\nand identifiability among many others. Classical results of nonparametric\nstatistics in terms of rate of convergence fail to explain the mentioned issues\nbecause of the curse of dimensionality problem. This note offers a general\napproach to studying a nonlinear regression problem which enables one to derive\nfinite sample expansions for the loss of the penalized maximum likelihood\nestimation (pMLE) with explicit error guarantees and obtain sharp loss and risk\nbounds. An important step of the study called calming allows to make the\nobjective function stochastically linear by extending the parameter space and\nto reduce the original problem to semiparametric estimation with a special\nstochastically linear structure. Such models are studied in this paper in the\nfull generality, the results provide finite sample expansions and risk bounds\nfor the full and target parameters. In all results, the remainder is given\nexplicitly and can be evaluated in terms of the effective sample size and\neffective parameter dimension which allows us to identify the so-called\ncritical parameter dimension. The results are also dimension and\ncoordinate-free. Despite generality, all the presented bounds are nearly sharp\nand the classical asymptotic results can be obtained as simple corollaries. The\nobtained general results are specified to nonlinear smooth regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear regression problem is one of the most popular and important\nstatistical tasks. The first methods like least squares estimation go back to\nGauss and Legendre. Recent models and developments in statistics and machine\nlearning like Deep Neuronal Networks or Bayesian methods for nonlinear PDE\nstimulate new research in this direction which has to address the important\nissues and challenges of modern statistical inference such as huge complexity\nand parameter dimension of the model, limited samples size, lack of convexity\nand identifiability among many others. Classical results of nonparametric\nstatistics in terms of rate of convergence fail to explain the mentioned issues\nbecause of the curse of dimensionality problem. This note offers a general\napproach to studying a nonlinear regression problem which enables one to derive\nfinite sample expansions for the loss of the penalized maximum likelihood\nestimation (pMLE) with explicit error guarantees and obtain sharp loss and risk\nbounds. An important step of the study called calming allows to make the\nobjective function stochastically linear by extending the parameter space and\nto reduce the original problem to semiparametric estimation with a special\nstochastically linear structure. Such models are studied in this paper in the\nfull generality, the results provide finite sample expansions and risk bounds\nfor the full and target parameters. In all results, the remainder is given\nexplicitly and can be evaluated in terms of the effective sample size and\neffective parameter dimension which allows us to identify the so-called\ncritical parameter dimension. The results are also dimension and\ncoordinate-free. Despite generality, all the presented bounds are nearly sharp\nand the classical asymptotic results can be obtained as simple corollaries. The\nobtained general results are specified to nonlinear smooth regression."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.08193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62F25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05965v2",
                "updated": "2024-09-01T15:13:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    15,
                    13,
                    52,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-08T14:04:58Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    14,
                    4,
                    58,
                    0,
                    190,
                    0
                ],
                "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models"
                },
                "summary": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI."
                },
                "authors": [
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Lijia Yu"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xiao-Shan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Shan Gao"
                },
                "author": "Xiao-Shan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10459v2",
                "updated": "2024-09-01T14:28:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    14,
                    28,
                    11,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-15T01:02:48Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    1,
                    2,
                    48,
                    5,
                    167,
                    0
                ],
                "title": "CancerLLM: A Large Language Model in Cancer Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerLLM: A Large Language Model in Cancer Domain"
                },
                "summary": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, and cancer diagnosis generation. Our evaluation demonstrated that\nCancerLLM achieves state-of-the-art results compared to other existing LLMs,\nwith an average F1 score improvement of 7.61 %. Additionally, CancerLLM\noutperforms other models on two proposed robustness testbeds. This illustrates\nthat CancerLLM can be effectively applied to clinical AI systems, enhancing\nclinical research and healthcare delivery in the field of cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, and cancer diagnosis generation. Our evaluation demonstrated that\nCancerLLM achieves state-of-the-art results compared to other existing LLMs,\nwith an average F1 score improvement of 7.61 %. Additionally, CancerLLM\noutperforms other models on two proposed robustness testbeds. This illustrates\nthat CancerLLM can be effectively applied to clinical AI systems, enhancing\nclinical research and healthcare delivery in the field of cancer."
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jiatan Huang"
                    },
                    {
                        "name": "Jeremy Yeung"
                    },
                    {
                        "name": "Anne Blaes"
                    },
                    {
                        "name": "Steven Johnson"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "add the diagnosis evaluation of ICD code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11108v2",
                "updated": "2024-09-01T13:33:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    33,
                    36,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-20T18:07:27Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    18,
                    7,
                    27,
                    1,
                    233,
                    0
                ],
                "title": "The densities in diffuse and translucent molecular clouds: estimates\n  from observations of C$_2$ and from 3-dimensional extinction maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The densities in diffuse and translucent molecular clouds: estimates\n  from observations of C$_2$ and from 3-dimensional extinction maps"
                },
                "summary": "Newly-computed collisional rate coefficients for the excitation of C$_2$ in\ncollisions with H$_2$, presented recently by Najar and Kalugina (2020), are\nsignificantly larger than the values adopted previously in models for the\nexcitation of the C$_2$ molecule, a widely used probe of the interstellar gas\ndensity. With these new rate coefficients, we have modeled the C$_2$ rotational\ndistributions inferred from visible and ultraviolet absorption observations of\nelectronic transitions of C$_2$ towards a collection of 46 nearby background\nsources. The inferred gas densities in the foreground interstellar clouds\nresponsible for the observed C$_2$ absorption are a factor 4 to 7 smaller than\nthose inferred previously, a direct reflection of the larger collisional rate\ncoefficients computed by Najar and Kalugina (2020). These lower density\nestimates are generally in good agreement with the peak densities inferred from\n3D extinction maps for the relevant sightlines. In cases where H$_3^+$\nabsorption has also been observed and used to estimate the cosmic-ray\nionization rate (CRIR), our estimates of the latter will also decrease\naccordingly because the H$_3^+$ abundance is a function of the ratio of the\nCRIR to the gas density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Newly-computed collisional rate coefficients for the excitation of C$_2$ in\ncollisions with H$_2$, presented recently by Najar and Kalugina (2020), are\nsignificantly larger than the values adopted previously in models for the\nexcitation of the C$_2$ molecule, a widely used probe of the interstellar gas\ndensity. With these new rate coefficients, we have modeled the C$_2$ rotational\ndistributions inferred from visible and ultraviolet absorption observations of\nelectronic transitions of C$_2$ towards a collection of 46 nearby background\nsources. The inferred gas densities in the foreground interstellar clouds\nresponsible for the observed C$_2$ absorption are a factor 4 to 7 smaller than\nthose inferred previously, a direct reflection of the larger collisional rate\ncoefficients computed by Najar and Kalugina (2020). These lower density\nestimates are generally in good agreement with the peak densities inferred from\n3D extinction maps for the relevant sightlines. In cases where H$_3^+$\nabsorption has also been observed and used to estimate the cosmic-ray\nionization rate (CRIR), our estimates of the latter will also decrease\naccordingly because the H$_3^+$ abundance is a function of the ratio of the\nCRIR to the gas density."
                },
                "authors": [
                    {
                        "name": "David A. Neufeld"
                    },
                    {
                        "name": "Daniel E. Welty"
                    },
                    {
                        "name": "Alexei V. Ivlev"
                    },
                    {
                        "name": "Paola Caselli"
                    },
                    {
                        "name": "Gordian Edenhofer"
                    },
                    {
                        "name": "Nick Indriolo"
                    },
                    {
                        "name": "Marta Obolentseva"
                    },
                    {
                        "name": "Kedron Silsbee"
                    },
                    {
                        "name": "Paule Sonnentrucker"
                    },
                    {
                        "name": "Mark G. Wolfire"
                    }
                ],
                "author_detail": {
                    "name": "Mark G. Wolfire"
                },
                "arxiv_affiliation": "UMD",
                "author": "Mark G. Wolfire",
                "arxiv_comment": "46 pages, 11 figures, 1 figure set with 23 figures, accepted for\n  publication in the Astrophysical Journal. Version 2: An error is corrected in\n  Table 2, in which values listed in two of the columns were jumbled in Version\n  1. There are no other changes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14073v3",
                "updated": "2024-09-01T13:26:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    26,
                    58,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-19T07:02:26Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    7,
                    2,
                    26,
                    4,
                    201,
                    0
                ],
                "title": "LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural\n  Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) have gained significant research attention in\nthe last decade due to their potential to drive resource-constrained edge\ndevices. Though existing SNN accelerators offer high efficiency in processing\nsparse spikes with dense weights, opportunities are less explored in SNNs with\nsparse weights, i.e., dual-sparsity. In this work, we study the acceleration of\ndual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix\nmultiplication (spMspM). We observe that naively running a dual-sparse SNN on\nexisting spMspM accelerators designed for dual-sparse Artificial Neural\nNetworks (ANNs) exhibits sub-optimal efficiency. The main challenge is that\nprocessing timesteps, a natural property of SNNs, introduces an extra loop to\nANN spMspM, leading to longer latency and more memory traffic. To address the\nproblem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes\nboth data movement across timesteps and the end-to-end latency of dual-sparse\nSNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly\nspike compression mechanism that efficiently compresses single-bit spikes and\nensures contiguous memory access. We further propose an FTP-friendly inner-join\ncircuit that can lower the cost of the expensive prefix-sum circuits with\nalmost no throughput penalty. All the above techniques for FTP dataflow are\nencapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs.\nWith FTP dataflow, compression, and inner-join, running dual-sparse SNN\nworkloads on LoAS demonstrates significant speedup (up to $8.51\\times$) and\nenergy reduction (up to $3.68\\times$) compared to running it on prior\ndual-sparse accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have gained significant research attention in\nthe last decade due to their potential to drive resource-constrained edge\ndevices. Though existing SNN accelerators offer high efficiency in processing\nsparse spikes with dense weights, opportunities are less explored in SNNs with\nsparse weights, i.e., dual-sparsity. In this work, we study the acceleration of\ndual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix\nmultiplication (spMspM). We observe that naively running a dual-sparse SNN on\nexisting spMspM accelerators designed for dual-sparse Artificial Neural\nNetworks (ANNs) exhibits sub-optimal efficiency. The main challenge is that\nprocessing timesteps, a natural property of SNNs, introduces an extra loop to\nANN spMspM, leading to longer latency and more memory traffic. To address the\nproblem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes\nboth data movement across timesteps and the end-to-end latency of dual-sparse\nSNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly\nspike compression mechanism that efficiently compresses single-bit spikes and\nensures contiguous memory access. We further propose an FTP-friendly inner-join\ncircuit that can lower the cost of the expensive prefix-sum circuits with\nalmost no throughput penalty. All the above techniques for FTP dataflow are\nencapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs.\nWith FTP dataflow, compression, and inner-join, running dual-sparse SNN\nworkloads on LoAS demonstrates significant speedup (up to $8.51\\times$) and\nenergy reduction (up to $3.68\\times$) compared to running it on prior\ndual-sparse accelerators."
                },
                "authors": [
                    {
                        "name": "Ruokai Yin"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "arxiv_comment": "Accepted to MICRO 2024. Will update with the camera-ready version\n  once ready. (Github: https://github.com/RuokaiYin/LoAS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v2",
                "updated": "2024-09-01T13:13:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    13,
                    5,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.04565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.04565v2",
                "updated": "2024-09-01T13:12:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    12,
                    34,
                    6,
                    245,
                    0
                ],
                "published": "2023-09-08T19:34:29Z",
                "published_parsed": [
                    2023,
                    9,
                    8,
                    19,
                    34,
                    29,
                    4,
                    251,
                    0
                ],
                "title": "A Versatile Graph Learning Approach through LLM-based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Versatile Graph Learning Approach through LLM-based Agent"
                },
                "summary": "Designing versatile graph learning approaches is important, considering the\ndiverse graphs and tasks existing in real-world applications. Existing methods\nhave attempted to achieve this target through automated machine learning\ntechniques, pre-training and fine-tuning strategies, and large language models.\nHowever, these methods are not versatile enough for graph learning, as they\nwork on either limited types of graphs or a single task. In this paper, we\npropose to explore versatile graph learning approaches with LLM-based agents,\nand the key insight is customizing the graph learning procedures for diverse\ngraphs and tasks. To achieve this, we develop several LLM-based agents,\nequipped with diverse profiles, tools, functions and human experience. They\ncollaborate to configure each procedure with task and data-specific settings\nstep by step towards versatile solutions, and the proposed method is dubbed\nGL-Agent. By evaluating on diverse tasks and graphs, the correct results of the\nagent and its comparable performance showcase the versatility of the proposed\nmethod, especially in complex scenarios.The low resource cost and the potential\nto use open-source LLMs highlight the efficiency of GL-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing versatile graph learning approaches is important, considering the\ndiverse graphs and tasks existing in real-world applications. Existing methods\nhave attempted to achieve this target through automated machine learning\ntechniques, pre-training and fine-tuning strategies, and large language models.\nHowever, these methods are not versatile enough for graph learning, as they\nwork on either limited types of graphs or a single task. In this paper, we\npropose to explore versatile graph learning approaches with LLM-based agents,\nand the key insight is customizing the graph learning procedures for diverse\ngraphs and tasks. To achieve this, we develop several LLM-based agents,\nequipped with diverse profiles, tools, functions and human experience. They\ncollaborate to configure each procedure with task and data-specific settings\nstep by step towards versatile solutions, and the proposed method is dubbed\nGL-Agent. By evaluating on diverse tasks and graphs, the correct results of the\nagent and its comparable performance showcase the versatility of the proposed\nmethod, especially in complex scenarios.The low resource cost and the potential\nto use open-source LLMs highlight the efficiency of GL-Agent."
                },
                "authors": [
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Xiaohan Zheng"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Quanming Yao"
                    }
                ],
                "author_detail": {
                    "name": "Quanming Yao"
                },
                "author": "Quanming Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.04565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.04565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18793v2",
                "updated": "2024-09-01T11:05:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    11,
                    5,
                    41,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-29T15:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    15,
                    27,
                    9,
                    0,
                    120,
                    0
                ],
                "title": "A real-time digital twin of azimuthal thermoacoustic instabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A real-time digital twin of azimuthal thermoacoustic instabilities"
                },
                "summary": "When they occur, azimuthal thermoacoustic oscillations can detrimentally\naffect the safe operation of gas turbines and aeroengines. We develop a\nreal-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular\ncombustor. The digital twin seamlessly combines two sources of information\nabout the system (i) a physics-based low-order model; and (ii) raw and sparse\nexperimental data from microphones, which contain both aleatoric noise and\nturbulent fluctuations. First, we derive a low-order thermoacoustic model for\nazimuthal instabilities, which is deterministic. Second, we propose a real-time\ndata assimilation framework to infer the acoustic pressure, the physical\nparameters, and the model and measurement biases simultaneously. This is the\nbias-regularized ensemble Kalman filter (r-EnKF), for which we find an\nanalytical solution that solves the optimization problem. Third, we propose a\nreservoir computer, which infers both the model bias and measurement bias to\nclose the assimilation equations. Fourth, we propose a real-time digital twin\nof the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular\ncombustor for a variety of equivalence ratios. We find that the real-time\ndigital twin (i) autonomously predicts azimuthal dynamics, in contrast to\nbias-unregularized methods; (ii) uncovers the physical acoustic pressure from\nthe raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying\nparameter system, which generalizes existing models that have constant\nparameters, and capture only slow-varying variables. The digital twin\ngeneralizes to all equivalence ratios, which bridges the gap of existing\nmodels. This work opens new opportunities for real-time digital twinning of\nmulti-physics problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When they occur, azimuthal thermoacoustic oscillations can detrimentally\naffect the safe operation of gas turbines and aeroengines. We develop a\nreal-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular\ncombustor. The digital twin seamlessly combines two sources of information\nabout the system (i) a physics-based low-order model; and (ii) raw and sparse\nexperimental data from microphones, which contain both aleatoric noise and\nturbulent fluctuations. First, we derive a low-order thermoacoustic model for\nazimuthal instabilities, which is deterministic. Second, we propose a real-time\ndata assimilation framework to infer the acoustic pressure, the physical\nparameters, and the model and measurement biases simultaneously. This is the\nbias-regularized ensemble Kalman filter (r-EnKF), for which we find an\nanalytical solution that solves the optimization problem. Third, we propose a\nreservoir computer, which infers both the model bias and measurement bias to\nclose the assimilation equations. Fourth, we propose a real-time digital twin\nof the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular\ncombustor for a variety of equivalence ratios. We find that the real-time\ndigital twin (i) autonomously predicts azimuthal dynamics, in contrast to\nbias-unregularized methods; (ii) uncovers the physical acoustic pressure from\nthe raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying\nparameter system, which generalizes existing models that have constant\nparameters, and capture only slow-varying variables. The digital twin\ngeneralizes to all equivalence ratios, which bridges the gap of existing\nmodels. This work opens new opportunities for real-time digital twinning of\nmulti-physics problems."
                },
                "authors": [
                    {
                        "name": "Andrea N√≥voa"
                    },
                    {
                        "name": "Nicolas Noiray"
                    },
                    {
                        "name": "James R. Dawson"
                    },
                    {
                        "name": "Luca Magri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Magri"
                },
                "author": "Luca Magri",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v6",
                "updated": "2024-09-01T10:12:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    10,
                    12,
                    45,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16644v2",
                "updated": "2024-09-01T09:57:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    9,
                    57,
                    4,
                    6,
                    245,
                    0
                ],
                "published": "2024-03-25T11:29:32Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    11,
                    29,
                    32,
                    0,
                    85,
                    0
                ],
                "title": "Bridging the Sim-to-Real Gap with Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Sim-to-Real Gap with Bayesian Inference"
                },
                "summary": "We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art."
                },
                "authors": [
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Bhavya Sukhija"
                    },
                    {
                        "name": "Lenart Treven"
                    },
                    {
                        "name": "Florian D√∂rfler"
                    },
                    {
                        "name": "Stelian Coros"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09726v2",
                "updated": "2024-09-01T09:00:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    9,
                    0,
                    13,
                    6,
                    245,
                    0
                ],
                "published": "2024-03-12T20:52:03Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    20,
                    52,
                    3,
                    1,
                    72,
                    0
                ],
                "title": "Quantile balancing inverse probability weighting for non-probability\n  samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile balancing inverse probability weighting for non-probability\n  samples"
                },
                "summary": "Usage of non-statistical data sources for statistical purposes have become\nincreasingly popular in recent years, also in official statistics. However,\nstatistical inference based on non-probability samples is made more difficult\nby nature of them being biased and not representative of the target population.\nIn this paper we propose quantile balancing inverse probability weighting\nestimator (QBIPW) for non-probability samples. We use the idea of Harms and\nDuchesne (2006) which allows to include quantile information in the estimation\nprocess so known totals and distribution for auxiliary variables are being\nreproduced. We discuss the estimation of the QBIPW probabilities and its\nvariance. Our simulation study has demonstrated that the proposed estimators\nare robust against model mis-specification and, as a result, help to reduce\nbias and mean squared error. Finally, we applied the proposed method to\nestimate the share of vacancies aimed at Ukrainian workers in Poland using an\nintegrated set of administrative and survey data about job vacancies",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usage of non-statistical data sources for statistical purposes have become\nincreasingly popular in recent years, also in official statistics. However,\nstatistical inference based on non-probability samples is made more difficult\nby nature of them being biased and not representative of the target population.\nIn this paper we propose quantile balancing inverse probability weighting\nestimator (QBIPW) for non-probability samples. We use the idea of Harms and\nDuchesne (2006) which allows to include quantile information in the estimation\nprocess so known totals and distribution for auxiliary variables are being\nreproduced. We discuss the estimation of the QBIPW probabilities and its\nvariance. Our simulation study has demonstrated that the proposed estimators\nare robust against model mis-specification and, as a result, help to reduce\nbias and mean squared error. Finally, we applied the proposed method to\nestimate the share of vacancies aimed at Ukrainian workers in Poland using an\nintegrated set of administrative and survey data about job vacancies"
                },
                "authors": [
                    {
                        "name": "Maciej Berƒôsewicz"
                    },
                    {
                        "name": "Marcin Szymkowiak"
                    },
                    {
                        "name": "Piotr Chlebicki"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Chlebicki"
                },
                "author": "Piotr Chlebicki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08506v2",
                "updated": "2024-09-01T08:30:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    8,
                    30,
                    58,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-16T03:06:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding"
                },
                "summary": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Guanhua He"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Shaoli Liu"
                    },
                    {
                        "name": "Tianshi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Chen"
                },
                "author": "Tianshi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.15444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15444v2",
                "updated": "2024-09-03T17:48:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    48,
                    55,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-30T18:07:13Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    7,
                    13,
                    3,
                    151,
                    0
                ],
                "title": "Investigating the Robustness of LLMs on Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of LLMs on Math Word Problems"
                },
                "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, ProbleMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and better ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to ~6%."
                },
                "authors": [
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Kevin Scaria"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Swaroop Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Mishra"
                },
                "author": "Swaroop Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15460v2",
                "updated": "2024-09-03T16:56:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    56,
                    16,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-28T00:52:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    0,
                    52,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "Lagrangian approach to origami vertex analysis: Kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lagrangian approach to origami vertex analysis: Kinematics"
                },
                "summary": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design."
                },
                "authors": [
                    {
                        "name": "Matthew Grasinger"
                    },
                    {
                        "name": "Andrew Gillman"
                    },
                    {
                        "name": "Philip Buskohl"
                    }
                ],
                "author_detail": {
                    "name": "Philip Buskohl"
                },
                "author": "Philip Buskohl",
                "arxiv_doi": "10.1098/rsta.2024.0203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsta.2024.0203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Electronic supplementary information can be found at the published\n  article, https://doi.org/10.1098/rsta.2024.0203",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06385v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06385v3",
                "updated": "2024-09-03T16:36:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    36,
                    6,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-10T15:44:22Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    15,
                    44,
                    22,
                    0,
                    162,
                    0
                ],
                "title": "Low-Rank Quantization-Aware Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Quantization-Aware Training for LLMs"
                },
                "summary": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT"
                },
                "authors": [
                    {
                        "name": "Yelysei Bondarenko"
                    },
                    {
                        "name": "Riccardo Del Chiaro"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06385v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06385v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16465v2",
                "updated": "2024-09-03T15:45:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    45,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-29T11:54:02Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    54,
                    2,
                    3,
                    242,
                    0
                ],
                "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework\n  for User Verbal and Nonverbal Behaviors"
                },
                "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions."
                },
                "authors": [
                    {
                        "name": "Szeyi Chan"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Mirjana Prpa"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v3",
                "updated": "2024-09-03T14:53:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    53,
                    34,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders √òland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Gy√∂rgy Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05731v2",
                "updated": "2024-09-03T14:14:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    14,
                    56,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-08T15:07:21Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    15,
                    7,
                    21,
                    3,
                    39,
                    0
                ],
                "title": "Face Recognition: to Deploy or not to Deploy? A Framework for Assessing\n  the Proportional Use of Face Recognition Systems in Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Recognition: to Deploy or not to Deploy? A Framework for Assessing\n  the Proportional Use of Face Recognition Systems in Real-World Scenarios"
                },
                "summary": "Face recognition (FR) has reached a high technical maturity. However, its use\nneeds to be carefully assessed from an ethical perspective, especially in\nsensitive scenarios. This is precisely the focus of this paper: the use of FR\nfor the identification of specific subjects in moderately to densely crowded\nspaces (e.g. public spaces, sports stadiums, train stations) and law\nenforcement scenarios. In particular, there is a need to consider the trade-off\nbetween the need to protect privacy and fundamental rights of citizens as well\nas their safety. Recent Artificial Intelligence (AI) policies, notably the\nEuropean AI Act, propose that such FR interventions should be proportionate and\ndeployed only when strictly necessary. Nevertheless, concrete guidelines on how\nto address the concept of proportional FR intervention are lacking to date.\nThis paper proposes a framework to contribute to assessing whether an FR\nintervention is proportionate or not for a given context of use in the above\nmentioned scenarios. It also identifies the main quantitative and qualitative\nvariables relevant to the FR intervention decision (e.g. number of people in\nthe scene, level of harm that the person(s) in search could perpetrate,\nconsequences to individual rights and freedoms) and propose a 2D graphical\nmodel making it possible to balance these variables in terms of ethical cost vs\nsecurity gain. Finally, different FR scenarios inspired by real-world\ndeployments validate the proposed model. The framework is conceived as a simple\nsupport tool for decision makers when confronted with the deployment of an FR\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face recognition (FR) has reached a high technical maturity. However, its use\nneeds to be carefully assessed from an ethical perspective, especially in\nsensitive scenarios. This is precisely the focus of this paper: the use of FR\nfor the identification of specific subjects in moderately to densely crowded\nspaces (e.g. public spaces, sports stadiums, train stations) and law\nenforcement scenarios. In particular, there is a need to consider the trade-off\nbetween the need to protect privacy and fundamental rights of citizens as well\nas their safety. Recent Artificial Intelligence (AI) policies, notably the\nEuropean AI Act, propose that such FR interventions should be proportionate and\ndeployed only when strictly necessary. Nevertheless, concrete guidelines on how\nto address the concept of proportional FR intervention are lacking to date.\nThis paper proposes a framework to contribute to assessing whether an FR\nintervention is proportionate or not for a given context of use in the above\nmentioned scenarios. It also identifies the main quantitative and qualitative\nvariables relevant to the FR intervention decision (e.g. number of people in\nthe scene, level of harm that the person(s) in search could perpetrate,\nconsequences to individual rights and freedoms) and propose a 2D graphical\nmodel making it possible to balance these variables in terms of ethical cost vs\nsecurity gain. Finally, different FR scenarios inspired by real-world\ndeployments validate the proposed model. The framework is conceived as a simple\nsupport tool for decision makers when confronted with the deployment of an FR\nsystem."
                },
                "authors": [
                    {
                        "name": "Pablo Negri"
                    },
                    {
                        "name": "Isabelle Hupont"
                    },
                    {
                        "name": "Emilia Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Emilia Gomez"
                },
                "author": "Emilia Gomez",
                "arxiv_doi": "10.1109/FG59268.2024.10581866",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/FG59268.2024.10581866",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published a shorter version in IEEE International Conference on\n  Automatic Face and Gesture Recognition 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00267v3",
                "updated": "2024-09-03T14:01:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    14,
                    1,
                    54,
                    1,
                    247,
                    0
                ],
                "published": "2023-09-01T05:53:33Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    5,
                    53,
                    33,
                    4,
                    244,
                    0
                ],
                "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with\n  AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with\n  AI Feedback"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but gathering\nhigh-quality preference labels is expensive. RL from AI Feedback (RLAIF),\nintroduced in Bai et al., offers a promising alternative that trains the reward\nmodel (RM) on preferences generated by an off-the-shelf LLM. Across the tasks\nof summarization, helpful dialogue generation, and harmless dialogue\ngeneration, we show that RLAIF achieves comparable performance to RLHF.\nFurthermore, we take a step towards \"self-improvement\" by demonstrating that\nRLAIF can outperform a supervised fine-tuned baseline even when the AI labeler\nis the same size as the policy, or even the exact same checkpoint as the\ninitial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that\ncircumvents RM training by obtaining rewards directly from an off-the-shelf LLM\nduring RL, which achieves superior performance to canonical RLAIF. Our results\nsuggest that RLAIF can achieve performance on-par with using human feedback,\noffering a potential solution to the scalability limitations of RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but gathering\nhigh-quality preference labels is expensive. RL from AI Feedback (RLAIF),\nintroduced in Bai et al., offers a promising alternative that trains the reward\nmodel (RM) on preferences generated by an off-the-shelf LLM. Across the tasks\nof summarization, helpful dialogue generation, and harmless dialogue\ngeneration, we show that RLAIF achieves comparable performance to RLHF.\nFurthermore, we take a step towards \"self-improvement\" by demonstrating that\nRLAIF can outperform a supervised fine-tuned baseline even when the AI labeler\nis the same size as the policy, or even the exact same checkpoint as the\ninitial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that\ncircumvents RM training by obtaining rewards directly from an off-the-shelf LLM\nduring RL, which achieves superior performance to canonical RLAIF. Our results\nsuggest that RLAIF can achieve performance on-par with using human feedback,\noffering a potential solution to the scalability limitations of RLHF."
                },
                "authors": [
                    {
                        "name": "Harrison Lee"
                    },
                    {
                        "name": "Samrat Phatale"
                    },
                    {
                        "name": "Hassan Mansoor"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Kellie Lu"
                    },
                    {
                        "name": "Colton Bishop"
                    },
                    {
                        "name": "Ethan Hall"
                    },
                    {
                        "name": "Victor Carbune"
                    },
                    {
                        "name": "Abhinav Rastogi"
                    },
                    {
                        "name": "Sushant Prakash"
                    }
                ],
                "author_detail": {
                    "name": "Sushant Prakash"
                },
                "author": "Sushant Prakash",
                "arxiv_comment": "Presented at ICML 2024",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning, PMLR 235:26874-26901, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07918v4",
                "updated": "2024-09-03T13:52:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    52,
                    24,
                    1,
                    247,
                    0
                ],
                "published": "2023-09-14T17:59:49Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    17,
                    59,
                    49,
                    3,
                    257,
                    0
                ],
                "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts"
                },
                "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI ."
                },
                "authors": [
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "A unified Human-Scene Interaction framework that supports versatile\n  interactions through language commands.Project URL:\n  https://xizaoqu.github.io/unihsi/ . Code:\n  https://github.com/OpenRobotLab/UniHSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02937v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02937v5",
                "updated": "2024-09-03T11:32:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    11,
                    32,
                    50,
                    1,
                    247,
                    0
                ],
                "published": "2024-04-03T07:14:15Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    14,
                    15,
                    2,
                    94,
                    0
                ],
                "title": "Towards Explainable Traffic Flow Prediction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Traffic Flow Prediction with Large Language Models"
                },
                "summary": "Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows."
                },
                "authors": [
                    {
                        "name": "Xusen Guo"
                    },
                    {
                        "name": "Qiming Zhang"
                    },
                    {
                        "name": "Junyue Jiang"
                    },
                    {
                        "name": "Mingxing Peng"
                    },
                    {
                        "name": "Meixin Zhu"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang"
                },
                "arxiv_affiliation": "Frank",
                "author": "Yang",
                "arxiv_comment": "31pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02937v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02937v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02031v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02031v8",
                "updated": "2024-09-03T10:19:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    19,
                    52,
                    1,
                    247,
                    0
                ],
                "published": "2023-10-03T13:17:35Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    13,
                    17,
                    35,
                    1,
                    276,
                    0
                ],
                "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OceanGPT: A Large Language Model for Ocean Science Tasks"
                },
                "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology."
                },
                "authors": [
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yida Xue"
                    },
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Daxiong Ji"
                    },
                    {
                        "name": "Guozhou Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "ACL2024. Project Website: http://oceangpt.zjukg.cn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02031v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02031v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02893v2",
                "updated": "2024-09-03T09:47:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    47,
                    30,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-05T11:01:57Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    11,
                    1,
                    57,
                    0,
                    36,
                    0
                ],
                "title": "On the Performance of RIS-Aided Spatial Modulation for Downlink\n  Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Performance of RIS-Aided Spatial Modulation for Downlink\n  Transmission"
                },
                "summary": "In this study, we explore the performance of a reconfigurable reflecting\nsurface (RIS)-assisted transmit spatial modulation (SM) system for downlink\ntransmission, wherein the deployment of RIS serves the purpose of blind area\ncoverage within the channel. At the receiving end, we present three detectors,\ni.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy\ndetector to recover the transmitted signal. By utilizing the ML detector, we\ninitially derive the conditional pair error probability expression for the\nproposed scheme. Subsequently, we leverage the central limit theorem (CLT) to\nobtain the probability density function of the combined channel. Following\nthis, the Gaussian-Chebyshev quadrature method is applied to derive a\nclosed-form expression for the unconditional pair error probability and\nestablish the union tight upper bound for the average bit error probability\n(ABEP). Furthermore, we derive a closed-form expression for the ergodic\ncapacity of the proposed RIS-SM scheme. Monte Carlo simulations are conducted\nnot only to assess the complexity and reliability of the three detection\nalgorithms but also to validate the results obtained through theoretical\nderivation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore the performance of a reconfigurable reflecting\nsurface (RIS)-assisted transmit spatial modulation (SM) system for downlink\ntransmission, wherein the deployment of RIS serves the purpose of blind area\ncoverage within the channel. At the receiving end, we present three detectors,\ni.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy\ndetector to recover the transmitted signal. By utilizing the ML detector, we\ninitially derive the conditional pair error probability expression for the\nproposed scheme. Subsequently, we leverage the central limit theorem (CLT) to\nobtain the probability density function of the combined channel. Following\nthis, the Gaussian-Chebyshev quadrature method is applied to derive a\nclosed-form expression for the unconditional pair error probability and\nestablish the union tight upper bound for the average bit error probability\n(ABEP). Furthermore, we derive a closed-form expression for the ergodic\ncapacity of the proposed RIS-SM scheme. Monte Carlo simulations are conducted\nnot only to assess the complexity and reliability of the three detection\nalgorithms but also to validate the results obtained through theoretical\nderivation results."
                },
                "authors": [
                    {
                        "name": "Xusheng Zhu"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wen Chen"
                },
                "author": "Wen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19047v2",
                "updated": "2024-09-03T09:25:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    25,
                    46,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-29T12:44:41Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    44,
                    41,
                    2,
                    150,
                    0
                ],
                "title": "Statistical Context Detection for Deep Lifelong Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Context Detection for Deep Lifelong Reinforcement Learning"
                },
                "summary": "Context detection involves labeling segments of an online stream of data as\nbelonging to different tasks. Task labels are used in lifelong learning\nalgorithms to perform consolidation or other procedures that prevent\ncatastrophic forgetting. Inferring task labels from online experiences remains\na challenging problem. Most approaches assume finite and low-dimension\nobservation spaces or a preliminary training phase during which task labels are\nlearned. Moreover, changes in the transition or reward functions can be\ndetected only in combination with a policy, and therefore are more difficult to\ndetect than changes in the input distribution. This paper presents an approach\nto learning both policies and labels in an online deep reinforcement learning\nsetting. The key idea is to use distance metrics, obtained via optimal\ntransport methods, i.e., Wasserstein distance, on suitable latent action-reward\nspaces to measure distances between sets of data points from past and current\nstreams. Such distances can then be used for statistical tests based on an\nadapted Kolmogorov-Smirnov calculation to assign labels to sequences of\nexperiences. A rollback procedure is introduced to learn multiple policies by\nensuring that only the appropriate data is used to train the corresponding\npolicy. The combination of task detection and policy deployment allows for the\noptimization of lifelong reinforcement learning agents without an oracle that\nprovides task labels. The approach is tested using two benchmarks and the\nresults show promising performance when compared with related context detection\nalgorithms. The results suggest that optimal transport statistical methods\nprovide an explainable and justifiable procedure for online context detection\nand reward optimization in lifelong reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context detection involves labeling segments of an online stream of data as\nbelonging to different tasks. Task labels are used in lifelong learning\nalgorithms to perform consolidation or other procedures that prevent\ncatastrophic forgetting. Inferring task labels from online experiences remains\na challenging problem. Most approaches assume finite and low-dimension\nobservation spaces or a preliminary training phase during which task labels are\nlearned. Moreover, changes in the transition or reward functions can be\ndetected only in combination with a policy, and therefore are more difficult to\ndetect than changes in the input distribution. This paper presents an approach\nto learning both policies and labels in an online deep reinforcement learning\nsetting. The key idea is to use distance metrics, obtained via optimal\ntransport methods, i.e., Wasserstein distance, on suitable latent action-reward\nspaces to measure distances between sets of data points from past and current\nstreams. Such distances can then be used for statistical tests based on an\nadapted Kolmogorov-Smirnov calculation to assign labels to sequences of\nexperiences. A rollback procedure is introduced to learn multiple policies by\nensuring that only the appropriate data is used to train the corresponding\npolicy. The combination of task detection and policy deployment allows for the\noptimization of lifelong reinforcement learning agents without an oracle that\nprovides task labels. The approach is tested using two benchmarks and the\nresults show promising performance when compared with related context detection\nalgorithms. The results suggest that optimal transport statistical methods\nprovide an explainable and justifiable procedure for online context detection\nand reward optimization in lifelong reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Jeffery Dick"
                    },
                    {
                        "name": "Saptarshi Nath"
                    },
                    {
                        "name": "Christos Peridis"
                    },
                    {
                        "name": "Eseoghene Benjamin"
                    },
                    {
                        "name": "Soheil Kolouri"
                    },
                    {
                        "name": "Andrea Soltoggio"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Soltoggio"
                },
                "author": "Andrea Soltoggio",
                "arxiv_comment": "10 pages excluding references and bibliography. Accepted at CoLLAs\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01306v3",
                "updated": "2024-09-03T07:41:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    41,
                    51,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-02T10:53:36Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    10,
                    53,
                    36,
                    4,
                    33,
                    0
                ],
                "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTO: Model Alignment as Prospect Theoretic Optimization"
                },
                "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration."
                },
                "authors": [
                    {
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Douwe Kiela"
                    }
                ],
                "author_detail": {
                    "name": "Douwe Kiela"
                },
                "author": "Douwe Kiela",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13740v3",
                "updated": "2024-09-03T07:34:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    34,
                    49,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-25T07:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    1,
                    37,
                    6,
                    238,
                    0
                ],
                "title": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots"
                },
                "summary": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains."
                },
                "authors": [
                    {
                        "name": "Shixin Luo"
                    },
                    {
                        "name": "Songbo Li"
                    },
                    {
                        "name": "Ruiqi Yu"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Qiuguo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuguo Zhu"
                },
                "author": "Qiuguo Zhu",
                "arxiv_comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01252v3",
                "updated": "2024-09-03T07:07:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    7,
                    59,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-03T12:10:26Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    12,
                    10,
                    26,
                    0,
                    155,
                    0
                ],
                "title": "Towards Scalable Automated Alignment of LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Automated Alignment of LLMs: A Survey"
                },
                "summary": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment."
                },
                "authors": [
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Xinyu Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Mengjie Ren"
                    },
                    {
                        "name": "Hao Xiang"
                    },
                    {
                        "name": "Peilin Liu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Yu"
                },
                "author": "Bowen Yu",
                "arxiv_comment": "Paper List: https://github.com/cascip/awesome-auto-alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01135v2",
                "updated": "2024-09-03T06:51:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    6,
                    51,
                    56,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-03T09:26:53Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    26,
                    53,
                    0,
                    155,
                    0
                ],
                "title": "The Danger Within: Insider Threat Modeling Using Business Process Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Danger Within: Insider Threat Modeling Using Business Process Models"
                },
                "summary": "Threat modeling has been successfully applied to model technical threats\nwithin information systems. However, a lack of methods focusing on\nnon-technical assets and their representation can be observed in theory and\npractice. Following the voices of industry practitioners, this paper explored\nhow to model insider threats based on business process models. Hence, this\nstudy developed a novel insider threat knowledge base and a threat modeling\napplication that leverages Business Process Modeling and Notation (BPMN).\nFinally, to understand how well the theoretic knowledge and its prototype\ntranslate into practice, the study conducted a real-world case study of an IT\nprovider's business process and an experimental deployment for a real voting\nprocess. The results indicate that even without annotation, BPMN diagrams can\nbe leveraged to automatically identify insider threats in an organization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threat modeling has been successfully applied to model technical threats\nwithin information systems. However, a lack of methods focusing on\nnon-technical assets and their representation can be observed in theory and\npractice. Following the voices of industry practitioners, this paper explored\nhow to model insider threats based on business process models. Hence, this\nstudy developed a novel insider threat knowledge base and a threat modeling\napplication that leverages Business Process Modeling and Notation (BPMN).\nFinally, to understand how well the theoretic knowledge and its prototype\ntranslate into practice, the study conducted a real-world case study of an IT\nprovider's business process and an experimental deployment for a real voting\nprocess. The results indicate that even without annotation, BPMN diagrams can\nbe leveraged to automatically identify insider threats in an organization."
                },
                "authors": [
                    {
                        "name": "Jan von der Assen"
                    },
                    {
                        "name": "Jasmin Hochuli"
                    },
                    {
                        "name": "Thomas Gr√ºbl"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11336v2",
                "updated": "2024-09-03T06:36:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    6,
                    36,
                    24,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-17T08:54:23Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    8,
                    54,
                    23,
                    0,
                    169,
                    0
                ],
                "title": "A General Framework for Load Forecasting based on Pre-trained Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Load Forecasting based on Pre-trained Large\n  Language Model"
                },
                "summary": "Accurate load forecasting is crucial for maintaining the power balance\nbetween generators and consumers,particularly with the increasing integration\nof renewable energy sources, which introduce significant intermittent\nvolatility. With the advancement of data-driven methods, machine learning and\ndeep learning models have become the predominant approaches for load\nforecasting tasks. In recent years, pre-trained large language models (LLMs)\nhave achieved significant progress, demonstrating superior performance across\nvarious fields. This paper proposes a load forecasting method based on LLMs,\noffering not only precise predictive capabilities but also broad and flexible\napplicability. Additionally, a data modeling method is introduced to\neffectively transform load sequence data into natural language suitable for LLM\ntraining. Furthermore, a data enhancement strategy is designed to mitigate the\nimpact of LLM hallucinations on forecasting results. The effectiveness of the\nproposed method is validated using two real-world datasets. Compared to\nexisting methods, our approach demonstrates state-of-the-art performance across\nall validation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate load forecasting is crucial for maintaining the power balance\nbetween generators and consumers,particularly with the increasing integration\nof renewable energy sources, which introduce significant intermittent\nvolatility. With the advancement of data-driven methods, machine learning and\ndeep learning models have become the predominant approaches for load\nforecasting tasks. In recent years, pre-trained large language models (LLMs)\nhave achieved significant progress, demonstrating superior performance across\nvarious fields. This paper proposes a load forecasting method based on LLMs,\noffering not only precise predictive capabilities but also broad and flexible\napplicability. Additionally, a data modeling method is introduced to\neffectively transform load sequence data into natural language suitable for LLM\ntraining. Furthermore, a data enhancement strategy is designed to mitigate the\nimpact of LLM hallucinations on forecasting results. The effectiveness of the\nproposed method is validated using two real-world datasets. Compared to\nexisting methods, our approach demonstrates state-of-the-art performance across\nall validation metrics."
                },
                "authors": [
                    {
                        "name": "Mingyang Gao"
                    },
                    {
                        "name": "Suyang Zhou"
                    },
                    {
                        "name": "Wei Gu"
                    },
                    {
                        "name": "Zhi Wu"
                    },
                    {
                        "name": "Haiquan Liu"
                    },
                    {
                        "name": "Aihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aihua Zhou"
                },
                "author": "Aihua Zhou",
                "arxiv_comment": "11 pages, 3 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11169v4",
                "updated": "2024-09-03T05:51:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    5,
                    51,
                    40,
                    1,
                    247,
                    0
                ],
                "published": "2024-03-17T10:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    10,
                    59,
                    9,
                    6,
                    77,
                    0
                ],
                "title": "Correcting misinformation on social media with a large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting misinformation on social media with a large language model"
                },
                "summary": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale."
                },
                "authors": [
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Ashish Sharma"
                    },
                    {
                        "name": "Amy X. Zhang"
                    },
                    {
                        "name": "Tim Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Tim Althoff"
                },
                "author": "Tim Althoff",
                "arxiv_comment": "50 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01481v2",
                "updated": "2024-09-03T05:47:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    5,
                    47,
                    42,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-02T17:13:40Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    17,
                    13,
                    40,
                    3,
                    123,
                    0
                ],
                "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to a thousand GPUs for training the largest open-source LLMs such as\nNemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized\nand scalable implementations for major paradigms of model alignment such as:\nReinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to a thousand GPUs for training the largest open-source LLMs such as\nNemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized\nand scalable implementations for major paradigms of model alignment such as:\nReinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner"
                },
                "authors": [
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Shengyang Sun"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Markel Sanz Ausin"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "arxiv_comment": "16 pages, 4 figures, Accepted to COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09600v2",
                "updated": "2024-09-03T03:45:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    45,
                    21,
                    1,
                    247,
                    0
                ],
                "published": "2024-08-18T21:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning"
                },
                "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}"
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Gautam Bhattacharya"
                    },
                    {
                        "name": "Pratik Joshi"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00648v2",
                "updated": "2024-09-03T03:40:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    40,
                    8,
                    1,
                    247,
                    0
                ],
                "published": "2024-05-01T17:24:42Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    17,
                    24,
                    42,
                    2,
                    122,
                    0
                ],
                "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination\n  Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination\n  Detection in Large Language Models"
                },
                "summary": "Large language models (LLMs) have transformed the landscape of language\nprocessing, yet struggle with significant challenges in terms of security,\nprivacy, and the generation of seemingly coherent but factually inaccurate\noutputs, commonly referred to as hallucinations. Among these challenges, one\nparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs\ngenerate content that directly contradicts established facts. Tackling FCH\nposes a formidable task due to two primary obstacles: Firstly, automating the\nconstruction and updating of benchmark datasets is challenging, as current\nmethods rely on static benchmarks that don't cover the diverse range of FCH\nscenarios. Secondly, validating LLM outputs' reasoning process is inherently\ncomplex, especially with intricate logical relations involved.\n  In addressing these obstacles, we propose an innovative approach leveraging\nlogic programming to enhance metamorphic testing for detecting Fact-Conflicting\nHallucinations (FCH). Our method gathers data from sources like Wikipedia,\nexpands it with logical reasoning to create diverse test cases, assesses LLMs\nthrough structured prompts, and validates their coherence using semantic-aware\nassessment mechanisms. Our method generates test cases and detects\nhallucinations across six different LLMs spanning nine domains, revealing\nhallucination rates ranging from 24.7% to 59.8%. Key observations indicate that\nLLMs encounter challenges, particularly with temporal concepts, handling\nout-of-distribution knowledge, and exhibiting deficiencies in logical reasoning\ncapabilities. The outcomes underscore the efficacy of logic-based test cases\ngenerated by our tool in both triggering and identifying hallucinations. These\nfindings underscore the imperative for ongoing collaborative endeavors within\nthe community to detect and address LLM hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed the landscape of language\nprocessing, yet struggle with significant challenges in terms of security,\nprivacy, and the generation of seemingly coherent but factually inaccurate\noutputs, commonly referred to as hallucinations. Among these challenges, one\nparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMs\ngenerate content that directly contradicts established facts. Tackling FCH\nposes a formidable task due to two primary obstacles: Firstly, automating the\nconstruction and updating of benchmark datasets is challenging, as current\nmethods rely on static benchmarks that don't cover the diverse range of FCH\nscenarios. Secondly, validating LLM outputs' reasoning process is inherently\ncomplex, especially with intricate logical relations involved.\n  In addressing these obstacles, we propose an innovative approach leveraging\nlogic programming to enhance metamorphic testing for detecting Fact-Conflicting\nHallucinations (FCH). Our method gathers data from sources like Wikipedia,\nexpands it with logical reasoning to create diverse test cases, assesses LLMs\nthrough structured prompts, and validates their coherence using semantic-aware\nassessment mechanisms. Our method generates test cases and detects\nhallucinations across six different LLMs spanning nine domains, revealing\nhallucination rates ranging from 24.7% to 59.8%. Key observations indicate that\nLLMs encounter challenges, particularly with temporal concepts, handling\nout-of-distribution knowledge, and exhibiting deficiencies in logical reasoning\ncapabilities. The outcomes underscore the efficacy of logic-based test cases\ngenerated by our tool in both triggering and identifying hallucinations. These\nfindings underscore the imperative for ongoing collaborative endeavors within\nthe community to detect and address LLM hallucinations."
                },
                "authors": [
                    {
                        "name": "Ningke Li"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "29 pages, 11 figures, 4 tables, to appear in OOPSLA'24 (Vol.8,\n  No.OOPSLA2, Article 336)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10802v3",
                "updated": "2024-09-03T02:37:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    2,
                    37,
                    48,
                    1,
                    247,
                    0
                ],
                "published": "2024-02-16T16:25:20Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    16,
                    25,
                    20,
                    4,
                    47,
                    0
                ],
                "title": "TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly\n  Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly\n  Detection Models"
                },
                "summary": "Time series anomaly detection (TSAD) has gained significant attention due to\nits real-world applications to improve the stability of modern software\nsystems. However, there is no effective way to verify whether they can meet the\nrequirements for real-world deployment. Firstly, current algorithms typically\ntrain a specific model for each time series. Maintaining such many models is\nimpractical in a large-scale system with tens of thousands of curves. The\nperformance of using merely one unified model to detect anomalies remains\nunknown. Secondly, most TSAD models are trained on the historical part of a\ntime series and are tested on its future segment. In distributed systems,\nhowever, there are frequent system deployments and upgrades, with new,\npreviously unseen time series emerging daily. The performance of testing newly\nincoming unseen time series on current TSAD algorithms remains unknown. Lastly,\nthe assumptions of the evaluation metrics in existing benchmarks are far from\npractical demands. To solve the above-mentioned problems, we propose an\nindustrial-grade benchmark TimeSeriesBench. We assess the performance of\nexisting algorithms across more than 168 evaluation settings and provide\ncomprehensive analysis for the future design of anomaly detection algorithms.\nAn industrial dataset is also released along with TimeSeriesBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection (TSAD) has gained significant attention due to\nits real-world applications to improve the stability of modern software\nsystems. However, there is no effective way to verify whether they can meet the\nrequirements for real-world deployment. Firstly, current algorithms typically\ntrain a specific model for each time series. Maintaining such many models is\nimpractical in a large-scale system with tens of thousands of curves. The\nperformance of using merely one unified model to detect anomalies remains\nunknown. Secondly, most TSAD models are trained on the historical part of a\ntime series and are tested on its future segment. In distributed systems,\nhowever, there are frequent system deployments and upgrades, with new,\npreviously unseen time series emerging daily. The performance of testing newly\nincoming unseen time series on current TSAD algorithms remains unknown. Lastly,\nthe assumptions of the evaluation metrics in existing benchmarks are far from\npractical demands. To solve the above-mentioned problems, we propose an\nindustrial-grade benchmark TimeSeriesBench. We assess the performance of\nexisting algorithms across more than 168 evaluation settings and provide\ncomprehensive analysis for the future design of anomaly detection algorithms.\nAn industrial dataset is also released along with TimeSeriesBench."
                },
                "authors": [
                    {
                        "name": "Haotian Si"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Hang Cui"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Jingjing Li"
                    },
                    {
                        "name": "Haiming Zhang"
                    },
                    {
                        "name": "Jing Han"
                    },
                    {
                        "name": "Dan Pei"
                    },
                    {
                        "name": "Gaogang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Gaogang Xie"
                },
                "author": "Gaogang Xie",
                "arxiv_comment": "Accepted by ISSRE'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12620v3",
                "updated": "2024-09-03T02:33:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    2,
                    33,
                    42,
                    1,
                    247,
                    0
                ],
                "published": "2024-03-19T10:40:18Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    10,
                    40,
                    18,
                    1,
                    79,
                    0
                ],
                "title": "Near-Field Channel Estimation in Dual-Band XL-MIMO with Side\n  Information-Assisted Compressed Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Field Channel Estimation in Dual-Band XL-MIMO with Side\n  Information-Assisted Compressed Sensing"
                },
                "summary": "Near-field communication comes to be an indispensable part of the future\nsixth generation (6G) communications at the arrival of the forth-coming\ndeployment of extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. Due to the huge array aperture and high-frequency bands, the\nelectromagnetic radiation field is modeled by the spherical waves instead of\nthe conventional planar waves, leading to severe weak sparsity to\nangular-domain near-field channel. Therefore, the channel estimation\nreminiscent of the conventional compression sensing (CS) approaches in the\nangular domain, judiciously utilized for low pilot overhead, may result in\nunprecedented challenges. To this end, this paper proposes a brand-new\nnear-field channel estimation scheme by exploiting the naturally occurring\nuseful side information. Specifically, we formulate the dual-band near-field\ncommunication model based on the fact that high-frequency systems are likely to\nbe deployed with lower-frequency systems. Representative side information,\ni.e., the structural characteristic information derived by the sparsity\nambiguity and the out-of-band spatial information stemming from the\nlower-frequency channel, is explored and tailored to materialize exceptional\nnear-field channel estimation. Furthermore, in-depth theoretical analyses are\ndeveloped to guarantee the minimum estimation error, based on which a suite of\nalgorithms leveraging the elaborating side information are proposed. Numerical\nsimulations demonstrate that the designed algorithms provide more assured\nresults than the off-the-shelf approaches in the context of the dual-band\nnear-field communications in both on- and off-grid scenarios, where the angle\nof departures/arrivals are discretely or continuously distributed,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-field communication comes to be an indispensable part of the future\nsixth generation (6G) communications at the arrival of the forth-coming\ndeployment of extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. Due to the huge array aperture and high-frequency bands, the\nelectromagnetic radiation field is modeled by the spherical waves instead of\nthe conventional planar waves, leading to severe weak sparsity to\nangular-domain near-field channel. Therefore, the channel estimation\nreminiscent of the conventional compression sensing (CS) approaches in the\nangular domain, judiciously utilized for low pilot overhead, may result in\nunprecedented challenges. To this end, this paper proposes a brand-new\nnear-field channel estimation scheme by exploiting the naturally occurring\nuseful side information. Specifically, we formulate the dual-band near-field\ncommunication model based on the fact that high-frequency systems are likely to\nbe deployed with lower-frequency systems. Representative side information,\ni.e., the structural characteristic information derived by the sparsity\nambiguity and the out-of-band spatial information stemming from the\nlower-frequency channel, is explored and tailored to materialize exceptional\nnear-field channel estimation. Furthermore, in-depth theoretical analyses are\ndeveloped to guarantee the minimum estimation error, based on which a suite of\nalgorithms leveraging the elaborating side information are proposed. Numerical\nsimulations demonstrate that the designed algorithms provide more assured\nresults than the off-the-shelf approaches in the context of the dual-band\nnear-field communications in both on- and off-grid scenarios, where the angle\nof departures/arrivals are discretely or continuously distributed,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Haochen Wu"
                    },
                    {
                        "name": "Liyang Lu"
                    },
                    {
                        "name": "Zhaocheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaocheng Wang"
                },
                "author": "Zhaocheng Wang",
                "arxiv_comment": "This paper has been accepted by IEEE Transactions on Communications\n  (IEEE TCOM). doi: 10.1109/TCOMM.2024.3445282",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06576v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06576v4",
                "updated": "2024-09-03T02:11:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    2,
                    11,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-06-04T04:17:40Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    4,
                    17,
                    40,
                    1,
                    156,
                    0
                ],
                "title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step"
                },
                "summary": "Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. Language model systems often enable LLMs to generate\ncode for arithmetic operations to achieve accurate calculations. However, this\napproach compromises speed and security, and fine-tuning risks the language\nmodel losing prior capabilities. We propose a framework that enables exact\narithmetic in a single autoregressive step, providing faster, more secure, and\nmore interpretable LLM systems with arithmetic capabilities. We use the hidden\nstates of a LLM to control a symbolic architecture that performs arithmetic.\nOur implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama)\nachieves 100\\% accuracy on single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nwith and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o\nwith and without a code interpreter on average across a range of mathematical\nproblem solving benchmarks, demonstrating that OccamLLMs can excel in\narithmetic tasks, even surpassing much larger models. We will make our code\npublic shortly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. Language model systems often enable LLMs to generate\ncode for arithmetic operations to achieve accurate calculations. However, this\napproach compromises speed and security, and fine-tuning risks the language\nmodel losing prior capabilities. We propose a framework that enables exact\narithmetic in a single autoregressive step, providing faster, more secure, and\nmore interpretable LLM systems with arithmetic capabilities. We use the hidden\nstates of a LLM to control a symbolic architecture that performs arithmetic.\nOur implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama)\nachieves 100\\% accuracy on single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nwith and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o\nwith and without a code interpreter on average across a range of mathematical\nproblem solving benchmarks, demonstrating that OccamLLMs can excel in\narithmetic tasks, even surpassing much larger models. We will make our code\npublic shortly."
                },
                "authors": [
                    {
                        "name": "Owen Dugan"
                    },
                    {
                        "name": "Donato Manuel Jimenez Beneto"
                    },
                    {
                        "name": "Charlotte Loh"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Marin Soljaƒçiƒá"
                    }
                ],
                "author_detail": {
                    "name": "Marin Soljaƒçiƒá"
                },
                "author": "Marin Soljaƒçiƒá",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06576v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06576v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v3",
                "updated": "2024-09-03T01:14:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    1,
                    14,
                    30,
                    1,
                    247,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates decision-making capabilities of LLMs through the lens of\nGame Theory. We focus specifically on games that support the simultaneous\nparticipation of more than two agents. We introduce GAMA($\\gamma$)-Bench, which\nevaluates LLMs' Gaming Ability in Multi-Agent environments. $\\gamma$-Bench\nincludes eight classical multi-agent games and a scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. Leveraging $\\gamma$-Bench,\nwe investigate LLMs' robustness, generalizability, and strategies for\nenhancement. Results reveal that while GPT-3.5 shows satisfying robustness, its\ngeneralizability is relatively limited. However, its performance can be\nimproved through approaches such as Chain-of-Thought. Additionally, we evaluate\ntwelve versions from six models, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1,\nMixtral, and Qwen-2. We find that Gemini-1.5-Pro outperforms other models with\na score of $63.8$ out of $100$, followed by LLaMA-3.1-70B and GPT-4 with scores\nof $60.9$ and $60.5$, respectively. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates decision-making capabilities of LLMs through the lens of\nGame Theory. We focus specifically on games that support the simultaneous\nparticipation of more than two agents. We introduce GAMA($\\gamma$)-Bench, which\nevaluates LLMs' Gaming Ability in Multi-Agent environments. $\\gamma$-Bench\nincludes eight classical multi-agent games and a scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. Leveraging $\\gamma$-Bench,\nwe investigate LLMs' robustness, generalizability, and strategies for\nenhancement. Results reveal that while GPT-3.5 shows satisfying robustness, its\ngeneralizability is relatively limited. However, its performance can be\nimproved through approaches such as Chain-of-Thought. Additionally, we evaluate\ntwelve versions from six models, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1,\nMixtral, and Qwen-2. We find that Gemini-1.5-Pro outperforms other models with\na score of $63.8$ out of $100$, followed by LLaMA-3.1-70B and GPT-4 with scores\nof $60.9$ and $60.5$, respectively. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "11 pages of main text. 20 pages of appendices. 12 figures, 9 tables.\n  Added models: Gemini-1.5-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15092v2",
                "updated": "2024-09-02T22:40:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    22,
                    40,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-23T22:38:58Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    22,
                    38,
                    58,
                    3,
                    144,
                    0
                ],
                "title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissociation of Faithful and Unfaithful Reasoning in LLMs"
                },
                "summary": "Large language models (LLMs) often improve their performance in downstream\ntasks when they generate Chain of Thought reasoning text before producing an\nanswer. We investigate how LLMs recover from errors in Chain of Thought.\nThrough analysis of error recovery behaviors, we find evidence for\nunfaithfulness in Chain of Thought, which occurs when models arrive at the\ncorrect answer despite invalid reasoning text. We identify factors that shift\nLLM recovery behavior: LLMs recover more frequently from obvious errors and in\ncontexts that provide more evidence for the correct answer. Critically, these\nfactors have divergent effects on faithful and unfaithful recoveries. Our\nresults indicate that there are distinct mechanisms driving faithful and\nunfaithful error recoveries. Selective targeting of these mechanisms may be\nable to drive down the rate of unfaithful reasoning and improve model\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often improve their performance in downstream\ntasks when they generate Chain of Thought reasoning text before producing an\nanswer. We investigate how LLMs recover from errors in Chain of Thought.\nThrough analysis of error recovery behaviors, we find evidence for\nunfaithfulness in Chain of Thought, which occurs when models arrive at the\ncorrect answer despite invalid reasoning text. We identify factors that shift\nLLM recovery behavior: LLMs recover more frequently from obvious errors and in\ncontexts that provide more evidence for the correct answer. Critically, these\nfactors have divergent effects on faithful and unfaithful recoveries. Our\nresults indicate that there are distinct mechanisms driving faithful and\nunfaithful error recoveries. Selective targeting of these mechanisms may be\nable to drive down the rate of unfaithful reasoning and improve model\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Evelyn Yee"
                    },
                    {
                        "name": "Alice Li"
                    },
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Yeon Ho Jung"
                    },
                    {
                        "name": "Ramamohan Paturi"
                    },
                    {
                        "name": "Leon Bergen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Bergen"
                },
                "author": "Leon Bergen",
                "arxiv_comment": "code published at\n  https://github.com/CoTErrorRecovery/CoTErrorRecovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07981v2",
                "updated": "2024-09-02T21:29:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    21,
                    29,
                    4,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-11T17:57:32Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    32,
                    3,
                    102,
                    0
                ],
                "title": "Manipulating Large Language Models to Increase Product Visibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulating Large Language Models to Increase Product Visibility"
                },
                "summary": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer."
                },
                "authors": [
                    {
                        "name": "Aounon Kumar"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    }
                ],
                "author_detail": {
                    "name": "Himabindu Lakkaraju"
                },
                "author": "Himabindu Lakkaraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17095v2",
                "updated": "2024-09-02T20:33:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    33,
                    49,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-30T08:26:55Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    8,
                    26,
                    55,
                    4,
                    243,
                    0
                ],
                "title": "RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation\n  and Retrieval-Guidance"
                },
                "summary": "Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models demonstrate impressive generation capabilities.\nHowever, they also have a massive number of parameters, resulting in enormous\nmodel sizes, thus making them unsuitable for deployment on resource-constraint\ndevices. Block-wise generation can be a promising alternative for designing\ncompact-sized (parameter-efficient) deep generative models since the model can\ngenerate one block at a time instead of generating the whole image at once.\nHowever, block-wise generation is also considerably challenging because\nensuring coherence across generated blocks can be non-trivial. To this end, we\ndesign a retrieval-augmented generation (RAG) approach and leverage the\ncorresponding blocks of the images retrieved by the RAG module to condition the\ntraining and generation stages of a block-wise denoising diffusion model. Our\nconditioning schemes ensure coherence across the different blocks during\ntraining and, consequently, during generation. While we showcase our approach\nusing the latent diffusion model (LDM) as the base model, it can be used with\nother variants of denoising diffusion models. We validate the solution of the\ncoherence problem through the proposed approach by reporting substantive\nexperiments to demonstrate our approach's effectiveness in compact model size\nand excellent generation quality."
                },
                "authors": [
                    {
                        "name": "Avideep Mukherjee"
                    },
                    {
                        "name": "Soumya Banerjee"
                    },
                    {
                        "name": "Piyush Rai"
                    },
                    {
                        "name": "Vinay P. Namboodiri"
                    }
                ],
                "author_detail": {
                    "name": "Vinay P. Namboodiri"
                },
                "author": "Vinay P. Namboodiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v2",
                "updated": "2024-09-02T20:26:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    26,
                    30,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Hanyang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Hanyang Zhong"
                },
                "author": "Hanyang Zhong",
                "arxiv_comment": "This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11927v3",
                "updated": "2024-09-02T20:26:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    26,
                    26,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-17T10:45:22Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    10,
                    45,
                    22,
                    0,
                    169,
                    0
                ],
                "title": "On the Impacts of Contexts on Repository-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impacts of Contexts on Repository-Level Code Generation"
                },
                "summary": "CodeLLMs have gained widespread adoption for code generation tasks, yet their\ncapacity to handle repository-level code generation with complex contextual\ndependencies remains underexplored. Our work underscores the critical\nimportance of leveraging repository-level contexts to generate executable and\nfunctionally correct code. We present \\textbf{\\methodnamews}, a novel benchmark\ndesigned to evaluate repository-level code generation, with a focus on three\nkey aspects: executability, functional correctness through comprehensive test\ncase generation, and accurate utilization of cross-file contexts. Our study\nexamines a controlled scenario where developers specify essential code\ndependencies (contexts), challenging models to integrate them effectively.\nAdditionally, we introduce an instruction-tuned dataset that enhances CodeLLMs'\nability to leverage dependencies, along with a new metric, \\textit{Dependency\nInvocation Rate (DIR)}, to quantify context utilization. Experimental results\nreveal that while pretrained LLMs demonstrate superior performance in terms of\ncorrectness, instruction-tuned models excel in context utilization and\ndebugging capabilities. \\methodnamews offers a comprehensive evaluation\nframework for assessing code functionality and alignment with developer intent,\nthereby advancing the development of more reliable CodeLLMs for real-world\napplications. The dataset and source code are available\nat~\\url{https://github.com/FSoft-AI4Code/RepoExec}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeLLMs have gained widespread adoption for code generation tasks, yet their\ncapacity to handle repository-level code generation with complex contextual\ndependencies remains underexplored. Our work underscores the critical\nimportance of leveraging repository-level contexts to generate executable and\nfunctionally correct code. We present \\textbf{\\methodnamews}, a novel benchmark\ndesigned to evaluate repository-level code generation, with a focus on three\nkey aspects: executability, functional correctness through comprehensive test\ncase generation, and accurate utilization of cross-file contexts. Our study\nexamines a controlled scenario where developers specify essential code\ndependencies (contexts), challenging models to integrate them effectively.\nAdditionally, we introduce an instruction-tuned dataset that enhances CodeLLMs'\nability to leverage dependencies, along with a new metric, \\textit{Dependency\nInvocation Rate (DIR)}, to quantify context utilization. Experimental results\nreveal that while pretrained LLMs demonstrate superior performance in terms of\ncorrectness, instruction-tuned models excel in context utilization and\ndebugging capabilities. \\methodnamews offers a comprehensive evaluation\nframework for assessing code functionality and alignment with developer intent,\nthereby advancing the development of more reliable CodeLLMs for real-world\napplications. The dataset and source code are available\nat~\\url{https://github.com/FSoft-AI4Code/RepoExec}."
                },
                "authors": [
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Dung Manh Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15077v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15077v4",
                "updated": "2024-09-02T20:25:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    20,
                    25,
                    36,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-23T21:56:12Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    21,
                    56,
                    12,
                    3,
                    144,
                    0
                ],
                "title": "Eliciting Informative Text Evaluations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Informative Text Evaluations with Large Language Models"
                },
                "summary": "Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Shengwei Xu"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuqing Kong"
                    },
                    {
                        "name": "Grant Schoenebeck"
                    }
                ],
                "author_detail": {
                    "name": "Grant Schoenebeck"
                },
                "author": "Grant Schoenebeck",
                "arxiv_comment": "Accepted by the Twenty-Fifth ACM Conference on Economics and\n  Computation (EC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15077v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15077v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08486v2",
                "updated": "2024-09-02T19:04:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    19,
                    4,
                    57,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-12T17:59:42Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    59,
                    42,
                    2,
                    164,
                    0
                ],
                "title": "On Evaluating Adversarial Robustness of Volumetric Medical Segmentation\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Evaluating Adversarial Robustness of Volumetric Medical Segmentation\n  Models"
                },
                "summary": "Volumetric medical segmentation models have achieved significant success on\norgan and tumor-based segmentation tasks in recent years. However, their\nvulnerability to adversarial attacks remains largely unexplored, raising\nserious concerns regarding the real-world deployment of tools employing such\nmodels in the healthcare sector. This underscores the importance of\ninvestigating the robustness of existing models. In this context, our work aims\nto empirically examine the adversarial robustness across current volumetric\nsegmentation architectures, encompassing Convolutional, Transformer, and\nMamba-based models. We extend this investigation across four volumetric\nsegmentation datasets, evaluating robustness under both white box and black box\nadversarial attacks. Overall, we observe that while both pixel and\nfrequency-based attacks perform reasonably well under \\emph{white box} setting,\nthe latter performs significantly better under transfer-based black box\nattacks. Across our experiments, we observe transformer-based models show\nhigher robustness than convolution-based models with Mamba-based models being\nthe most vulnerable. Additionally, we show that large-scale training of\nvolumetric segmentation models improves the model's robustness against\nadversarial attacks. The code and robust models are available at\nhttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetric medical segmentation models have achieved significant success on\norgan and tumor-based segmentation tasks in recent years. However, their\nvulnerability to adversarial attacks remains largely unexplored, raising\nserious concerns regarding the real-world deployment of tools employing such\nmodels in the healthcare sector. This underscores the importance of\ninvestigating the robustness of existing models. In this context, our work aims\nto empirically examine the adversarial robustness across current volumetric\nsegmentation architectures, encompassing Convolutional, Transformer, and\nMamba-based models. We extend this investigation across four volumetric\nsegmentation datasets, evaluating robustness under both white box and black box\nadversarial attacks. Overall, we observe that while both pixel and\nfrequency-based attacks perform reasonably well under \\emph{white box} setting,\nthe latter performs significantly better under transfer-based black box\nattacks. Across our experiments, we observe transformer-based models show\nhigher robustness than convolution-based models with Mamba-based models being\nthe most vulnerable. Additionally, we show that large-scale training of\nvolumetric segmentation models improves the model's robustness against\nadversarial attacks. The code and robust models are available at\nhttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models."
                },
                "authors": [
                    {
                        "name": "Hashmat Shadab Malik"
                    },
                    {
                        "name": "Numan Saeed"
                    },
                    {
                        "name": "Asif Hanif"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Accepted at British Machine Vision Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09147v3",
                "updated": "2024-09-02T19:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    19,
                    1,
                    44,
                    0,
                    246,
                    0
                ],
                "published": "2024-02-14T12:56:58Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    12,
                    56,
                    58,
                    2,
                    45,
                    0
                ],
                "title": "Into the Unknown: Self-Learning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Into the Unknown: Self-Learning Large Language Models"
                },
                "summary": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. We introduce a concept called Point in the Unknown\n(PiU) to identify atomic knowledge unknown to a model, along with four methods\nfor automatic PiUs identification, facilitating the creation of a self-learning\nloop that focuses exclusively on the absorption of currently unknown knowledge\ninto the model. Additionally, we developed evaluation metrics to gauge an LLM's\nself-learning capability. Our experiments revealed that LLMs with at least 3B\nparameters that have undergone some instruction training would be able to\nperform self-learning well. We further proved the effectiveness of\nself-learning by comparing the performance of a model that has undergone\nself-learning to a model that has not. Our self-learning concept allows more\nefficient LLM updates and opens new perspectives for LLM knowledge exchange.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the main problem of self-learning LLM: the question of what to\nlearn. We propose a self-learning LLM framework that enables an LLM to\nindependently learn previously unknown knowledge through self-assessment of\ntheir own hallucinations. We introduce a concept called Point in the Unknown\n(PiU) to identify atomic knowledge unknown to a model, along with four methods\nfor automatic PiUs identification, facilitating the creation of a self-learning\nloop that focuses exclusively on the absorption of currently unknown knowledge\ninto the model. Additionally, we developed evaluation metrics to gauge an LLM's\nself-learning capability. Our experiments revealed that LLMs with at least 3B\nparameters that have undergone some instruction training would be able to\nperform self-learning well. We further proved the effectiveness of\nself-learning by comparing the performance of a model that has undergone\nself-learning to a model that has not. Our self-learning concept allows more\nefficient LLM updates and opens new perspectives for LLM knowledge exchange."
                },
                "authors": [
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Jan Koco≈Ñ"
                    },
                    {
                        "name": "Przemys≈Çaw Kazienko"
                    }
                ],
                "author_detail": {
                    "name": "Przemys≈Çaw Kazienko"
                },
                "author": "Przemys≈Çaw Kazienko",
                "arxiv_comment": "10 pages, 3 figures, 3 tables, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21009v2",
                "updated": "2024-09-02T18:01:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    18,
                    1,
                    44,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-30T17:55:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    55,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "AI-Assisted Generation of Difficult Math Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Generation of Difficult Math Questions"
                },
                "summary": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills."
                },
                "authors": [
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17702v2",
                "updated": "2024-09-02T17:32:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    32,
                    10,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-27T23:18:12Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    23,
                    18,
                    12,
                    0,
                    148,
                    0
                ],
                "title": "A Two-sided Model for EV Market Dynamics and Policy Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-sided Model for EV Market Dynamics and Policy Implications"
                },
                "summary": "The diffusion of Electric Vehicles (EVs) plays a pivotal role in mitigating\ngreenhouse gas emissions, particularly in the U.S., where ambitious\nzero-emission and carbon neutrality objectives have been set. In pursuit of\nthese goals, many states have implemented a range of incentive policies aimed\nat stimulating EV adoption and charging infrastructure development, especially\npublic EV charging stations (EVCS). This study examines the indirect network\neffect observed between EV adoption and EVCS deployment within urban\nlandscapes. We developed a two-sided log-log regression model with historical\ndata on EV purchases and EVCS development to quantify this effect. To test the\nrobustness, we then conducted a case study of the EV market in Los Angeles (LA)\nCounty, which suggests that a 1% increase in EVCS correlates with a 0.35%\nincrease in EV sales. Additionally, we forecasted the future EV market dynamics\nin LA County, revealing a notable disparity between current policies and the\ntargeted 80% EV market share for private cars by 2045. To bridge this gap, we\nproposed a combined policy recommendation that enhances EV incentives by 60%\nand EVCS rebates by 66%, facilitating the achievement of future EV market\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffusion of Electric Vehicles (EVs) plays a pivotal role in mitigating\ngreenhouse gas emissions, particularly in the U.S., where ambitious\nzero-emission and carbon neutrality objectives have been set. In pursuit of\nthese goals, many states have implemented a range of incentive policies aimed\nat stimulating EV adoption and charging infrastructure development, especially\npublic EV charging stations (EVCS). This study examines the indirect network\neffect observed between EV adoption and EVCS deployment within urban\nlandscapes. We developed a two-sided log-log regression model with historical\ndata on EV purchases and EVCS development to quantify this effect. To test the\nrobustness, we then conducted a case study of the EV market in Los Angeles (LA)\nCounty, which suggests that a 1% increase in EVCS correlates with a 0.35%\nincrease in EV sales. Additionally, we forecasted the future EV market dynamics\nin LA County, revealing a notable disparity between current policies and the\ntargeted 80% EV market share for private cars by 2045. To bridge this gap, we\nproposed a combined policy recommendation that enhances EV incentives by 60%\nand EVCS rebates by 66%, facilitating the achievement of future EV market\nobjectives."
                },
                "authors": [
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Brian Yueshuai He"
                    },
                    {
                        "name": "Tomas Kaljevic"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "arxiv_comment": "Conference preprint, 6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12288v3",
                "updated": "2024-09-02T17:12:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    12,
                    48,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-18T05:49:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    5,
                    49,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "An Investigation of Neuron Activation as a Unified Lens to Explain\n  Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Neuron Activation as a Unified Lens to Explain\n  Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs"
                },
                "summary": "Large language models (LLMs) have shown strong arithmetic reasoning\ncapabilities when prompted with Chain-of-Thought (CoT) prompts. However, we\nhave only a limited understanding of how they are processed by LLMs. To\ndemystify it, prior work has primarily focused on ablating different components\nin the CoT prompt and empirically observing their resulting LLM performance\nchange. Yet, the reason why these components are important to LLM reasoning is\nnot explored. To fill this gap, in this work, we investigate ``neuron\nactivation'' as a lens to provide a unified explanation to observations made by\nprior work. Specifically, we look into neurons within the feed-forward layers\nof LLMs that may have activated their arithmetic reasoning capabilities, using\nLlama2 as an example. To facilitate this investigation, we also propose an\napproach based on GPT-4 to automatically identify neurons that imply arithmetic\nreasoning. Our analyses revealed that the activation of reasoning neurons in\nthe feed-forward layers of an LLM can explain the importance of various\ncomponents in a CoT prompt, and future research can extend it for a more\ncomplete understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong arithmetic reasoning\ncapabilities when prompted with Chain-of-Thought (CoT) prompts. However, we\nhave only a limited understanding of how they are processed by LLMs. To\ndemystify it, prior work has primarily focused on ablating different components\nin the CoT prompt and empirically observing their resulting LLM performance\nchange. Yet, the reason why these components are important to LLM reasoning is\nnot explored. To fill this gap, in this work, we investigate ``neuron\nactivation'' as a lens to provide a unified explanation to observations made by\nprior work. Specifically, we look into neurons within the feed-forward layers\nof LLMs that may have activated their arithmetic reasoning capabilities, using\nLlama2 as an example. To facilitate this investigation, we also propose an\napproach based on GPT-4 to automatically identify neurons that imply arithmetic\nreasoning. Our analyses revealed that the activation of reasoning neurons in\nthe feed-forward layers of an LLM can explain the importance of various\ncomponents in a CoT prompt, and future research can extend it for a more\ncomplete understanding."
                },
                "authors": [
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "arxiv_comment": "9 pages, 1 figure, to be published in ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10108v2",
                "updated": "2024-09-02T17:00:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    0,
                    21,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-15T06:30:55Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    6,
                    30,
                    55,
                    4,
                    349,
                    0
                ],
                "title": "Privacy-Aware Document Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Aware Document Visual Question Answering"
                },
                "summary": "Document Visual Question Answering (DocVQA) has quickly grown into a central\ntask of document understanding. But despite the fact that documents contain\nsensitive or copyrighted information, none of the current DocVQA methods offers\nstrong privacy guarantees. In this work, we explore privacy in the domain of\nDocVQA for the first time, highlighting privacy issues in state of the art\nmulti-modal LLM models used for DocVQA, and explore possible solutions.\nSpecifically, we focus on invoice processing as a realistic document\nunderstanding scenario, and propose a large scale DocVQA dataset comprising\ninvoice documents and associated questions and answers. We employ a federated\nlearning scheme, that reflects the real-life distribution of documents in\ndifferent businesses, and we explore the use case where the data of the invoice\nprovider is the sensitive information to be protected. We demonstrate that\nnon-private models tend to memorise, a behaviour that can lead to exposing\nprivate information. We then evaluate baseline training schemes employing\nfederated learning and differential privacy in this multi-modal scenario, where\nthe sensitive information might be exposed through either or both of the two\ninput modalities: vision (document image) or language (OCR tokens). Finally, we\ndesign attacks exploiting the memorisation effect of the model, and demonstrate\ntheir effectiveness in probing a representative DocVQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (DocVQA) has quickly grown into a central\ntask of document understanding. But despite the fact that documents contain\nsensitive or copyrighted information, none of the current DocVQA methods offers\nstrong privacy guarantees. In this work, we explore privacy in the domain of\nDocVQA for the first time, highlighting privacy issues in state of the art\nmulti-modal LLM models used for DocVQA, and explore possible solutions.\nSpecifically, we focus on invoice processing as a realistic document\nunderstanding scenario, and propose a large scale DocVQA dataset comprising\ninvoice documents and associated questions and answers. We employ a federated\nlearning scheme, that reflects the real-life distribution of documents in\ndifferent businesses, and we explore the use case where the data of the invoice\nprovider is the sensitive information to be protected. We demonstrate that\nnon-private models tend to memorise, a behaviour that can lead to exposing\nprivate information. We then evaluate baseline training schemes employing\nfederated learning and differential privacy in this multi-modal scenario, where\nthe sensitive information might be exposed through either or both of the two\ninput modalities: vision (document image) or language (OCR tokens). Finally, we\ndesign attacks exploiting the memorisation effect of the model, and demonstrate\ntheir effectiveness in probing a representative DocVQA models."
                },
                "authors": [
                    {
                        "name": "Rub√®n Tito"
                    },
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Marlon Tobaben"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mohamed Ali Souibgui"
                    },
                    {
                        "name": "Kangsoo Jung"
                    },
                    {
                        "name": "Joonas J√§lk√∂"
                    },
                    {
                        "name": "Vincent Poulain D'Andecy"
                    },
                    {
                        "name": "Aurelie Joseph"
                    },
                    {
                        "name": "Lei Kang"
                    },
                    {
                        "name": "Ernest Valveny"
                    },
                    {
                        "name": "Antti Honkela"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Dimosthenis Karatzas"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Karatzas"
                },
                "author": "Dimosthenis Karatzas",
                "arxiv_comment": "35 pages, 12 figures, accepted for publication at the 18th\n  International Conference on Document Analysis and Recognition, ICDAR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16160v2",
                "updated": "2024-09-02T16:33:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    33,
                    29,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-24T19:30:18Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    19,
                    30,
                    18,
                    2,
                    115,
                    0
                ],
                "title": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive generalization\ncapabilities on specific tasks with human-written instruction data. However,\nthe limited quantity, diversity, and professional expertise of such instruction\ndata raise concerns about the performance of LLMs in psychotherapy tasks when\nprovided with domain-specific instructions. To address this, we firstly propose\nDomain-Specific Assistant Instructions based on AlexanderStreet therapy, and\nsecondly, we use an adaption fine-tuning method and retrieval augmented\ngeneration method to improve pre-trained LLMs. Through quantitative evaluation\nof linguistic quality using automatic and human evaluation, we observe that\npre-trained LLMs on Psychotherapy Assistant Instructions outperform\nstate-of-the-art LLMs response baselines. Our Assistant-Instruction approach\noffers a half-annotation method to align pre-trained LLMs with instructions and\nprovide pre-trained LLMs with more psychotherapy knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive generalization\ncapabilities on specific tasks with human-written instruction data. However,\nthe limited quantity, diversity, and professional expertise of such instruction\ndata raise concerns about the performance of LLMs in psychotherapy tasks when\nprovided with domain-specific instructions. To address this, we firstly propose\nDomain-Specific Assistant Instructions based on AlexanderStreet therapy, and\nsecondly, we use an adaption fine-tuning method and retrieval augmented\ngeneration method to improve pre-trained LLMs. Through quantitative evaluation\nof linguistic quality using automatic and human evaluation, we observe that\npre-trained LLMs on Psychotherapy Assistant Instructions outperform\nstate-of-the-art LLMs response baselines. Our Assistant-Instruction approach\noffers a half-annotation method to align pre-trained LLMs with instructions and\nprovide pre-trained LLMs with more psychotherapy knowledge."
                },
                "authors": [
                    {
                        "name": "Cheng Kang"
                    },
                    {
                        "name": "Daniel Novak"
                    },
                    {
                        "name": "Katerina Urbanova"
                    },
                    {
                        "name": "Yuqing Cheng"
                    },
                    {
                        "name": "Yong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Hu"
                },
                "author": "Yong Hu",
                "arxiv_doi": "10.1109/ICASSPW62465.2024.10626529",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSPW62465.2024.10626529",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.16160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ICASSP 2024 EIHRC Workshop",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14738v2",
                "updated": "2024-09-02T16:23:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    23,
                    24,
                    0,
                    246,
                    0
                ],
                "published": "2024-04-23T04:42:01Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    42,
                    1,
                    1,
                    114,
                    0
                ],
                "title": "Unmanned Vehicles in 6G Networks: A Unifying Treatment of Problems,\n  Formulations, and Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Vehicles in 6G Networks: A Unifying Treatment of Problems,\n  Formulations, and Tools"
                },
                "summary": "Unmanned Vehicles (UVs) functioning as autonomous agents are anticipated to\nplay a crucial role in the 6th Generation of wireless networks. Their seamless\nintegration, cost-effectiveness, and the additional controllability through\nmotion planning make them an attractive deployment option for a wide range of\napplications, both as assets in the network (e.g., mobile base stations) and as\nconsumers of network services (e.g., autonomous delivery systems). However,\ndespite their potential, the convergence of UVs and wireless systems brings\nforth numerous challenges that require attention from both academia and\nindustry. This paper then aims to offer a comprehensive overview encompassing\nthe transformative possibilities as well as the significant challenges\nassociated with UV-assisted next-generation wireless communications.\nConsidering the diverse landscape of possible application scenarios, problem\nformulations, and mathematical tools related to UV-assisted wireless systems,\nthe underlying core theme of this paper is the unification of the problem\nspace, providing a structured framework to understand the use cases, problem\nformulations, and necessary mathematical tools. Overall, the paper sets forth a\nclear understanding of how unmanned vehicles can be integrated in the 6G\necosystem, paving the way towards harnessing the full potential at this\nintersection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Vehicles (UVs) functioning as autonomous agents are anticipated to\nplay a crucial role in the 6th Generation of wireless networks. Their seamless\nintegration, cost-effectiveness, and the additional controllability through\nmotion planning make them an attractive deployment option for a wide range of\napplications, both as assets in the network (e.g., mobile base stations) and as\nconsumers of network services (e.g., autonomous delivery systems). However,\ndespite their potential, the convergence of UVs and wireless systems brings\nforth numerous challenges that require attention from both academia and\nindustry. This paper then aims to offer a comprehensive overview encompassing\nthe transformative possibilities as well as the significant challenges\nassociated with UV-assisted next-generation wireless communications.\nConsidering the diverse landscape of possible application scenarios, problem\nformulations, and mathematical tools related to UV-assisted wireless systems,\nthe underlying core theme of this paper is the unification of the problem\nspace, providing a structured framework to understand the use cases, problem\nformulations, and necessary mathematical tools. Overall, the paper sets forth a\nclear understanding of how unmanned vehicles can be integrated in the 6G\necosystem, paving the way towards harnessing the full potential at this\nintersection."
                },
                "authors": [
                    {
                        "name": "Winston Hurst"
                    },
                    {
                        "name": "Spilios Evmorfos"
                    },
                    {
                        "name": "Athina Petropulu"
                    },
                    {
                        "name": "Yasamin Mostofi"
                    }
                ],
                "author_detail": {
                    "name": "Yasamin Mostofi"
                },
                "author": "Yasamin Mostofi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13152v2",
                "updated": "2024-09-02T15:42:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    42,
                    3,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-19T02:00:51Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    0,
                    51,
                    2,
                    171,
                    0
                ],
                "title": "Analyzing Diversity in Healthcare LLM Research: A Scientometric\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Diversity in Healthcare LLM Research: A Scientometric\n  Perspective"
                },
                "summary": "The deployment of large language models (LLMs) in healthcare has demonstrated\nsubstantial potential for enhancing clinical decision-making, administrative\nefficiency, and patient outcomes. However, the underrepresentation of diverse\ngroups in the development and application of these models can perpetuate\nbiases, leading to inequitable healthcare delivery. This paper presents a\ncomprehensive scientometric analysis of LLM research for healthcare, including\ndata from January 1, 2021, to July 1, 2024. By analyzing metadata from PubMed\nand Dimensions, including author affiliations, countries, and funding sources,\nwe assess the diversity of contributors to LLM research. Our findings highlight\nsignificant gender and geographic disparities, with a predominance of male\nauthors and contributions primarily from high-income countries (HICs). We\nintroduce a novel journal diversity index based on Gini diversity to measure\nthe inclusiveness of scientific publications. Our results underscore the\nnecessity for greater representation in order to ensure the equitable\napplication of LLMs in healthcare. We propose actionable strategies to enhance\ndiversity and inclusivity in artificial intelligence research, with the\nultimate goal of fostering a more inclusive and equitable future in healthcare\ninnovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) in healthcare has demonstrated\nsubstantial potential for enhancing clinical decision-making, administrative\nefficiency, and patient outcomes. However, the underrepresentation of diverse\ngroups in the development and application of these models can perpetuate\nbiases, leading to inequitable healthcare delivery. This paper presents a\ncomprehensive scientometric analysis of LLM research for healthcare, including\ndata from January 1, 2021, to July 1, 2024. By analyzing metadata from PubMed\nand Dimensions, including author affiliations, countries, and funding sources,\nwe assess the diversity of contributors to LLM research. Our findings highlight\nsignificant gender and geographic disparities, with a predominance of male\nauthors and contributions primarily from high-income countries (HICs). We\nintroduce a novel journal diversity index based on Gini diversity to measure\nthe inclusiveness of scientific publications. Our results underscore the\nnecessity for greater representation in order to ensure the equitable\napplication of LLMs in healthcare. We propose actionable strategies to enhance\ndiversity and inclusivity in artificial intelligence research, with the\nultimate goal of fostering a more inclusive and equitable future in healthcare\ninnovation."
                },
                "authors": [
                    {
                        "name": "David Restrepo"
                    },
                    {
                        "name": "Chenwei Wu"
                    },
                    {
                        "name": "Constanza V√°squez-Venegas"
                    },
                    {
                        "name": "Jo√£o Matos"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Luis Filipe Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Luis Filipe Nakayama"
                },
                "author": "Luis Filipe Nakayama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.17708v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.17708v4",
                "updated": "2024-09-02T15:23:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    23,
                    52,
                    0,
                    246,
                    0
                ],
                "published": "2023-03-30T21:00:38Z",
                "published_parsed": [
                    2023,
                    3,
                    30,
                    21,
                    0,
                    38,
                    3,
                    89,
                    0
                ],
                "title": "Analysis of Failures and Risks in Deep Learning Model Converters: A Case\n  Study in the ONNX Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Failures and Risks in Deep Learning Model Converters: A Case\n  Study in the ONNX Ecosystem"
                },
                "summary": "Software engineers develop, fine-tune, and deploy deep learning (DL) models\nusing a variety of development frameworks and runtime environments. DL model\nconverters move models between frameworks and to runtime environments.\nConversion errors compromise model quality and disrupt deployment. However, the\nfailure characteristics of DL model converters are unknown, adding risk when\nusing DL interoperability technologies.\n  This paper analyzes failures in DL model converters. We survey software\nengineers about DL interoperability tools, use cases, and pain points (N=92).\nThen, we characterize failures in model converters associated with the main\ninteroperability tool, ONNX (N=200 issues in PyTorch and TensorFlow). Finally,\nwe formulate and test two hypotheses about structural causes for the failures\nwe studied. We find that the node conversion stage of a model converter\naccounts for ~75% of the defects and 33% of reported failure are related to\nsemantically incorrect models. The cause of semantically incorrect models is\nelusive, but models with behaviour inconsistencies share operator sequences.\nOur results motivate future research on making DL interoperability software\nsimpler to maintain, extend, and validate. Research into behavioural tolerances\nand architectural coverage metrics could be fruitful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers develop, fine-tune, and deploy deep learning (DL) models\nusing a variety of development frameworks and runtime environments. DL model\nconverters move models between frameworks and to runtime environments.\nConversion errors compromise model quality and disrupt deployment. However, the\nfailure characteristics of DL model converters are unknown, adding risk when\nusing DL interoperability technologies.\n  This paper analyzes failures in DL model converters. We survey software\nengineers about DL interoperability tools, use cases, and pain points (N=92).\nThen, we characterize failures in model converters associated with the main\ninteroperability tool, ONNX (N=200 issues in PyTorch and TensorFlow). Finally,\nwe formulate and test two hypotheses about structural causes for the failures\nwe studied. We find that the node conversion stage of a model converter\naccounts for ~75% of the defects and 33% of reported failure are related to\nsemantically incorrect models. The cause of semantically incorrect models is\nelusive, but models with behaviour inconsistencies share operator sequences.\nOur results motivate future research on making DL interoperability software\nsimpler to maintain, extend, and validate. Research into behavioural tolerances\nand architectural coverage metrics could be fruitful."
                },
                "authors": [
                    {
                        "name": "Purvish Jajal"
                    },
                    {
                        "name": "Wenxin Jiang"
                    },
                    {
                        "name": "Arav Tewari"
                    },
                    {
                        "name": "Erik Kocinare"
                    },
                    {
                        "name": "Joseph Woo"
                    },
                    {
                        "name": "Anusha Sarraf"
                    },
                    {
                        "name": "Yung-Hsiang Lu"
                    },
                    {
                        "name": "George K. Thiruvathukal"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "arxiv_comment": "[ISSTA'24] Proceedings of the 33rd ACM SIGSOFT International\n  Symposium on Software Testing and Analysis (ISSTA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.17708v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.17708v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v4",
                "updated": "2024-09-02T15:08:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    8,
                    32,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Evolving Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Shize Wei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12537v3",
                "updated": "2024-09-02T12:36:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    36,
                    6,
                    0,
                    246,
                    0
                ],
                "published": "2023-10-19T07:39:00Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    7,
                    39,
                    0,
                    3,
                    292,
                    0
                ],
                "title": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction"
                },
                "summary": "In order to facilitate features such as faceted product search and product\ncomparison, e-commerce platforms require accurately structured product data,\nincluding precise attribute/value pairs. Vendors often times provide\nunstructured product descriptions consisting only of an offer title and a\ntextual description. Consequently, extracting attribute values from titles and\ndescriptions is vital for e-commerce platforms. State-of-the-art attribute\nvalue extraction methods based on pre-trained language models, such as BERT,\nface two drawbacks (i) the methods require significant amounts of task-specific\ntraining data and (ii) the fine-tuned models have problems with generalising to\nunseen attribute values that were not part of the training data. This paper\nexplores the potential of using large language models as a more training\ndata-efficient and more robust alternative to existing AVE methods. We propose\nprompt templates for describing the target attributes of the extraction to the\nLLM, covering both zero-shot and few-shot scenarios. In the zero-shot scenario,\ntextual and JSON-based target schema representations of the attributes are\ncompared. In the few-shot scenario, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. We\nevaluate the prompt templates in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs which can be run locally. We compare the\nperformance of the LLMs to the PLM-based methods SU-OpenTag, AVEQA, and MAVEQA.\nThe highest average F1-score of 86% was achieved by GPT-4. Llama-3-70B performs\nonly 3% worse than GPT-4, making it a competitive open-source alternative.\nGiven the same training data, this prompt/GPT-4 combination outperforms the\nbest PLM baseline by an average of 6% F1-score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to facilitate features such as faceted product search and product\ncomparison, e-commerce platforms require accurately structured product data,\nincluding precise attribute/value pairs. Vendors often times provide\nunstructured product descriptions consisting only of an offer title and a\ntextual description. Consequently, extracting attribute values from titles and\ndescriptions is vital for e-commerce platforms. State-of-the-art attribute\nvalue extraction methods based on pre-trained language models, such as BERT,\nface two drawbacks (i) the methods require significant amounts of task-specific\ntraining data and (ii) the fine-tuned models have problems with generalising to\nunseen attribute values that were not part of the training data. This paper\nexplores the potential of using large language models as a more training\ndata-efficient and more robust alternative to existing AVE methods. We propose\nprompt templates for describing the target attributes of the extraction to the\nLLM, covering both zero-shot and few-shot scenarios. In the zero-shot scenario,\ntextual and JSON-based target schema representations of the attributes are\ncompared. In the few-shot scenario, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. We\nevaluate the prompt templates in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs which can be run locally. We compare the\nperformance of the LLMs to the PLM-based methods SU-OpenTag, AVEQA, and MAVEQA.\nThe highest average F1-score of 86% was achieved by GPT-4. Llama-3-70B performs\nonly 3% worse than GPT-4, making it a competitive open-source alternative.\nGiven the same training data, this prompt/GPT-4 combination outperforms the\nbest PLM baseline by an average of 6% F1-score."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Roee Shraga"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05141v3",
                "updated": "2024-09-02T10:55:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    55,
                    30,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-09T15:53:55Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}."
                },
                "authors": [
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Chengwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02091v2",
                "updated": "2024-09-02T10:02:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    2,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-04T18:06:41Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    6,
                    41,
                    0,
                    338,
                    0
                ],
                "title": "Physics simulation capabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics simulation capabilities of LLMs"
                },
                "summary": "[Abridged abstract] Large Language Models (LLMs) can solve some\nundergraduate-level to graduate-level physics textbook problems and are\nproficient at coding. Combining these two capabilities could one day enable AI\nsystems to simulate and predict the physical world.\n  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to\nresearch-level computational physics problems. We condition LLM generation on\nthe use of well-documented and widely-used packages to elicit coding\ncapabilities in the physics and astrophysics domains. We contribute $\\sim 50$\noriginal and challenging problems in celestial mechanics (with REBOUND),\nstellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear\ndynamics (with SciPy). Since our problems do not admit unique solutions, we\nevaluate LLM performance on several soft metrics: counts of lines that contain\ndifferent types of errors (coding, physics, necessity and sufficiency) as well\nas a more \"educational\" Pass-Fail metric focused on capturing the salient\nphysical ingredients of the problem at hand.\n  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,\nalthough about 40\\% of the solutions could plausibly get a passing grade. About\n$70-90 \\%$ of the code lines produced are necessary, sufficient and correct\n(coding \\& physics). Physics and coding errors are the most common, with some\nunnecessary or insufficient lines. We observe significant variations across\nproblem class and difficulty. We identify several failure modes of GPT4 in the\ncomputational physics domain.\n  Our reconnaissance work provides a snapshot of current computational\ncapabilities in classical physics and points to obvious improvement targets if\nAI systems are ever to reach a basic level of autonomy in physics simulation\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Abridged abstract] Large Language Models (LLMs) can solve some\nundergraduate-level to graduate-level physics textbook problems and are\nproficient at coding. Combining these two capabilities could one day enable AI\nsystems to simulate and predict the physical world.\n  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to\nresearch-level computational physics problems. We condition LLM generation on\nthe use of well-documented and widely-used packages to elicit coding\ncapabilities in the physics and astrophysics domains. We contribute $\\sim 50$\noriginal and challenging problems in celestial mechanics (with REBOUND),\nstellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear\ndynamics (with SciPy). Since our problems do not admit unique solutions, we\nevaluate LLM performance on several soft metrics: counts of lines that contain\ndifferent types of errors (coding, physics, necessity and sufficiency) as well\nas a more \"educational\" Pass-Fail metric focused on capturing the salient\nphysical ingredients of the problem at hand.\n  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,\nalthough about 40\\% of the solutions could plausibly get a passing grade. About\n$70-90 \\%$ of the code lines produced are necessary, sufficient and correct\n(coding \\& physics). Physics and coding errors are the most common, with some\nunnecessary or insufficient lines. We observe significant variations across\nproblem class and difficulty. We identify several failure modes of GPT4 in the\ncomputational physics domain.\n  Our reconnaissance work provides a snapshot of current computational\ncapabilities in classical physics and points to obvious improvement targets if\nAI systems are ever to reach a basic level of autonomy in physics simulation\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Mohamad Ali-Dib"
                    },
                    {
                        "name": "Kristen Menou"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Menou"
                },
                "author": "Kristen Menou",
                "arxiv_comment": "Accepted for publication in Physica Scripta. Abridged abstract. 15\n  pages + appendix, 1 figure. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03816v2",
                "updated": "2024-09-02T09:48:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    48,
                    18,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-06T07:40:00Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    7,
                    40,
                    0,
                    3,
                    158,
                    0
                ],
                "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"
                },
                "summary": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16069v2",
                "updated": "2024-09-02T09:13:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    13,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-23T10:36:35Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    10,
                    36,
                    35,
                    6,
                    175,
                    0
                ],
                "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by fine-tuning only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by fine-tuning only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem"
                },
                "authors": [
                    {
                        "name": "Junyi Zhu"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Matthew B. Blaschko"
                    }
                ],
                "author_detail": {
                    "name": "Matthew B. Blaschko"
                },
                "author": "Matthew B. Blaschko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01371v2",
                "updated": "2024-09-02T07:57:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    7,
                    57,
                    17,
                    0,
                    246,
                    0
                ],
                "published": "2023-12-29T01:17:58Z",
                "published_parsed": [
                    2023,
                    12,
                    29,
                    1,
                    17,
                    58,
                    4,
                    363,
                    0
                ],
                "title": "A model-based assessment of social isolation practices for COVID-19\n  outbreak response in residential care facilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model-based assessment of social isolation practices for COVID-19\n  outbreak response in residential care facilities"
                },
                "summary": "Residential aged-care facilities (RACFs, also called long-term care\nfacilities, aged care homes, or nursing homes) have elevated risks of\nrespiratory infection outbreaks and associated disease burden. During the\nCOVID-19 pandemic, social isolation policies were commonly used in these\nfacilities to prevent and mitigate outbreaks. We refer specifically to general\nisolation policies that were intended to reduce contact between residents,\nwithout regard to confirmed infection status. Such policies are controversial\nbecause of their association with adverse mental and physical health indicators\nand there is a lack of modelling that assesses their effectiveness.\n  We developed an agent-based model of COVID-19 transmission in a structured\npopulation, intended to represent the salient characteristics of a residential\ncare environment. Using our model, we generated stochastic ensembles of\nsimulated outbreaks and compared summary statistics of outbreaks simulated}\nunder different mitigation conditions. Our study focuses on the marginal impact\nof general isolation (reducing social contact between residents), regardless of\nconfirmed infection.\n  In the absence of any asymptomatic screening, general isolation of residents\nto their rooms reduced median cumulative cases by approximately 27%. However,\nwhen conducted concurrently with asymptomatic screening and isolation of\nconfirmed cases, general isolation reduced the median number of cumulative\ninfections by only 12% in our simulations.\n  Our simulations showed that general isolation of residents did not provide\nsubstantial benefits beyond those achieved through screening, isolation of\nconfirmed cases, and deployment of PPE. Our conclusions are sensitive to\nassumptions about the proportion of total contacts in a facility accounted for\nby casual interactions between residents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residential aged-care facilities (RACFs, also called long-term care\nfacilities, aged care homes, or nursing homes) have elevated risks of\nrespiratory infection outbreaks and associated disease burden. During the\nCOVID-19 pandemic, social isolation policies were commonly used in these\nfacilities to prevent and mitigate outbreaks. We refer specifically to general\nisolation policies that were intended to reduce contact between residents,\nwithout regard to confirmed infection status. Such policies are controversial\nbecause of their association with adverse mental and physical health indicators\nand there is a lack of modelling that assesses their effectiveness.\n  We developed an agent-based model of COVID-19 transmission in a structured\npopulation, intended to represent the salient characteristics of a residential\ncare environment. Using our model, we generated stochastic ensembles of\nsimulated outbreaks and compared summary statistics of outbreaks simulated}\nunder different mitigation conditions. Our study focuses on the marginal impact\nof general isolation (reducing social contact between residents), regardless of\nconfirmed infection.\n  In the absence of any asymptomatic screening, general isolation of residents\nto their rooms reduced median cumulative cases by approximately 27%. However,\nwhen conducted concurrently with asymptomatic screening and isolation of\nconfirmed cases, general isolation reduced the median number of cumulative\ninfections by only 12% in our simulations.\n  Our simulations showed that general isolation of residents did not provide\nsubstantial benefits beyond those achieved through screening, isolation of\nconfirmed cases, and deployment of PPE. Our conclusions are sensitive to\nassumptions about the proportion of total contacts in a facility accounted for\nby casual interactions between residents."
                },
                "authors": [
                    {
                        "name": "Cameron Zachreson"
                    },
                    {
                        "name": "Ruarai Tobin"
                    },
                    {
                        "name": "Camelia Walker"
                    },
                    {
                        "name": "Eamon Conway"
                    },
                    {
                        "name": "Freya M Shearer"
                    },
                    {
                        "name": "Jodie McVernon"
                    },
                    {
                        "name": "Nicholas Geard"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Geard"
                },
                "author": "Nicholas Geard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09067v3",
                "updated": "2024-09-02T07:26:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    7,
                    26,
                    46,
                    0,
                    246,
                    0
                ],
                "published": "2023-08-17T15:54:38Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    15,
                    54,
                    38,
                    3,
                    229,
                    0
                ],
                "title": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text"
                },
                "summary": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs."
                },
                "authors": [
                    {
                        "name": "Alberto Mu√±oz-Ortiz"
                    },
                    {
                        "name": "Carlos G√≥mez-Rodr√≠guez"
                    },
                    {
                        "name": "David Vilares"
                    }
                ],
                "author_detail": {
                    "name": "David Vilares"
                },
                "author": "David Vilares",
                "arxiv_doi": "10.1007/s10462-024-10903-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-024-10903-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at Artificial Intelligence Review vol. 57, 265",
                "arxiv_journal_ref": "Artificial Intelligence Review 57, 265 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16237v2",
                "updated": "2024-09-02T07:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    7,
                    25,
                    21,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-23T07:22:25Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    7,
                    22,
                    25,
                    1,
                    205,
                    0
                ],
                "title": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection"
                },
                "summary": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Fan Cui"
                    },
                    {
                        "name": "Chenyang Yin"
                    },
                    {
                        "name": "Kexing Zhou"
                    },
                    {
                        "name": "Youwei Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Qiang Xu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Demin Song"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Yun"
                    },
                    {
                        "name": "Liang"
                    }
                ],
                "author_detail": {
                    "name": "Liang"
                },
                "arxiv_affiliation": "Eric",
                "author": "Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03963v3",
                "updated": "2024-09-02T06:51:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    51,
                    36,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-07T02:49:59Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    2,
                    49,
                    59,
                    1,
                    128,
                    0
                ],
                "title": "ERATTA: Extreme RAG for Table To Answers with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERATTA: Extreme RAG for Table To Answers with Large Language Models"
                },
                "summary": "Large language models (LLMs) with retrieval augmented-generation (RAG) have\nbeen the optimal choice for scalable generative AI solutions in the recent\npast. Although RAG implemented with AI agents (agentic-RAG) has been recently\npopularized, its suffers from unstable cost and unreliable performances for\nEnterprise-level data-practices. Most existing use-cases that incorporate RAG\nwith LLMs have been either generic or extremely domain specific, thereby\nquestioning the scalability and generalizability of RAG-LLM approaches. In this\nwork, we propose a unique LLM-based system where multiple LLMs can be invoked\nto enable data authentication, user-query routing, data-retrieval and custom\nprompting for question-answering capabilities from Enterprise-data tables. The\nsource tables here are highly fluctuating and large in size and the proposed\nframework enables structured responses in under 10 seconds per query.\nAdditionally, we propose a five metric scoring module that detects and reports\nhallucinations in the LLM responses. Our proposed system and scoring metrics\nachieve >90% confidence scores across hundreds of user queries in the\nsustainability, financial health and social media domains. Extensions to the\nproposed extreme RAG architectures can enable heterogeneous source querying\nusing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with retrieval augmented-generation (RAG) have\nbeen the optimal choice for scalable generative AI solutions in the recent\npast. Although RAG implemented with AI agents (agentic-RAG) has been recently\npopularized, its suffers from unstable cost and unreliable performances for\nEnterprise-level data-practices. Most existing use-cases that incorporate RAG\nwith LLMs have been either generic or extremely domain specific, thereby\nquestioning the scalability and generalizability of RAG-LLM approaches. In this\nwork, we propose a unique LLM-based system where multiple LLMs can be invoked\nto enable data authentication, user-query routing, data-retrieval and custom\nprompting for question-answering capabilities from Enterprise-data tables. The\nsource tables here are highly fluctuating and large in size and the proposed\nframework enables structured responses in under 10 seconds per query.\nAdditionally, we propose a five metric scoring module that detects and reports\nhallucinations in the LLM responses. Our proposed system and scoring metrics\nachieve >90% confidence scores across hundreds of user queries in the\nsustainability, financial health and social media domains. Extensions to the\nproposed extreme RAG architectures can enable heterogeneous source querying\nusing LLMs."
                },
                "authors": [
                    {
                        "name": "Sohini Roychowdhury"
                    },
                    {
                        "name": "Marko Krema"
                    },
                    {
                        "name": "Anvar Mahammad"
                    },
                    {
                        "name": "Brian Moore"
                    },
                    {
                        "name": "Arijit Mukherjee"
                    },
                    {
                        "name": "Punit Prakashchandra"
                    }
                ],
                "author_detail": {
                    "name": "Punit Prakashchandra"
                },
                "author": "Punit Prakashchandra",
                "arxiv_comment": "5 pages, 4 tables, IEEE Big Data, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05191v2",
                "updated": "2024-09-02T06:24:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    6,
                    24,
                    32,
                    0,
                    246,
                    0
                ],
                "published": "2023-10-08T15:00:04Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    15,
                    0,
                    4,
                    6,
                    281,
                    0
                ],
                "title": "LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of\n  Student-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of\n  Student-LLM Interaction"
                },
                "summary": "In the context of English as a Foreign Language (EFL) writing education,\nLLM-as-a-tutor can assist students by providing real-time feedback on their\nessays. However, challenges arise in assessing LLM-as-a-tutor due to differing\nstandards between educational and general use cases. To bridge this gap, we\nintegrate pedagogical principles to assess student-LLM interaction. First, we\nexplore how LLMs can function as English tutors, providing effective essay\nfeedback tailored to students. Second, we propose three metrics to evaluate\nLLM-as-a-tutor specifically designed for EFL writing education, emphasizing\npedagogical aspects. In this process, EFL experts evaluate the feedback from\nLLM-as-a-tutor regarding quality and characteristics. On the other hand, EFL\nlearners assess their learning outcomes from interaction with LLM-as-a-tutor.\nThis approach lays the groundwork for developing LLMs-as-a-tutor tailored to\nthe needs of EFL learners, advancing the effectiveness of writing education in\nthis context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of English as a Foreign Language (EFL) writing education,\nLLM-as-a-tutor can assist students by providing real-time feedback on their\nessays. However, challenges arise in assessing LLM-as-a-tutor due to differing\nstandards between educational and general use cases. To bridge this gap, we\nintegrate pedagogical principles to assess student-LLM interaction. First, we\nexplore how LLMs can function as English tutors, providing effective essay\nfeedback tailored to students. Second, we propose three metrics to evaluate\nLLM-as-a-tutor specifically designed for EFL writing education, emphasizing\npedagogical aspects. In this process, EFL experts evaluate the feedback from\nLLM-as-a-tutor regarding quality and characteristics. On the other hand, EFL\nlearners assess their learning outcomes from interaction with LLM-as-a-tutor.\nThis approach lays the groundwork for developing LLMs-as-a-tutor tailored to\nthe needs of EFL learners, advancing the effectiveness of writing education in\nthis context."
                },
                "authors": [
                    {
                        "name": "Jieun Han"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Minsun Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Yoonsu Kim"
                    },
                    {
                        "name": "Tak Yeon Lee"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Juho Kim"
                    },
                    {
                        "name": "So-Yeon Ahn"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14033v2",
                "updated": "2024-09-02T05:55:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    55,
                    6,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-26T05:55:48Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    55,
                    48,
                    0,
                    239,
                    0
                ],
                "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Copilot: Autonomous Machine Learning Research based on Large\n  Language Models Agents"
                },
                "summary": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations."
                },
                "authors": [
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06764v3",
                "updated": "2024-09-02T05:48:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    5,
                    48,
                    54,
                    0,
                    246,
                    0
                ],
                "published": "2024-03-11T14:35:32Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    14,
                    35,
                    32,
                    0,
                    71,
                    0
                ],
                "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models"
                },
                "summary": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "Accepted to ECCV 2024 (Oral), code is released at\n  https://github.com/pkunlp-icler/FastV,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10311v2",
                "updated": "2024-09-02T03:37:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    3,
                    37,
                    35,
                    0,
                    246,
                    0
                ],
                "published": "2024-06-14T06:47:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    6,
                    47,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large\n  Language Models"
                },
                "summary": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench."
                },
                "authors": [
                    {
                        "name": "Wenjing Zhang"
                    },
                    {
                        "name": "Xuejiao Lei"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Meijuan An"
                    },
                    {
                        "name": "Bikun Yang"
                    },
                    {
                        "name": "KaiKai Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04818v2",
                "updated": "2024-09-02T02:44:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    44,
                    11,
                    0,
                    246,
                    0
                ],
                "published": "2024-05-08T05:36:52Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    5,
                    36,
                    52,
                    2,
                    129,
                    0
                ],
                "title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation"
                },
                "summary": "Evaluating the quality of free-text explanations is a multifaceted,\nsubjective, and labor-intensive task. Large language models (LLMs) present an\nappealing alternative due to their potential for consistency, scalability, and\ncost-efficiency. In this work, we present ACORN, a new dataset of 3,500\nfree-text explanations and aspect-wise quality ratings, and use it to evaluate\nhow LLMs rate explanations. We observed that larger models outputted labels\nthat maintained or increased the inter-annotator agreement, suggesting that\nthey are within the expected variance between human raters. However, their\ncorrelation with majority-voted human ratings varied across different quality\naspects, indicating that they are not a complete replacement. In turn, using\nLLMs as a supplement to a smaller group of human raters in some cases improved\nthe correlation with the original majority labels. However, the effect was\nlimited to cases where human raters were scarce, and an additional human rater\nhad a more pronounced effect in all cases. Overall, we recommend against using\nLLMs as a complete replacement for human raters but encourage using them in\nconfigurations that end with targeted human involvement. Data available here:\nhttps://github.com/a-brassard/ACORN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of free-text explanations is a multifaceted,\nsubjective, and labor-intensive task. Large language models (LLMs) present an\nappealing alternative due to their potential for consistency, scalability, and\ncost-efficiency. In this work, we present ACORN, a new dataset of 3,500\nfree-text explanations and aspect-wise quality ratings, and use it to evaluate\nhow LLMs rate explanations. We observed that larger models outputted labels\nthat maintained or increased the inter-annotator agreement, suggesting that\nthey are within the expected variance between human raters. However, their\ncorrelation with majority-voted human ratings varied across different quality\naspects, indicating that they are not a complete replacement. In turn, using\nLLMs as a supplement to a smaller group of human raters in some cases improved\nthe correlation with the original majority labels. However, the effect was\nlimited to cases where human raters were scarce, and an additional human rater\nhad a more pronounced effect in all cases. Overall, we recommend against using\nLLMs as a complete replacement for human raters but encourage using them in\nconfigurations that end with targeted human involvement. Data available here:\nhttps://github.com/a-brassard/ACORN"
                },
                "authors": [
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "arxiv_comment": "18 pages, 7 figures, accepted to COLM 2024. Data available here:\n  https://github.com/a-brassard/ACORN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08254v2",
                "updated": "2024-09-02T02:35:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    35,
                    50,
                    0,
                    246,
                    0
                ],
                "published": "2024-07-11T07:55:50Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    55,
                    50,
                    3,
                    193,
                    0
                ],
                "title": "United We Stand: Decentralized Multi-Agent Planning With Attrition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "United We Stand: Decentralized Multi-Agent Planning With Attrition"
                },
                "summary": "Decentralized planning is a key element of cooperative multi-agent systems\nfor information gathering tasks. However, despite the high frequency of agent\nfailures in realistic large deployment scenarios, current approaches perform\npoorly in the presence of failures, by not converging at all, and/or by making\nvery inefficient use of resources (e.g. energy). In this work, we propose\nAttritable MCTS (A-MCTS), a decentralized MCTS algorithm capable of timely and\nefficient adaptation to changes in the set of active agents. It is based on the\nuse of a global reward function for the estimation of each agent's local\ncontribution, and regret matching for coordination. We evaluate its\neffectiveness in realistic data-harvesting problems under different scenarios.\nWe show both theoretically and experimentally that A-MCTS enables efficient\nadaptation even under high failure rates. Results suggest that, in the presence\nof frequent failures, our solution improves substantially over the best\nexisting approaches in terms of global utility and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized planning is a key element of cooperative multi-agent systems\nfor information gathering tasks. However, despite the high frequency of agent\nfailures in realistic large deployment scenarios, current approaches perform\npoorly in the presence of failures, by not converging at all, and/or by making\nvery inefficient use of resources (e.g. energy). In this work, we propose\nAttritable MCTS (A-MCTS), a decentralized MCTS algorithm capable of timely and\nefficient adaptation to changes in the set of active agents. It is based on the\nuse of a global reward function for the estimation of each agent's local\ncontribution, and regret matching for coordination. We evaluate its\neffectiveness in realistic data-harvesting problems under different scenarios.\nWe show both theoretically and experimentally that A-MCTS enables efficient\nadaptation even under high failure rates. Results suggest that, in the presence\nof frequent failures, our solution improves substantially over the best\nexisting approaches in terms of global utility and scalability."
                },
                "authors": [
                    {
                        "name": "Nhat Nguyen"
                    },
                    {
                        "name": "Duong Nguyen"
                    },
                    {
                        "name": "Gianluca Rizzo"
                    },
                    {
                        "name": "Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Hung Nguyen"
                },
                "author": "Hung Nguyen",
                "arxiv_comment": "To appear in ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15879v2",
                "updated": "2024-09-02T02:30:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    30,
                    51,
                    0,
                    246,
                    0
                ],
                "published": "2024-08-28T15:50:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Persuasion Games using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Games using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape user perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nuser agents through persuasive dialogue, while the auxiliary agents perform\ntasks such as information retrieval, response analysis, development of\npersuasion strategies, and validation of facts. Empirical evidence from our\nexperiments demonstrates that this collaborative methodology significantly\nenhances the persuasive efficacy of the LLM. We continuously analyze the\nresistance of the user agent to persuasive efforts and counteract it by\nemploying a combination of rule-based and LLM-based resistance-persuasion\nmapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase)."
                },
                "authors": [
                    {
                        "name": "Ganesh Prasath Ramani"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Santhosh V"
                    },
                    {
                        "name": "Yash Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Yash Bhatia"
                },
                "author": "Yash Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08414v2",
                "updated": "2024-09-01T22:58:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    22,
                    58,
                    51,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-12T16:58:41Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    16,
                    58,
                    41,
                    2,
                    164,
                    0
                ],
                "title": "Discovering Preference Optimization Algorithms with and for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Preference Optimization Algorithms with and for Large\n  Language Models"
                },
                "summary": "Offline preference optimization is a key method for enhancing and controlling\nthe quality of Large Language Model (LLM) outputs. Typically, preference\noptimization is approached as an offline supervised learning task using\nmanually-crafted convex loss functions. While these methods are based on\ntheoretical insights, they are inherently constrained by human creativity, so\nthe large search space of possible loss functions remains under explored. We\naddress this by performing LLM-driven objective discovery to automatically\ndiscover new state-of-the-art preference optimization algorithms without\n(expert) human intervention. Specifically, we iteratively prompt an LLM to\npropose and implement new preference optimization loss functions based on\npreviously-evaluated performance metrics. This process leads to the discovery\nof previously-unknown and performant preference optimization algorithms. The\nbest performing of these we call Discovered Preference Optimization (DiscoPOP),\na novel algorithm that adaptively blends logistic and exponential losses.\nExperiments demonstrate the state-of-the-art performance of DiscoPOP and its\nsuccessful transfer to held-out tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline preference optimization is a key method for enhancing and controlling\nthe quality of Large Language Model (LLM) outputs. Typically, preference\noptimization is approached as an offline supervised learning task using\nmanually-crafted convex loss functions. While these methods are based on\ntheoretical insights, they are inherently constrained by human creativity, so\nthe large search space of possible loss functions remains under explored. We\naddress this by performing LLM-driven objective discovery to automatically\ndiscover new state-of-the-art preference optimization algorithms without\n(expert) human intervention. Specifically, we iteratively prompt an LLM to\npropose and implement new preference optimization loss functions based on\npreviously-evaluated performance metrics. This process leads to the discovery\nof previously-unknown and performant preference optimization algorithms. The\nbest performing of these we call Discovered Preference Optimization (DiscoPOP),\na novel algorithm that adaptively blends logistic and exponential losses.\nExperiments demonstrate the state-of-the-art performance of DiscoPOP and its\nsuccessful transfer to held-out tasks."
                },
                "authors": [
                    {
                        "name": "Chris Lu"
                    },
                    {
                        "name": "Samuel Holt"
                    },
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Alex J. Chan"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "Robert Tjarko Lange"
                    }
                ],
                "author_detail": {
                    "name": "Robert Tjarko Lange"
                },
                "author": "Robert Tjarko Lange",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01663v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01663v4",
                "updated": "2024-09-01T22:02:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    22,
                    2,
                    32,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-02T06:07:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    6,
                    7,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models"
                },
                "summary": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "JingSong Yang"
                    }
                ],
                "author_detail": {
                    "name": "JingSong Yang"
                },
                "author": "JingSong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01663v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01663v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16767v3",
                "updated": "2024-09-01T19:54:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    54,
                    56,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-25T17:20:45Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    20,
                    45,
                    3,
                    116,
                    0
                ],
                "title": "REBEL: Reinforcement Learning via Regressing Relative Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBEL: Reinforcement Learning via Regressing Relative Rewards"
                },
                "summary": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While originally developed for continuous control problems, Proximal Policy\nOptimization (PPO) has emerged as the work-horse of a variety of reinforcement\nlearning (RL) applications, including the fine-tuning of generative models.\nUnfortunately, PPO requires multiple heuristics to enable stable convergence\n(e.g. value networks, clipping), and is notorious for its sensitivity to the\nprecise implementation of these components. In response, we take a step back\nand ask what a minimalist RL algorithm for the era of generative models would\nlook like. We propose REBEL, an algorithm that cleanly reduces the problem of\npolicy optimization to regressing the relative reward between two completions\nto a prompt in terms of the policy, enabling strikingly lightweight\nimplementation. In theory, we prove that fundamental RL algorithms like Natural\nPolicy Gradient can be seen as variants of REBEL, which allows us to match the\nstrongest known theoretical guarantees in terms of convergence and sample\ncomplexity in the RL literature. REBEL can also cleanly incorporate offline\ndata and be extended to handle the intransitive preferences we frequently see\nin practice. Empirically, we find that REBEL provides a unified approach to\nlanguage modeling and image generation with stronger or similar performance as\nPPO and DPO, all while being simpler to implement and more computationally\nefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong\nperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "name": "Wenhao Zhan"
                    },
                    {
                        "name": "Owen Oertell"
                    },
                    {
                        "name": "Gokul Swamy"
                    },
                    {
                        "name": "Kiant√© Brantley"
                    },
                    {
                        "name": "Thorsten Joachims"
                    },
                    {
                        "name": "J. Andrew Bagnell"
                    },
                    {
                        "name": "Jason D. Lee"
                    },
                    {
                        "name": "Wen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wen Sun"
                },
                "author": "Wen Sun",
                "arxiv_comment": "New experimental results on general chat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06573v2",
                "updated": "2024-09-01T19:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    38,
                    2,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-03T18:15:56Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    15,
                    56,
                    0,
                    155,
                    0
                ],
                "title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering"
                },
                "summary": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings."
                },
                "authors": [
                    {
                        "name": "Robert Osazuwa Ness"
                    },
                    {
                        "name": "Katie Matton"
                    },
                    {
                        "name": "Hayden Helm"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Junaid Bajwa"
                    },
                    {
                        "name": "Carey E. Priebe"
                    },
                    {
                        "name": "Eric Horvitz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Horvitz"
                },
                "author": "Eric Horvitz",
                "arxiv_comment": "9 pages, 3 figures, 2 algorithms, appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04668v2",
                "updated": "2024-09-01T19:00:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    0,
                    25,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-07T01:50:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    1,
                    50,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "Forecasting Live Chat Intent from Browsing History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Live Chat Intent from Browsing History"
                },
                "summary": "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage."
                },
                "authors": [
                    {
                        "name": "Se-eun Yoon"
                    },
                    {
                        "name": "Ahmad Bin Rabiah"
                    },
                    {
                        "name": "Zaid Alibadi"
                    },
                    {
                        "name": "Surya Kallumadi"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15751v2",
                "updated": "2024-09-01T17:32:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    17,
                    32,
                    47,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-28T12:35:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    35,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Reinforcement Learning for Adaptive Traffic Signal Control: Turn-Based\n  and Time-Based Approaches to Reduce Congestion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Adaptive Traffic Signal Control: Turn-Based\n  and Time-Based Approaches to Reduce Congestion"
                },
                "summary": "The growing demand for road use in urban areas has led to significant traffic\ncongestion, posing challenges that are costly to mitigate through\ninfrastructure expansion alone. As an alternative, optimizing existing traffic\nmanagement systems, particularly through adaptive traffic signal control,\noffers a promising solution. This paper explores the use of Reinforcement\nLearning (RL) to enhance traffic signal operations at intersections, aiming to\nreduce congestion without extensive sensor networks. We introduce two RL-based\nalgorithms: a turn-based agent, which dynamically prioritizes traffic signals\nbased on real-time queue lengths, and a time-based agent, which adjusts signal\nphase durations according to traffic conditions while following a fixed phase\ncycle. By representing the state as a scalar queue length, our approach\nsimplifies the learning process and lowers deployment costs. The algorithms\nwere tested in four distinct traffic scenarios using seven evaluation metrics\nto comprehensively assess performance. Simulation results demonstrate that both\nRL algorithms significantly outperform conventional traffic signal control\nsystems, highlighting their potential to improve urban traffic flow\nefficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for road use in urban areas has led to significant traffic\ncongestion, posing challenges that are costly to mitigate through\ninfrastructure expansion alone. As an alternative, optimizing existing traffic\nmanagement systems, particularly through adaptive traffic signal control,\noffers a promising solution. This paper explores the use of Reinforcement\nLearning (RL) to enhance traffic signal operations at intersections, aiming to\nreduce congestion without extensive sensor networks. We introduce two RL-based\nalgorithms: a turn-based agent, which dynamically prioritizes traffic signals\nbased on real-time queue lengths, and a time-based agent, which adjusts signal\nphase durations according to traffic conditions while following a fixed phase\ncycle. By representing the state as a scalar queue length, our approach\nsimplifies the learning process and lowers deployment costs. The algorithms\nwere tested in four distinct traffic scenarios using seven evaluation metrics\nto comprehensively assess performance. Simulation results demonstrate that both\nRL algorithms significantly outperform conventional traffic signal control\nsystems, highlighting their potential to improve urban traffic flow\nefficiently."
                },
                "authors": [
                    {
                        "name": "Muhammad Tahir Rafique"
                    },
                    {
                        "name": "Ahmed Mustafa"
                    },
                    {
                        "name": "Hasan Sajid"
                    }
                ],
                "author_detail": {
                    "name": "Hasan Sajid"
                },
                "author": "Hasan Sajid",
                "arxiv_comment": "9 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15319v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15319v3",
                "updated": "2024-09-01T17:21:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    17,
                    21,
                    18,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-21T17:23:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    23,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"
                },
                "summary": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15319v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15319v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10237v3",
                "updated": "2024-09-01T16:39:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    16,
                    39,
                    31,
                    6,
                    245,
                    0
                ],
                "published": "2024-04-16T02:35:17Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    2,
                    35,
                    17,
                    1,
                    107,
                    0
                ],
                "title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models"
                },
                "summary": "Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method."
                },
                "authors": [
                    {
                        "name": "Songtao Jiang"
                    },
                    {
                        "name": "Tuo Zheng"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05965v2",
                "updated": "2024-09-01T15:13:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    15,
                    13,
                    52,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-08T14:04:58Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    14,
                    4,
                    58,
                    0,
                    190,
                    0
                ],
                "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models"
                },
                "summary": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI."
                },
                "authors": [
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Lijia Yu"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xiao-Shan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Shan Gao"
                },
                "author": "Xiao-Shan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10459v2",
                "updated": "2024-09-01T14:28:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    14,
                    28,
                    11,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-15T01:02:48Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    1,
                    2,
                    48,
                    5,
                    167,
                    0
                ],
                "title": "CancerLLM: A Large Language Model in Cancer Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CancerLLM: A Large Language Model in Cancer Domain"
                },
                "summary": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, and cancer diagnosis generation. Our evaluation demonstrated that\nCancerLLM achieves state-of-the-art results compared to other existing LLMs,\nwith an average F1 score improvement of 7.61 %. Additionally, CancerLLM\noutperforms other models on two proposed robustness testbeds. This illustrates\nthat CancerLLM can be effectively applied to clinical AI systems, enhancing\nclinical research and healthcare delivery in the field of cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B,\nLlama3-OpenBioLLM 70B have demonstrated impressive performance on a wide\nvariety of medical NLP task.However, there still lacks a large language model\n(LLM) specifically designed for cancer domain. Moreover, these LLMs typically\nhave billions of parameters, making them computationally expensive for\nhealthcare systems.Thus, in this study, we propose CancerLLM, a model with 7\nbillion parameters and a Mistral-style architecture, pre-trained on 2,676,642\nclinical notes and 515,524 pathology reports covering 17 cancer types, followed\nby fine-tuning on three cancer-relevant tasks, including cancer phenotypes\nextraction, and cancer diagnosis generation. Our evaluation demonstrated that\nCancerLLM achieves state-of-the-art results compared to other existing LLMs,\nwith an average F1 score improvement of 7.61 %. Additionally, CancerLLM\noutperforms other models on two proposed robustness testbeds. This illustrates\nthat CancerLLM can be effectively applied to clinical AI systems, enhancing\nclinical research and healthcare delivery in the field of cancer."
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jiatan Huang"
                    },
                    {
                        "name": "Jeremy Yeung"
                    },
                    {
                        "name": "Anne Blaes"
                    },
                    {
                        "name": "Steven Johnson"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "add the diagnosis evaluation of ICD code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v2",
                "updated": "2024-09-01T13:13:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    13,
                    5,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.04565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.04565v2",
                "updated": "2024-09-01T13:12:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    13,
                    12,
                    34,
                    6,
                    245,
                    0
                ],
                "published": "2023-09-08T19:34:29Z",
                "published_parsed": [
                    2023,
                    9,
                    8,
                    19,
                    34,
                    29,
                    4,
                    251,
                    0
                ],
                "title": "A Versatile Graph Learning Approach through LLM-based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Versatile Graph Learning Approach through LLM-based Agent"
                },
                "summary": "Designing versatile graph learning approaches is important, considering the\ndiverse graphs and tasks existing in real-world applications. Existing methods\nhave attempted to achieve this target through automated machine learning\ntechniques, pre-training and fine-tuning strategies, and large language models.\nHowever, these methods are not versatile enough for graph learning, as they\nwork on either limited types of graphs or a single task. In this paper, we\npropose to explore versatile graph learning approaches with LLM-based agents,\nand the key insight is customizing the graph learning procedures for diverse\ngraphs and tasks. To achieve this, we develop several LLM-based agents,\nequipped with diverse profiles, tools, functions and human experience. They\ncollaborate to configure each procedure with task and data-specific settings\nstep by step towards versatile solutions, and the proposed method is dubbed\nGL-Agent. By evaluating on diverse tasks and graphs, the correct results of the\nagent and its comparable performance showcase the versatility of the proposed\nmethod, especially in complex scenarios.The low resource cost and the potential\nto use open-source LLMs highlight the efficiency of GL-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing versatile graph learning approaches is important, considering the\ndiverse graphs and tasks existing in real-world applications. Existing methods\nhave attempted to achieve this target through automated machine learning\ntechniques, pre-training and fine-tuning strategies, and large language models.\nHowever, these methods are not versatile enough for graph learning, as they\nwork on either limited types of graphs or a single task. In this paper, we\npropose to explore versatile graph learning approaches with LLM-based agents,\nand the key insight is customizing the graph learning procedures for diverse\ngraphs and tasks. To achieve this, we develop several LLM-based agents,\nequipped with diverse profiles, tools, functions and human experience. They\ncollaborate to configure each procedure with task and data-specific settings\nstep by step towards versatile solutions, and the proposed method is dubbed\nGL-Agent. By evaluating on diverse tasks and graphs, the correct results of the\nagent and its comparable performance showcase the versatility of the proposed\nmethod, especially in complex scenarios.The low resource cost and the potential\nto use open-source LLMs highlight the efficiency of GL-Agent."
                },
                "authors": [
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Xiaohan Zheng"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Quanming Yao"
                    }
                ],
                "author_detail": {
                    "name": "Quanming Yao"
                },
                "author": "Quanming Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.04565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.04565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v6",
                "updated": "2024-09-01T10:12:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    10,
                    12,
                    45,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08506v2",
                "updated": "2024-09-01T08:30:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    8,
                    30,
                    58,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-16T03:06:57Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    6,
                    57,
                    4,
                    229,
                    0
                ],
                "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding"
                },
                "summary": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Guanhua He"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Shaoli Liu"
                    },
                    {
                        "name": "Tianshi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Chen"
                },
                "author": "Tianshi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12680v2",
                "updated": "2024-09-01T05:24:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    5,
                    24,
                    15,
                    6,
                    245,
                    0
                ],
                "published": "2024-08-22T18:39:00Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    39,
                    0,
                    3,
                    235,
                    0
                ],
                "title": "Can LLMs Understand Social Norms in Autonomous Driving Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Understand Social Norms in Autonomous Driving Games?"
                },
                "summary": "Social norm is defined as a shared standard of acceptable behavior in a\nsociety. The emergence of social norms fosters coordination among agents\nwithout any hard-coded rules, which is crucial for the large-scale deployment\nof AVs in an intelligent transportation system. This paper explores the\napplication of LLMs in understanding and modeling social norms in autonomous\ndriving games. We introduce LLMs into autonomous driving games as intelligent\nagents who make decisions according to text prompts. These agents are referred\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\nof social norms among individual agents. We aim to identify social norms by\ndesigning prompts and utilizing LLMs on textual information related to the\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\nand evaluate the performance of LLM-based agents in two driving scenarios:\nunsignalized intersection and highway platoon. The results show that LLM-based\nagents can handle dynamically changing environments in Markov games, and social\nnorms evolve among LLM-based agents in both scenarios. In the intersection\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\na potential car crash. The advantage of LLM-based agents in games lies in their\nstrong operability and analyzability, which facilitate experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social norm is defined as a shared standard of acceptable behavior in a\nsociety. The emergence of social norms fosters coordination among agents\nwithout any hard-coded rules, which is crucial for the large-scale deployment\nof AVs in an intelligent transportation system. This paper explores the\napplication of LLMs in understanding and modeling social norms in autonomous\ndriving games. We introduce LLMs into autonomous driving games as intelligent\nagents who make decisions according to text prompts. These agents are referred\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\nof social norms among individual agents. We aim to identify social norms by\ndesigning prompts and utilizing LLMs on textual information related to the\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\nand evaluate the performance of LLM-based agents in two driving scenarios:\nunsignalized intersection and highway platoon. The results show that LLM-based\nagents can handle dynamically changing environments in Markov games, and social\nnorms evolve among LLM-based agents in both scenarios. In the intersection\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\na potential car crash. The advantage of LLM-based agents in games lies in their\nstrong operability and analyzability, which facilitate experimental design."
                },
                "authors": [
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Yanhao Feng"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yongjie Fu"
                    },
                    {
                        "name": "Zhaobin Mo"
                    },
                    {
                        "name": "Xuan Di"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Di"
                },
                "author": "Xuan Di",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02168v4",
                "updated": "2024-09-01T05:21:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    5,
                    21,
                    46,
                    6,
                    245,
                    0
                ],
                "published": "2023-10-03T16:02:36Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    16,
                    2,
                    36,
                    1,
                    276,
                    0
                ],
                "title": "Editing Personality for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing Personality for Large Language Models"
                },
                "summary": "This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct PersonalityEdit, a new benchmark dataset to address this task.\nDrawing on the theory in Social Psychology, we isolate three representative\ntraits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation\nfor our benchmark. We then gather data using GPT-4, generating responses that\nalign with a specified topic and embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our findings uncover potential\nchallenges of the proposed task, illustrating several remaining issues. We\nanticipate that our work can stimulate further annotation in model editing and\npersonality-related research. Code is available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct PersonalityEdit, a new benchmark dataset to address this task.\nDrawing on the theory in Social Psychology, we isolate three representative\ntraits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation\nfor our benchmark. We then gather data using GPT-4, generating responses that\nalign with a specified topic and embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our findings uncover potential\nchallenges of the proposed task, illustrating several remaining issues. We\nanticipate that our work can stimulate further annotation in model editing and\npersonality-related research. Code is available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Shengyu Mao"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "NLPCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04220v3",
                "updated": "2024-09-01T01:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    1,
                    17,
                    21,
                    6,
                    245,
                    0
                ],
                "published": "2024-06-06T16:18:30Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    16,
                    18,
                    30,
                    3,
                    158,
                    0
                ],
                "title": "BEADs: Bias Evaluation Across Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEADs: Bias Evaluation Across Domains"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets, most are limited to one or two NLP tasks (typically\nclassification or evaluation) and lack comprehensive evaluations across a\nbroader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains (BEADs) dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label subset of BEADs, an important portion of the data verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. The benchmarking results highlight the\nefficacy of fine-tuning LLMs for bias identification and the necessity of\ncomprehensive bias evaluation. We make BEADs publicly available to promote more\nresponsible AI development. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets, most are limited to one or two NLP tasks (typically\nclassification or evaluation) and lack comprehensive evaluations across a\nbroader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains (BEADs) dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label subset of BEADs, an important portion of the data verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. The benchmarking results highlight the\nefficacy of fine-tuning LLMs for bias identification and the necessity of\ncomprehensive bias evaluation. We make BEADs publicly available to promote more\nresponsible AI development. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD ."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Michael R. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Zhang"
                },
                "author": "Michael R. Zhang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09105v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09105v6",
                "updated": "2024-09-01T00:26:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    1,
                    0,
                    26,
                    46,
                    6,
                    245,
                    0
                ],
                "published": "2024-07-12T09:10:37Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    9,
                    10,
                    37,
                    4,
                    194,
                    0
                ],
                "title": "Enhancing Training Efficiency Using Packing with Flash Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Training Efficiency Using Packing with Flash Attention"
                },
                "summary": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing."
                },
                "authors": [
                    {
                        "name": "Achintya Kundu"
                    },
                    {
                        "name": "Rhui Dih Lee"
                    },
                    {
                        "name": "Laura Wynter"
                    },
                    {
                        "name": "Raghu Kiran Ganti"
                    },
                    {
                        "name": "Mayank Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Mishra"
                },
                "author": "Mayank Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09105v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09105v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17493v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17493v5",
                "updated": "2024-08-31T19:42:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    19,
                    42,
                    31,
                    5,
                    244,
                    0
                ],
                "published": "2024-02-27T13:18:00Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    13,
                    18,
                    0,
                    1,
                    58,
                    0
                ],
                "title": "The Foundational Capabilities of Large Language Models in Predicting\n  Postoperative Risks Using Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Foundational Capabilities of Large Language Models in Predicting\n  Postoperative Risks Using Clinical Notes"
                },
                "summary": "Clinical notes recorded during a patient's perioperative journey holds\nimmense informational value. Advances in large language models (LLMs) offer\nopportunities for bridging this gap. Using 84,875 pre-operative notes and its\nassociated surgical cases from 2018 to 2021, we examine the performance of LLMs\nin predicting six postoperative risks using various fine-tuning strategies.\nPretrained LLMs outperformed traditional word embeddings by an absolute AUROC\nof 38.3% and AUPRC of 33.2%. Self-supervised fine-tuning further improved\nperformance by 3.2% and 1.5%. Incorporating labels into training further\nincreased AUROC by 1.8% and AUPRC by 2%. The highest performance was achieved\nwith a unified foundation model, with improvements of 3.6% for AUROC and 2.6%\nfor AUPRC compared to self-supervision, highlighting the foundational\ncapabilities of LLMs in predicting postoperative risks, which could be\npotentially beneficial when deployed for perioperative care",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical notes recorded during a patient's perioperative journey holds\nimmense informational value. Advances in large language models (LLMs) offer\nopportunities for bridging this gap. Using 84,875 pre-operative notes and its\nassociated surgical cases from 2018 to 2021, we examine the performance of LLMs\nin predicting six postoperative risks using various fine-tuning strategies.\nPretrained LLMs outperformed traditional word embeddings by an absolute AUROC\nof 38.3% and AUPRC of 33.2%. Self-supervised fine-tuning further improved\nperformance by 3.2% and 1.5%. Incorporating labels into training further\nincreased AUROC by 1.8% and AUPRC by 2%. The highest performance was achieved\nwith a unified foundation model, with improvements of 3.6% for AUROC and 2.6%\nfor AUPRC compared to self-supervision, highlighting the foundational\ncapabilities of LLMs in predicting postoperative risks, which could be\npotentially beneficial when deployed for perioperative care"
                },
                "authors": [
                    {
                        "name": "Charles Alba"
                    },
                    {
                        "name": "Bing Xue"
                    },
                    {
                        "name": "Joanna Abraham"
                    },
                    {
                        "name": "Thomas Kannampallil"
                    },
                    {
                        "name": "Chenyang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Lu"
                },
                "author": "Chenyang Lu",
                "arxiv_comment": "Codes are publicly available at:\n  https://github.com/cja5553/LLMs_in_perioperative_care",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17493v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17493v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12535v2",
                "updated": "2024-08-31T17:18:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    17,
                    18,
                    29,
                    5,
                    244,
                    0
                ],
                "published": "2024-04-18T22:56:57Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    22,
                    56,
                    57,
                    3,
                    109,
                    0
                ],
                "title": "Is There No Such Thing as a Bad Question? H4R: HalluciBot For\n  Ratiocination, Rewriting, Ranking, and Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is There No Such Thing as a Bad Question? H4R: HalluciBot For\n  Ratiocination, Rewriting, Ranking, and Routing"
                },
                "summary": "Hallucination continues to be one of the most critical challenges in the\ninstitutional adoption journey of Large Language Models (LLMs). While prior\nstudies have primarily focused on the post-generation analysis and refinement\nof outputs, this paper centers on the effectiveness of queries in eliciting\naccurate responses from LLMs. We present HalluciBot, a model that estimates the\nquery's propensity to hallucinate before generation, without invoking any LLMs\nduring inference. HalluciBot can serve as a proxy reward model for query\nrewriting, offering a general framework to estimate query quality based on\naccuracy and consensus. In essence, HalluciBot investigates how poorly\nconstructed queries can lead to erroneous outputs - moreover, by employing\nquery rewriting guided by HalluciBot's empirical estimates, we demonstrate that\n95.7% output accuracy can be achieved for Multiple Choice questions. The\ntraining procedure for HalluciBot consists of perturbing 369,837 queries n\ntimes, employing n+1 independent LLM agents, sampling an output from each\nquery, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs,\nand training an encoder classifier. The idea of perturbation is the outcome of\nour ablation studies that measures the increase in output diversity (+12.5\nagreement spread) by perturbing a query in lexically different but semantically\nsimilar ways. Therefore, HalluciBot paves the way to ratiocinate (76.0% test F1\nscore, 46.6% in saved computation on hallucinatory queries), rewrite (+30.2%\npositive class transition from hallucinatory to non-hallucinatory), rank\n(+50.6% positive class transition from hallucinatory to non-hallucinatory), and\nroute queries to effective pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination continues to be one of the most critical challenges in the\ninstitutional adoption journey of Large Language Models (LLMs). While prior\nstudies have primarily focused on the post-generation analysis and refinement\nof outputs, this paper centers on the effectiveness of queries in eliciting\naccurate responses from LLMs. We present HalluciBot, a model that estimates the\nquery's propensity to hallucinate before generation, without invoking any LLMs\nduring inference. HalluciBot can serve as a proxy reward model for query\nrewriting, offering a general framework to estimate query quality based on\naccuracy and consensus. In essence, HalluciBot investigates how poorly\nconstructed queries can lead to erroneous outputs - moreover, by employing\nquery rewriting guided by HalluciBot's empirical estimates, we demonstrate that\n95.7% output accuracy can be achieved for Multiple Choice questions. The\ntraining procedure for HalluciBot consists of perturbing 369,837 queries n\ntimes, employing n+1 independent LLM agents, sampling an output from each\nquery, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs,\nand training an encoder classifier. The idea of perturbation is the outcome of\nour ablation studies that measures the increase in output diversity (+12.5\nagreement spread) by perturbing a query in lexically different but semantically\nsimilar ways. Therefore, HalluciBot paves the way to ratiocinate (76.0% test F1\nscore, 46.6% in saved computation on hallucinatory queries), rewrite (+30.2%\npositive class transition from hallucinatory to non-hallucinatory), rank\n(+50.6% positive class transition from hallucinatory to non-hallucinatory), and\nroute queries to effective pipelines."
                },
                "authors": [
                    {
                        "name": "William Watson"
                    },
                    {
                        "name": "Nicole Cho"
                    },
                    {
                        "name": "Nishan Srishankar"
                    }
                ],
                "author_detail": {
                    "name": "Nishan Srishankar"
                },
                "author": "Nishan Srishankar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08800v3",
                "updated": "2024-08-31T16:56:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    16,
                    56,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2023-12-14T10:35:13Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    10,
                    35,
                    13,
                    3,
                    348,
                    0
                ],
                "title": "Evaluating Large Language Models for Health-related Queries with\n  Presuppositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Health-related Queries with\n  Presuppositions"
                },
                "summary": "As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios."
                },
                "authors": [
                    {
                        "name": "Navreet Kaur"
                    },
                    {
                        "name": "Monojit Choudhury"
                    },
                    {
                        "name": "Danish Pruthi"
                    }
                ],
                "author_detail": {
                    "name": "Danish Pruthi"
                },
                "author": "Danish Pruthi",
                "arxiv_comment": "Findings of ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15173v2",
                "updated": "2024-08-31T15:36:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    36,
                    32,
                    5,
                    244,
                    0
                ],
                "published": "2024-02-23T08:11:55Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    8,
                    11,
                    55,
                    4,
                    54,
                    0
                ],
                "title": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed\n  Zeroth-Order Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed\n  Zeroth-Order Optimizer"
                },
                "summary": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8."
                },
                "authors": [
                    {
                        "name": "Yanjun Zhao"
                    },
                    {
                        "name": "Sizhe Dang"
                    },
                    {
                        "name": "Haishan Ye"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Yi Qian"
                    },
                    {
                        "name": "Ivor W. Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor W. Tsang"
                },
                "author": "Ivor W. Tsang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14158v2",
                "updated": "2024-08-31T13:33:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    13,
                    33,
                    22,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-26T10:11:56Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    10,
                    11,
                    56,
                    0,
                    239,
                    0
                ],
                "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep\n  Learning"
                },
                "summary": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC."
                },
                "authors": [
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Yuheng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Zou"
                },
                "author": "Yuheng Zou",
                "arxiv_comment": "This is the preprint version of the paper accepted for presentation\n  at the 2024 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'24). \\c{opyright} 2024 IEEE. Personal\n  use of this material is permitted. For other uses, permission from IEEE must\n  be obtained. Please refer to IEEE Xplore for the final published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01020v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01020v4",
                "updated": "2024-08-31T12:46:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    12,
                    46,
                    51,
                    5,
                    244,
                    0
                ],
                "published": "2023-11-02T06:24:38Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    6,
                    24,
                    38,
                    3,
                    306,
                    0
                ],
                "title": "Exploring the Problems, their Causes and Solutions of AI Pair\n  Programming: A Study on GitHub and Stack Overflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Problems, their Causes and Solutions of AI Pair\n  Programming: A Study on GitHub and Stack Overflow"
                },
                "summary": "With the recent advancement of Artificial Intelligence (AI) and Large\nLanguage Models (LLMs), AI-based code generation tools become a practical\nsolution for software development. GitHub Copilot, the AI pair programmer,\nutilizes machine learning models trained on a large corpus of code snippets to\ngenerate code suggestions using natural language processing. Despite its\npopularity in software development, there is limited empirical evidence on the\nactual experiences of practitioners who work with Copilot. To this end, we\nconducted an empirical study to understand the problems that practitioners face\nwhen using Copilot, as well as their underlying causes and potential solutions.\nWe collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack\nOverflow posts. Our results reveal that (1) Operation Issue and Compatibility\nIssue are the most common problems faced by Copilot users, (2) Copilot Internal\nError, Network Connection Error, and Editor/IDE Compatibility Issue are\nidentified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify\nConfiguration/Setting, and Use Suitable Version are the predominant solutions.\nBased on the results, we discuss the potential areas of Copilot for\nenhancement, and provide the implications for the Copilot users, the Copilot\nteam, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent advancement of Artificial Intelligence (AI) and Large\nLanguage Models (LLMs), AI-based code generation tools become a practical\nsolution for software development. GitHub Copilot, the AI pair programmer,\nutilizes machine learning models trained on a large corpus of code snippets to\ngenerate code suggestions using natural language processing. Despite its\npopularity in software development, there is limited empirical evidence on the\nactual experiences of practitioners who work with Copilot. To this end, we\nconducted an empirical study to understand the problems that practitioners face\nwhen using Copilot, as well as their underlying causes and potential solutions.\nWe collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack\nOverflow posts. Our results reveal that (1) Operation Issue and Compatibility\nIssue are the most common problems faced by Copilot users, (2) Copilot Internal\nError, Network Connection Error, and Editor/IDE Compatibility Issue are\nidentified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify\nConfiguration/Setting, and Use Suitable Version are the predominant solutions.\nBased on the results, we discuss the potential areas of Copilot for\nenhancement, and provide the implications for the Copilot users, the Copilot\nteam, and researchers."
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Aakash Ahmad"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Muhammad Waseem"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Waseem"
                },
                "author": "Muhammad Waseem",
                "arxiv_comment": "Preprint accepted for publication in Journal of Systems and Software,\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01020v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01020v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06152v2",
                "updated": "2024-08-31T12:32:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    12,
                    32,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-12T13:48:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    48,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming"
                },
                "summary": "Neural enhancement through super-resolution (SR) deep neural networks (DNNs)\nopens up new possibilities for ultra-high-definition (UHD) live streaming over\nexisting encoding and networking infrastructure. Yet, the heavy SR DNN\ninference overhead leads to severe deployment challenges. To reduce the\noverhead, existing systems propose to apply DNN-based SR only on carefully\nselected anchor frames while upscaling non-anchor frames via the lightweight\nreusing-based SR approach. However, frame-level scheduling is coarse-grained\nand fails to deliver optimal efficiency. In this work, we propose Palantir, the\nfirst neural-enhanced UHD live streaming system with fine-grained patch-level\nscheduling. Two novel techniques are incorporated into Palantir to select the\nmost beneficial anchor patches and support latency-sensitive UHD live streaming\napplications. Firstly, under the guidance of our pioneering and theoretical\nanalysis, Palantir constructs a directed acyclic graph (DAG) for lightweight\nyet accurate SR quality estimation under any possible anchor patch set.\nSecondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation.\n  The evaluation results suggest that Palantir incurs a negligible scheduling\nlatency accounting for less than 5.7% of the end-to-end latency requirement.\nWhen compared to the naive method of applying DNN-based SR on all the frames,\nPalantir can reduce the SR DNN inference overhead by 20 times (or 60 times)\nwhile preserving 54.0-82.6% (or 32.8-64.0%) of the quality gain. When compared\nto the state-of-the-art real-time frame-level scheduling strategy, Palantir can\nreduce the SR DNN inference overhead by 80.1% at most (and 38.4% on average)\nwithout sacrificing the video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural enhancement through super-resolution (SR) deep neural networks (DNNs)\nopens up new possibilities for ultra-high-definition (UHD) live streaming over\nexisting encoding and networking infrastructure. Yet, the heavy SR DNN\ninference overhead leads to severe deployment challenges. To reduce the\noverhead, existing systems propose to apply DNN-based SR only on carefully\nselected anchor frames while upscaling non-anchor frames via the lightweight\nreusing-based SR approach. However, frame-level scheduling is coarse-grained\nand fails to deliver optimal efficiency. In this work, we propose Palantir, the\nfirst neural-enhanced UHD live streaming system with fine-grained patch-level\nscheduling. Two novel techniques are incorporated into Palantir to select the\nmost beneficial anchor patches and support latency-sensitive UHD live streaming\napplications. Firstly, under the guidance of our pioneering and theoretical\nanalysis, Palantir constructs a directed acyclic graph (DAG) for lightweight\nyet accurate SR quality estimation under any possible anchor patch set.\nSecondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation.\n  The evaluation results suggest that Palantir incurs a negligible scheduling\nlatency accounting for less than 5.7% of the end-to-end latency requirement.\nWhen compared to the naive method of applying DNN-based SR on all the frames,\nPalantir can reduce the SR DNN inference overhead by 20 times (or 60 times)\nwhile preserving 54.0-82.6% (or 32.8-64.0%) of the quality gain. When compared\nto the state-of-the-art real-time frame-level scheduling strategy, Palantir can\nreduce the SR DNN inference overhead by 80.1% at most (and 38.4% on average)\nwithout sacrificing the video quality."
                },
                "authors": [
                    {
                        "name": "Xinqi Jin"
                    },
                    {
                        "name": "Zhui Zhu"
                    },
                    {
                        "name": "Xikai Sun"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Kebin Liu"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yunhao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Liu"
                },
                "author": "Yunhao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17277v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17277v3",
                "updated": "2024-08-31T11:50:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    11,
                    50,
                    41,
                    5,
                    244,
                    0
                ],
                "published": "2023-09-29T14:30:03Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    14,
                    30,
                    3,
                    4,
                    272,
                    0
                ],
                "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind\n  Aware GPT-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind\n  Aware GPT-4"
                },
                "summary": "Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available."
                },
                "authors": [
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Paul Yoo"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17277v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19465v2",
                "updated": "2024-08-31T11:31:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    11,
                    31,
                    2,
                    5,
                    244,
                    0
                ],
                "published": "2024-02-29T18:55:06Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    55,
                    6,
                    3,
                    60,
                    0
                ],
                "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period\n  of Large Language Models"
                },
                "summary": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}."
                },
                "authors": [
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Accepted at ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12673v2",
                "updated": "2024-08-31T09:38:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    9,
                    38,
                    41,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-22T18:26:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    26,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A\n  Comprehensive Framework for Gradient Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A\n  Comprehensive Framework for Gradient Editing"
                },
                "summary": "Transferable adversarial attacks pose significant threats to deep neural\nnetworks, particularly in black-box scenarios where internal model information\nis inaccessible. Studying adversarial attack methods helps advance the\nperformance of defense mechanisms and explore model vulnerabilities. These\nmethods can uncover and exploit weaknesses in models, promoting the development\nof more robust architectures. However, current methods for transferable attacks\noften come with substantial computational costs, limiting their deployment and\napplication, especially in edge computing scenarios. Adversarial generative\nmodels, such as Generative Adversarial Networks (GANs), are characterized by\ntheir ability to generate samples without the need for retraining after an\ninitial training phase. GE-AdvGAN, a recent method for transferable adversarial\nattacks, is based on this principle. In this paper, we propose a novel general\nframework for gradient editing-based transferable attacks, named GE-AdvGAN+,\nwhich integrates nearly all mainstream attack methods to enhance\ntransferability while significantly reducing computational resource\nconsumption. Our experiments demonstrate the compatibility and effectiveness of\nour framework. Compared to the baseline AdvGAN, our best-performing method,\nGE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it\nsurpasses the latest competing algorithm, GE-AdvGAN, with an average ASR\nincrease of 5.9. The framework also exhibits enhanced computational efficiency,\nachieving 2217.7 FPS, outperforming traditional methods such as BIM and\nMI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at\nhttps://github.com/GEAdvGANP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable adversarial attacks pose significant threats to deep neural\nnetworks, particularly in black-box scenarios where internal model information\nis inaccessible. Studying adversarial attack methods helps advance the\nperformance of defense mechanisms and explore model vulnerabilities. These\nmethods can uncover and exploit weaknesses in models, promoting the development\nof more robust architectures. However, current methods for transferable attacks\noften come with substantial computational costs, limiting their deployment and\napplication, especially in edge computing scenarios. Adversarial generative\nmodels, such as Generative Adversarial Networks (GANs), are characterized by\ntheir ability to generate samples without the need for retraining after an\ninitial training phase. GE-AdvGAN, a recent method for transferable adversarial\nattacks, is based on this principle. In this paper, we propose a novel general\nframework for gradient editing-based transferable attacks, named GE-AdvGAN+,\nwhich integrates nearly all mainstream attack methods to enhance\ntransferability while significantly reducing computational resource\nconsumption. Our experiments demonstrate the compatibility and effectiveness of\nour framework. Compared to the baseline AdvGAN, our best-performing method,\nGE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it\nsurpasses the latest competing algorithm, GE-AdvGAN, with an average ASR\nincrease of 5.9. The framework also exhibits enhanced computational efficiency,\nachieving 2217.7 FPS, outperforming traditional methods such as BIM and\nMI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at\nhttps://github.com/GEAdvGANP"
                },
                "authors": [
                    {
                        "name": "Zhibo Jin"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Zhiyu Zhu"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Jianlong Zhou"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02270v3",
                "updated": "2024-08-31T07:30:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    7,
                    30,
                    59,
                    5,
                    244,
                    0
                ],
                "published": "2024-03-04T17:57:18Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    17,
                    57,
                    18,
                    0,
                    64,
                    0
                ],
                "title": "FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction"
                },
                "summary": "Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization. In the hope of fostering research in\nsummarization factuality evaluation, we release the code of our metric and our\nfactuality annotations of long-form summarization at\nhttps://github.com/Babelscape/FENICE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization. In the hope of fostering research in\nsummarization factuality evaluation, we release the code of our metric and our\nfactuality annotations of long-form summarization at\nhttps://github.com/Babelscape/FENICE."
                },
                "authors": [
                    {
                        "name": "Alessandro Scir√®"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "ACL 2024 camera ready. Code and data at\n  https://github.com/Babelscape/FENICE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18638v2",
                "updated": "2024-08-31T05:17:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    5,
                    17,
                    17,
                    5,
                    244,
                    0
                ],
                "published": "2024-05-28T22:45:28Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    22,
                    45,
                    28,
                    1,
                    149,
                    0
                ],
                "title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation\n  for Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation\n  for Generative Large Language Models"
                },
                "summary": "In this position paper, we argue that human evaluation of generative large\nlanguage models (LLMs) should be a multidisciplinary undertaking that draws\nupon insights from disciplines such as user experience research and human\nbehavioral psychology to ensure that the experimental design and results are\nreliable. The conclusions from these evaluations, thus, must consider factors\nsuch as usability, aesthetics, and cognitive biases. We highlight how cognitive\nbiases can conflate fluent information and truthfulness, and how cognitive\nuncertainty affects the reliability of rating scores such as Likert.\nFurthermore, the evaluation should differentiate the capabilities and\nweaknesses of increasingly powerful large language models -- which requires\neffective test sets. The scalability of human evaluation is also crucial to\nwider adoption. Hence, to design an effective human evaluation system in the\nage of generative NLP, we propose the ConSiDERS-The-Human evaluation framework\nconsisting of 6 pillars -- Consistency, Scoring Criteria, Differentiating, User\nExperience, Responsible, and Scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we argue that human evaluation of generative large\nlanguage models (LLMs) should be a multidisciplinary undertaking that draws\nupon insights from disciplines such as user experience research and human\nbehavioral psychology to ensure that the experimental design and results are\nreliable. The conclusions from these evaluations, thus, must consider factors\nsuch as usability, aesthetics, and cognitive biases. We highlight how cognitive\nbiases can conflate fluent information and truthfulness, and how cognitive\nuncertainty affects the reliability of rating scores such as Likert.\nFurthermore, the evaluation should differentiate the capabilities and\nweaknesses of increasingly powerful large language models -- which requires\neffective test sets. The scalability of human evaluation is also crucial to\nwider adoption. Hence, to design an effective human evaluation system in the\nage of generative NLP, we propose the ConSiDERS-The-Human evaluation framework\nconsisting of 6 pillars -- Consistency, Scoring Criteria, Differentiating, User\nExperience, Responsible, and Scalability."
                },
                "authors": [
                    {
                        "name": "Aparna Elangovan"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Sravan Bodapati"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "Accepted in ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12590v2",
                "updated": "2024-08-31T05:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    5,
                    12,
                    9,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-22T17:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations"
                },
                "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models."
                },
                "authors": [
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Krithika Ramakrishnan"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Lifu Tu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Anas Awadalla"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Accepted by ECCV24 AI4VA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01725v2",
                "updated": "2024-08-31T04:27:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    27,
                    8,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-03T09:40:26Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    9,
                    40,
                    26,
                    5,
                    216,
                    0
                ],
                "title": "The Drama Machine: Simulating Character Development with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Drama Machine: Simulating Character Development with LLM Agents"
                },
                "summary": "This paper explores use of multiple large language model (LLM) agents to\nsimulate complex, dynamic characters in dramatic scenarios. We introduce a\ndrama machine framework that coordinates interactions between LLM agents\nplaying different 'Ego' and 'Superego' psychological roles. In roleplay\nsimulations, this design allows intersubjective dialogue and intra-subjective\ninternal monologue to develop in parallel. We apply this framework to two\ndramatic scenarios - an interview and a detective story - and compare character\ndevelopment with and without the Superego's influence. Though exploratory,\nresults suggest this multi-agent approach can produce more nuanced, adaptive\nnarratives that evolve over a sequence of dialogical turns. We discuss\ndifferent modalities of LLM-based roleplay and character development, along\nwith what this might mean for conceptualization of AI subjectivity. The paper\nconcludes by considering how this approach opens possibilities for thinking of\nthe roles of internal conflict and social performativity in AI-based\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores use of multiple large language model (LLM) agents to\nsimulate complex, dynamic characters in dramatic scenarios. We introduce a\ndrama machine framework that coordinates interactions between LLM agents\nplaying different 'Ego' and 'Superego' psychological roles. In roleplay\nsimulations, this design allows intersubjective dialogue and intra-subjective\ninternal monologue to develop in parallel. We apply this framework to two\ndramatic scenarios - an interview and a detective story - and compare character\ndevelopment with and without the Superego's influence. Though exploratory,\nresults suggest this multi-agent approach can produce more nuanced, adaptive\nnarratives that evolve over a sequence of dialogical turns. We discuss\ndifferent modalities of LLM-based roleplay and character development, along\nwith what this might mean for conceptualization of AI subjectivity. The paper\nconcludes by considering how this approach opens possibilities for thinking of\nthe roles of internal conflict and social performativity in AI-based\nsimulation."
                },
                "authors": [
                    {
                        "name": "Liam Magee"
                    },
                    {
                        "name": "Vanicka Arora"
                    },
                    {
                        "name": "Gus Gollings"
                    },
                    {
                        "name": "Norma Lam-Saw"
                    }
                ],
                "author_detail": {
                    "name": "Norma Lam-Saw"
                },
                "author": "Norma Lam-Saw",
                "arxiv_comment": "28 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; J.5; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03886v2",
                "updated": "2024-08-30T23:40:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    23,
                    40,
                    5,
                    4,
                    243,
                    0
                ],
                "published": "2023-12-06T20:24:17Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    20,
                    24,
                    17,
                    2,
                    340,
                    0
                ],
                "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Fairness Impacts of Hardware Selection in Machine Learning"
                },
                "summary": "In the machine learning ecosystem, hardware selection is often regarded as a\nmere utility, overshadowed by the spotlight on algorithms and data. This\noversight is particularly problematic in contexts like ML-as-a-service\nplatforms, where users often lack control over the hardware used for model\ndeployment. How does the choice of hardware impact generalization properties?\nThis paper investigates the influence of hardware on the delicate balance\nbetween model performance and fairness. We demonstrate that hardware choices\ncan exacerbate existing disparities, attributing these discrepancies to\nvariations in gradient flows and loss surfaces across different demographic\ngroups. Through both theoretical and empirical analysis, the paper not only\nidentifies the underlying factors but also proposes an effective strategy for\nmitigating hardware-induced performance imbalances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the machine learning ecosystem, hardware selection is often regarded as a\nmere utility, overshadowed by the spotlight on algorithms and data. This\noversight is particularly problematic in contexts like ML-as-a-service\nplatforms, where users often lack control over the hardware used for model\ndeployment. How does the choice of hardware impact generalization properties?\nThis paper investigates the influence of hardware on the delicate balance\nbetween model performance and fairness. We demonstrate that hardware choices\ncan exacerbate existing disparities, attributing these discrepancies to\nvariations in gradient flows and loss surfaces across different demographic\ngroups. Through both theoretical and empirical analysis, the paper not only\nidentifies the underlying factors but also proposes an effective strategy for\nmitigating hardware-induced performance imbalances."
                },
                "authors": [
                    {
                        "name": "Sree Harsha Nelaturu"
                    },
                    {
                        "name": "Nishaanth Kanna Ravichandran"
                    },
                    {
                        "name": "Cuong Tran"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Ferdinando Fioretto"
                    }
                ],
                "author_detail": {
                    "name": "Ferdinando Fioretto"
                },
                "author": "Ferdinando Fioretto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04691v2",
                "updated": "2024-08-30T23:09:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    23,
                    9,
                    46,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-08T13:10:51Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    10,
                    51,
                    3,
                    221,
                    0
                ],
                "title": "Improving Relational Database Interactions with Large Language Models:\n  Column Descriptions and Their Impact on Text-to-SQL Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Relational Database Interactions with Large Language Models:\n  Column Descriptions and Their Impact on Text-to-SQL Performance"
                },
                "summary": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and Text-to-SQL models. This paper explores the use of large\nlanguage models (LLMs) to generate informative column descriptions as a\nsemantic layer for relational databases. Using the BIRD-Bench development set,\nwe created ColSQL, a dataset with gold-standard column descriptions generated\nand refined by LLMs and human annotators. We evaluated several\ninstruction-tuned models, finding that GPT-4o and Command R+ excelled in\ngenerating high-quality descriptions. Additionally, we applied an\nLLM-as-a-judge to evaluate model performance. Although this method does not\nalign well with human evaluations, we included it to explore its potential and\nto identify areas for improvement. More work is needed to improve the\nreliability of automatic evaluations for this task. We also find that detailed\ncolumn descriptions significantly improve Text-to-SQL execution accuracy,\nespecially when columns are uninformative. This study establishes LLMs as\neffective tools for generating detailed metadata, enhancing the usability of\nrelational databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and Text-to-SQL models. This paper explores the use of large\nlanguage models (LLMs) to generate informative column descriptions as a\nsemantic layer for relational databases. Using the BIRD-Bench development set,\nwe created ColSQL, a dataset with gold-standard column descriptions generated\nand refined by LLMs and human annotators. We evaluated several\ninstruction-tuned models, finding that GPT-4o and Command R+ excelled in\ngenerating high-quality descriptions. Additionally, we applied an\nLLM-as-a-judge to evaluate model performance. Although this method does not\nalign well with human evaluations, we included it to explore its potential and\nto identify areas for improvement. More work is needed to improve the\nreliability of automatic evaluations for this task. We also find that detailed\ncolumn descriptions significantly improve Text-to-SQL execution accuracy,\nespecially when columns are uninformative. This study establishes LLMs as\neffective tools for generating detailed metadata, enhancing the usability of\nrelational databases."
                },
                "authors": [
                    {
                        "name": "Niklas Wretblad"
                    },
                    {
                        "name": "Oskar Holmstr√∂m"
                    },
                    {
                        "name": "Erik Larsson"
                    },
                    {
                        "name": "Axel Wiks√§ter"
                    },
                    {
                        "name": "Oscar S√∂derlund"
                    },
                    {
                        "name": "Hjalmar √ñhman"
                    },
                    {
                        "name": "Ture Pont√©n"
                    },
                    {
                        "name": "Martin Forsberg"
                    },
                    {
                        "name": "Martin S√∂rme"
                    },
                    {
                        "name": "Fredrik Heintz"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Heintz"
                },
                "author": "Fredrik Heintz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.00012v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.00012v4",
                "updated": "2024-08-30T22:49:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    22,
                    49,
                    32,
                    4,
                    243,
                    0
                ],
                "published": "2023-06-21T19:34:16Z",
                "published_parsed": [
                    2023,
                    6,
                    21,
                    19,
                    34,
                    16,
                    2,
                    172,
                    0
                ],
                "title": "FlakyFix: Using Large Language Models for Predicting Flaky Test Fix\n  Categories and Test Code Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlakyFix: Using Large Language Models for Predicting Flaky Test Fix\n  Categories and Test Code Repair"
                },
                "summary": "Flaky tests are problematic because they non-deterministically pass or fail\nfor the same software version under test, causing confusion and wasting\ndevelopment effort. While machine learning models have been used to predict\nflakiness and its root causes, there is much less work on providing support to\nfix the problem. To address this gap, in this paper, we focus on predicting the\ntype of fix that is required to remove flakiness and then repair the test code\non that basis. We do this for a subset of flaky tests where the root cause of\nflakiness is in the test itself and not in the production code. One key idea is\nto guide the repair process with additional knowledge about the test's\nflakiness in the form of its predicted fix category. Thus, we first propose a\nframework that automatically generates labeled datasets for 13 fix categories\nand trains models to predict the fix category of a flaky test by analyzing the\ntest code only. Our experimental results using code models and few-shot\nlearning show that we can correctly predict most of the fix categories. To show\nthe usefulness of such fix category labels for automatically repairing\nflakiness, we augment the prompts of GPT-3.5 Turbo, a Large Language Model\n(LLM), with such extra knowledge to request repair suggestions. The results\nshow that our suggested fix category labels, complemented with in-context\nlearning, significantly enhance the capability of GPT-3.5 Turbo in generating\nfixes for flaky tests. Based on the execution and analysis of a sample of\nGPT-repaired flaky tests, we estimate that a large percentage of such repairs\n(roughly between 51% and 83%) can be expected to pass. For the failing repaired\ntests, on average, 16% of the test code needs to be further changed for them to\npass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flaky tests are problematic because they non-deterministically pass or fail\nfor the same software version under test, causing confusion and wasting\ndevelopment effort. While machine learning models have been used to predict\nflakiness and its root causes, there is much less work on providing support to\nfix the problem. To address this gap, in this paper, we focus on predicting the\ntype of fix that is required to remove flakiness and then repair the test code\non that basis. We do this for a subset of flaky tests where the root cause of\nflakiness is in the test itself and not in the production code. One key idea is\nto guide the repair process with additional knowledge about the test's\nflakiness in the form of its predicted fix category. Thus, we first propose a\nframework that automatically generates labeled datasets for 13 fix categories\nand trains models to predict the fix category of a flaky test by analyzing the\ntest code only. Our experimental results using code models and few-shot\nlearning show that we can correctly predict most of the fix categories. To show\nthe usefulness of such fix category labels for automatically repairing\nflakiness, we augment the prompts of GPT-3.5 Turbo, a Large Language Model\n(LLM), with such extra knowledge to request repair suggestions. The results\nshow that our suggested fix category labels, complemented with in-context\nlearning, significantly enhance the capability of GPT-3.5 Turbo in generating\nfixes for flaky tests. Based on the execution and analysis of a sample of\nGPT-repaired flaky tests, we estimate that a large percentage of such repairs\n(roughly between 51% and 83%) can be expected to pass. For the failing repaired\ntests, on average, 16% of the test code needs to be further changed for them to\npass."
                },
                "authors": [
                    {
                        "name": "Sakina Fatima"
                    },
                    {
                        "name": "Hadi Hemmati"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_comment": "26 pages, 20 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.00012v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.00012v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13402v2",
                "updated": "2024-08-30T20:57:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    20,
                    57,
                    39,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-23T23:00:19Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    23,
                    0,
                    19,
                    4,
                    236,
                    0
                ],
                "title": "LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!"
                },
                "summary": "Multimodal Large Language Models (MM-LLMs) have seen significant advancements\nin the last year, demonstrating impressive performance across tasks. However,\nto truly democratize AI, models must exhibit strong capabilities and be able to\nrun efficiently on small compute footprints accessible by most. Part of this\nquest, we introduce LLaVaOLMoBitnet1B - the first Ternary Multimodal LLM\ncapable of accepting Image(s)+Text inputs to produce coherent textual\nresponses. The model is fully open-sourced along with training scripts to\nencourage further research in this space. This accompanying technical report\nhighlights the training process, evaluation details, challenges associated with\nternary models and future opportunities. Link to the model:\nhttps://huggingface.co/IntelLabs/LlavaOLMoBitnet1B",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MM-LLMs) have seen significant advancements\nin the last year, demonstrating impressive performance across tasks. However,\nto truly democratize AI, models must exhibit strong capabilities and be able to\nrun efficiently on small compute footprints accessible by most. Part of this\nquest, we introduce LLaVaOLMoBitnet1B - the first Ternary Multimodal LLM\ncapable of accepting Image(s)+Text inputs to produce coherent textual\nresponses. The model is fully open-sourced along with training scripts to\nencourage further research in this space. This accompanying technical report\nhighlights the training process, evaluation details, challenges associated with\nternary models and future opportunities. Link to the model:\nhttps://huggingface.co/IntelLabs/LlavaOLMoBitnet1B"
                },
                "authors": [
                    {
                        "name": "Jainaveen Sundaram"
                    },
                    {
                        "name": "Ravi Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Iyer"
                },
                "author": "Ravi Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02946v2",
                "updated": "2024-08-30T20:22:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    20,
                    22,
                    18,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-06T04:14:29Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    4,
                    14,
                    29,
                    1,
                    219,
                    0
                ],
                "title": "Scaling Laws for Data Poisoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Data Poisoning in LLMs"
                },
                "summary": "Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\nsignificantly more quickly than smaller LLMs with even minimal data poisoning.\nThese results underscore the need for robust safeguards against data poisoning\nin larger LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\nsignificantly more quickly than smaller LLMs with even minimal data poisoning.\nThese results underscore the need for robust safeguards against data poisoning\nin larger LLMs."
                },
                "authors": [
                    {
                        "name": "Dillon Bowen"
                    },
                    {
                        "name": "Brendan Murphy"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "Kellin Pelrine"
                    }
                ],
                "author_detail": {
                    "name": "Kellin Pelrine"
                },
                "author": "Kellin Pelrine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05600v2",
                "updated": "2024-08-30T20:05:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    20,
                    5,
                    36,
                    4,
                    243,
                    0
                ],
                "published": "2024-06-09T00:23:20Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    0,
                    23,
                    20,
                    6,
                    161,
                    0
                ],
                "title": "61A-Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "61A-Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)"
                },
                "summary": "Chatbot interfaces for LLMs enable students to get immediate, interactive\nhelp on homework assignments, but even a thoughtfully-designed bot may not\nserve all pedagogical goals. In this paper, we report on the development and\ndeployment of a GPT-4-based interactive homework assistant (\"61A Bot\") for\nstudents in a large CS1 course; over 2000 students made over 100,000 requests\nof our bot across two semesters. Our assistant offers one-shot, contextual\nfeedback, primarily through a low-friction \"get feedback\" prompt within the\ncommand-line \"autograder\" our students already run to test their code. Our Bot\nwraps student code in a custom prompt that supports our pedagogical goals and\navoids providing solutions directly. We discuss our deployment and then analyze\nthe impacts of our Bot on students, primarily through student-reported feedback\nand tracking of student homework progress. We find reductions in\nhomework-related question rates in our course forum, as well as substantial\nreductions in homework completion time when our Bot is available. For students\nin the 50th-80th percentile, these reductions typically exceed 30 minutes per\nassignment, over 4 standard deviations faster than the mean in prior semesters.\nFinally, we conclude with a discussion of these observations, the potential\nimpacts on student learning, as well as other potential costs and benefits of\nAI assistance in CS1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot interfaces for LLMs enable students to get immediate, interactive\nhelp on homework assignments, but even a thoughtfully-designed bot may not\nserve all pedagogical goals. In this paper, we report on the development and\ndeployment of a GPT-4-based interactive homework assistant (\"61A Bot\") for\nstudents in a large CS1 course; over 2000 students made over 100,000 requests\nof our bot across two semesters. Our assistant offers one-shot, contextual\nfeedback, primarily through a low-friction \"get feedback\" prompt within the\ncommand-line \"autograder\" our students already run to test their code. Our Bot\nwraps student code in a custom prompt that supports our pedagogical goals and\navoids providing solutions directly. We discuss our deployment and then analyze\nthe impacts of our Bot on students, primarily through student-reported feedback\nand tracking of student homework progress. We find reductions in\nhomework-related question rates in our course forum, as well as substantial\nreductions in homework completion time when our Bot is available. For students\nin the 50th-80th percentile, these reductions typically exceed 30 minutes per\nassignment, over 4 standard deviations faster than the mean in prior semesters.\nFinally, we conclude with a discussion of these observations, the potential\nimpacts on student learning, as well as other potential costs and benefits of\nAI assistance in CS1."
                },
                "authors": [
                    {
                        "name": "J. D. Zamfirescu-Pereira"
                    },
                    {
                        "name": "Laryn Qi"
                    },
                    {
                        "name": "Bj√∂rn Hartmann"
                    },
                    {
                        "name": "John DeNero"
                    },
                    {
                        "name": "Narges Norouzi"
                    }
                ],
                "author_detail": {
                    "name": "Narges Norouzi"
                },
                "author": "Narges Norouzi",
                "arxiv_comment": "6 pages, 3 figures, 1 table, 2 pages of references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11286v2",
                "updated": "2024-08-30T19:17:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    17,
                    41,
                    4,
                    243,
                    0
                ],
                "published": "2024-05-18T13:21:14Z",
                "published_parsed": [
                    2024,
                    5,
                    18,
                    13,
                    21,
                    14,
                    5,
                    139,
                    0
                ],
                "title": "Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion"
                },
                "summary": "In recent years, there has been significant interest in creating 3D avatars\nand motions, driven by their diverse applications in areas like film-making,\nvideo games, AR/VR, and human-robot interaction. However, current efforts\nprimarily concentrate on either generating the 3D avatar mesh alone or\nproducing motion sequences, with integrating these two aspects proving to be a\npersistent challenge. Additionally, while avatar and motion generation\npredominantly target humans, extending these techniques to animals remains a\nsignificant challenge due to inadequate training data and methods. To bridge\nthese gaps, our paper presents three key contributions. Firstly, we proposed a\nnovel agent-based approach named Motion Avatar, which allows for the automatic\ngeneration of high-quality customizable human and animal avatars with motions\nthrough text queries. The method significantly advanced the progress in dynamic\n3D character generation. Secondly, we introduced a LLM planner that coordinates\nboth motion and avatar generation, which transforms a discriminative planning\ninto a customizable Q&A fashion. Lastly, we presented an animal motion dataset\nnamed Zoo-300K, comprising approximately 300,000 text-motion pairs across 65\nanimal categories and its building pipeline ZooGen, which serves as a valuable\nresource for the community. See project website\nhttps://steve-zeyu-zhang.github.io/MotionAvatar/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been significant interest in creating 3D avatars\nand motions, driven by their diverse applications in areas like film-making,\nvideo games, AR/VR, and human-robot interaction. However, current efforts\nprimarily concentrate on either generating the 3D avatar mesh alone or\nproducing motion sequences, with integrating these two aspects proving to be a\npersistent challenge. Additionally, while avatar and motion generation\npredominantly target humans, extending these techniques to animals remains a\nsignificant challenge due to inadequate training data and methods. To bridge\nthese gaps, our paper presents three key contributions. Firstly, we proposed a\nnovel agent-based approach named Motion Avatar, which allows for the automatic\ngeneration of high-quality customizable human and animal avatars with motions\nthrough text queries. The method significantly advanced the progress in dynamic\n3D character generation. Secondly, we introduced a LLM planner that coordinates\nboth motion and avatar generation, which transforms a discriminative planning\ninto a customizable Q&A fashion. Lastly, we presented an animal motion dataset\nnamed Zoo-300K, comprising approximately 300,000 text-motion pairs across 65\nanimal categories and its building pipeline ZooGen, which serves as a valuable\nresource for the community. See project website\nhttps://steve-zeyu-zhang.github.io/MotionAvatar/"
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Yiran Wang"
                    },
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Shiya Huang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "arxiv_comment": "Accepted to BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11934v2",
                "updated": "2024-08-30T18:54:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    54,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-21T18:37:08Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    37,
                    8,
                    2,
                    234,
                    0
                ],
                "title": "Decoupling Power Quality Issues in Grid-Microgrid Network Using\n  Microgrid Building Blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling Power Quality Issues in Grid-Microgrid Network Using\n  Microgrid Building Blocks"
                },
                "summary": "Microgrids are evolving as promising options to enhance reliability of the\nconnected transmission and distribution systems. Traditional design and\ndeployment of microgrids require significant engineering analysis. Microgrid\nBuilding Blocks (MBB), consisting of modular blocks that integrate seamlessly\nto form effective microgrids, is an enabling concept for faster and broader\nadoption of microgrids. Back-to-Back converter placed at the point of common\ncoupling of microgrid is an integral part of the MBB. This paper presents\napplications of MBB to decouple power quality issues in grid-microgrid network\nserving power quality sensitive loads such as data centers, new grid-edge\ntechnologies such as vehicle-to-grid generation, and serving electric vehicle\ncharging loads during evacuation before disaster events. Simulation results\nshow that MBB effectively decouples the power quality issues across networks\nand helps maintain good power quality in the power quality sensitive network\nbased on the operational scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microgrids are evolving as promising options to enhance reliability of the\nconnected transmission and distribution systems. Traditional design and\ndeployment of microgrids require significant engineering analysis. Microgrid\nBuilding Blocks (MBB), consisting of modular blocks that integrate seamlessly\nto form effective microgrids, is an enabling concept for faster and broader\nadoption of microgrids. Back-to-Back converter placed at the point of common\ncoupling of microgrid is an integral part of the MBB. This paper presents\napplications of MBB to decouple power quality issues in grid-microgrid network\nserving power quality sensitive loads such as data centers, new grid-edge\ntechnologies such as vehicle-to-grid generation, and serving electric vehicle\ncharging loads during evacuation before disaster events. Simulation results\nshow that MBB effectively decouples the power quality issues across networks\nand helps maintain good power quality in the power quality sensitive network\nbased on the operational scenario."
                },
                "authors": [
                    {
                        "name": "Samrat Acharya"
                    },
                    {
                        "name": "Priya Mana"
                    },
                    {
                        "name": "Hisham Mahmood"
                    },
                    {
                        "name": "Francis Tuffner"
                    },
                    {
                        "name": "Alok Kumar Bharati"
                    }
                ],
                "author_detail": {
                    "name": "Alok Kumar Bharati"
                },
                "author": "Alok Kumar Bharati",
                "arxiv_comment": "This paper is accepted for publication in IEEE IECON 2024, Chicago,\n  IL. The complete copyright version will be available on IEEE Xplore when the\n  conference proceedings are published",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04315v3",
                "updated": "2024-08-30T18:24:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    24,
                    27,
                    4,
                    243,
                    0
                ],
                "published": "2024-02-06T19:00:40Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    19,
                    0,
                    40,
                    1,
                    37,
                    0
                ],
                "title": "Training Language Models to Generate Text with Citations via\n  Fine-grained Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Models to Generate Text with Citations via\n  Fine-grained Rewards"
                },
                "summary": "While recent Large Language Models (LLMs) have proven useful in answering\nuser queries, they are prone to hallucination, and their responses often lack\ncredibility due to missing references to reliable sources. An intuitive\nsolution to these issues would be to include in-text citations referring to\nexternal documents as evidence. While previous works have directly prompted\nLLMs to generate in-text citations, their performances are far from\nsatisfactory, especially when it comes to smaller LLMs. In this work, we\npropose an effective training framework using fine-grained rewards to teach\nLLMs to generate highly supportive and relevant citations, while ensuring the\ncorrectness of their responses. We also conduct a systematic analysis of\napplying these fine-grained rewards to common LLM training strategies,\ndemonstrating its advantage over conventional practices. We conduct extensive\nexperiments on Question Answering (QA) datasets taken from the ALCE benchmark\nand validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the\nincorporation of fine-grained rewards achieves the best performance among the\nbaselines, even surpassing that of GPT-3.5-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent Large Language Models (LLMs) have proven useful in answering\nuser queries, they are prone to hallucination, and their responses often lack\ncredibility due to missing references to reliable sources. An intuitive\nsolution to these issues would be to include in-text citations referring to\nexternal documents as evidence. While previous works have directly prompted\nLLMs to generate in-text citations, their performances are far from\nsatisfactory, especially when it comes to smaller LLMs. In this work, we\npropose an effective training framework using fine-grained rewards to teach\nLLMs to generate highly supportive and relevant citations, while ensuring the\ncorrectness of their responses. We also conduct a systematic analysis of\napplying these fine-grained rewards to common LLM training strategies,\ndemonstrating its advantage over conventional practices. We conduct extensive\nexperiments on Question Answering (QA) datasets taken from the ALCE benchmark\nand validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the\nincorporation of fine-grained rewards achieves the best performance among the\nbaselines, even surpassing that of GPT-3.5-turbo."
                },
                "authors": [
                    {
                        "name": "Chengyu Huang"
                    },
                    {
                        "name": "Zeqiu Wu"
                    },
                    {
                        "name": "Yushi Hu"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "Accepted by ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17437v1",
                "updated": "2024-08-30T17:41:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T17:41:30Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    17,
                    41,
                    30,
                    4,
                    243,
                    0
                ],
                "title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists"
                },
                "summary": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif K√∂ksal"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Sch√ºtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Sch√ºtze"
                },
                "author": "Hinrich Sch√ºtze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]