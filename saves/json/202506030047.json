[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaños"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustín Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v1",
                "updated": "2025-05-30T13:32:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v2",
                "updated": "2025-05-30T11:43:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    43,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v1",
                "updated": "2025-05-30T08:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24221v1",
                "updated": "2025-05-30T05:17:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T05:17:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management"
                },
                "summary": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Wenzhe Zhu"
                    },
                    {
                        "name": "Yongkun Li"
                    },
                    {
                        "name": "Yinlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinlong Xu"
                },
                "author": "Yinlong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v1",
                "updated": "2025-05-30T02:03:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v1",
                "updated": "2025-05-30T00:46:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v3",
                "updated": "2025-05-30T00:36:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    36,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23970v1",
                "updated": "2025-05-29T19:52:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T19:52:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving"
                },
                "summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23938v1",
                "updated": "2025-05-29T18:41:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T18:41:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Forensic Investigation of the ChatGPT Windows Application"
                },
                "summary": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics."
                },
                "authors": [
                    {
                        "name": "Malithi Wanniarachchi Kankanamge"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Santiago Carmona"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v2",
                "updated": "2025-05-27T03:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    57,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v2",
                "updated": "2025-05-24T04:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    37,
                    34,
                    5,
                    144,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v1",
                "updated": "2025-05-23T18:46:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20325v1",
                "updated": "2025-05-23T18:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence"
                },
                "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v3",
                "updated": "2025-05-23T17:02:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v3",
                "updated": "2025-05-23T16:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    36,
                    12,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18013v1",
                "updated": "2025-05-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence"
                },
                "summary": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively."
                },
                "authors": [
                    {
                        "name": "Hanze Zhang"
                    },
                    {
                        "name": "Kaiming Wang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17934v1",
                "updated": "2025-05-23T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications"
                },
                "summary": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Łukasz Szustak"
                    },
                    {
                        "name": "László Környei"
                    },
                    {
                        "name": "Flavio Cesar Cunha Galeazzo"
                    },
                    {
                        "name": "Paweł Bratek"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Bratek"
                },
                "author": "Paweł Bratek",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18231v1",
                "updated": "2025-05-23T12:40:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache"
                },
                "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17787v1",
                "updated": "2025-05-23T12:00:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to GLSVLSI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v2",
                "updated": "2025-05-23T11:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    59,
                    22,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v4",
                "updated": "2025-05-23T10:45:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    45,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17694v1",
                "updated": "2025-05-23T10:03:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T10:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding"
                },
                "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Rui Ning"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v3",
                "updated": "2025-05-23T10:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    1,
                    57,
                    4,
                    143,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v3",
                "updated": "2025-05-23T09:33:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    33,
                    32,
                    4,
                    143,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "ACL25 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v2",
                "updated": "2025-05-23T08:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    12,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v2",
                "updated": "2025-05-23T07:07:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    7,
                    7,
                    29,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v2",
                "updated": "2025-05-23T04:58:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    58,
                    47,
                    4,
                    143,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values"
                },
                "summary": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Mohammadreza Nemati"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v1",
                "updated": "2025-05-22T22:54:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17272v1",
                "updated": "2025-05-22T20:39:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T20:39:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-Llama: Towards Extremely Efficient Hybrid Models"
                },
                "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v2",
                "updated": "2025-05-22T20:10:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    10,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16271v1",
                "updated": "2025-05-22T06:04:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T06:04:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms"
                },
                "summary": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime."
                },
                "authors": [
                    {
                        "name": "A. Chilingarian"
                    }
                ],
                "author_detail": {
                    "name": "A. Chilingarian"
                },
                "author": "A. Chilingarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16210v1",
                "updated": "2025-05-22T04:23:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T04:23:19Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
                },
                "authors": [
                    {
                        "name": "Zhihang Cai"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Zheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wei"
                },
                "author": "Zheng Wei",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v1",
                "updated": "2025-05-22T03:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16076v1",
                "updated": "2025-05-21T23:23:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T23:23:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models"
                },
                "summary": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url."
                },
                "authors": [
                    {
                        "name": "Jinhua Liang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v1",
                "updated": "2025-05-21T22:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v2",
                "updated": "2025-05-21T20:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    52,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache. Accepted\n  at CVPR eLVM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v2",
                "updated": "2025-05-21T20:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    42,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.24878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24878v1",
                "updated": "2025-05-30T17:59:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents"
                },
                "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL."
                },
                "authors": [
                    {
                        "name": "Yaxin Luo"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Jiacheng Cui"
                    },
                    {
                        "name": "Xiaohan Zhao"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24873v1",
                "updated": "2025-05-30T17:59:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    45,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:45Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    45,
                    4,
                    150,
                    0
                ],
                "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal"
                },
                "summary": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io."
                },
                "authors": [
                    {
                        "name": "Bojia Zi"
                    },
                    {
                        "name": "Weixuan Peng"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Jianan Wang"
                    },
                    {
                        "name": "Shihao Zhao"
                    },
                    {
                        "name": "Rong Xiao"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24872v1",
                "updated": "2025-05-30T17:59:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    43,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:43Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    43,
                    4,
                    150,
                    0
                ],
                "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners"
                },
                "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker."
                },
                "authors": [
                    {
                        "name": "Zilin Xiao"
                    },
                    {
                        "name": "Jaywon Koo"
                    },
                    {
                        "name": "Siru Ouyang"
                    },
                    {
                        "name": "Jefferson Hernandez"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Vicente Ordonez"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Ordonez"
                },
                "author": "Vicente Ordonez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24871v1",
                "updated": "2025-05-30T17:59:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    38,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."
                },
                "authors": [
                    {
                        "name": "Yiqing Liang"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Wenhao Ding"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "James Tompkin"
                    },
                    {
                        "name": "Mengdi Xu"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Laixi Shi"
                    },
                    {
                        "name": "Jiacheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jiacheng Zhu"
                },
                "author": "Jiacheng Zhu",
                "arxiv_comment": "Project Webpage: https://modomodo-rl.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24869v1",
                "updated": "2025-05-30T17:59:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    19,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:19Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    19,
                    4,
                    150,
                    0
                ],
                "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLVR: A Simple Language-based Video Reasoning Framework"
                },
                "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR."
                },
                "authors": [
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Yan-Bo Lin"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24859v1",
                "updated": "2025-05-30T17:57:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    57,
                    15,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:57:15Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    57,
                    15,
                    4,
                    150,
                    0
                ],
                "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive\n  Free-Form Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive\n  Free-Form Summarization"
                },
                "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks."
                },
                "authors": [
                    {
                        "name": "Joschka Braun"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    },
                    {
                        "name": "Seyed Ali Bahrainian"
                    }
                ],
                "author_detail": {
                    "name": "Seyed Ali Bahrainian"
                },
                "author": "Seyed Ali Bahrainian",
                "arxiv_comment": "29 pages, 21 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24858v1",
                "updated": "2025-05-30T17:54:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    54,
                    8,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:54:08Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    54,
                    8,
                    4,
                    150,
                    0
                ],
                "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs"
                },
                "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans."
                },
                "authors": [
                    {
                        "name": "Gabrielle Kaili-May Liu"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Avi Caciularu"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Tim G. J. Rudner"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13467v3",
                "updated": "2025-05-30T17:53:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    53,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2024-08-24T05:03:08Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    5,
                    3,
                    8,
                    5,
                    237,
                    0
                ],
                "title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs"
                },
                "summary": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is automatically improved through additional fine-tuning using\nextra similar data generated by the service LLM. This multi-turn process\nguarantees that the smaller model can eventually match or even surpass the\nservice LLM's capabilities in specific downstream tasks, offering a practical\nand scalable solution for managing AI deployments in constrained environments.\nExtensive experiments with leading-edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is automatically improved through additional fine-tuning using\nextra similar data generated by the service LLM. This multi-turn process\nguarantees that the smaller model can eventually match or even surpass the\nservice LLM's capabilities in specific downstream tasks, offering a practical\nand scalable solution for managing AI deployments in constrained environments.\nExtensive experiments with leading-edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo."
                },
                "authors": [
                    {
                        "name": "Chansung Park"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "The first three authors contributed equally to this work; Accepted by\n  ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02339v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02339v3",
                "updated": "2025-05-30T17:53:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    53,
                    6,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-04T14:18:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    18,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "Boosting Multimodal Reasoning with Automated Structured Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Multimodal Reasoning with Automated Structured Thinking"
                },
                "summary": "Multimodal large language models excel across diverse domains but struggle\nwith complex visual reasoning tasks. Current approaches aim to incorporate\nstructured thinking via two strategies: explicit search methods and\npost-training techniques. However, both approaches face significant\nlimitations: Search-based methods suffer from computational inefficiency due to\nextensive solution space exploration, while post-training methods require\nsubstantial data, computational resources, and often encounter training\ninstability. To address these limitations, we propose AStar, an\n\\textbf{A}utomated \\textbf{S}tructured \\textbf{t}hinking paradigm for\nmultimod\\textbf{a}l \\textbf{r}easoning. Our method introduces \"thought cards\",\na lightweight library of high-level reasoning patterns abstracted from 500\nprior samples using Monte Carlo Tree Search. For each test problem, AStar\nadaptively retrieves the optimal thought cards and seamlessly integrates these\nexternal explicit guidelines with the model's internal implicit reasoning\ncapabilities. Extensive experiments demonstrate AStar's effectiveness and\nefficiency: using only 500 prior samples and a 7B backbone, our training-free\nframework achieves 53.9$\\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)\nand 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that\nAStar generalizes beyond multimodal reasoning to visual perception and\nunderstanding domains, and serves as a plug-and-play test-time inference method\ncompatible with mainstream post-training techniques like GRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models excel across diverse domains but struggle\nwith complex visual reasoning tasks. Current approaches aim to incorporate\nstructured thinking via two strategies: explicit search methods and\npost-training techniques. However, both approaches face significant\nlimitations: Search-based methods suffer from computational inefficiency due to\nextensive solution space exploration, while post-training methods require\nsubstantial data, computational resources, and often encounter training\ninstability. To address these limitations, we propose AStar, an\n\\textbf{A}utomated \\textbf{S}tructured \\textbf{t}hinking paradigm for\nmultimod\\textbf{a}l \\textbf{r}easoning. Our method introduces \"thought cards\",\na lightweight library of high-level reasoning patterns abstracted from 500\nprior samples using Monte Carlo Tree Search. For each test problem, AStar\nadaptively retrieves the optimal thought cards and seamlessly integrates these\nexternal explicit guidelines with the model's internal implicit reasoning\ncapabilities. Extensive experiments demonstrate AStar's effectiveness and\nefficiency: using only 500 prior samples and a 7B backbone, our training-free\nframework achieves 53.9$\\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)\nand 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that\nAStar generalizes beyond multimodal reasoning to visual perception and\nunderstanding domains, and serves as a plug-and-play test-time inference method\ncompatible with mainstream post-training techniques like GRPO."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Fangrui Lv"
                    },
                    {
                        "name": "Ruihan Jin"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02339v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02339v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24852v1",
                "updated": "2025-05-30T17:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    49,
                    30,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:49:30Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    49,
                    30,
                    4,
                    150,
                    0
                ],
                "title": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for\n  End-to-End Few-Shot and Continual Learning from Sequential Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for\n  End-to-End Few-Shot and Continual Learning from Sequential Data"
                },
                "summary": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W."
                },
                "authors": [
                    {
                        "name": "Douwe den Blanken"
                    },
                    {
                        "name": "Charlotte Frenkel"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Frenkel"
                },
                "author": "Charlotte Frenkel",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; B.6.0; B.7.0; I.2.6; B.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24850v1",
                "updated": "2025-05-30T17:47:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    47,
                    17,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:47:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    47,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning"
                },
                "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data."
                },
                "authors": [
                    {
                        "name": "Shuyao Xu"
                    },
                    {
                        "name": "Cheng Peng"
                    },
                    {
                        "name": "Jiangxuan Long"
                    },
                    {
                        "name": "Weidi Xu"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "author": "Yuan Qi",
                "arxiv_comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00459v2",
                "updated": "2025-05-30T17:45:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    45,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-01T06:34:02Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    34,
                    2,
                    1,
                    91,
                    0
                ],
                "title": "Graphical Models and Efficient Inference Methods for Multivariate Phase\n  Probability Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical Models and Efficient Inference Methods for Multivariate Phase\n  Probability Distributions"
                },
                "summary": "Multivariate phase relationships are important to characterize and understand\nnumerous physical, biological, and chemical systems, from electromagnetic waves\nto neural oscillations. These systems exhibit complex spatiotemporal dynamics\nand intricate interdependencies among their constituent elements. While\nclassical models of multivariate phase relationships, such as the wave equation\nand Kuramoto model, give theoretical models to describe phenomena, the\ndevelopment of statistical tools for hypothesis testing and inference for\nmultivariate phase relationships in complex systems remains limited. This paper\nintroduces a novel probabilistic modeling framework to characterize\nmultivariate phase relationships, with wave-like phenomena serving as a key\nexample. This approach describes spatial patterns and interactions between\noscillators through a pairwise exponential family distribution. Building upon\nthe literature of graphical model inference, including methods like Ising\nmodels, graphical lasso, and interaction screening, this work bridges the gap\nbetween classical wave dynamics and modern statistical approaches. Efficient\ninference methods are introduced, leveraging the Chow-Liu algorithm for\ndirected tree approximations and interaction screening for general graphical\nmodels. Simulated experiments demonstrate the utility of these methods for\nuncovering wave properties and sparse interaction structures, highlighting\ntheir applicability to diverse scientific domains. This framework establishes a\nnew paradigm for statistical modeling of multivariate phase relationships,\nproviding a powerful toolset for exploring the complexity of these systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate phase relationships are important to characterize and understand\nnumerous physical, biological, and chemical systems, from electromagnetic waves\nto neural oscillations. These systems exhibit complex spatiotemporal dynamics\nand intricate interdependencies among their constituent elements. While\nclassical models of multivariate phase relationships, such as the wave equation\nand Kuramoto model, give theoretical models to describe phenomena, the\ndevelopment of statistical tools for hypothesis testing and inference for\nmultivariate phase relationships in complex systems remains limited. This paper\nintroduces a novel probabilistic modeling framework to characterize\nmultivariate phase relationships, with wave-like phenomena serving as a key\nexample. This approach describes spatial patterns and interactions between\noscillators through a pairwise exponential family distribution. Building upon\nthe literature of graphical model inference, including methods like Ising\nmodels, graphical lasso, and interaction screening, this work bridges the gap\nbetween classical wave dynamics and modern statistical approaches. Efficient\ninference methods are introduced, leveraging the Chow-Liu algorithm for\ndirected tree approximations and interaction screening for general graphical\nmodels. Simulated experiments demonstrate the utility of these methods for\nuncovering wave properties and sparse interaction structures, highlighting\ntheir applicability to diverse scientific domains. This framework establishes a\nnew paradigm for statistical modeling of multivariate phase relationships,\nproviding a powerful toolset for exploring the complexity of these systems."
                },
                "authors": [
                    {
                        "name": "Andrew S. Perley"
                    },
                    {
                        "name": "Todd P. Coleman"
                    }
                ],
                "author_detail": {
                    "name": "Todd P. Coleman"
                },
                "author": "Todd P. Coleman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24846v1",
                "updated": "2025-05-30T17:44:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    44,
                    28,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:44:28Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    44,
                    28,
                    4,
                    150,
                    0
                ],
                "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning"
                },
                "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization."
                },
                "authors": [
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Han Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhao"
                },
                "author": "Han Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08972v2",
                "updated": "2025-05-30T17:43:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    43,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-12T06:08:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    8,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios"
                },
                "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena."
                },
                "authors": [
                    {
                        "name": "Ruiwen Zhou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24842v1",
                "updated": "2025-05-30T17:41:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    41,
                    58,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:41:58Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    41,
                    58,
                    4,
                    150,
                    0
                ],
                "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models"
                },
                "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24840v1",
                "updated": "2025-05-30T17:40:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    40,
                    46,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:40:46Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    40,
                    46,
                    4,
                    150,
                    0
                ],
                "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are\n  the Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are\n  the Bottleneck"
                },
                "summary": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge."
                },
                "authors": [
                    {
                        "name": "Yuwen Tan"
                    },
                    {
                        "name": "Yuan Qing"
                    },
                    {
                        "name": "Boqing Gong"
                    }
                ],
                "author_detail": {
                    "name": "Boqing Gong"
                },
                "author": "Boqing Gong",
                "arxiv_comment": "28 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24838v1",
                "updated": "2025-05-30T17:39:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:39:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software"
                },
                "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies."
                },
                "authors": [
                    {
                        "name": "Brandon Man"
                    },
                    {
                        "name": "Ghadi Nehme"
                    },
                    {
                        "name": "Md Ferdous Alam"
                    },
                    {
                        "name": "Faez Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Faez Ahmed"
                },
                "author": "Faez Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02355v3",
                "updated": "2025-05-30T17:39:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    6,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-04T18:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization"
                },
                "summary": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Alexandre Marques"
                    },
                    {
                        "name": "Shubhra Pandit"
                    },
                    {
                        "name": "Mark Kurtz"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24832v1",
                "updated": "2025-05-30T17:34:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    34,
                    3,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:34:03Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    34,
                    3,
                    4,
                    150,
                    0
                ],
                "title": "How much do language models memorize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much do language models memorize?"
                },
                "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference."
                },
                "authors": [
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Chawin Sitawarin"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Narine Kokhlikyan"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Mahloujifar"
                },
                "author": "Saeed Mahloujifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24830v1",
                "updated": "2025-05-30T17:33:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    33,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:33:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    33,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs"
                },
                "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Annika Domres"
                    },
                    {
                        "name": "Mai Nguyen"
                    },
                    {
                        "name": "Rebecca Moser"
                    },
                    {
                        "name": "Jana Nano"
                    },
                    {
                        "name": "Felix Busch"
                    },
                    {
                        "name": "Lisa C. Adams"
                    },
                    {
                        "name": "Keno K. Bressem"
                    },
                    {
                        "name": "Denise Bernhardt"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Kai J. Borm"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Jan C. Peeken"
                    }
                ],
                "author_detail": {
                    "name": "Jan C. Peeken"
                },
                "author": "Jan C. Peeken",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09284v3",
                "updated": "2025-05-30T17:30:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-13T12:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "SparQLe: Speech Queries to Text Translation Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQLe: Speech Queries to Text Translation Through LLMs"
                },
                "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications."
                },
                "authors": [
                    {
                        "name": "Amirbek Djanibekov"
                    },
                    {
                        "name": "Hanan Aldarmaki"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Aldarmaki"
                },
                "author": "Hanan Aldarmaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24826v1",
                "updated": "2025-05-30T17:30:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:30:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text"
                },
                "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q."
                },
                "authors": [
                    {
                        "name": "Li yunhan"
                    },
                    {
                        "name": "Wu gengshen"
                    }
                ],
                "author_detail": {
                    "name": "Wu gengshen"
                },
                "author": "Wu gengshen",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16181v2",
                "updated": "2025-05-30T17:29:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    29,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-22T18:14:43Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    18,
                    14,
                    43,
                    1,
                    112,
                    0
                ],
                "title": "CLIP-IT: CLIP-based Pairing for Histology Images Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP-IT: CLIP-based Pairing for Histology Images Classification"
                },
                "summary": "Multimodal learning has shown significant promise for improving medical image\nanalysis by integrating information from complementary data sources. This is\nwidely employed for training vision-language models (VLMs) for cancer detection\nbased on histology images and text reports. However, one of the main\nlimitations in training these VLMs is the requirement for large paired\ndatasets, raising concerns over privacy, and data collection, annotation, and\nmaintenance costs. To address this challenge, we introduce CLIP-IT method to\ntrain a vision backbone model to classify histology images by pairing them with\nprivileged textual information from an external source. At first, the modality\npairing step relies on a CLIP-based model to match histology images with\nsemantically relevant textual report data from external sources, creating an\naugmented multimodal dataset without the need for manually paired samples.\nThen, we propose a multimodal training procedure that distills the knowledge\nfrom the paired text modality to the unimodal image classifier for enhanced\nperformance without the need for the textual data during inference. A\nparameter-efficient fine-tuning method is used to efficiently address the\nmisalignment between the main (image) and paired (text) modalities. During\ninference, the improved unimodal histology classifier is used, with only\nminimal additional computational complexity. Our experiments on challenging\nPCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a\ncost-effective approach to leverage privileged textual information and\noutperform unimodal classifiers for histology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal learning has shown significant promise for improving medical image\nanalysis by integrating information from complementary data sources. This is\nwidely employed for training vision-language models (VLMs) for cancer detection\nbased on histology images and text reports. However, one of the main\nlimitations in training these VLMs is the requirement for large paired\ndatasets, raising concerns over privacy, and data collection, annotation, and\nmaintenance costs. To address this challenge, we introduce CLIP-IT method to\ntrain a vision backbone model to classify histology images by pairing them with\nprivileged textual information from an external source. At first, the modality\npairing step relies on a CLIP-based model to match histology images with\nsemantically relevant textual report data from external sources, creating an\naugmented multimodal dataset without the need for manually paired samples.\nThen, we propose a multimodal training procedure that distills the knowledge\nfrom the paired text modality to the unimodal image classifier for enhanced\nperformance without the need for the textual data during inference. A\nparameter-efficient fine-tuning method is used to efficiently address the\nmisalignment between the main (image) and paired (text) modalities. During\ninference, the improved unimodal histology classifier is used, with only\nminimal additional computational complexity. Our experiments on challenging\nPCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a\ncost-effective approach to leverage privileged textual information and\noutperform unimodal classifiers for histology."
                },
                "authors": [
                    {
                        "name": "Banafsheh Karimian"
                    },
                    {
                        "name": "Giulia Avanzato"
                    },
                    {
                        "name": "Soufian Belharbi"
                    },
                    {
                        "name": "Luke McCaffrey"
                    },
                    {
                        "name": "Mohammadhadi Shateri"
                    },
                    {
                        "name": "Eric Granger"
                    }
                ],
                "author_detail": {
                    "name": "Eric Granger"
                },
                "author": "Eric Granger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24823v1",
                "updated": "2025-05-30T17:25:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    20,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:25:20Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    20,
                    4,
                    150,
                    0
                ],
                "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning."
                },
                "authors": [
                    {
                        "name": "Yinggan Xu"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "name": "Changnan Peng"
                    },
                    {
                        "name": "Di Luo"
                    }
                ],
                "author_detail": {
                    "name": "Di Luo"
                },
                "author": "Di Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05368v2",
                "updated": "2025-05-30T17:25:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    4,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-07T22:41:31Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    41,
                    31,
                    4,
                    38,
                    0
                ],
                "title": "Otter: Generating Tests from Issues to Validate SWE Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Otter: Generating Tests from Issues to Validate SWE Patches"
                },
                "summary": "While there has been plenty of work on generating tests from existing code,\nthere has been limited work on generating tests from issues. A correct test\nmust validate the code patch that resolves the issue. This paper focuses on the\nscenario where that code patch does not yet exist. Doing so supports two major\nuse-cases. First, it supports TDD (test-driven development), the discipline of\n\"test first, write code later\" that has well-documented benefits for human\nsoftware engineers. Second, it also validates SWE (software engineering)\nagents, which generate code patches for resolving issues. This paper introduces\nTDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an\nLLM-based solution for this task. Otter augments LLMs with rule-based analysis\nto check and repair their outputs, and introduces a novel self-reflective\naction planner. Experiments show Otter outperforming state-of-the-art systems\nfor generating tests from issues, in addition to enhancing systems that\ngenerate patches from issues. We hope that Otter helps make developers more\nproductive at resolving issues and leads to more robust, well-tested code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been plenty of work on generating tests from existing code,\nthere has been limited work on generating tests from issues. A correct test\nmust validate the code patch that resolves the issue. This paper focuses on the\nscenario where that code patch does not yet exist. Doing so supports two major\nuse-cases. First, it supports TDD (test-driven development), the discipline of\n\"test first, write code later\" that has well-documented benefits for human\nsoftware engineers. Second, it also validates SWE (software engineering)\nagents, which generate code patches for resolving issues. This paper introduces\nTDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an\nLLM-based solution for this task. Otter augments LLMs with rule-based analysis\nto check and repair their outputs, and introduces a novel self-reflective\naction planner. Experiments show Otter outperforming state-of-the-art systems\nfor generating tests from issues, in addition to enhancing systems that\ngenerate patches from issues. We hope that Otter helps make developers more\nproductive at resolving issues and leads to more robust, well-tested code."
                },
                "authors": [
                    {
                        "name": "Toufique Ahmed"
                    },
                    {
                        "name": "Jatin Ganhotra"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Avraham Shinnar"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "arxiv_comment": "Accepted to the main technical track of the International Conference\n  on Machine Learning (ICML), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13001v2",
                "updated": "2025-05-30T17:21:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    21,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-18T16:21:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations"
                },
                "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Muneeb Khan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "Accepted at ACL 2025 (Findings)",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v2",
                "updated": "2025-05-30T17:20:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    20,
                    43,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24816v1",
                "updated": "2025-05-30T17:19:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    19,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:19:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    19,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free\n  Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free\n  Class-Incremental Learning"
                },
                "summary": "Class-Incremental Learning (CIL) aims to learn new classes sequentially while\nretaining the knowledge of previously learned classes. Recently, pre-trained\nmodels (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown\nremarkable performance in rehearsal-free CIL without requiring exemplars from\nprevious tasks. However, existing adapter-based methods, which incorporate\nlightweight learnable modules into PTMs for CIL, create new adapters for each\nnew task, leading to both parameter redundancy and failure to leverage shared\nknowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation\n(CL-LoRA), which introduces a novel dual-adapter architecture combining\n\\textbf{task-shared adapters} to learn cross-task knowledge and\n\\textbf{task-specific adapters} to capture unique features of each new task.\nSpecifically, the shared adapters utilize random orthogonal matrices and\nleverage knowledge distillation with gradient reassignment to preserve\nessential shared knowledge. In addition, we introduce learnable block-wise\nweights for task-specific adapters, which mitigate inter-task interference\nwhile maintaining the model's plasticity. We demonstrate CL-LoRA consistently\nachieves promising performance under multiple benchmarks with reduced training\nand inference computation, establishing a more efficient and scalable paradigm\nfor continual learning with pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) aims to learn new classes sequentially while\nretaining the knowledge of previously learned classes. Recently, pre-trained\nmodels (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown\nremarkable performance in rehearsal-free CIL without requiring exemplars from\nprevious tasks. However, existing adapter-based methods, which incorporate\nlightweight learnable modules into PTMs for CIL, create new adapters for each\nnew task, leading to both parameter redundancy and failure to leverage shared\nknowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation\n(CL-LoRA), which introduces a novel dual-adapter architecture combining\n\\textbf{task-shared adapters} to learn cross-task knowledge and\n\\textbf{task-specific adapters} to capture unique features of each new task.\nSpecifically, the shared adapters utilize random orthogonal matrices and\nleverage knowledge distillation with gradient reassignment to preserve\nessential shared knowledge. In addition, we introduce learnable block-wise\nweights for task-specific adapters, which mitigate inter-task interference\nwhile maintaining the model's plasticity. We demonstrate CL-LoRA consistently\nachieves promising performance under multiple benchmarks with reduced training\nand inference computation, establishing a more efficient and scalable paradigm\nfor continual learning with pre-trained models."
                },
                "authors": [
                    {
                        "name": "Jiangpeng He"
                    },
                    {
                        "name": "Zhihao Duan"
                    },
                    {
                        "name": "Fengqing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Fengqing Zhu"
                },
                "author": "Fengqing Zhu",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11361v3",
                "updated": "2025-05-30T17:17:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    17,
                    11,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-17T02:18:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    18,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment"
                },
                "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ashmal Vayani"
                    },
                    {
                        "name": "Aditya Jain"
                    },
                    {
                        "name": "Aravind Narayanan"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    },
                    {
                        "name": "Rizwan Qureshi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06824v2",
                "updated": "2025-05-30T17:17:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    17,
                    4,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-11T09:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs"
                },
                "summary": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs."
                },
                "authors": [
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Amal Zouaq"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14449v3",
                "updated": "2025-05-30T17:10:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    10,
                    8,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-20T14:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"
                },
                "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 28% with less than a 2% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 4.6% improvement in fairness metrics with a\ndrop of less than 3.6% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential when explicit demographic information is unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 28% with less than a 2% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 4.6% improvement in fairness metrics with a\ndrop of less than 3.6% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential when explicit demographic information is unavailable."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Huang-Cheng Chou"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22918v2",
                "updated": "2025-05-30T17:09:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    9,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-28T22:39:12Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    39,
                    12,
                    2,
                    148,
                    0
                ],
                "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape"
                },
                "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}"
                },
                "authors": [
                    {
                        "name": "Ruichen Chen"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Liyao Jiang"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Submitted before obtaining agreement of all authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17333v2",
                "updated": "2025-05-30T17:09:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    9,
                    31,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-24T17:05:14Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    5,
                    14,
                    0,
                    55,
                    0
                ],
                "title": "Jet rates in Higgs boson decay at third order in QCD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jet rates in Higgs boson decay at third order in QCD"
                },
                "summary": "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables."
                },
                "authors": [
                    {
                        "name": "Elliot Fox"
                    },
                    {
                        "name": "Aude Gehrmann-De Ridder"
                    },
                    {
                        "name": "Thomas Gehrmann"
                    },
                    {
                        "name": "Nigel Glover"
                    },
                    {
                        "name": "Matteo Marcoli"
                    },
                    {
                        "name": "Christian T. Preuss"
                    }
                ],
                "author_detail": {
                    "name": "Christian T. Preuss"
                },
                "author": "Christian T. Preuss",
                "arxiv_comment": "Version 2 to match journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24803v1",
                "updated": "2025-05-30T17:08:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:08:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Guiding Generative Storytelling with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Generative Storytelling with Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."
                },
                "authors": [
                    {
                        "name": "Zhijun Pan"
                    },
                    {
                        "name": "Antonios Andronis"
                    },
                    {
                        "name": "Eva Hayek"
                    },
                    {
                        "name": "Oscar AP Wilkinson"
                    },
                    {
                        "name": "Ilya Lasy"
                    },
                    {
                        "name": "Annette Parry"
                    },
                    {
                        "name": "Guy Gadney"
                    },
                    {
                        "name": "Tim J. Smith"
                    },
                    {
                        "name": "Mick Grierson"
                    }
                ],
                "author_detail": {
                    "name": "Mick Grierson"
                },
                "author": "Mick Grierson",
                "arxiv_comment": "This manuscript was submitted for peer review in January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14248v2",
                "updated": "2025-05-30T17:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    1,
                    57,
                    4,
                    150,
                    0
                ],
                "published": "2024-10-18T07:52:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    52,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models"
                },
                "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models."
                },
                "authors": [
                    {
                        "name": "Olga Loginova"
                    },
                    {
                        "name": "Oleksandr Bezrukov"
                    },
                    {
                        "name": "Ravi Shekhar"
                    },
                    {
                        "name": "Alexey Kravets"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Kravets"
                },
                "author": "Alexey Kravets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11404v2",
                "updated": "2025-05-30T16:59:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    59,
                    23,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-17T03:42:28Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    42,
                    28,
                    0,
                    48,
                    0
                ],
                "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large\n  Language Models"
                },
                "summary": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning."
                },
                "authors": [
                    {
                        "name": "Hanxing Ding"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24795v1",
                "updated": "2025-05-30T16:56:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    56,
                    47,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:56:47Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    56,
                    47,
                    4,
                    150,
                    0
                ],
                "title": "Early warning signals in rumor models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early warning signals in rumor models"
                },
                "summary": "We study the emission and control of a rumor using the modified Maki-Thomson\nmodel. A key challenge in social networks is distinguishing between natural\nincreases in transmissibility and artificial injections of rumor spreaders,\nsuch as through broadcast events or astroturfing. Using stochastic simulations,\nwe compare two scenarios: one with organic growth in transmissibility, and\nanother with externally injected spreaders. Although both lead to high\nautocorrelation, only the organic growth produces oscillatory patterns in\nautocorrelation at multiple lags, an effect we can analytically explain using\nthe N-intertwined mean-field (NIMFA) approximation. This distinction offers a\npractical tool to identify the origin of rumor virality and also infer its\ntransmissibility. Our approach is validated analytically and tested on\nreal-world data from Twitter during the announcement of the Higgs boson\ndiscovery. In addition to detection, we also explore control strategies. We\nshow that the average lifetime of a rumor can be manipulated through targeted\ninterventions: placing spreaders at specific locations in the network.\nDepending on their placement, these interventions can either extend or shorten\nthe lifespan of the rumor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the emission and control of a rumor using the modified Maki-Thomson\nmodel. A key challenge in social networks is distinguishing between natural\nincreases in transmissibility and artificial injections of rumor spreaders,\nsuch as through broadcast events or astroturfing. Using stochastic simulations,\nwe compare two scenarios: one with organic growth in transmissibility, and\nanother with externally injected spreaders. Although both lead to high\nautocorrelation, only the organic growth produces oscillatory patterns in\nautocorrelation at multiple lags, an effect we can analytically explain using\nthe N-intertwined mean-field (NIMFA) approximation. This distinction offers a\npractical tool to identify the origin of rumor virality and also infer its\ntransmissibility. Our approach is validated analytically and tested on\nreal-world data from Twitter during the announcement of the Higgs boson\ndiscovery. In addition to detection, we also explore control strategies. We\nshow that the average lifetime of a rumor can be manipulated through targeted\ninterventions: placing spreaders at specific locations in the network.\nDepending on their placement, these interventions can either extend or shorten\nthe lifespan of the rumor."
                },
                "authors": [
                    {
                        "name": "Eva Rifà"
                    },
                    {
                        "name": "Julian Vicens"
                    },
                    {
                        "name": "Emanuele Cozzo"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Cozzo"
                },
                "author": "Emanuele Cozzo",
                "arxiv_comment": "Working paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24791v1",
                "updated": "2025-05-30T16:53:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    53,
                    15,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:53:15Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    53,
                    15,
                    4,
                    150,
                    0
                ],
                "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective\n  Jacobi Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Acceleration of Autoregressive Normalizing Flows by Selective\n  Jacobi Decoding"
                },
                "summary": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity."
                },
                "authors": [
                    {
                        "name": "Jiaru Zhang"
                    },
                    {
                        "name": "Juanwu Lu"
                    },
                    {
                        "name": "Ziran Wang"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24788v1",
                "updated": "2025-05-30T16:48:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    48,
                    38,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:48:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    48,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "Drop Dropout on Single-Epoch Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drop Dropout on Single-Epoch Language Model Pretraining"
                },
                "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining."
                },
                "authors": [
                    {
                        "name": "Houjun Liu"
                    },
                    {
                        "name": "John Bauer"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "arxiv_comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24785v1",
                "updated": "2025-05-30T16:46:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    29,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:46:29Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    29,
                    4,
                    150,
                    0
                ],
                "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXP-Bench: Can AI Conduct AI Research Experiments?"
                },
                "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench."
                },
                "authors": [
                    {
                        "name": "Patrick Tser Jern Kon"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Xinyi Zhu"
                    },
                    {
                        "name": "Qiuyi Ding"
                    },
                    {
                        "name": "Jingjia Peng"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Yibo Huang"
                    },
                    {
                        "name": "Yiming Qiu"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ang Chen"
                },
                "author": "Ang Chen",
                "arxiv_comment": "45 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24784v1",
                "updated": "2025-05-30T16:46:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    20,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:46:20Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    20,
                    4,
                    150,
                    0
                ],
                "title": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric\n  Models"
                },
                "summary": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art\nperformance in various domains, but struggle with data efficiency compared to\nhuman learning, which leverages core priors about objects and their\ninteractions. Active inference offers a principled framework for integrating\nsensory information with prior knowledge to learn a world model and quantify\nthe uncertainty of its own beliefs and predictions. However, active inference\nmodels are usually crafted for a single task with bespoke knowledge, so they\nlack the domain flexibility typical of DRL approaches. To bridge this gap, we\npropose a novel architecture that integrates a minimal yet expressive set of\ncore priors about object-centric dynamics and interactions to accelerate\nlearning in low-data regimes. The resulting approach, which we call AXIOM,\ncombines the usual data efficiency and interpretability of Bayesian approaches\nwith the across-task generalization usually associated with DRL. AXIOM\nrepresents scenes as compositions of objects, whose dynamics are modeled as\npiecewise linear trajectories that capture sparse object-object interactions.\nThe structure of the generative model is expanded online by growing and\nlearning mixture models from single events and periodically refined through\nBayesian model reduction to induce generalization. AXIOM masters various games\nwithin only 10,000 interaction steps, with both a small number of parameters\ncompared to DRL, and without the computational expense of gradient-based\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art\nperformance in various domains, but struggle with data efficiency compared to\nhuman learning, which leverages core priors about objects and their\ninteractions. Active inference offers a principled framework for integrating\nsensory information with prior knowledge to learn a world model and quantify\nthe uncertainty of its own beliefs and predictions. However, active inference\nmodels are usually crafted for a single task with bespoke knowledge, so they\nlack the domain flexibility typical of DRL approaches. To bridge this gap, we\npropose a novel architecture that integrates a minimal yet expressive set of\ncore priors about object-centric dynamics and interactions to accelerate\nlearning in low-data regimes. The resulting approach, which we call AXIOM,\ncombines the usual data efficiency and interpretability of Bayesian approaches\nwith the across-task generalization usually associated with DRL. AXIOM\nrepresents scenes as compositions of objects, whose dynamics are modeled as\npiecewise linear trajectories that capture sparse object-object interactions.\nThe structure of the generative model is expanded online by growing and\nlearning mixture models from single events and periodically refined through\nBayesian model reduction to induce generalization. AXIOM masters various games\nwithin only 10,000 interaction steps, with both a small number of parameters\ncompared to DRL, and without the computational expense of gradient-based\noptimization."
                },
                "authors": [
                    {
                        "name": "Conor Heins"
                    },
                    {
                        "name": "Toon Van de Maele"
                    },
                    {
                        "name": "Alexander Tschantz"
                    },
                    {
                        "name": "Hampus Linander"
                    },
                    {
                        "name": "Dimitrije Markovic"
                    },
                    {
                        "name": "Tommaso Salvatori"
                    },
                    {
                        "name": "Corrado Pezzato"
                    },
                    {
                        "name": "Ozan Catal"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Magnus Koudahl"
                    },
                    {
                        "name": "Marco Perin"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Tim Verbelen"
                    },
                    {
                        "name": "Christopher Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Buckley"
                },
                "author": "Christopher Buckley",
                "arxiv_comment": "10 pages main text, 4 figures, 2 tables; 25 pages supplementary\n  material, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17794v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17794v4",
                "updated": "2025-05-30T16:44:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    44,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-22T15:05:21Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    15,
                    5,
                    21,
                    5,
                    81,
                    0
                ],
                "title": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image\n  Generative Models"
                },
                "summary": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of more than +8 in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n83% of the prompts from the GenAI-Bench dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of more than +8 in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n83% of the prompts from the GenAI-Bench dataset."
                },
                "authors": [
                    {
                        "name": "Ketan Suhaas Saichandran"
                    },
                    {
                        "name": "Xavier Thomas"
                    },
                    {
                        "name": "Prakhar Kaushik"
                    },
                    {
                        "name": "Deepti Ghadiyaram"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Ghadiyaram"
                },
                "author": "Deepti Ghadiyaram",
                "arxiv_comment": "Accepted at CVPR 2025 workshops (AI4CC (oral) & GMCV (poster))",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17794v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17794v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04378v2",
                "updated": "2025-05-30T16:42:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    42,
                    57,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-06T12:30:24Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    30,
                    24,
                    3,
                    65,
                    0
                ],
                "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower\n  Inference-Time Scaling in Open-Ended General-Domain Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower\n  Inference-Time Scaling in Open-Ended General-Domain Tasks"
                },
                "summary": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect\nHelpSteer3 data to train dedicated Feedback and Edit Models that are capable of\nperforming inference-time scaling for open-ended general-domain tasks. In our\nsetup, one model generates an initial response, which are given feedback by a\nsecond model, that are then used by a third model to edit the response. We show\nthat performance on Arena Hard, a benchmark strongly predictive of Chatbot\nArena Elo can be boosted by scaling the number of initial response drafts,\neffective feedback and edited responses. When scaled optimally, our setup based\non 70B models from the Llama 3 family can reach SoTA performance on Arena Hard\nat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect\nHelpSteer3 data to train dedicated Feedback and Edit Models that are capable of\nperforming inference-time scaling for open-ended general-domain tasks. In our\nsetup, one model generates an initial response, which are given feedback by a\nsecond model, that are then used by a third model to edit the response. We show\nthat performance on Arena Hard, a benchmark strongly predictive of Chatbot\nArena Elo can be boosted by scaling the number of initial response drafts,\neffective feedback and edited responses. When scaled optimally, our setup based\non 70B models from the Llama 3 family can reach SoTA performance on Arena Hard\nat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Hoo-Chang Shin"
                    },
                    {
                        "name": "Felipe Soares"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "arxiv_comment": "23 pages, 2 figures, Accepted to ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24778v1",
                "updated": "2025-05-30T16:41:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:41:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?"
                },
                "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24774v1",
                "updated": "2025-05-30T16:36:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    36,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:36:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    36,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "A studentized permutation test for the treatment effect in individual\n  participant data meta-analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A studentized permutation test for the treatment effect in individual\n  participant data meta-analysis"
                },
                "summary": "Meta-analysis is a well-established tool used to combine data from several\nindependent studies, each of which usually compares the effect of an\nexperimental treatment with a control group. While meta-analyses are often\nperformed using aggregated study summaries, they may also be conducted using\nindividual participant data (IPD). Classical meta-analysis models may be\ngeneralized to handle continuous IPD by formulating them within a linear mixed\nmodel framework. IPD meta-analyses are commonly based on a small number of\nstudies. Technically, inference for the overall treatment effect can be\nperformed using Student-t approximation. However, as some approaches may not\nadequately control the type I error, Satterthwaite's or Kenward-Roger's method\nhave been suggested to set the degrees-of-freedom parameter. The latter also\nadjusts the standard error of the treatment effect estimator. Nevertheless,\nthese methods may be conservative. Since permutation tests are known to control\nthe type I error and offer robustness to violations of distributional\nassumptions, we propose a studentized permutation test for the treatment effect\nbased on permutations of standardized residuals across studies in IPD\nmeta-analysis. Also, we construct confidence intervals for the treatment effect\nbased on this test. The first interval is derived from the percentiles of the\npermutation distribution. The second interval is obtained by searching values\nclosest to the effect estimate that are just significantly different from the\ntrue effect. In a simulation study, we demonstrate satisfactory performance of\nthe proposed methods, often producing shorter confidence intervals compared\nwith competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-analysis is a well-established tool used to combine data from several\nindependent studies, each of which usually compares the effect of an\nexperimental treatment with a control group. While meta-analyses are often\nperformed using aggregated study summaries, they may also be conducted using\nindividual participant data (IPD). Classical meta-analysis models may be\ngeneralized to handle continuous IPD by formulating them within a linear mixed\nmodel framework. IPD meta-analyses are commonly based on a small number of\nstudies. Technically, inference for the overall treatment effect can be\nperformed using Student-t approximation. However, as some approaches may not\nadequately control the type I error, Satterthwaite's or Kenward-Roger's method\nhave been suggested to set the degrees-of-freedom parameter. The latter also\nadjusts the standard error of the treatment effect estimator. Nevertheless,\nthese methods may be conservative. Since permutation tests are known to control\nthe type I error and offer robustness to violations of distributional\nassumptions, we propose a studentized permutation test for the treatment effect\nbased on permutations of standardized residuals across studies in IPD\nmeta-analysis. Also, we construct confidence intervals for the treatment effect\nbased on this test. The first interval is derived from the percentiles of the\npermutation distribution. The second interval is obtained by searching values\nclosest to the effect estimate that are just significantly different from the\ntrue effect. In a simulation study, we demonstrate satisfactory performance of\nthe proposed methods, often producing shorter confidence intervals compared\nwith competitors."
                },
                "authors": [
                    {
                        "name": "Phuc Thien Tran"
                    },
                    {
                        "name": "Long-Hao Xu"
                    },
                    {
                        "name": "Christian Röver"
                    },
                    {
                        "name": "Tim Friede"
                    }
                ],
                "author_detail": {
                    "name": "Tim Friede"
                },
                "author": "Tim Friede",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15712v2",
                "updated": "2025-05-30T16:35:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-20T09:33:31Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining"
                },
                "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."
                },
                "authors": [
                    {
                        "name": "Maike Züfle"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24773v1",
                "updated": "2025-05-30T16:35:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption"
                },
                "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world."
                },
                "authors": [
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Xiaoyi Pang"
                    },
                    {
                        "name": "Zhibo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Wang"
                },
                "author": "Zhibo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18491v2",
                "updated": "2025-05-30T16:34:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    34,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-24T09:45:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    45,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge\n  for Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge\n  for Visual Question Answering"
                },
                "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Siwen Luo"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    },
                    {
                        "name": "Eduard Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Hovy"
                },
                "author": "Eduard Hovy",
                "arxiv_comment": "Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22767v2",
                "updated": "2025-05-30T16:31:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    58,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-28T18:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    36,
                    0,
                    2,
                    148,
                    0
                ],
                "title": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge"
                },
                "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems."
                },
                "authors": [
                    {
                        "name": "Eleni Vasilaki"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Vasilaki"
                },
                "author": "Eleni Vasilaki",
                "arxiv_comment": "6 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24768v1",
                "updated": "2025-05-30T16:31:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    5,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:31:05Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    5,
                    4,
                    150,
                    0
                ],
                "title": "From Macro to Micro: Probing Dataset Diversity in Language Model\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Macro to Micro: Probing Dataset Diversity in Language Model\n  Fine-Tuning"
                },
                "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xuhong Li"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Kun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kun Liu"
                },
                "author": "Kun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24767v1",
                "updated": "2025-05-30T16:30:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    30,
                    54,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:30:54Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    30,
                    54,
                    4,
                    150,
                    0
                ],
                "title": "A survey of using EHR as real-world evidence for discovering and\n  validating new drug indications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of using EHR as real-world evidence for discovering and\n  validating new drug indications"
                },
                "summary": "Electronic Health Records (EHRs) have been increasingly used as real-world\nevidence (RWE) to support the discovery and validation of new drug indications.\nThis paper surveys current approaches to EHR-based drug repurposing, covering\ndata sources, processing methodologies, and representation techniques. It\ndiscusses study designs and statistical frameworks for evaluating drug\nefficacy. Key challenges in validation are discussed, with emphasis on the role\nof large language models (LLMs) and target trial emulation. By synthesizing\nrecent developments and methodological advances, this work provides a\nfoundational resource for researchers aiming to translate real-world data into\nactionable drug-repurposing evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) have been increasingly used as real-world\nevidence (RWE) to support the discovery and validation of new drug indications.\nThis paper surveys current approaches to EHR-based drug repurposing, covering\ndata sources, processing methodologies, and representation techniques. It\ndiscusses study designs and statistical frameworks for evaluating drug\nefficacy. Key challenges in validation are discussed, with emphasis on the role\nof large language models (LLMs) and target trial emulation. By synthesizing\nrecent developments and methodological advances, this work provides a\nfoundational resource for researchers aiming to translate real-world data into\nactionable drug-repurposing evidence."
                },
                "authors": [
                    {
                        "name": "Nabasmita Talukdar"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Shreya Paithankar"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Bin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Chen"
                },
                "author": "Bin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24765v1",
                "updated": "2025-05-30T16:29:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    29,
                    12,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:29:12Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    29,
                    12,
                    4,
                    150,
                    0
                ],
                "title": "Supervised Quantum Machine Learning: A Future Outlook from Qubits to\n  Enterprise Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Quantum Machine Learning: A Future Outlook from Qubits to\n  Enterprise Applications"
                },
                "summary": "Supervised Quantum Machine Learning (QML) represents an intersection of\nquantum computing and classical machine learning, aiming to use quantum\nresources to support model training and inference. This paper reviews recent\ndevelopments in supervised QML, focusing on methods such as variational quantum\ncircuits, quantum neural networks, and quantum kernel methods, along with\nhybrid quantum-classical workflows. We examine recent experimental studies that\nshow partial indications of quantum advantage and describe current limitations\nincluding noise, barren plateaus, scalability issues, and the lack of formal\nproofs of performance improvement over classical methods. The main contribution\nis a ten-year outlook (2025-2035) that outlines possible developments in\nsupervised QML, including a roadmap describing conditions under which QML may\nbe used in applied research and enterprise systems over the next decade.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Quantum Machine Learning (QML) represents an intersection of\nquantum computing and classical machine learning, aiming to use quantum\nresources to support model training and inference. This paper reviews recent\ndevelopments in supervised QML, focusing on methods such as variational quantum\ncircuits, quantum neural networks, and quantum kernel methods, along with\nhybrid quantum-classical workflows. We examine recent experimental studies that\nshow partial indications of quantum advantage and describe current limitations\nincluding noise, barren plateaus, scalability issues, and the lack of formal\nproofs of performance improvement over classical methods. The main contribution\nis a ten-year outlook (2025-2035) that outlines possible developments in\nsupervised QML, including a roadmap describing conditions under which QML may\nbe used in applied research and enterprise systems over the next decade."
                },
                "authors": [
                    {
                        "name": "Srikanth Thudumu"
                    },
                    {
                        "name": "Jason Fisher"
                    },
                    {
                        "name": "Hung Du"
                    }
                ],
                "author_detail": {
                    "name": "Hung Du"
                },
                "author": "Hung Du",
                "arxiv_comment": "Future outlook and roadmap of QML with 7 pages and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24757v1",
                "updated": "2025-05-30T16:18:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    50,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:18:50Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    50,
                    4,
                    150,
                    0
                ],
                "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in\n  Systematic Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in\n  Systematic Literature Reviews"
                },
                "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available."
                },
                "authors": [
                    {
                        "name": "Christian Jaumann"
                    },
                    {
                        "name": "Andreas Wiedholz"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Annemarie Friedrich"
                },
                "author": "Annemarie Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00651v2",
                "updated": "2025-05-30T16:15:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    15,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2024-08-01T15:41:07Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    15,
                    41,
                    7,
                    3,
                    214,
                    0
                ],
                "title": "A Dirichlet stochastic block model for composition-weighted networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dirichlet stochastic block model for composition-weighted networks"
                },
                "summary": "Network data are observed in various applications where the individual\nentities of the system interact with or are connected to each other, and often\nthese interactions are defined by their associated strength or importance.\nClustering is a common task in network analysis that involves finding groups of\nnodes displaying similarities in the way they interact with the rest of the\nnetwork. However, most clustering methods use the strengths of connections\nbetween entities in their original form, ignoring the possible differences in\nthe capacities of individual nodes to send or receive edges. This often leads\nto clustering solutions that are heavily influenced by the nodes' capacities.\nOne way to overcome this is to analyse the strengths of connections in relative\nrather than absolute terms, expressing each edge weight as a proportion of the\nsending (or receiving) capacity of the respective node. This, however, induces\nadditional modelling constraints that most existing clustering methods are not\ndesigned to handle. In this work we propose a stochastic block model for\ncomposition-weighted networks based on direct modelling of compositional weight\nvectors using a Dirichlet mixture, with the parameters determined by the\ncluster labels of the sender and the receiver nodes. Inference is implemented\nvia an extension of the classification expectation-maximisation algorithm that\nuses a working independence assumption, expressing the complete data likelihood\nof each node of the network as a function of fixed cluster labels of the\nremaining nodes. A model selection criterion is derived to aid the choice of\nthe number of clusters. The model is validated using simulation studies, and\nshowcased on network data from the Erasmus exchange program and a bike sharing\nnetwork for the city of London.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network data are observed in various applications where the individual\nentities of the system interact with or are connected to each other, and often\nthese interactions are defined by their associated strength or importance.\nClustering is a common task in network analysis that involves finding groups of\nnodes displaying similarities in the way they interact with the rest of the\nnetwork. However, most clustering methods use the strengths of connections\nbetween entities in their original form, ignoring the possible differences in\nthe capacities of individual nodes to send or receive edges. This often leads\nto clustering solutions that are heavily influenced by the nodes' capacities.\nOne way to overcome this is to analyse the strengths of connections in relative\nrather than absolute terms, expressing each edge weight as a proportion of the\nsending (or receiving) capacity of the respective node. This, however, induces\nadditional modelling constraints that most existing clustering methods are not\ndesigned to handle. In this work we propose a stochastic block model for\ncomposition-weighted networks based on direct modelling of compositional weight\nvectors using a Dirichlet mixture, with the parameters determined by the\ncluster labels of the sender and the receiver nodes. Inference is implemented\nvia an extension of the classification expectation-maximisation algorithm that\nuses a working independence assumption, expressing the complete data likelihood\nof each node of the network as a function of fixed cluster labels of the\nremaining nodes. A model selection criterion is derived to aid the choice of\nthe number of clusters. The model is validated using simulation studies, and\nshowcased on network data from the Erasmus exchange program and a bike sharing\nnetwork for the city of London."
                },
                "authors": [
                    {
                        "name": "Iuliia Promskaia"
                    },
                    {
                        "name": "Adrian O'Hagan"
                    },
                    {
                        "name": "Michael Fop"
                    }
                ],
                "author_detail": {
                    "name": "Michael Fop"
                },
                "author": "Michael Fop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24749v1",
                "updated": "2025-05-30T16:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    8,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:08:40Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    8,
                    40,
                    4,
                    150,
                    0
                ],
                "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating\n  Memory-Efficient LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating\n  Memory-Efficient LLM Training"
                },
                "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Guy Smorodinsky"
                    },
                    {
                        "name": "Tom Tirer"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24726v1",
                "updated": "2025-05-30T15:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    49,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:49:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    49,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning"
                },
                "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback."
                },
                "authors": [
                    {
                        "name": "Shelly Bensal"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Dmytro Mozolevskyi"
                    },
                    {
                        "name": "Muayad Ali"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24724v1",
                "updated": "2025-05-30T15:47:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    47,
                    13,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:47:13Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    47,
                    13,
                    4,
                    150,
                    0
                ],
                "title": "Talking Transactions: Decentralized Communication through Ethereum Input\n  Data Messages (IDMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking Transactions: Decentralized Communication through Ethereum Input\n  Data Messages (IDMs)"
                },
                "summary": "Can you imagine, blockchain transactions can talk! In this paper, we study\nhow they talk and what they talk about. We focus on the input data field of\nEthereum transactions, which is designed to allow external callers to interact\nwith smart contracts. In practice, this field also enables users to embed\nnatural language messages into transactions. Users can leverage these Input\nData Messages (IDMs) for peer-to-peer communication. This means that, beyond\nEthereum's well-known role as a financial infrastructure, it also serves as a\ndecentralized communication medium.\n  We present the first large-scale analysis of Ethereum IDMs from the genesis\nblock to February 2024 (3134 days). We filter IDMs to extract 867,140\ntransactions with informative IDMs and use LLMs for language detection. We find\nthat English (95.4%) and Chinese (4.4%) dominate the use of natural languages\nin IDMs. Interestingly, English IDMs center on security and scam warnings (24%)\nwith predominantly negative emotions, while Chinese IDMs emphasize emotional\nexpression and social connection (44%) with a more positive tone. We also\nobserve that longer English IDMs often transfer high ETH values for\nprotocol-level purposes, while longer Chinese IDMs tend to involve symbolic\ntransfer amounts for emotional intent. Moreover, we find that the IDM\nparticipants tend to form small, loosely connected communities (59.99%). Our\nfindings highlight culturally and functionally divergent use cases of the IDM\nchannel across user communities. We further examine the security relevance of\nIDMs in on-chain attacks. Many victims use them to appeal to attackers for fund\nrecovery. IDMs containing negotiations or reward offers are linked to higher\nreply rates. We also analyze IDMs' regulatory implications. Their misuse for\nabuse, threats, and sexual solicitation reveals the urgent need for content\nmoderation and regulation in decentralized systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can you imagine, blockchain transactions can talk! In this paper, we study\nhow they talk and what they talk about. We focus on the input data field of\nEthereum transactions, which is designed to allow external callers to interact\nwith smart contracts. In practice, this field also enables users to embed\nnatural language messages into transactions. Users can leverage these Input\nData Messages (IDMs) for peer-to-peer communication. This means that, beyond\nEthereum's well-known role as a financial infrastructure, it also serves as a\ndecentralized communication medium.\n  We present the first large-scale analysis of Ethereum IDMs from the genesis\nblock to February 2024 (3134 days). We filter IDMs to extract 867,140\ntransactions with informative IDMs and use LLMs for language detection. We find\nthat English (95.4%) and Chinese (4.4%) dominate the use of natural languages\nin IDMs. Interestingly, English IDMs center on security and scam warnings (24%)\nwith predominantly negative emotions, while Chinese IDMs emphasize emotional\nexpression and social connection (44%) with a more positive tone. We also\nobserve that longer English IDMs often transfer high ETH values for\nprotocol-level purposes, while longer Chinese IDMs tend to involve symbolic\ntransfer amounts for emotional intent. Moreover, we find that the IDM\nparticipants tend to form small, loosely connected communities (59.99%). Our\nfindings highlight culturally and functionally divergent use cases of the IDM\nchannel across user communities. We further examine the security relevance of\nIDMs in on-chain attacks. Many victims use them to appeal to attackers for fund\nrecovery. IDMs containing negotiations or reward offers are linked to higher\nreply rates. We also analyze IDMs' regulatory implications. Their misuse for\nabuse, threats, and sexual solicitation reveals the urgent need for content\nmoderation and regulation in decentralized systems."
                },
                "authors": [
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Endong Liu"
                    },
                    {
                        "name": "Pascal Berrang"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13772v2",
                "updated": "2025-05-30T15:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    44,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-19T23:18:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    23,
                    18,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Krikri: Advancing Open Large Language Models for Greek",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krikri: Advancing Open Large Language Models for Greek"
                },
                "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation."
                },
                "authors": [
                    {
                        "name": "Dimitris Roussis"
                    },
                    {
                        "name": "Leon Voukoutis"
                    },
                    {
                        "name": "Georgios Paraskevopoulos"
                    },
                    {
                        "name": "Sokratis Sofianopoulos"
                    },
                    {
                        "name": "Prokopis Prokopidis"
                    },
                    {
                        "name": "Vassilis Papavasileiou"
                    },
                    {
                        "name": "Athanasios Katsamanis"
                    },
                    {
                        "name": "Stelios Piperidis"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    }
                ],
                "author_detail": {
                    "name": "Vassilis Katsouros"
                },
                "author": "Vassilis Katsouros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05258v2",
                "updated": "2025-05-30T15:37:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    37,
                    19,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-07T16:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Adrián Bazaga"
                    },
                    {
                        "name": "Rexhina Blloshmi"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Adrià de Gispert"
                    }
                ],
                "author_detail": {
                    "name": "Adrià de Gispert"
                },
                "author": "Adrià de Gispert",
                "arxiv_comment": "ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24716v1",
                "updated": "2025-05-30T15:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    36,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    36,
                    56,
                    4,
                    150,
                    0
                ],
                "title": "Towards Scalable Schema Mapping using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Schema Mapping using Large Language Models"
                },
                "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering."
                },
                "authors": [
                    {
                        "name": "Christopher Buss"
                    },
                    {
                        "name": "Mahdis Safari"
                    },
                    {
                        "name": "Arash Termehchy"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "David Maier"
                    }
                ],
                "author_detail": {
                    "name": "David Maier"
                },
                "author": "David Maier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24712v1",
                "updated": "2025-05-30T15:32:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    32,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:32:48Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    32,
                    48,
                    4,
                    150,
                    0
                ],
                "title": "HESEIA: A community-based dataset for evaluating social biases in large\n  language models, co-designed in real school settings in Latin America",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HESEIA: A community-based dataset for evaluating social biases in large\n  language models, co-designed in real school settings in Latin America"
                },
                "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities."
                },
                "authors": [
                    {
                        "name": "Guido Ivetta"
                    },
                    {
                        "name": "Marcos J. Gomez"
                    },
                    {
                        "name": "Sofía Martinelli"
                    },
                    {
                        "name": "Pietro Palombini"
                    },
                    {
                        "name": "M. Emilia Echeveste"
                    },
                    {
                        "name": "Nair Carolina Mazzeo"
                    },
                    {
                        "name": "Beatriz Busaniche"
                    },
                    {
                        "name": "Luciana Benotti"
                    }
                ],
                "author_detail": {
                    "name": "Luciana Benotti"
                },
                "arxiv_affiliation": "Fundación Vía Libre",
                "author": "Luciana Benotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24710v1",
                "updated": "2025-05-30T15:30:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    30,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:30:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    30,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through\n  Learning, Adapting and Acting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-aware Large Language Models: Enhancing Decision-Making Through\n  Learning, Adapting and Acting"
                },
                "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Haipeng Zhu"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Zhifeng Hao"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Junjian Ye"
                    },
                    {
                        "name": "Ruichu Cai"
                    }
                ],
                "author_detail": {
                    "name": "Ruichu Cai"
                },
                "author": "Ruichu Cai",
                "arxiv_comment": "Accepted by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24708v1",
                "updated": "2025-05-30T15:29:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    29,
                    36,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:29:36Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    29,
                    36,
                    4,
                    150,
                    0
                ],
                "title": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions"
                },
                "summary": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels. We demonstrate that BMFIA solves Bayesian inverse problems for\nscenarios that used to be prohibitive, such as finely-resolved spatial\nreconstruction problems for nonlinear and transient coupled poro-elastic media\nphysics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels. We demonstrate that BMFIA solves Bayesian inverse problems for\nscenarios that used to be prohibitive, such as finely-resolved spatial\nreconstruction problems for nonlinear and transient coupled poro-elastic media\nphysics."
                },
                "authors": [
                    {
                        "name": "Jonas Nitzler"
                    },
                    {
                        "name": "Bugrahan Z. Temür"
                    },
                    {
                        "name": "Phaedon-Stelios Koutsourelakis"
                    },
                    {
                        "name": "Wolfgang A. Wall"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang A. Wall"
                },
                "author": "Wolfgang A. Wall",
                "arxiv_comment": "42 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24701v1",
                "updated": "2025-05-30T15:24:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    24,
                    17,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:24:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    24,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for\n  Real-World Evaluation and Model Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for\n  Real-World Evaluation and Model Comparison"
                },
                "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data."
                },
                "authors": [
                    {
                        "name": "Tejul Pandit"
                    },
                    {
                        "name": "Meet Raval"
                    },
                    {
                        "name": "Dhvani Upadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Dhvani Upadhyay"
                },
                "author": "Dhvani Upadhyay",
                "arxiv_comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17439v2",
                "updated": "2025-05-30T15:19:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    19,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-21T17:59:10Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    10,
                    4,
                    80,
                    0
                ],
                "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines."
                },
                "authors": [
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chenlin Ming"
                    },
                    {
                        "name": "H. Vicky Zhao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "ACL'25 Findings, Code is available at https://github.com/pzs19/LEMMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03621v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03621v4",
                "updated": "2025-05-30T15:19:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    19,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-04T15:26:10Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    26,
                    10,
                    2,
                    339,
                    0
                ],
                "title": "JPPO++: Joint Power and Denoising-inspired Prompt Optimization for\n  Mobile LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JPPO++: Joint Power and Denoising-inspired Prompt Optimization for\n  Mobile LLM Services"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into mobile services\nover wireless networks to support complex user requests. This trend has led to\nlonger prompts, which improve LLMs' performance but increase data transmission\ncosts and require more processing time, thereby reducing overall system\nefficiency and negatively impacting user experience. To address these\nchallenges, we propose Joint Prompt and Power Optimization (JPPO), a framework\nthat jointly optimizes prompt compression and wireless transmission power for\nmobile LLM services. JPPO leverages a Small Language Model (SLM) deployed at\nedge devices to perform lightweight prompt compression, reducing communication\nload before transmission to the cloud-based LLM. A Deep Reinforcement Learning\n(DRL) agent dynamically adjusts both the compression ratio and transmission\npower based on network conditions and service constraints, aiming to minimize\nservice time while preserving response fidelity. We further extend the\nframework to JPPO++, which introduces a denoising-inspired compression scheme.\nThis design performs iterative prompt refinement by progressively removing less\ninformative tokens, allowing for more aggressive yet controlled compression.\nExperimental results show that JPPO++ reduces service time by 17% compared to\nthe no-compression baseline while maintaining output quality. Under\ncompression-prioritized settings, a reduction of up to 16x in prompt length can\nbe achieved with an acceptable loss in accuracy. Specifically, JPPO with a 16x\nratio reduces total service time by approximately 42.3%, and JPPO++ further\nimproves this reduction to 46.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into mobile services\nover wireless networks to support complex user requests. This trend has led to\nlonger prompts, which improve LLMs' performance but increase data transmission\ncosts and require more processing time, thereby reducing overall system\nefficiency and negatively impacting user experience. To address these\nchallenges, we propose Joint Prompt and Power Optimization (JPPO), a framework\nthat jointly optimizes prompt compression and wireless transmission power for\nmobile LLM services. JPPO leverages a Small Language Model (SLM) deployed at\nedge devices to perform lightweight prompt compression, reducing communication\nload before transmission to the cloud-based LLM. A Deep Reinforcement Learning\n(DRL) agent dynamically adjusts both the compression ratio and transmission\npower based on network conditions and service constraints, aiming to minimize\nservice time while preserving response fidelity. We further extend the\nframework to JPPO++, which introduces a denoising-inspired compression scheme.\nThis design performs iterative prompt refinement by progressively removing less\ninformative tokens, allowing for more aggressive yet controlled compression.\nExperimental results show that JPPO++ reduces service time by 17% compared to\nthe no-compression baseline while maintaining output quality. Under\ncompression-prioritized settings, a reduction of up to 16x in prompt length can\nbe achieved with an acceptable loss in accuracy. Specifically, JPPO with a 16x\nratio reduces total service time by approximately 42.3%, and JPPO++ further\nimproves this reduction to 46.5%."
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2411.18010",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03621v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03621v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24695v1",
                "updated": "2025-05-30T15:18:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    18,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:18:51Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    18,
                    51,
                    4,
                    150,
                    0
                ],
                "title": "Likelihoods for Stochastic Gravitational Wave Background Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihoods for Stochastic Gravitational Wave Background Data Analysis"
                },
                "summary": "We present a systematic study of likelihood functions used for Stochastic\nGravitational Wave Background (SGWB) searches. By dividing the data into many\nshort segments, one customarily takes advantage of the Central Limit Theorem to\njustify a Gaussian crosscorrelation likelihood. We show, with a hierarchy of\never more realistic examples, beginning with a single frequency bin and one\ndetector, and then moving to two and three detectors with white and colored\nsignal and noise, that approximating the exact Whittle likelihood by various\nGaussian alternatives can induce systematic biases in the estimation of the\nSGWB parameters. We derive several approximations for the full likelihood and\nidentify regimes where Gaussianity breaks down. We also discuss the possibility\nof conditioning the full likelihood on fiducial noise estimates to produce\nunbiased SGWB parameter estimation. We show that for some segment durations and\nbandwidths, particularly in space-based and pulsar-timing arrays, the bias can\nexceed the statistical uncertainty. Our results provide practical guidance for\nsegment choice, likelihood selection, and data-compression strategies to ensure\nrobust SGWB inference in current and next-generation gravitational wave\ndetectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of likelihood functions used for Stochastic\nGravitational Wave Background (SGWB) searches. By dividing the data into many\nshort segments, one customarily takes advantage of the Central Limit Theorem to\njustify a Gaussian crosscorrelation likelihood. We show, with a hierarchy of\never more realistic examples, beginning with a single frequency bin and one\ndetector, and then moving to two and three detectors with white and colored\nsignal and noise, that approximating the exact Whittle likelihood by various\nGaussian alternatives can induce systematic biases in the estimation of the\nSGWB parameters. We derive several approximations for the full likelihood and\nidentify regimes where Gaussianity breaks down. We also discuss the possibility\nof conditioning the full likelihood on fiducial noise estimates to produce\nunbiased SGWB parameter estimation. We show that for some segment durations and\nbandwidths, particularly in space-based and pulsar-timing arrays, the bias can\nexceed the statistical uncertainty. Our results provide practical guidance for\nsegment choice, likelihood selection, and data-compression strategies to ensure\nrobust SGWB inference in current and next-generation gravitational wave\ndetectors."
                },
                "authors": [
                    {
                        "name": "Gabriele Franciolini"
                    },
                    {
                        "name": "Mauro Pieroni"
                    },
                    {
                        "name": "Angelo Ricciardone"
                    },
                    {
                        "name": "Joseph D. Romano"
                    }
                ],
                "author_detail": {
                    "name": "Joseph D. Romano"
                },
                "author": "Joseph D. Romano",
                "arxiv_comment": "25 pages, 9 figures, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17261v2",
                "updated": "2025-05-30T15:15:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T09:37:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator"
                },
                "summary": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails, and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments. Our\nproject is at https://yfthu.github.io/HEIE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails, and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments. Our\nproject is at https://yfthu.github.io/HEIE/."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ru Zhen"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24691v1",
                "updated": "2025-05-30T15:15:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:15:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing\n  Cross-Lingual Transfer in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing\n  Cross-Lingual Transfer in Low-Resource Scenarios"
                },
                "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages."
                },
                "authors": [
                    {
                        "name": "Gerard I. Gállego"
                    },
                    {
                        "name": "Oriol Pareras"
                    },
                    {
                        "name": "Martí Cortada Garcia"
                    },
                    {
                        "name": "Lucas Takanori"
                    },
                    {
                        "name": "Javier Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernando"
                },
                "author": "Javier Hernando",
                "arxiv_comment": "Accepted at Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24688v1",
                "updated": "2025-05-30T15:11:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:11:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration"
                },
                "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24684v1",
                "updated": "2025-05-30T15:08:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    50,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:08:50Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    50,
                    4,
                    150,
                    0
                ],
                "title": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs"
                },
                "summary": "Despite the success in learning semantically meaningful, unsupervised\ndisentangled representations, variational autoencoders (VAEs) and their\nvariants face a fundamental theoretical challenge: substantial evidence\nindicates that unsupervised disentanglement is unattainable without implicit\ninductive bias, yet such bias remains elusive. In this work, we focus on\nexploring the implicit inductive bias that drive disentanglement in VAEs with\nfactorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we\nuncover a crucial implicit inductive bias called disentangling granularity,\nwhich leads to the discovery of an interesting \"V\"-shaped optimal Evidence\nLower Bound (ELBO) trajectory within the parameter space. This finding is\nvalidated through over 100K experiments using factorized VAEs and our newly\nproposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that\nconventional factorized VAEs, constrained by fixed disentangling granularity,\ninherently tend to disentangle low-complexity feature. Whereas, appropriately\ntuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the\nrange of disentangled representations, allowing for the disentanglement of\nhigh-complexity features. Our findings unveil that disentangling granularity as\nan implicit inductive bias in factorized VAEs influence both disentanglement\nperformance and the inference of the ELBO, offering fresh insights into the\ninterpretability and inherent biases of VAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success in learning semantically meaningful, unsupervised\ndisentangled representations, variational autoencoders (VAEs) and their\nvariants face a fundamental theoretical challenge: substantial evidence\nindicates that unsupervised disentanglement is unattainable without implicit\ninductive bias, yet such bias remains elusive. In this work, we focus on\nexploring the implicit inductive bias that drive disentanglement in VAEs with\nfactorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we\nuncover a crucial implicit inductive bias called disentangling granularity,\nwhich leads to the discovery of an interesting \"V\"-shaped optimal Evidence\nLower Bound (ELBO) trajectory within the parameter space. This finding is\nvalidated through over 100K experiments using factorized VAEs and our newly\nproposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that\nconventional factorized VAEs, constrained by fixed disentangling granularity,\ninherently tend to disentangle low-complexity feature. Whereas, appropriately\ntuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the\nrange of disentangled representations, allowing for the disentanglement of\nhigh-complexity features. Our findings unveil that disentangling granularity as\nan implicit inductive bias in factorized VAEs influence both disentanglement\nperformance and the inference of the ELBO, offering fresh insights into the\ninterpretability and inherent biases of VAEs."
                },
                "authors": [
                    {
                        "name": "Zihao Chen"
                    },
                    {
                        "name": "Yu Xiang"
                    },
                    {
                        "name": "Wenyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyong Wang"
                },
                "author": "Wenyong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24683v1",
                "updated": "2025-05-30T15:08:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:08:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation"
                },
                "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Kevin Duh"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "22 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24680v1",
                "updated": "2025-05-30T15:06:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    6,
                    8,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:06:08Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    6,
                    8,
                    4,
                    150,
                    0
                ],
                "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Linear Patch Revives Layer-Pruned Large Language Models"
                },
                "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card."
                },
                "authors": [
                    {
                        "name": "Xinrui Chen"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Tian Guan"
                    },
                    {
                        "name": "Yonghong He"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24672v1",
                "updated": "2025-05-30T15:02:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    2,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    2,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis"
                },
                "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset."
                },
                "authors": [
                    {
                        "name": "Xiaorui Wu"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xuanhong Li"
                    },
                    {
                        "name": "Chong Teng"
                    },
                    {
                        "name": "Donghong Ji"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24671v1",
                "updated": "2025-05-30T15:01:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple LLM Agents Debate for Equitable Cultural Alignment"
                },
                "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Rachel Rudinger"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "37 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03380v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03380v4",
                "updated": "2025-05-30T15:00:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    0,
                    27,
                    4,
                    150,
                    0
                ],
                "published": "2024-10-04T12:48:21Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    12,
                    48,
                    21,
                    4,
                    278,
                    0
                ],
                "title": "Identifying biological perturbation targets through causal differential\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying biological perturbation targets through causal differential\n  networks"
                },
                "summary": "Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data."
                },
                "authors": [
                    {
                        "name": "Menghua Wu"
                    },
                    {
                        "name": "Umesh Padia"
                    },
                    {
                        "name": "Sean H. Murphy"
                    },
                    {
                        "name": "Regina Barzilay"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Jaakkola"
                },
                "author": "Tommi Jaakkola",
                "arxiv_journal_ref": "Proceedings of the 42nd International Conference on Machine\n  Learning, Vancouver, Canada. PMLR 267, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03380v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03380v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17229v2",
                "updated": "2025-05-30T14:59:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    59,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-21T15:32:24Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples."
                },
                "authors": [
                    {
                        "name": "Albert Sawczyn"
                    },
                    {
                        "name": "Jakub Binkowski"
                    },
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Bogdan Gabrys"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23473v2",
                "updated": "2025-05-30T14:54:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    54,
                    29,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-29T14:26:46Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    46,
                    3,
                    149,
                    0
                ],
                "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions"
                },
                "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context."
                },
                "authors": [
                    {
                        "name": "Xiaorui Wu"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Chong Teng"
                    },
                    {
                        "name": "Yuxiang Peng"
                    },
                    {
                        "name": "Li Zheng"
                    },
                    {
                        "name": "Donghong Ji"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24664v1",
                "updated": "2025-05-30T14:53:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    53,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    53,
                    40,
                    4,
                    150,
                    0
                ],
                "title": "Learning Distributions over Permutations and Rankings with Factorized\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Distributions over Permutations and Rankings with Factorized\n  Representations"
                },
                "summary": "Learning distributions over permutations is a fundamental problem in machine\nlearning, with applications in ranking, combinatorial optimization, structured\nprediction, and data association. Existing methods rely on mixtures of\nparametric families or neural networks with expensive variational inference\nprocedures. In this work, we propose a novel approach that leverages\nalternative representations for permutations, including Lehmer codes,\nFisher-Yates draws, and Insertion-Vectors. These representations form a\nbijection with the symmetric group, allowing for unconstrained learning using\nconventional deep learning techniques, and can represent any probability\ndistribution over permutations. Our approach enables a trade-off between\nexpressivity of the model family and computational requirements. In the least\nexpressive and most computationally efficient case, our method subsumes\nprevious families of well established probabilistic models over permutations,\nincluding Mallow's and the Repeated Insertion Model. Experiments indicate our\nmethod significantly outperforms current approaches on the jigsaw puzzle\nbenchmark, a common task for permutation learning. However, we argue this\nbenchmark is limited in its ability to assess learning probability\ndistributions, as the target is a delta distribution (i.e., a single correct\nsolution exists). We therefore propose two additional benchmarks: learning\ncyclic permutations and re-ranking movies based on user preference. We show\nthat our method learns non-trivial distributions even in the least expressive\nmode, while traditional models fail to even generate valid permutations in this\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning distributions over permutations is a fundamental problem in machine\nlearning, with applications in ranking, combinatorial optimization, structured\nprediction, and data association. Existing methods rely on mixtures of\nparametric families or neural networks with expensive variational inference\nprocedures. In this work, we propose a novel approach that leverages\nalternative representations for permutations, including Lehmer codes,\nFisher-Yates draws, and Insertion-Vectors. These representations form a\nbijection with the symmetric group, allowing for unconstrained learning using\nconventional deep learning techniques, and can represent any probability\ndistribution over permutations. Our approach enables a trade-off between\nexpressivity of the model family and computational requirements. In the least\nexpressive and most computationally efficient case, our method subsumes\nprevious families of well established probabilistic models over permutations,\nincluding Mallow's and the Repeated Insertion Model. Experiments indicate our\nmethod significantly outperforms current approaches on the jigsaw puzzle\nbenchmark, a common task for permutation learning. However, we argue this\nbenchmark is limited in its ability to assess learning probability\ndistributions, as the target is a delta distribution (i.e., a single correct\nsolution exists). We therefore propose two additional benchmarks: learning\ncyclic permutations and re-ranking movies based on user preference. We show\nthat our method learns non-trivial distributions even in the least expressive\nmode, while traditional models fail to even generate valid permutations in this\nsetting."
                },
                "authors": [
                    {
                        "name": "Daniel Severo"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Niklas Nolte"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Nolte"
                },
                "author": "Niklas Nolte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00816v2",
                "updated": "2025-05-30T14:52:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    52,
                    53,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-02T14:52:50Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    52,
                    50,
                    6,
                    33,
                    0
                ],
                "title": "Sundial: A Family of Highly Capable Time Series Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sundial: A Family of Highly Capable Time Series Foundation Models"
                },
                "summary": "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on continuous-valued time series without discrete tokenization.\nConditioned on arbitrary-length time series, our models are pre-trained without\nspecifying any prior distribution and can generate multiple probable\npredictions, achieving more flexibility in representation learning than using\nparametric densities. Towards time series foundation models, we leverage\nminimal but crucial adaptations of Transformers and curate TimeBench with one\ntrillion time points, comprising mostly real-world datasets and synthetic data.\nBy mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial\nmodels on TimeBench, which achieve unprecedented model capacity and\ngeneralization performance. In addition to excellent scalability, Sundial\nachieves state-of-the-art results on both point and probabilistic forecasting\nbenchmarks with a just-in-time inference speed, i.e., making zero-shot\npredictions within a few milliseconds. We believe that Sundial's pioneering\ngenerative forecasting capability can improve model reliability in real-world\ndecision-making. Code is available at: https://github.com/thuml/Sundial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on continuous-valued time series without discrete tokenization.\nConditioned on arbitrary-length time series, our models are pre-trained without\nspecifying any prior distribution and can generate multiple probable\npredictions, achieving more flexibility in representation learning than using\nparametric densities. Towards time series foundation models, we leverage\nminimal but crucial adaptations of Transformers and curate TimeBench with one\ntrillion time points, comprising mostly real-world datasets and synthetic data.\nBy mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial\nmodels on TimeBench, which achieve unprecedented model capacity and\ngeneralization performance. In addition to excellent scalability, Sundial\nachieves state-of-the-art results on both point and probabilistic forecasting\nbenchmarks with a just-in-time inference speed, i.e., making zero-shot\npredictions within a few milliseconds. We believe that Sundial's pioneering\ngenerative forecasting capability can improve model reliability in real-world\ndecision-making. Code is available at: https://github.com/thuml/Sundial."
                },
                "authors": [
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Guo Qin"
                    },
                    {
                        "name": "Zhiyuan Shi"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Caiyin Yang"
                    },
                    {
                        "name": "Xiangdong Huang"
                    },
                    {
                        "name": "Jianmin Wang"
                    },
                    {
                        "name": "Mingsheng Long"
                    }
                ],
                "author_detail": {
                    "name": "Mingsheng Long"
                },
                "author": "Mingsheng Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22366v3",
                "updated": "2025-05-30T14:51:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    51,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2024-10-28T19:01:18Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    1,
                    18,
                    0,
                    302,
                    0
                ],
                "title": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models"
                },
                "summary": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Surkov"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Antonio Mari"
                    },
                    {
                        "name": "Mikhail Terekhov"
                    },
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18134v2",
                "updated": "2025-05-30T14:50:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    50,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-23T17:43:27Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    43,
                    27,
                    4,
                    143,
                    0
                ],
                "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoGameBench: Can Vision-Language Models complete popular video games?"
                },
                "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions."
                },
                "authors": [
                    {
                        "name": "Alex L. Zhang"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    },
                    {
                        "name": "Karthik R. Narasimhan"
                    },
                    {
                        "name": "Ofir Press"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Press"
                },
                "author": "Ofir Press",
                "arxiv_comment": "9 pages, 33 pages including supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24658v1",
                "updated": "2025-05-30T14:46:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    46,
                    22,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:46:22Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    46,
                    22,
                    4,
                    150,
                    0
                ],
                "title": "Can LLMs and humans be friends? Uncovering factors affecting human-AI\n  intimacy formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs and humans be friends? Uncovering factors affecting human-AI\n  intimacy formation"
                },
                "summary": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation."
                },
                "authors": [
                    {
                        "name": "Yeseon Hong"
                    },
                    {
                        "name": "Junhyuk Choi"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "30 pages, 2figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00592v2",
                "updated": "2025-05-30T14:40:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    40,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-01T23:13:10Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    23,
                    13,
                    10,
                    5,
                    32,
                    0
                ],
                "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M+: Extending MemoryLLM with Scalable Long-Term Memory"
                },
                "summary": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dmitry Krotov"
                    },
                    {
                        "name": "Yuanzhe Hu"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Dan Gutfreund"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Zexue He"
                    }
                ],
                "author_detail": {
                    "name": "Zexue He"
                },
                "author": "Zexue He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21701v2",
                "updated": "2025-05-30T14:39:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    39,
                    34,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-27T19:39:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    39,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge\n  Probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge\n  Probing"
                },
                "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21693v2",
                "updated": "2025-05-30T14:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    37,
                    57,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-27T19:29:40Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    29,
                    40,
                    1,
                    147,
                    0
                ],
                "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs"
                },
                "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaños"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustín Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01986v2",
                "updated": "2025-05-30T14:28:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    28,
                    59,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-31T07:43:12Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    43,
                    12,
                    0,
                    90,
                    0
                ],
                "title": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation"
                },
                "summary": "The rapid advancements in LLMs have driven the adoption of generative AI in\nvarious domains, including Electronic Design Automation (EDA). Unlike\ntraditional software development, EDA presents unique challenges, as generated\nRTL code must not only be syntactically correct and functionally accurate but\nalso synthesizable by hardware generators while meeting performance, power, and\narea constraints. These additional requirements introduce complexities that\nexisting code-generation benchmarks often fail to capture, limiting their\neffectiveness in evaluating LLMs for RTL generation. To address this gap, we\npropose TuRTLe, a unified evaluation framework designed to systematically\nassess LLMs across key RTL generation tasks. TuRTLe integrates multiple\nexisting benchmarks and automates the evaluation process, enabling a\ncomprehensive assessment of LLM performance in syntax correctness, functional\ncorrectness, synthesis, PPA optimization, and exact line completion. Using this\nframework, we benchmark a diverse set of open LLMs and analyze their strengths\nand weaknesses in EDA-specific tasks. Our results show that reasoning-based\nmodels, such as DeepSeek R1, consistently outperform others across multiple\nevaluation criteria, but at the cost of increased computational overhead and\ninference latency. Additionally, base models are better suited in module\ncompletion tasks, while instruct-tuned models perform better in\nspecification-to-RTL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in LLMs have driven the adoption of generative AI in\nvarious domains, including Electronic Design Automation (EDA). Unlike\ntraditional software development, EDA presents unique challenges, as generated\nRTL code must not only be syntactically correct and functionally accurate but\nalso synthesizable by hardware generators while meeting performance, power, and\narea constraints. These additional requirements introduce complexities that\nexisting code-generation benchmarks often fail to capture, limiting their\neffectiveness in evaluating LLMs for RTL generation. To address this gap, we\npropose TuRTLe, a unified evaluation framework designed to systematically\nassess LLMs across key RTL generation tasks. TuRTLe integrates multiple\nexisting benchmarks and automates the evaluation process, enabling a\ncomprehensive assessment of LLM performance in syntax correctness, functional\ncorrectness, synthesis, PPA optimization, and exact line completion. Using this\nframework, we benchmark a diverse set of open LLMs and analyze their strengths\nand weaknesses in EDA-specific tasks. Our results show that reasoning-based\nmodels, such as DeepSeek R1, consistently outperform others across multiple\nevaluation criteria, but at the cost of increased computational overhead and\ninference latency. Additionally, base models are better suited in module\ncompletion tasks, while instruct-tuned models perform better in\nspecification-to-RTL tasks."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Gokcen Kestor"
                    },
                    {
                        "name": "Emanuele Parisi"
                    },
                    {
                        "name": "Miquel Albertí-Binimelis"
                    },
                    {
                        "name": "Cristian Gutierrez"
                    },
                    {
                        "name": "Razine Moundir Ghorab"
                    },
                    {
                        "name": "Orlando Montenegro"
                    },
                    {
                        "name": "Bernat Homs"
                    },
                    {
                        "name": "Miquel Moreto"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moreto"
                },
                "author": "Miquel Moreto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.5; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24640v1",
                "updated": "2025-05-30T14:27:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    27,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:27:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    27,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "Efficient Text Encoders for Labor Market Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Text Encoders for Labor Market Analysis"
                },
                "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis."
                },
                "authors": [
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Jeroen Van Hautte"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24635v1",
                "updated": "2025-05-30T14:25:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    45,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:25:45Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    45,
                    4,
                    150,
                    0
                ],
                "title": "Disentangling Language and Culture for Evaluating Multilingual Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Language and Culture for Evaluating Multilingual Large\n  Language Models"
                },
                "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/."
                },
                "authors": [
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24634v1",
                "updated": "2025-05-30T14:25:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:25:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR\n  Semantic Segmentation"
                },
                "summary": "LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}."
                },
                "authors": [
                    {
                        "name": "Xuzhi Wang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_doi": "10.1109/TCSVT.2025.3554182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCSVT.2025.3554182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.24634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24630v1",
                "updated": "2025-05-30T14:23:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    23,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:23:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    23,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v1",
                "updated": "2025-05-30T14:16:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23272v2",
                "updated": "2025-05-30T14:14:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    14,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-29T09:20:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    20,
                    12,
                    3,
                    149,
                    0
                ],
                "title": "Are MLMs Trapped in the Visual Room?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are MLMs Trapped in the Visual Room?"
                },
                "summary": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs."
                },
                "authors": [
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Chunwang Zou"
                    },
                    {
                        "name": "Qimeng Liu"
                    },
                    {
                        "name": "Lu Rong"
                    },
                    {
                        "name": "Ben Yao"
                    },
                    {
                        "name": "Zheng Lian"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24622v1",
                "updated": "2025-05-30T14:13:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    13,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:13:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    13,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated\n  Questions for Predicting Startup Success",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated\n  Questions for Predicting Startup Success"
                },
                "summary": "Predicting startup success requires models that are both accurate and\ninterpretable. We present a lightweight ensemble framework that combines YES/NO\nquestions generated by large language models (LLMs), forming a transparent\ndecision-making system. Each question acts as a weak heuristic, and by\nfiltering, ranking, and aggregating them through a threshold-based voting\nmechanism, we construct a strong ensemble predictor. On a test set where 10% of\nstartups are classified as successful, our approach achieves a precision rate\nof 50%, representing a 5x improvement over random selection, while remaining\nfully transparent. When we incorporate expert-guided heuristics into the\ngeneration process, performance improves further to 54% precision. These\nresults highlight the value of combining LLM reasoning with human insight and\ndemonstrate that simple, interpretable ensembles can support high-stakes\ndecisions in domains such as venture capital (VC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting startup success requires models that are both accurate and\ninterpretable. We present a lightweight ensemble framework that combines YES/NO\nquestions generated by large language models (LLMs), forming a transparent\ndecision-making system. Each question acts as a weak heuristic, and by\nfiltering, ranking, and aggregating them through a threshold-based voting\nmechanism, we construct a strong ensemble predictor. On a test set where 10% of\nstartups are classified as successful, our approach achieves a precision rate\nof 50%, representing a 5x improvement over random selection, while remaining\nfully transparent. When we incorporate expert-guided heuristics into the\ngeneration process, performance improves further to 54% precision. These\nresults highlight the value of combining LLM reasoning with human insight and\ndemonstrate that simple, interpretable ensembles can support high-stakes\ndecisions in domains such as venture capital (VC)."
                },
                "authors": [
                    {
                        "name": "Ben Griffin"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14189v2",
                "updated": "2025-05-30T14:12:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    54,
                    4,
                    150,
                    0
                ],
                "published": "2024-05-23T05:31:41Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    5,
                    31,
                    41,
                    3,
                    144,
                    0
                ],
                "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt\n  Organization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Universal Goal Hijacking with Semantics-guided Prompt\n  Organization"
                },
                "summary": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24621v1",
                "updated": "2025-05-30T14:12:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:12:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Benchmarking Large Language Models for Cryptanalysis and\n  Mismatched-Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Cryptanalysis and\n  Mismatched-Generalization"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Chencheng Zhu"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14604v2",
                "updated": "2025-05-30T14:11:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    11,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-18T18:03:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    3,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives"
                },
                "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."
                },
                "authors": [
                    {
                        "name": "Sara Sarto"
                    },
                    {
                        "name": "Marcella Cornia"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "IJCAI 2025. Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24618v1",
                "updated": "2025-05-30T14:10:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    10,
                    33,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:10:33Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    10,
                    33,
                    4,
                    150,
                    0
                ],
                "title": "Distributed Intelligence in the Computing Continuum with Active\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Intelligence in the Computing Continuum with Active\n  Inference"
                },
                "summary": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm\nthat spans from local Internet of Things sensors and constrained edge devices\nto large-scale cloud data centers. Its goal is to orchestrate a vast array of\ndiverse and distributed computing resources to support the next generation of\nInternet-based applications. However, the distributed, heterogeneous, and\ndynamic nature of CC platforms demands distributed intelligence for adaptive\nand resilient service management. This article introduces a distributed stream\nprocessing pipeline as a CC use case, where each service is managed by an\nActive Inference (AIF) agent. These agents collaborate to fulfill service needs\nspecified by SLOiDs, a term we introduce to denote Service Level Objectives\nthat are aware of its deployed devices, meaning that non-functional\nrequirements must consider the characteristics of the hosting device. We\ndemonstrate how AIF agents can be modeled and deployed alongside distributed\nservices to manage them autonomously. Our experiments show that AIF agents\nachieve over 90% SLOiD fulfillment when using tested transition models, and\naround 80% when learning the models during deployment. We compare their\nperformance to a multi-agent reinforcement learning algorithm, finding that\nwhile both approaches yield similar results, MARL requires extensive training,\nwhereas AIF agents can operate effectively from the start. Additionally, we\nevaluate the behavior of AIF agents in offloading scenarios, observing a strong\ncapacity for adaptation. Finally, we outline key research directions to advance\nAIF integration in CC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm\nthat spans from local Internet of Things sensors and constrained edge devices\nto large-scale cloud data centers. Its goal is to orchestrate a vast array of\ndiverse and distributed computing resources to support the next generation of\nInternet-based applications. However, the distributed, heterogeneous, and\ndynamic nature of CC platforms demands distributed intelligence for adaptive\nand resilient service management. This article introduces a distributed stream\nprocessing pipeline as a CC use case, where each service is managed by an\nActive Inference (AIF) agent. These agents collaborate to fulfill service needs\nspecified by SLOiDs, a term we introduce to denote Service Level Objectives\nthat are aware of its deployed devices, meaning that non-functional\nrequirements must consider the characteristics of the hosting device. We\ndemonstrate how AIF agents can be modeled and deployed alongside distributed\nservices to manage them autonomously. Our experiments show that AIF agents\nachieve over 90% SLOiD fulfillment when using tested transition models, and\naround 80% when learning the models during deployment. We compare their\nperformance to a multi-agent reinforcement learning algorithm, finding that\nwhile both approaches yield similar results, MARL requires extensive training,\nwhereas AIF agents can operate effectively from the start. Additionally, we\nevaluate the behavior of AIF agents in offloading scenarios, observing a strong\ncapacity for adaptation. Finally, we outline key research directions to advance\nAIF integration in CC platforms."
                },
                "authors": [
                    {
                        "name": "Victor Casamayor Pujol"
                    },
                    {
                        "name": "Boris Sedlak"
                    },
                    {
                        "name": "Tommaso Salvatori"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.24878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24878v1",
                "updated": "2025-05-30T17:59:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents"
                },
                "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL."
                },
                "authors": [
                    {
                        "name": "Yaxin Luo"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Jiacheng Cui"
                    },
                    {
                        "name": "Xiaohan Zhao"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24872v1",
                "updated": "2025-05-30T17:59:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    43,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:43Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    43,
                    4,
                    150,
                    0
                ],
                "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners"
                },
                "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker."
                },
                "authors": [
                    {
                        "name": "Zilin Xiao"
                    },
                    {
                        "name": "Jaywon Koo"
                    },
                    {
                        "name": "Siru Ouyang"
                    },
                    {
                        "name": "Jefferson Hernandez"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Vicente Ordonez"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Ordonez"
                },
                "author": "Vicente Ordonez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24871v1",
                "updated": "2025-05-30T17:59:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    38,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."
                },
                "authors": [
                    {
                        "name": "Yiqing Liang"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Wenhao Ding"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "James Tompkin"
                    },
                    {
                        "name": "Mengdi Xu"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Laixi Shi"
                    },
                    {
                        "name": "Jiacheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jiacheng Zhu"
                },
                "author": "Jiacheng Zhu",
                "arxiv_comment": "Project Webpage: https://modomodo-rl.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24869v1",
                "updated": "2025-05-30T17:59:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    19,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:59:19Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    59,
                    19,
                    4,
                    150,
                    0
                ],
                "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLVR: A Simple Language-based Video Reasoning Framework"
                },
                "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR."
                },
                "authors": [
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Yan-Bo Lin"
                    },
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24858v1",
                "updated": "2025-05-30T17:54:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    54,
                    8,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:54:08Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    54,
                    8,
                    4,
                    150,
                    0
                ],
                "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs"
                },
                "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans."
                },
                "authors": [
                    {
                        "name": "Gabrielle Kaili-May Liu"
                    },
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Avi Caciularu"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Tim G. J. Rudner"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13467v3",
                "updated": "2025-05-30T17:53:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    53,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2024-08-24T05:03:08Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    5,
                    3,
                    8,
                    5,
                    237,
                    0
                ],
                "title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to\n  Small-Scale Local LLMs"
                },
                "summary": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is automatically improved through additional fine-tuning using\nextra similar data generated by the service LLM. This multi-turn process\nguarantees that the smaller model can eventually match or even surpass the\nservice LLM's capabilities in specific downstream tasks, offering a practical\nand scalable solution for managing AI deployments in constrained environments.\nExtensive experiments with leading-edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is automatically improved through additional fine-tuning using\nextra similar data generated by the service LLM. This multi-turn process\nguarantees that the smaller model can eventually match or even surpass the\nservice LLM's capabilities in specific downstream tasks, offering a practical\nand scalable solution for managing AI deployments in constrained environments.\nExtensive experiments with leading-edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo."
                },
                "authors": [
                    {
                        "name": "Chansung Park"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "The first three authors contributed equally to this work; Accepted by\n  ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24851v1",
                "updated": "2025-05-30T17:49:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    49,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:49:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    49,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "Realistic quantum network simulation for experimental BBM92 key\n  distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic quantum network simulation for experimental BBM92 key\n  distribution"
                },
                "summary": "Quantum key distribution (QKD) can provide secure key material between two\nparties without relying on assumptions about the computational power of an\neavesdropper. QKD is performed over quantum links and quantum networks, systems\nwhich are resource-intensive to deploy and maintain. To evaluate and optimize\nperformance prior to, during, and after deployment, realistic simulations with\nattention to physical realism are necessary. Quantum network simulators can\nsimulate a variety of quantum and classical protocols and can assist in quantum\nnetwork design and optimization by offering realism and flexibility beyond\nmathematical models which rely on simplifying assumptions and can be\nintractable to solve as network complexity increases. We use a versatile\ndiscrete event quantum network simulator to simulate the entanglement-based QKD\nprotocol BBM92 and compare it to our experimental implementation and to\nexisting theory. Furthermore, we simulate secure key rates in a repeater key\ndistribution scenario for which no experimental implementations exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum key distribution (QKD) can provide secure key material between two\nparties without relying on assumptions about the computational power of an\neavesdropper. QKD is performed over quantum links and quantum networks, systems\nwhich are resource-intensive to deploy and maintain. To evaluate and optimize\nperformance prior to, during, and after deployment, realistic simulations with\nattention to physical realism are necessary. Quantum network simulators can\nsimulate a variety of quantum and classical protocols and can assist in quantum\nnetwork design and optimization by offering realism and flexibility beyond\nmathematical models which rely on simplifying assumptions and can be\nintractable to solve as network complexity increases. We use a versatile\ndiscrete event quantum network simulator to simulate the entanglement-based QKD\nprotocol BBM92 and compare it to our experimental implementation and to\nexisting theory. Furthermore, we simulate secure key rates in a repeater key\ndistribution scenario for which no experimental implementations exist."
                },
                "authors": [
                    {
                        "name": "Michelle Chalupnik"
                    },
                    {
                        "name": "Brian Doolittle"
                    },
                    {
                        "name": "Suparna Seshadri"
                    },
                    {
                        "name": "Eric G. Brown"
                    },
                    {
                        "name": "Keith Kenemer"
                    },
                    {
                        "name": "Daniel Winton"
                    },
                    {
                        "name": "Daniel Sanchez-Rosales"
                    },
                    {
                        "name": "Matthew Skrzypczyk"
                    },
                    {
                        "name": "Cara Alexander"
                    },
                    {
                        "name": "Eric Ostby"
                    },
                    {
                        "name": "Michael Cubeddu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cubeddu"
                },
                "author": "Michael Cubeddu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24850v1",
                "updated": "2025-05-30T17:47:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    47,
                    17,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:47:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    47,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning"
                },
                "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data."
                },
                "authors": [
                    {
                        "name": "Shuyao Xu"
                    },
                    {
                        "name": "Cheng Peng"
                    },
                    {
                        "name": "Jiangxuan Long"
                    },
                    {
                        "name": "Weidi Xu"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Yuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Qi"
                },
                "author": "Yuan Qi",
                "arxiv_comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24846v1",
                "updated": "2025-05-30T17:44:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    44,
                    28,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:44:28Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    44,
                    28,
                    4,
                    150,
                    0
                ],
                "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning"
                },
                "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization."
                },
                "authors": [
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Han Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhao"
                },
                "author": "Han Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08972v2",
                "updated": "2025-05-30T17:43:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    43,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-12T06:08:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    6,
                    8,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World\n  Scenarios"
                },
                "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena."
                },
                "authors": [
                    {
                        "name": "Ruiwen Zhou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24842v1",
                "updated": "2025-05-30T17:41:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    41,
                    58,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:41:58Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    41,
                    58,
                    4,
                    150,
                    0
                ],
                "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models"
                },
                "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24840v1",
                "updated": "2025-05-30T17:40:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    40,
                    46,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:40:46Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    40,
                    46,
                    4,
                    150,
                    0
                ],
                "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are\n  the Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are\n  the Bottleneck"
                },
                "summary": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge."
                },
                "authors": [
                    {
                        "name": "Yuwen Tan"
                    },
                    {
                        "name": "Yuan Qing"
                    },
                    {
                        "name": "Boqing Gong"
                    }
                ],
                "author_detail": {
                    "name": "Boqing Gong"
                },
                "author": "Boqing Gong",
                "arxiv_comment": "28 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24838v1",
                "updated": "2025-05-30T17:39:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:39:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software"
                },
                "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies."
                },
                "authors": [
                    {
                        "name": "Brandon Man"
                    },
                    {
                        "name": "Ghadi Nehme"
                    },
                    {
                        "name": "Md Ferdous Alam"
                    },
                    {
                        "name": "Faez Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Faez Ahmed"
                },
                "author": "Faez Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02355v3",
                "updated": "2025-05-30T17:39:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    39,
                    6,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-04T18:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization"
                },
                "summary": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Alexandre Marques"
                    },
                    {
                        "name": "Shubhra Pandit"
                    },
                    {
                        "name": "Mark Kurtz"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24830v1",
                "updated": "2025-05-30T17:33:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    33,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:33:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    33,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs"
                },
                "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Annika Domres"
                    },
                    {
                        "name": "Mai Nguyen"
                    },
                    {
                        "name": "Rebecca Moser"
                    },
                    {
                        "name": "Jana Nano"
                    },
                    {
                        "name": "Felix Busch"
                    },
                    {
                        "name": "Lisa C. Adams"
                    },
                    {
                        "name": "Keno K. Bressem"
                    },
                    {
                        "name": "Denise Bernhardt"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Kai J. Borm"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Jan C. Peeken"
                    }
                ],
                "author_detail": {
                    "name": "Jan C. Peeken"
                },
                "author": "Jan C. Peeken",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09284v3",
                "updated": "2025-05-30T17:30:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-13T12:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "SparQLe: Speech Queries to Text Translation Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQLe: Speech Queries to Text Translation Through LLMs"
                },
                "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications."
                },
                "authors": [
                    {
                        "name": "Amirbek Djanibekov"
                    },
                    {
                        "name": "Hanan Aldarmaki"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Aldarmaki"
                },
                "author": "Hanan Aldarmaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24826v1",
                "updated": "2025-05-30T17:30:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:30:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    30,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text"
                },
                "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q."
                },
                "authors": [
                    {
                        "name": "Li yunhan"
                    },
                    {
                        "name": "Wu gengshen"
                    }
                ],
                "author_detail": {
                    "name": "Wu gengshen"
                },
                "author": "Wu gengshen",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24823v1",
                "updated": "2025-05-30T17:25:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    20,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:25:20Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    20,
                    4,
                    150,
                    0
                ],
                "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning."
                },
                "authors": [
                    {
                        "name": "Yinggan Xu"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "name": "Changnan Peng"
                    },
                    {
                        "name": "Di Luo"
                    }
                ],
                "author_detail": {
                    "name": "Di Luo"
                },
                "author": "Di Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05368v2",
                "updated": "2025-05-30T17:25:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    25,
                    4,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-07T22:41:31Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    41,
                    31,
                    4,
                    38,
                    0
                ],
                "title": "Otter: Generating Tests from Issues to Validate SWE Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Otter: Generating Tests from Issues to Validate SWE Patches"
                },
                "summary": "While there has been plenty of work on generating tests from existing code,\nthere has been limited work on generating tests from issues. A correct test\nmust validate the code patch that resolves the issue. This paper focuses on the\nscenario where that code patch does not yet exist. Doing so supports two major\nuse-cases. First, it supports TDD (test-driven development), the discipline of\n\"test first, write code later\" that has well-documented benefits for human\nsoftware engineers. Second, it also validates SWE (software engineering)\nagents, which generate code patches for resolving issues. This paper introduces\nTDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an\nLLM-based solution for this task. Otter augments LLMs with rule-based analysis\nto check and repair their outputs, and introduces a novel self-reflective\naction planner. Experiments show Otter outperforming state-of-the-art systems\nfor generating tests from issues, in addition to enhancing systems that\ngenerate patches from issues. We hope that Otter helps make developers more\nproductive at resolving issues and leads to more robust, well-tested code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been plenty of work on generating tests from existing code,\nthere has been limited work on generating tests from issues. A correct test\nmust validate the code patch that resolves the issue. This paper focuses on the\nscenario where that code patch does not yet exist. Doing so supports two major\nuse-cases. First, it supports TDD (test-driven development), the discipline of\n\"test first, write code later\" that has well-documented benefits for human\nsoftware engineers. Second, it also validates SWE (software engineering)\nagents, which generate code patches for resolving issues. This paper introduces\nTDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an\nLLM-based solution for this task. Otter augments LLMs with rule-based analysis\nto check and repair their outputs, and introduces a novel self-reflective\naction planner. Experiments show Otter outperforming state-of-the-art systems\nfor generating tests from issues, in addition to enhancing systems that\ngenerate patches from issues. We hope that Otter helps make developers more\nproductive at resolving issues and leads to more robust, well-tested code."
                },
                "authors": [
                    {
                        "name": "Toufique Ahmed"
                    },
                    {
                        "name": "Jatin Ganhotra"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Avraham Shinnar"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "arxiv_comment": "Accepted to the main technical track of the International Conference\n  on Machine Learning (ICML), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19577v2",
                "updated": "2025-05-30T17:24:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    24,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-26T06:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    6,
                    47,
                    43,
                    0,
                    146,
                    0
                ],
                "title": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding"
                },
                "summary": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xiaoyu Gu"
                    },
                    {
                        "name": "Yidi Jiang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "TASLP under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13001v2",
                "updated": "2025-05-30T17:21:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    21,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-18T16:21:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations"
                },
                "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Muneeb Khan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "Accepted at ACL 2025 (Findings)",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v2",
                "updated": "2025-05-30T17:20:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    20,
                    43,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11361v3",
                "updated": "2025-05-30T17:17:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    17,
                    11,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-17T02:18:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    18,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment"
                },
                "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ashmal Vayani"
                    },
                    {
                        "name": "Aditya Jain"
                    },
                    {
                        "name": "Aravind Narayanan"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    },
                    {
                        "name": "Rizwan Qureshi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06824v2",
                "updated": "2025-05-30T17:17:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    17,
                    4,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-11T09:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs"
                },
                "summary": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs."
                },
                "authors": [
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Amal Zouaq"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24803v1",
                "updated": "2025-05-30T17:08:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T17:08:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    8,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Guiding Generative Storytelling with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Generative Storytelling with Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."
                },
                "authors": [
                    {
                        "name": "Zhijun Pan"
                    },
                    {
                        "name": "Antonios Andronis"
                    },
                    {
                        "name": "Eva Hayek"
                    },
                    {
                        "name": "Oscar AP Wilkinson"
                    },
                    {
                        "name": "Ilya Lasy"
                    },
                    {
                        "name": "Annette Parry"
                    },
                    {
                        "name": "Guy Gadney"
                    },
                    {
                        "name": "Tim J. Smith"
                    },
                    {
                        "name": "Mick Grierson"
                    }
                ],
                "author_detail": {
                    "name": "Mick Grierson"
                },
                "author": "Mick Grierson",
                "arxiv_comment": "This manuscript was submitted for peer review in January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14248v2",
                "updated": "2025-05-30T17:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    1,
                    57,
                    4,
                    150,
                    0
                ],
                "published": "2024-10-18T07:52:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    52,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models"
                },
                "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models."
                },
                "authors": [
                    {
                        "name": "Olga Loginova"
                    },
                    {
                        "name": "Oleksandr Bezrukov"
                    },
                    {
                        "name": "Ravi Shekhar"
                    },
                    {
                        "name": "Alexey Kravets"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Kravets"
                },
                "author": "Alexey Kravets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11404v2",
                "updated": "2025-05-30T16:59:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    59,
                    23,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-17T03:42:28Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    42,
                    28,
                    0,
                    48,
                    0
                ],
                "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large\n  Language Models"
                },
                "summary": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning."
                },
                "authors": [
                    {
                        "name": "Hanxing Ding"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24791v1",
                "updated": "2025-05-30T16:53:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    53,
                    15,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:53:15Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    53,
                    15,
                    4,
                    150,
                    0
                ],
                "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective\n  Jacobi Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Acceleration of Autoregressive Normalizing Flows by Selective\n  Jacobi Decoding"
                },
                "summary": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity."
                },
                "authors": [
                    {
                        "name": "Jiaru Zhang"
                    },
                    {
                        "name": "Juanwu Lu"
                    },
                    {
                        "name": "Ziran Wang"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24788v1",
                "updated": "2025-05-30T16:48:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    48,
                    38,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:48:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    48,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "Drop Dropout on Single-Epoch Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drop Dropout on Single-Epoch Language Model Pretraining"
                },
                "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining."
                },
                "authors": [
                    {
                        "name": "Houjun Liu"
                    },
                    {
                        "name": "John Bauer"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "arxiv_comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24785v1",
                "updated": "2025-05-30T16:46:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    29,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:46:29Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    46,
                    29,
                    4,
                    150,
                    0
                ],
                "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXP-Bench: Can AI Conduct AI Research Experiments?"
                },
                "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench."
                },
                "authors": [
                    {
                        "name": "Patrick Tser Jern Kon"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Xinyi Zhu"
                    },
                    {
                        "name": "Qiuyi Ding"
                    },
                    {
                        "name": "Jingjia Peng"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Yibo Huang"
                    },
                    {
                        "name": "Yiming Qiu"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ang Chen"
                },
                "author": "Ang Chen",
                "arxiv_comment": "45 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24778v1",
                "updated": "2025-05-30T16:41:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:41:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?"
                },
                "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15712v2",
                "updated": "2025-05-30T16:35:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-20T09:33:31Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    9,
                    33,
                    31,
                    4,
                    355,
                    0
                ],
                "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining"
                },
                "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."
                },
                "authors": [
                    {
                        "name": "Maike Züfle"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24773v1",
                "updated": "2025-05-30T16:35:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    35,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with\n  Resource-Aware Low-Rank Adaption"
                },
                "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world."
                },
                "authors": [
                    {
                        "name": "Yajie Zhou"
                    },
                    {
                        "name": "Xiaoyi Pang"
                    },
                    {
                        "name": "Zhibo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Wang"
                },
                "author": "Zhibo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22767v2",
                "updated": "2025-05-30T16:31:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    58,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-28T18:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    36,
                    0,
                    2,
                    148,
                    0
                ],
                "title": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge"
                },
                "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems."
                },
                "authors": [
                    {
                        "name": "Eleni Vasilaki"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Vasilaki"
                },
                "author": "Eleni Vasilaki",
                "arxiv_comment": "6 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20771v4",
                "updated": "2025-05-30T16:31:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    22,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-26T17:53:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data"
                },
                "summary": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health diagnosis and\nmonitoring (e.g., assessing pain and depression). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable inter-subject variability in\nexpressions. Source-free (unsupervised) domain adaptation (SFDA) methods may be\nemployed to adapt a pre-trained source model using only unlabeled target domain\ndata, thereby avoiding data privacy, storage, and transmission issues.\nTypically, SFDA methods adapt to a target domain dataset corresponding to an\nentire population and assume it includes data from all recognition classes.\nHowever, collecting such comprehensive target data can be difficult or even\nimpossible for FER in healthcare applications. In many real-world scenarios, it\nmay be feasible to collect a short neutral control video (which displays only\nneutral expressions) from target subjects before deployment. These videos can\nbe used to adapt a model to better handle the variability of expressions among\nsubjects. This paper introduces the Disentangled SFDA (DSFDA) method to address\nthe challenge posed by adapting models with missing target expression data.\nDSFDA leverages data from a neutral target control video for end-to-end\ngeneration and adaptation of target data with missing non-neutral data. Our\nmethod learns to disentangle features related to expressions and identity while\ngenerating the missing non-neutral expression data for the target subject,\nthereby enhancing model accuracy. Additionally, our self-supervision strategy\nimproves model adaptation by reconstructing target images that maintain the\nsame identity and source expression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health diagnosis and\nmonitoring (e.g., assessing pain and depression). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable inter-subject variability in\nexpressions. Source-free (unsupervised) domain adaptation (SFDA) methods may be\nemployed to adapt a pre-trained source model using only unlabeled target domain\ndata, thereby avoiding data privacy, storage, and transmission issues.\nTypically, SFDA methods adapt to a target domain dataset corresponding to an\nentire population and assume it includes data from all recognition classes.\nHowever, collecting such comprehensive target data can be difficult or even\nimpossible for FER in healthcare applications. In many real-world scenarios, it\nmay be feasible to collect a short neutral control video (which displays only\nneutral expressions) from target subjects before deployment. These videos can\nbe used to adapt a model to better handle the variability of expressions among\nsubjects. This paper introduces the Disentangled SFDA (DSFDA) method to address\nthe challenge posed by adapting models with missing target expression data.\nDSFDA leverages data from a neutral target control video for end-to-end\ngeneration and adaptation of target data with missing non-neutral data. Our\nmethod learns to disentangle features related to expressions and identity while\ngenerating the missing non-neutral expression data for the target subject,\nthereby enhancing model accuracy. Additionally, our self-supervision strategy\nimproves model adaptation by reconstructing target images that maintain the\nsame identity and source expression."
                },
                "authors": [
                    {
                        "name": "Masoumeh Sharafi"
                    },
                    {
                        "name": "Emma Ollivier"
                    },
                    {
                        "name": "Muhammad Osama Zeeshan"
                    },
                    {
                        "name": "Soufiane Belharbi"
                    },
                    {
                        "name": "Marco Pedersoli"
                    },
                    {
                        "name": "Alessandro Lameiras Koerich"
                    },
                    {
                        "name": "Simon Bacon"
                    },
                    {
                        "name": "Eric Granger"
                    }
                ],
                "author_detail": {
                    "name": "Eric Granger"
                },
                "author": "Eric Granger",
                "arxiv_comment": "13 pages, 13 figures, FG 2025: IEEE Conf. on Automatic Face and\n  Gesture Recognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24768v1",
                "updated": "2025-05-30T16:31:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    5,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:31:05Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    31,
                    5,
                    4,
                    150,
                    0
                ],
                "title": "From Macro to Micro: Probing Dataset Diversity in Language Model\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Macro to Micro: Probing Dataset Diversity in Language Model\n  Fine-Tuning"
                },
                "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xuhong Li"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Kun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kun Liu"
                },
                "author": "Kun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24767v1",
                "updated": "2025-05-30T16:30:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    30,
                    54,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:30:54Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    30,
                    54,
                    4,
                    150,
                    0
                ],
                "title": "A survey of using EHR as real-world evidence for discovering and\n  validating new drug indications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of using EHR as real-world evidence for discovering and\n  validating new drug indications"
                },
                "summary": "Electronic Health Records (EHRs) have been increasingly used as real-world\nevidence (RWE) to support the discovery and validation of new drug indications.\nThis paper surveys current approaches to EHR-based drug repurposing, covering\ndata sources, processing methodologies, and representation techniques. It\ndiscusses study designs and statistical frameworks for evaluating drug\nefficacy. Key challenges in validation are discussed, with emphasis on the role\nof large language models (LLMs) and target trial emulation. By synthesizing\nrecent developments and methodological advances, this work provides a\nfoundational resource for researchers aiming to translate real-world data into\nactionable drug-repurposing evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) have been increasingly used as real-world\nevidence (RWE) to support the discovery and validation of new drug indications.\nThis paper surveys current approaches to EHR-based drug repurposing, covering\ndata sources, processing methodologies, and representation techniques. It\ndiscusses study designs and statistical frameworks for evaluating drug\nefficacy. Key challenges in validation are discussed, with emphasis on the role\nof large language models (LLMs) and target trial emulation. By synthesizing\nrecent developments and methodological advances, this work provides a\nfoundational resource for researchers aiming to translate real-world data into\nactionable drug-repurposing evidence."
                },
                "authors": [
                    {
                        "name": "Nabasmita Talukdar"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Shreya Paithankar"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Bin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Chen"
                },
                "author": "Bin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24758v1",
                "updated": "2025-05-30T16:18:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    58,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:18:58Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    58,
                    4,
                    150,
                    0
                ],
                "title": "Survey: Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey: Graph Databases"
                },
                "summary": "Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions."
                },
                "authors": [
                    {
                        "name": "Miguel E. Coimbra"
                    },
                    {
                        "name": "Lucie Svitáková"
                    },
                    {
                        "name": "Alexandre P. Francisco"
                    },
                    {
                        "name": "Luís Veiga"
                    }
                ],
                "author_detail": {
                    "name": "Luís Veiga"
                },
                "author": "Luís Veiga",
                "arxiv_comment": "47 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24757v1",
                "updated": "2025-05-30T16:18:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    50,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:18:50Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    18,
                    50,
                    4,
                    150,
                    0
                ],
                "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in\n  Systematic Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in\n  Systematic Literature Reviews"
                },
                "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available."
                },
                "authors": [
                    {
                        "name": "Christian Jaumann"
                    },
                    {
                        "name": "Andreas Wiedholz"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Annemarie Friedrich"
                },
                "author": "Annemarie Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24749v1",
                "updated": "2025-05-30T16:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    8,
                    40,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T16:08:40Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    8,
                    40,
                    4,
                    150,
                    0
                ],
                "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating\n  Memory-Efficient LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating\n  Memory-Efficient LLM Training"
                },
                "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Guy Smorodinsky"
                    },
                    {
                        "name": "Tom Tirer"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24726v1",
                "updated": "2025-05-30T15:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    49,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:49:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    49,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning"
                },
                "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback."
                },
                "authors": [
                    {
                        "name": "Shelly Bensal"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Dmytro Mozolevskyi"
                    },
                    {
                        "name": "Muayad Ali"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24724v1",
                "updated": "2025-05-30T15:47:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    47,
                    13,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:47:13Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    47,
                    13,
                    4,
                    150,
                    0
                ],
                "title": "Talking Transactions: Decentralized Communication through Ethereum Input\n  Data Messages (IDMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking Transactions: Decentralized Communication through Ethereum Input\n  Data Messages (IDMs)"
                },
                "summary": "Can you imagine, blockchain transactions can talk! In this paper, we study\nhow they talk and what they talk about. We focus on the input data field of\nEthereum transactions, which is designed to allow external callers to interact\nwith smart contracts. In practice, this field also enables users to embed\nnatural language messages into transactions. Users can leverage these Input\nData Messages (IDMs) for peer-to-peer communication. This means that, beyond\nEthereum's well-known role as a financial infrastructure, it also serves as a\ndecentralized communication medium.\n  We present the first large-scale analysis of Ethereum IDMs from the genesis\nblock to February 2024 (3134 days). We filter IDMs to extract 867,140\ntransactions with informative IDMs and use LLMs for language detection. We find\nthat English (95.4%) and Chinese (4.4%) dominate the use of natural languages\nin IDMs. Interestingly, English IDMs center on security and scam warnings (24%)\nwith predominantly negative emotions, while Chinese IDMs emphasize emotional\nexpression and social connection (44%) with a more positive tone. We also\nobserve that longer English IDMs often transfer high ETH values for\nprotocol-level purposes, while longer Chinese IDMs tend to involve symbolic\ntransfer amounts for emotional intent. Moreover, we find that the IDM\nparticipants tend to form small, loosely connected communities (59.99%). Our\nfindings highlight culturally and functionally divergent use cases of the IDM\nchannel across user communities. We further examine the security relevance of\nIDMs in on-chain attacks. Many victims use them to appeal to attackers for fund\nrecovery. IDMs containing negotiations or reward offers are linked to higher\nreply rates. We also analyze IDMs' regulatory implications. Their misuse for\nabuse, threats, and sexual solicitation reveals the urgent need for content\nmoderation and regulation in decentralized systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can you imagine, blockchain transactions can talk! In this paper, we study\nhow they talk and what they talk about. We focus on the input data field of\nEthereum transactions, which is designed to allow external callers to interact\nwith smart contracts. In practice, this field also enables users to embed\nnatural language messages into transactions. Users can leverage these Input\nData Messages (IDMs) for peer-to-peer communication. This means that, beyond\nEthereum's well-known role as a financial infrastructure, it also serves as a\ndecentralized communication medium.\n  We present the first large-scale analysis of Ethereum IDMs from the genesis\nblock to February 2024 (3134 days). We filter IDMs to extract 867,140\ntransactions with informative IDMs and use LLMs for language detection. We find\nthat English (95.4%) and Chinese (4.4%) dominate the use of natural languages\nin IDMs. Interestingly, English IDMs center on security and scam warnings (24%)\nwith predominantly negative emotions, while Chinese IDMs emphasize emotional\nexpression and social connection (44%) with a more positive tone. We also\nobserve that longer English IDMs often transfer high ETH values for\nprotocol-level purposes, while longer Chinese IDMs tend to involve symbolic\ntransfer amounts for emotional intent. Moreover, we find that the IDM\nparticipants tend to form small, loosely connected communities (59.99%). Our\nfindings highlight culturally and functionally divergent use cases of the IDM\nchannel across user communities. We further examine the security relevance of\nIDMs in on-chain attacks. Many victims use them to appeal to attackers for fund\nrecovery. IDMs containing negotiations or reward offers are linked to higher\nreply rates. We also analyze IDMs' regulatory implications. Their misuse for\nabuse, threats, and sexual solicitation reveals the urgent need for content\nmoderation and regulation in decentralized systems."
                },
                "authors": [
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Endong Liu"
                    },
                    {
                        "name": "Pascal Berrang"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13772v2",
                "updated": "2025-05-30T15:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    44,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-19T23:18:27Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    23,
                    18,
                    27,
                    0,
                    139,
                    0
                ],
                "title": "Krikri: Advancing Open Large Language Models for Greek",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krikri: Advancing Open Large Language Models for Greek"
                },
                "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation."
                },
                "authors": [
                    {
                        "name": "Dimitris Roussis"
                    },
                    {
                        "name": "Leon Voukoutis"
                    },
                    {
                        "name": "Georgios Paraskevopoulos"
                    },
                    {
                        "name": "Sokratis Sofianopoulos"
                    },
                    {
                        "name": "Prokopis Prokopidis"
                    },
                    {
                        "name": "Vassilis Papavasileiou"
                    },
                    {
                        "name": "Athanasios Katsamanis"
                    },
                    {
                        "name": "Stelios Piperidis"
                    },
                    {
                        "name": "Vassilis Katsouros"
                    }
                ],
                "author_detail": {
                    "name": "Vassilis Katsouros"
                },
                "author": "Vassilis Katsouros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05258v2",
                "updated": "2025-05-30T15:37:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    37,
                    19,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-07T16:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Adrián Bazaga"
                    },
                    {
                        "name": "Rexhina Blloshmi"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Adrià de Gispert"
                    }
                ],
                "author_detail": {
                    "name": "Adrià de Gispert"
                },
                "author": "Adrià de Gispert",
                "arxiv_comment": "ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24716v1",
                "updated": "2025-05-30T15:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    36,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    36,
                    56,
                    4,
                    150,
                    0
                ],
                "title": "Towards Scalable Schema Mapping using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Schema Mapping using Large Language Models"
                },
                "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering."
                },
                "authors": [
                    {
                        "name": "Christopher Buss"
                    },
                    {
                        "name": "Mahdis Safari"
                    },
                    {
                        "name": "Arash Termehchy"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "David Maier"
                    }
                ],
                "author_detail": {
                    "name": "David Maier"
                },
                "author": "David Maier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24712v1",
                "updated": "2025-05-30T15:32:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    32,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:32:48Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    32,
                    48,
                    4,
                    150,
                    0
                ],
                "title": "HESEIA: A community-based dataset for evaluating social biases in large\n  language models, co-designed in real school settings in Latin America",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HESEIA: A community-based dataset for evaluating social biases in large\n  language models, co-designed in real school settings in Latin America"
                },
                "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities."
                },
                "authors": [
                    {
                        "name": "Guido Ivetta"
                    },
                    {
                        "name": "Marcos J. Gomez"
                    },
                    {
                        "name": "Sofía Martinelli"
                    },
                    {
                        "name": "Pietro Palombini"
                    },
                    {
                        "name": "M. Emilia Echeveste"
                    },
                    {
                        "name": "Nair Carolina Mazzeo"
                    },
                    {
                        "name": "Beatriz Busaniche"
                    },
                    {
                        "name": "Luciana Benotti"
                    }
                ],
                "author_detail": {
                    "name": "Luciana Benotti"
                },
                "arxiv_affiliation": "Fundación Vía Libre",
                "author": "Luciana Benotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24710v1",
                "updated": "2025-05-30T15:30:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    30,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:30:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    30,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through\n  Learning, Adapting and Acting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-aware Large Language Models: Enhancing Decision-Making Through\n  Learning, Adapting and Acting"
                },
                "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Haipeng Zhu"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Zhifeng Hao"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Junjian Ye"
                    },
                    {
                        "name": "Ruichu Cai"
                    }
                ],
                "author_detail": {
                    "name": "Ruichu Cai"
                },
                "author": "Ruichu Cai",
                "arxiv_comment": "Accepted by IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24701v1",
                "updated": "2025-05-30T15:24:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    24,
                    17,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:24:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    24,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for\n  Real-World Evaluation and Model Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for\n  Real-World Evaluation and Model Comparison"
                },
                "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data."
                },
                "authors": [
                    {
                        "name": "Tejul Pandit"
                    },
                    {
                        "name": "Meet Raval"
                    },
                    {
                        "name": "Dhvani Upadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Dhvani Upadhyay"
                },
                "author": "Dhvani Upadhyay",
                "arxiv_comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17439v2",
                "updated": "2025-05-30T15:19:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    19,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-21T17:59:10Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    10,
                    4,
                    80,
                    0
                ],
                "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines."
                },
                "authors": [
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chenlin Ming"
                    },
                    {
                        "name": "H. Vicky Zhao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "ACL'25 Findings, Code is available at https://github.com/pzs19/LEMMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03621v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03621v4",
                "updated": "2025-05-30T15:19:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    19,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-04T15:26:10Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    26,
                    10,
                    2,
                    339,
                    0
                ],
                "title": "JPPO++: Joint Power and Denoising-inspired Prompt Optimization for\n  Mobile LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JPPO++: Joint Power and Denoising-inspired Prompt Optimization for\n  Mobile LLM Services"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into mobile services\nover wireless networks to support complex user requests. This trend has led to\nlonger prompts, which improve LLMs' performance but increase data transmission\ncosts and require more processing time, thereby reducing overall system\nefficiency and negatively impacting user experience. To address these\nchallenges, we propose Joint Prompt and Power Optimization (JPPO), a framework\nthat jointly optimizes prompt compression and wireless transmission power for\nmobile LLM services. JPPO leverages a Small Language Model (SLM) deployed at\nedge devices to perform lightweight prompt compression, reducing communication\nload before transmission to the cloud-based LLM. A Deep Reinforcement Learning\n(DRL) agent dynamically adjusts both the compression ratio and transmission\npower based on network conditions and service constraints, aiming to minimize\nservice time while preserving response fidelity. We further extend the\nframework to JPPO++, which introduces a denoising-inspired compression scheme.\nThis design performs iterative prompt refinement by progressively removing less\ninformative tokens, allowing for more aggressive yet controlled compression.\nExperimental results show that JPPO++ reduces service time by 17% compared to\nthe no-compression baseline while maintaining output quality. Under\ncompression-prioritized settings, a reduction of up to 16x in prompt length can\nbe achieved with an acceptable loss in accuracy. Specifically, JPPO with a 16x\nratio reduces total service time by approximately 42.3%, and JPPO++ further\nimproves this reduction to 46.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into mobile services\nover wireless networks to support complex user requests. This trend has led to\nlonger prompts, which improve LLMs' performance but increase data transmission\ncosts and require more processing time, thereby reducing overall system\nefficiency and negatively impacting user experience. To address these\nchallenges, we propose Joint Prompt and Power Optimization (JPPO), a framework\nthat jointly optimizes prompt compression and wireless transmission power for\nmobile LLM services. JPPO leverages a Small Language Model (SLM) deployed at\nedge devices to perform lightweight prompt compression, reducing communication\nload before transmission to the cloud-based LLM. A Deep Reinforcement Learning\n(DRL) agent dynamically adjusts both the compression ratio and transmission\npower based on network conditions and service constraints, aiming to minimize\nservice time while preserving response fidelity. We further extend the\nframework to JPPO++, which introduces a denoising-inspired compression scheme.\nThis design performs iterative prompt refinement by progressively removing less\ninformative tokens, allowing for more aggressive yet controlled compression.\nExperimental results show that JPPO++ reduces service time by 17% compared to\nthe no-compression baseline while maintaining output quality. Under\ncompression-prioritized settings, a reduction of up to 16x in prompt length can\nbe achieved with an acceptable loss in accuracy. Specifically, JPPO with a 16x\nratio reduces total service time by approximately 42.3%, and JPPO++ further\nimproves this reduction to 46.5%."
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2411.18010",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03621v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03621v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17261v2",
                "updated": "2025-05-30T15:15:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T09:37:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator"
                },
                "summary": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails, and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments. Our\nproject is at https://yfthu.github.io/HEIE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails, and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments. Our\nproject is at https://yfthu.github.io/HEIE/."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ru Zhen"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24691v1",
                "updated": "2025-05-30T15:15:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:15:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    15,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing\n  Cross-Lingual Transfer in Low-Resource Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing\n  Cross-Lingual Transfer in Low-Resource Scenarios"
                },
                "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages."
                },
                "authors": [
                    {
                        "name": "Gerard I. Gállego"
                    },
                    {
                        "name": "Oriol Pareras"
                    },
                    {
                        "name": "Martí Cortada Garcia"
                    },
                    {
                        "name": "Lucas Takanori"
                    },
                    {
                        "name": "Javier Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernando"
                },
                "author": "Javier Hernando",
                "arxiv_comment": "Accepted at Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24688v1",
                "updated": "2025-05-30T15:11:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:11:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration"
                },
                "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24683v1",
                "updated": "2025-05-30T15:08:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:08:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation"
                },
                "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Kevin Duh"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "22 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24680v1",
                "updated": "2025-05-30T15:06:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    6,
                    8,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:06:08Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    6,
                    8,
                    4,
                    150,
                    0
                ],
                "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Linear Patch Revives Layer-Pruned Large Language Models"
                },
                "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card."
                },
                "authors": [
                    {
                        "name": "Xinrui Chen"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Tian Guan"
                    },
                    {
                        "name": "Yonghong He"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24672v1",
                "updated": "2025-05-30T15:02:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    2,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    2,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis"
                },
                "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset."
                },
                "authors": [
                    {
                        "name": "Xiaorui Wu"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xuanhong Li"
                    },
                    {
                        "name": "Chong Teng"
                    },
                    {
                        "name": "Donghong Ji"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24671v1",
                "updated": "2025-05-30T15:01:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple LLM Agents Debate for Equitable Cultural Alignment"
                },
                "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Rachel Rudinger"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "37 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17229v2",
                "updated": "2025-05-30T14:59:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    59,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-21T15:32:24Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples."
                },
                "authors": [
                    {
                        "name": "Albert Sawczyn"
                    },
                    {
                        "name": "Jakub Binkowski"
                    },
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Bogdan Gabrys"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16772v3",
                "updated": "2025-05-30T14:57:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    57,
                    33,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-24T01:35:32Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    35,
                    32,
                    0,
                    55,
                    0
                ],
                "title": "Model-Based Exploration in Monitored Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Exploration in Monitored Markov Decision Processes"
                },
                "summary": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable."
                },
                "authors": [
                    {
                        "name": "Alireza Kazemipour"
                    },
                    {
                        "name": "Simone Parisi"
                    },
                    {
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "name": "Michael Bowling"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bowling"
                },
                "author": "Michael Bowling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23473v2",
                "updated": "2025-05-30T14:54:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    54,
                    29,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-29T14:26:46Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    26,
                    46,
                    3,
                    149,
                    0
                ],
                "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions"
                },
                "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context."
                },
                "authors": [
                    {
                        "name": "Xiaorui Wu"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Chong Teng"
                    },
                    {
                        "name": "Yuxiang Peng"
                    },
                    {
                        "name": "Li Zheng"
                    },
                    {
                        "name": "Donghong Ji"
                    },
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22366v3",
                "updated": "2025-05-30T14:51:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    51,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2024-10-28T19:01:18Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    1,
                    18,
                    0,
                    302,
                    0
                ],
                "title": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models"
                },
                "summary": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models."
                },
                "authors": [
                    {
                        "name": "Viacheslav Surkov"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Antonio Mari"
                    },
                    {
                        "name": "Mikhail Terekhov"
                    },
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24658v1",
                "updated": "2025-05-30T14:46:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    46,
                    22,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:46:22Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    46,
                    22,
                    4,
                    150,
                    0
                ],
                "title": "Can LLMs and humans be friends? Uncovering factors affecting human-AI\n  intimacy formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs and humans be friends? Uncovering factors affecting human-AI\n  intimacy formation"
                },
                "summary": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation."
                },
                "authors": [
                    {
                        "name": "Yeseon Hong"
                    },
                    {
                        "name": "Junhyuk Choi"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "30 pages, 2figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05532v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05532v2",
                "updated": "2025-05-30T14:45:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    45,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-08T15:10:29Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    15,
                    10,
                    29,
                    3,
                    128,
                    0
                ],
                "title": "Design and FPGA Implementation of WOMBAT: A Deep Neural Network Level-1\n  Trigger System for Jet Substructure Identification and Boosted $H\\rightarrow\n  b\\bar{b}$ Tagging at the CMS Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and FPGA Implementation of WOMBAT: A Deep Neural Network Level-1\n  Trigger System for Jet Substructure Identification and Boosted $H\\rightarrow\n  b\\bar{b}$ Tagging at the CMS Experiment"
                },
                "summary": "This thesis investigates the physics performance, trigger efficiency, and\nField Programmable Gate Array (FPGA) implementation of machine learning\n(ML)-based algorithms for Lorentz-boosted $H\\rightarrow b\\bar{b}$ tagging\nwithin the CMS Level-1 Trigger (L1T) under Phase-1 conditions. The proposed\nalgorithm, WOMBAT (Wide Object ML Boosted Algorithm Trigger), comprises a\nhigh-performance Master Model (W-MM) and a quantized, FPGA-synthesizable\nApprentice Model (W-AM), benchmarked against the standard Single Jet 180 and\nthe custom rule-based JEDI (Jet Event Deterministic Identifier) triggers.\n  All algorithms process calorimeter trigger primitive data to localize boosted\n$H\\rightarrow b\\bar{b}$ jets. Outputs are post-processed minimally to yield\nreal-valued $(\\eta, \\phi)$ jet coordinates at trigger tower granularity.\n  Trigger rates are evaluated using 2023 CMS ZeroBias data (0.64 fb$^{-1}$),\nwith efficiency assessed via a Monte Carlo sample of $H\\rightarrow b\\bar{b}$\noffline reconstructed AK8 jets. W-MM achieves a 1 kHz rate at an offline jet\n$p_T$ threshold of 146.8 GeV, 40.6 GeV lower than Single Jet 180, while\nmaintaining comparable signal efficiency. W-AM reduces the threshold further to\n140.4 GeV, with reduced efficiency due to fixed-output constraints and limited\nmulti-jet handling.\n  FPGA implementation targeting the Xilinx Virtex-7 XC7VX690T confirms that\nW-AM meets resource constraints with a pre-place-and-route latency of 22 clock\ncycles (137.5 ns). In contrast, JEDI requires excessive resource usage and a\n56-cycle latency, surpassing the 14-cycle L1T budget.\n  Originally developed for Run-3 CMS L1T, WOMBAT serves as a proof-of-concept\nfor Phase-2 triggers, where hardware advances will enable online deployment of\nmore sophisticated ML-based L1T systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This thesis investigates the physics performance, trigger efficiency, and\nField Programmable Gate Array (FPGA) implementation of machine learning\n(ML)-based algorithms for Lorentz-boosted $H\\rightarrow b\\bar{b}$ tagging\nwithin the CMS Level-1 Trigger (L1T) under Phase-1 conditions. The proposed\nalgorithm, WOMBAT (Wide Object ML Boosted Algorithm Trigger), comprises a\nhigh-performance Master Model (W-MM) and a quantized, FPGA-synthesizable\nApprentice Model (W-AM), benchmarked against the standard Single Jet 180 and\nthe custom rule-based JEDI (Jet Event Deterministic Identifier) triggers.\n  All algorithms process calorimeter trigger primitive data to localize boosted\n$H\\rightarrow b\\bar{b}$ jets. Outputs are post-processed minimally to yield\nreal-valued $(\\eta, \\phi)$ jet coordinates at trigger tower granularity.\n  Trigger rates are evaluated using 2023 CMS ZeroBias data (0.64 fb$^{-1}$),\nwith efficiency assessed via a Monte Carlo sample of $H\\rightarrow b\\bar{b}$\noffline reconstructed AK8 jets. W-MM achieves a 1 kHz rate at an offline jet\n$p_T$ threshold of 146.8 GeV, 40.6 GeV lower than Single Jet 180, while\nmaintaining comparable signal efficiency. W-AM reduces the threshold further to\n140.4 GeV, with reduced efficiency due to fixed-output constraints and limited\nmulti-jet handling.\n  FPGA implementation targeting the Xilinx Virtex-7 XC7VX690T confirms that\nW-AM meets resource constraints with a pre-place-and-route latency of 22 clock\ncycles (137.5 ns). In contrast, JEDI requires excessive resource usage and a\n56-cycle latency, surpassing the 14-cycle L1T budget.\n  Originally developed for Run-3 CMS L1T, WOMBAT serves as a proof-of-concept\nfor Phase-2 triggers, where hardware advances will enable online deployment of\nmore sophisticated ML-based L1T systems."
                },
                "authors": [
                    {
                        "name": "Mila Bileska"
                    }
                ],
                "author_detail": {
                    "name": "Mila Bileska"
                },
                "author": "Mila Bileska",
                "arxiv_comment": "Submitted as Princeton University's Senior Thesis requirement.\n  Published at jinst.sissa.it/jinst/theses/2025_JINST_TH_001.jsp",
                "arxiv_journal_ref": "Journal of Instrumentation, 2025 JINST TH 001",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05532v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05532v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24654v1",
                "updated": "2025-05-30T14:41:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    41,
                    38,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:41:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    41,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms"
                },
                "summary": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system."
                },
                "authors": [
                    {
                        "name": "Maria Rafaela Gkeka"
                    },
                    {
                        "name": "Bowen Sun"
                    },
                    {
                        "name": "Evgenia Smirni"
                    },
                    {
                        "name": "Christos D. Antonopoulos"
                    },
                    {
                        "name": "Spyros Lalis"
                    },
                    {
                        "name": "Nikolaos Bellas"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Bellas"
                },
                "author": "Nikolaos Bellas",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40, 68T45, 68M25,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00592v2",
                "updated": "2025-05-30T14:40:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    40,
                    56,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-01T23:13:10Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    23,
                    13,
                    10,
                    5,
                    32,
                    0
                ],
                "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M+: Extending MemoryLLM with Scalable Long-Term Memory"
                },
                "summary": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dmitry Krotov"
                    },
                    {
                        "name": "Yuanzhe Hu"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Dan Gutfreund"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Zexue He"
                    }
                ],
                "author_detail": {
                    "name": "Zexue He"
                },
                "author": "Zexue He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21701v2",
                "updated": "2025-05-30T14:39:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    39,
                    34,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-27T19:39:49Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    39,
                    49,
                    1,
                    147,
                    0
                ],
                "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge\n  Probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge\n  Probing"
                },
                "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21693v2",
                "updated": "2025-05-30T14:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    37,
                    57,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-27T19:29:40Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    29,
                    40,
                    1,
                    147,
                    0
                ],
                "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs"
                },
                "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaños"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustín Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01986v2",
                "updated": "2025-05-30T14:28:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    28,
                    59,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-31T07:43:12Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    43,
                    12,
                    0,
                    90,
                    0
                ],
                "title": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation"
                },
                "summary": "The rapid advancements in LLMs have driven the adoption of generative AI in\nvarious domains, including Electronic Design Automation (EDA). Unlike\ntraditional software development, EDA presents unique challenges, as generated\nRTL code must not only be syntactically correct and functionally accurate but\nalso synthesizable by hardware generators while meeting performance, power, and\narea constraints. These additional requirements introduce complexities that\nexisting code-generation benchmarks often fail to capture, limiting their\neffectiveness in evaluating LLMs for RTL generation. To address this gap, we\npropose TuRTLe, a unified evaluation framework designed to systematically\nassess LLMs across key RTL generation tasks. TuRTLe integrates multiple\nexisting benchmarks and automates the evaluation process, enabling a\ncomprehensive assessment of LLM performance in syntax correctness, functional\ncorrectness, synthesis, PPA optimization, and exact line completion. Using this\nframework, we benchmark a diverse set of open LLMs and analyze their strengths\nand weaknesses in EDA-specific tasks. Our results show that reasoning-based\nmodels, such as DeepSeek R1, consistently outperform others across multiple\nevaluation criteria, but at the cost of increased computational overhead and\ninference latency. Additionally, base models are better suited in module\ncompletion tasks, while instruct-tuned models perform better in\nspecification-to-RTL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in LLMs have driven the adoption of generative AI in\nvarious domains, including Electronic Design Automation (EDA). Unlike\ntraditional software development, EDA presents unique challenges, as generated\nRTL code must not only be syntactically correct and functionally accurate but\nalso synthesizable by hardware generators while meeting performance, power, and\narea constraints. These additional requirements introduce complexities that\nexisting code-generation benchmarks often fail to capture, limiting their\neffectiveness in evaluating LLMs for RTL generation. To address this gap, we\npropose TuRTLe, a unified evaluation framework designed to systematically\nassess LLMs across key RTL generation tasks. TuRTLe integrates multiple\nexisting benchmarks and automates the evaluation process, enabling a\ncomprehensive assessment of LLM performance in syntax correctness, functional\ncorrectness, synthesis, PPA optimization, and exact line completion. Using this\nframework, we benchmark a diverse set of open LLMs and analyze their strengths\nand weaknesses in EDA-specific tasks. Our results show that reasoning-based\nmodels, such as DeepSeek R1, consistently outperform others across multiple\nevaluation criteria, but at the cost of increased computational overhead and\ninference latency. Additionally, base models are better suited in module\ncompletion tasks, while instruct-tuned models perform better in\nspecification-to-RTL tasks."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Gokcen Kestor"
                    },
                    {
                        "name": "Emanuele Parisi"
                    },
                    {
                        "name": "Miquel Albertí-Binimelis"
                    },
                    {
                        "name": "Cristian Gutierrez"
                    },
                    {
                        "name": "Razine Moundir Ghorab"
                    },
                    {
                        "name": "Orlando Montenegro"
                    },
                    {
                        "name": "Bernat Homs"
                    },
                    {
                        "name": "Miquel Moreto"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moreto"
                },
                "author": "Miquel Moreto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.5; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24640v1",
                "updated": "2025-05-30T14:27:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    27,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:27:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    27,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "Efficient Text Encoders for Labor Market Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Text Encoders for Labor Market Analysis"
                },
                "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis."
                },
                "authors": [
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Jeroen Van Hautte"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24635v1",
                "updated": "2025-05-30T14:25:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    45,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:25:45Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    25,
                    45,
                    4,
                    150,
                    0
                ],
                "title": "Disentangling Language and Culture for Evaluating Multilingual Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Language and Culture for Evaluating Multilingual Large\n  Language Models"
                },
                "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/."
                },
                "authors": [
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24630v1",
                "updated": "2025-05-30T14:23:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    23,
                    32,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:23:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    23,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v1",
                "updated": "2025-05-30T14:16:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24622v1",
                "updated": "2025-05-30T14:13:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    13,
                    21,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:13:21Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    13,
                    21,
                    4,
                    150,
                    0
                ],
                "title": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated\n  Questions for Predicting Startup Success",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated\n  Questions for Predicting Startup Success"
                },
                "summary": "Predicting startup success requires models that are both accurate and\ninterpretable. We present a lightweight ensemble framework that combines YES/NO\nquestions generated by large language models (LLMs), forming a transparent\ndecision-making system. Each question acts as a weak heuristic, and by\nfiltering, ranking, and aggregating them through a threshold-based voting\nmechanism, we construct a strong ensemble predictor. On a test set where 10% of\nstartups are classified as successful, our approach achieves a precision rate\nof 50%, representing a 5x improvement over random selection, while remaining\nfully transparent. When we incorporate expert-guided heuristics into the\ngeneration process, performance improves further to 54% precision. These\nresults highlight the value of combining LLM reasoning with human insight and\ndemonstrate that simple, interpretable ensembles can support high-stakes\ndecisions in domains such as venture capital (VC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting startup success requires models that are both accurate and\ninterpretable. We present a lightweight ensemble framework that combines YES/NO\nquestions generated by large language models (LLMs), forming a transparent\ndecision-making system. Each question acts as a weak heuristic, and by\nfiltering, ranking, and aggregating them through a threshold-based voting\nmechanism, we construct a strong ensemble predictor. On a test set where 10% of\nstartups are classified as successful, our approach achieves a precision rate\nof 50%, representing a 5x improvement over random selection, while remaining\nfully transparent. When we incorporate expert-guided heuristics into the\ngeneration process, performance improves further to 54% precision. These\nresults highlight the value of combining LLM reasoning with human insight and\ndemonstrate that simple, interpretable ensembles can support high-stakes\ndecisions in domains such as venture capital (VC)."
                },
                "authors": [
                    {
                        "name": "Ben Griffin"
                    },
                    {
                        "name": "Joseph Ternasky"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14189v2",
                "updated": "2025-05-30T14:12:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    54,
                    4,
                    150,
                    0
                ],
                "published": "2024-05-23T05:31:41Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    5,
                    31,
                    41,
                    3,
                    144,
                    0
                ],
                "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt\n  Organization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Universal Goal Hijacking with Semantics-guided Prompt\n  Organization"
                },
                "summary": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24621v1",
                "updated": "2025-05-30T14:12:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:12:07Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    12,
                    7,
                    4,
                    150,
                    0
                ],
                "title": "Benchmarking Large Language Models for Cryptanalysis and\n  Mismatched-Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Cryptanalysis and\n  Mismatched-Generalization"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Chencheng Zhu"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14604v2",
                "updated": "2025-05-30T14:11:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    11,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-18T18:03:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    3,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives"
                },
                "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."
                },
                "authors": [
                    {
                        "name": "Sara Sarto"
                    },
                    {
                        "name": "Marcella Cornia"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "IJCAI 2025. Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24618v1",
                "updated": "2025-05-30T14:10:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    10,
                    33,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:10:33Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    10,
                    33,
                    4,
                    150,
                    0
                ],
                "title": "Distributed Intelligence in the Computing Continuum with Active\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Intelligence in the Computing Continuum with Active\n  Inference"
                },
                "summary": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm\nthat spans from local Internet of Things sensors and constrained edge devices\nto large-scale cloud data centers. Its goal is to orchestrate a vast array of\ndiverse and distributed computing resources to support the next generation of\nInternet-based applications. However, the distributed, heterogeneous, and\ndynamic nature of CC platforms demands distributed intelligence for adaptive\nand resilient service management. This article introduces a distributed stream\nprocessing pipeline as a CC use case, where each service is managed by an\nActive Inference (AIF) agent. These agents collaborate to fulfill service needs\nspecified by SLOiDs, a term we introduce to denote Service Level Objectives\nthat are aware of its deployed devices, meaning that non-functional\nrequirements must consider the characteristics of the hosting device. We\ndemonstrate how AIF agents can be modeled and deployed alongside distributed\nservices to manage them autonomously. Our experiments show that AIF agents\nachieve over 90% SLOiD fulfillment when using tested transition models, and\naround 80% when learning the models during deployment. We compare their\nperformance to a multi-agent reinforcement learning algorithm, finding that\nwhile both approaches yield similar results, MARL requires extensive training,\nwhereas AIF agents can operate effectively from the start. Additionally, we\nevaluate the behavior of AIF agents in offloading scenarios, observing a strong\ncapacity for adaptation. Finally, we outline key research directions to advance\nAIF integration in CC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm\nthat spans from local Internet of Things sensors and constrained edge devices\nto large-scale cloud data centers. Its goal is to orchestrate a vast array of\ndiverse and distributed computing resources to support the next generation of\nInternet-based applications. However, the distributed, heterogeneous, and\ndynamic nature of CC platforms demands distributed intelligence for adaptive\nand resilient service management. This article introduces a distributed stream\nprocessing pipeline as a CC use case, where each service is managed by an\nActive Inference (AIF) agent. These agents collaborate to fulfill service needs\nspecified by SLOiDs, a term we introduce to denote Service Level Objectives\nthat are aware of its deployed devices, meaning that non-functional\nrequirements must consider the characteristics of the hosting device. We\ndemonstrate how AIF agents can be modeled and deployed alongside distributed\nservices to manage them autonomously. Our experiments show that AIF agents\nachieve over 90% SLOiD fulfillment when using tested transition models, and\naround 80% when learning the models during deployment. We compare their\nperformance to a multi-agent reinforcement learning algorithm, finding that\nwhile both approaches yield similar results, MARL requires extensive training,\nwhereas AIF agents can operate effectively from the start. Additionally, we\nevaluate the behavior of AIF agents in offloading scenarios, observing a strong\ncapacity for adaptation. Finally, we outline key research directions to advance\nAIF integration in CC platforms."
                },
                "authors": [
                    {
                        "name": "Victor Casamayor Pujol"
                    },
                    {
                        "name": "Boris Sedlak"
                    },
                    {
                        "name": "Tommaso Salvatori"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18893v4",
                "updated": "2025-05-30T14:09:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    9,
                    51,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-24T22:35:32Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    22,
                    35,
                    32,
                    5,
                    144,
                    0
                ],
                "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand\n  AI's Real World Effects"
                },
                "summary": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem."
                },
                "authors": [
                    {
                        "name": "Reva Schwartz"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Heather Frase"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Gabriella Waters"
                    },
                    {
                        "name": "Afaf Taik"
                    },
                    {
                        "name": "Morgan Briggs"
                    },
                    {
                        "name": "Patrick Hall"
                    },
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Kyra Yee"
                    },
                    {
                        "name": "Spencer Thomas"
                    },
                    {
                        "name": "Sundeep Bhandari"
                    },
                    {
                        "name": "Paul Duncan"
                    },
                    {
                        "name": "Andrew Thompson"
                    },
                    {
                        "name": "Maya Carlyle"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Matthew Holmes"
                    },
                    {
                        "name": "Theodora Skeadas"
                    }
                ],
                "author_detail": {
                    "name": "Theodora Skeadas"
                },
                "author": "Theodora Skeadas",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24616v1",
                "updated": "2025-05-30T14:08:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:08:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX"
                },
                "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."
                },
                "authors": [
                    {
                        "name": "Nikita Martynov"
                    },
                    {
                        "name": "Anastasia Mordasheva"
                    },
                    {
                        "name": "Dmitriy Gorbetskiy"
                    },
                    {
                        "name": "Danil Astafurov"
                    },
                    {
                        "name": "Ulyana Isaeva"
                    },
                    {
                        "name": "Elina Basyrova"
                    },
                    {
                        "name": "Sergey Skachkov"
                    },
                    {
                        "name": "Victoria Berestova"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Valeriia Zanina"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "arxiv_comment": "179 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24615v1",
                "updated": "2025-05-30T14:08:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    13,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:08:13Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    13,
                    4,
                    150,
                    0
                ],
                "title": "Harnessing Large Language Models for Scientific Novelty Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Scientific Novelty Detection"
                },
                "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/."
                },
                "authors": [
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Thanh-Son Nguyen"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "arxiv_comment": "15 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09823v3",
                "updated": "2025-05-30T14:06:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    6,
                    34,
                    4,
                    150,
                    0
                ],
                "published": "2024-07-13T09:34:00Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    9,
                    34,
                    0,
                    5,
                    195,
                    0
                ],
                "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs"
                },
                "summary": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped and some work has been done in parallel, there is a notable lack of a\nframework and large scale region-specific datasets queried by native users in\ntheir own languages. This gap hinders the effective benchmarking and the\ndevelopment of fine-tuned models for regional and cultural specificities. In\nthis study, we propose a scalable, language-independent framework, NativQA, to\nseamlessly construct culturally and regionally aligned QA datasets in native\nlanguages, for LLM evaluation and tuning. We demonstrate the efficacy of the\nproposed framework by designing a multilingual natural QA dataset,\nMultiNativQA, consisting of ~64k manually annotated QA pairs in seven\nlanguages, ranging from high to extremely low resource, based on queries from\nnative speakers from 9 regions covering 18 topics. We benchmark open- and\nclosed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA\ndataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other\nexperimental scripts(https://gitlab.com/nativqa/multinativqa) publicly\navailable for the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped and some work has been done in parallel, there is a notable lack of a\nframework and large scale region-specific datasets queried by native users in\ntheir own languages. This gap hinders the effective benchmarking and the\ndevelopment of fine-tuned models for regional and cultural specificities. In\nthis study, we propose a scalable, language-independent framework, NativQA, to\nseamlessly construct culturally and regionally aligned QA datasets in native\nlanguages, for LLM evaluation and tuning. We demonstrate the efficacy of the\nproposed framework by designing a multilingual natural QA dataset,\nMultiNativQA, consisting of ~64k manually annotated QA pairs in seven\nlanguages, ranging from high to extremely low resource, based on queries from\nnative speakers from 9 regions covering 18 topics. We benchmark open- and\nclosed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA\ndataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other\nexperimental scripts(https://gitlab.com/nativqa/multinativqa) publicly\navailable for the community."
                },
                "authors": [
                    {
                        "name": "Md. Arid Hasan"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Fatema Ahmad"
                    },
                    {
                        "name": "Sahinur Rahman Laskar"
                    },
                    {
                        "name": "Sunaya Upadhyay"
                    },
                    {
                        "name": "Vrunda N Sukhadia"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24613v1",
                "updated": "2025-05-30T14:04:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    4,
                    30,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:04:30Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    4,
                    30,
                    4,
                    150,
                    0
                ],
                "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based\n  Dialogue Generation"
                },
                "summary": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task."
                },
                "authors": [
                    {
                        "name": "Daniela Occhipinti"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Malvina Nissim"
                    }
                ],
                "author_detail": {
                    "name": "Malvina Nissim"
                },
                "author": "Malvina Nissim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05268v2",
                "updated": "2025-05-30T14:02:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    2,
                    52,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-07T09:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    9,
                    33,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs"
                },
                "summary": "In recent years, the need for natural language interfaces to knowledge graphs\nhas become increasingly important since they enable easy and efficient access\nto the information contained in them. In particular, property graphs (PGs) have\nseen increased adoption as a means of representing complex structured\ninformation. Despite their growing popularity in industry, PGs remain\nrelatively underrepresented in semantic parsing research with a lack of\nresources for evaluation. To address this gap, we introduce ZOGRASCOPE, a\nbenchmark designed specifically for PGs and queries written in Cypher. Our\nbenchmark includes a diverse set of manually annotated queries of varying\ncomplexity and is organized into three partitions: iid, compositional and\nlength. We complement this paper with a set of experiments that test the\nperformance of different LLMs in a variety of learning settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the need for natural language interfaces to knowledge graphs\nhas become increasingly important since they enable easy and efficient access\nto the information contained in them. In particular, property graphs (PGs) have\nseen increased adoption as a means of representing complex structured\ninformation. Despite their growing popularity in industry, PGs remain\nrelatively underrepresented in semantic parsing research with a lack of\nresources for evaluation. To address this gap, we introduce ZOGRASCOPE, a\nbenchmark designed specifically for PGs and queries written in Cypher. Our\nbenchmark includes a diverse set of manually annotated queries of varying\ncomplexity and is organized into three partitions: iid, compositional and\nlength. We complement this paper with a set of experiments that test the\nperformance of different LLMs in a variety of learning settings."
                },
                "authors": [
                    {
                        "name": "Francesco Cazzaro"
                    },
                    {
                        "name": "Justin Kleindienst"
                    },
                    {
                        "name": "Sofia Marquez Gomez"
                    },
                    {
                        "name": "Ariadna Quattoni"
                    }
                ],
                "author_detail": {
                    "name": "Ariadna Quattoni"
                },
                "author": "Ariadna Quattoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23083v3",
                "updated": "2025-05-30T14:02:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    2,
                    7,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-29T13:49:11Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    49,
                    11,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adaptation For Remote Sensing Visual Grounding"
                },
                "summary": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training."
                },
                "authors": [
                    {
                        "name": "Hasan Moughnieh"
                    },
                    {
                        "name": "Mohamad Chalhoub"
                    },
                    {
                        "name": "Hasan Nasrallah"
                    },
                    {
                        "name": "Cristiano Nattero"
                    },
                    {
                        "name": "Paolo Campanella"
                    },
                    {
                        "name": "Giovanni Nico"
                    },
                    {
                        "name": "Ali J. Ghandour"
                    }
                ],
                "author_detail": {
                    "name": "Ali J. Ghandour"
                },
                "author": "Ali J. Ghandour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01014v2",
                "updated": "2025-05-30T14:00:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    0,
                    25,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-01T17:57:18Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    57,
                    18,
                    1,
                    91,
                    0
                ],
                "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction"
                },
                "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer."
                },
                "authors": [
                    {
                        "name": "Junhao Cheng"
                    },
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Jing Liao"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project released at: https://howe125.github.io/AnimeGamer.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12929v2",
                "updated": "2025-05-30T13:56:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    56,
                    47,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-18T15:11:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    11,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options"
                },
                "summary": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). Flow-of-Options\nenables LLMs to systematically explore a diverse range of possibilities in\ntheir reasoning, as demonstrated by an FoO-based agentic framework developed\nfor autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in\nLLM solutions through compressed and interpretable task representations,\nresulting in improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art\nbaselines. With an overall operation cost under $1 per task, our framework is\nwell-suited for cost-sensitive applications. Going beyond tabular\nclassification and regression, we show the broader applicability of our\nFoO-based agentic system to tasks such as reinforcement learning and image\ngeneration. Our code is open-sourced at:\nhttps://github.com/flagshippioneering/Flow-of-Options.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). Flow-of-Options\nenables LLMs to systematically explore a diverse range of possibilities in\ntheir reasoning, as demonstrated by an FoO-based agentic framework developed\nfor autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in\nLLM solutions through compressed and interpretable task representations,\nresulting in improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art\nbaselines. With an overall operation cost under $1 per task, our framework is\nwell-suited for cost-sensitive applications. Going beyond tabular\nclassification and regression, we show the broader applicability of our\nFoO-based agentic system to tasks such as reinforcement learning and image\ngeneration. Our code is open-sourced at:\nhttps://github.com/flagshippioneering/Flow-of-Options."
                },
                "authors": [
                    {
                        "name": "Lakshmi Nair"
                    },
                    {
                        "name": "Ian Trase"
                    },
                    {
                        "name": "Mark Kim"
                    }
                ],
                "author_detail": {
                    "name": "Mark Kim"
                },
                "author": "Mark Kim",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01388v3",
                "updated": "2025-05-30T13:55:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    55,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2024-06-03T14:51:24Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    14,
                    51,
                    24,
                    0,
                    155,
                    0
                ],
                "title": "AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image\n  Generation"
                },
                "summary": "As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity."
                },
                "authors": [
                    {
                        "name": "Junhao Cheng"
                    },
                    {
                        "name": "Xi Lu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Khun Loun Zai"
                    },
                    {
                        "name": "Baiqiao Yin"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Yiqiang Yan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "Multi-turn interactive image generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18919v2",
                "updated": "2025-05-30T13:52:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    52,
                    39,
                    4,
                    150,
                    0
                ],
                "published": "2024-04-29T17:58:14Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    17,
                    58,
                    14,
                    0,
                    120,
                    0
                ],
                "title": "TheaterGen: Character Management with LLM for Consistent Multi-turn\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheaterGen: Character Management with LLM for Consistent Multi-turn\n  Image Generation"
                },
                "summary": "Recent advances in diffusion models can generate high-quality and stunning\nimages from text. However, multi-turn image generation, which is of high demand\nin real-world scenarios, still faces challenges in maintaining semantic\nconsistency between images and texts, as well as contextual consistency of the\nsame subject across multiple interactive turns. To address this issue, we\nintroduce TheaterGen, a training-free framework that integrates large language\nmodels (LLMs) and text-to-image (T2I) models to provide the capability of\nmulti-turn image generation. Within this framework, LLMs, acting as a\n\"Screenwriter\", engage in multi-turn interaction, generating and managing a\nstandardized prompt book that encompasses prompts and layout designs for each\ncharacter in the target image. Based on these, Theatergen generate a list of\ncharacter images and extract guidance information, akin to the \"Rehearsal\".\nSubsequently, through incorporating the prompt book and guidance information\ninto the reverse denoising process of T2I diffusion models, Theatergen generate\nthe final image, as conducting the \"Final Performance\". With the effective\nmanagement of prompt books and character images, TheaterGen significantly\nimproves semantic and contextual consistency in synthesized images.\nFurthermore, we introduce a dedicated benchmark, CMIGBench (Consistent\nMulti-turn Image Generation Benchmark) with 8000 multi-turn instructions.\nDifferent from previous multi-turn benchmarks, CMIGBench does not define\ncharacters in advance. Both the tasks of story generation and multi-turn\nediting are included on CMIGBench for comprehensive evaluation. Extensive\nexperimental results show that TheaterGen outperforms state-of-the-art methods\nsignificantly. It raises the performance bar of the cutting-edge Mini DALLE 3\nmodel by 21% in average character-character similarity and 19% in average\ntext-image similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models can generate high-quality and stunning\nimages from text. However, multi-turn image generation, which is of high demand\nin real-world scenarios, still faces challenges in maintaining semantic\nconsistency between images and texts, as well as contextual consistency of the\nsame subject across multiple interactive turns. To address this issue, we\nintroduce TheaterGen, a training-free framework that integrates large language\nmodels (LLMs) and text-to-image (T2I) models to provide the capability of\nmulti-turn image generation. Within this framework, LLMs, acting as a\n\"Screenwriter\", engage in multi-turn interaction, generating and managing a\nstandardized prompt book that encompasses prompts and layout designs for each\ncharacter in the target image. Based on these, Theatergen generate a list of\ncharacter images and extract guidance information, akin to the \"Rehearsal\".\nSubsequently, through incorporating the prompt book and guidance information\ninto the reverse denoising process of T2I diffusion models, Theatergen generate\nthe final image, as conducting the \"Final Performance\". With the effective\nmanagement of prompt books and character images, TheaterGen significantly\nimproves semantic and contextual consistency in synthesized images.\nFurthermore, we introduce a dedicated benchmark, CMIGBench (Consistent\nMulti-turn Image Generation Benchmark) with 8000 multi-turn instructions.\nDifferent from previous multi-turn benchmarks, CMIGBench does not define\ncharacters in advance. Both the tasks of story generation and multi-turn\nediting are included on CMIGBench for comprehensive evaluation. Extensive\nexperimental results show that TheaterGen outperforms state-of-the-art methods\nsignificantly. It raises the performance bar of the cutting-edge Mini DALLE 3\nmodel by 21% in average character-character similarity and 19% in average\ntext-image similarity."
                },
                "authors": [
                    {
                        "name": "Junhao Cheng"
                    },
                    {
                        "name": "Baiqiao Yin"
                    },
                    {
                        "name": "Kaixin Cai"
                    },
                    {
                        "name": "Minbin Huang"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Yuxin He"
                    },
                    {
                        "name": "Xi Lu"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Yiqiang Yan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01372v2",
                "updated": "2025-05-30T13:48:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    48,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-03T10:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    10,
                    10,
                    30,
                    0,
                    62,
                    0
                ],
                "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiLTra-Bench: The Swiss Legal Translation Benchmark"
                },
                "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments."
                },
                "authors": [
                    {
                        "name": "Joel Niklaus"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Luka Nenadic"
                    },
                    {
                        "name": "Sina Ahmadi"
                    },
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Cyrill A. H. Chevalley"
                    },
                    {
                        "name": "Claude Humbel"
                    },
                    {
                        "name": "Christophe Gösken"
                    },
                    {
                        "name": "Lorenzo Tanzi"
                    },
                    {
                        "name": "Thomas Lüthi"
                    },
                    {
                        "name": "Stefan Palombo"
                    },
                    {
                        "name": "Spencer Poff"
                    },
                    {
                        "name": "Boling Yang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Matthew Guillod"
                    },
                    {
                        "name": "Robin Mamié"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Julio Pereyra"
                    },
                    {
                        "name": "Niko Grupen"
                    }
                ],
                "author_detail": {
                    "name": "Niko Grupen"
                },
                "author": "Niko Grupen",
                "arxiv_comment": "Accepted at ACL main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24597v1",
                "updated": "2025-05-30T13:45:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    45,
                    19,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:45:19Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    45,
                    19,
                    4,
                    150,
                    0
                ],
                "title": "Mixture-of-Experts for Personalized and Semantic-Aware Next Location\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts for Personalized and Semantic-Aware Next Location\n  Prediction"
                },
                "summary": "Next location prediction plays a critical role in understanding human\nmobility patterns. However, existing approaches face two core limitations: (1)\nthey fall short in capturing the complex, multi-functional semantics of\nreal-world locations; and (2) they lack the capacity to model heterogeneous\nbehavioral dynamics across diverse user groups. To tackle these challenges, we\nintroduce NextLocMoE, a novel framework built upon large language models (LLMs)\nand structured around a dual-level Mixture-of-Experts (MoE) design. Our\narchitecture comprises two specialized modules: a Location Semantics MoE that\noperates at the embedding level to encode rich functional semantics of\nlocations, and a Personalized MoE embedded within the Transformer backbone to\ndynamically adapt to individual user mobility patterns. In addition, we\nincorporate a history-aware routing mechanism that leverages long-term\ntrajectory data to enhance expert selection and ensure prediction stability.\nEmpirical evaluations across several real-world urban datasets show that\nNextLocMoE achieves superior performance in terms of predictive accuracy,\ncross-domain generalization, and interpretability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next location prediction plays a critical role in understanding human\nmobility patterns. However, existing approaches face two core limitations: (1)\nthey fall short in capturing the complex, multi-functional semantics of\nreal-world locations; and (2) they lack the capacity to model heterogeneous\nbehavioral dynamics across diverse user groups. To tackle these challenges, we\nintroduce NextLocMoE, a novel framework built upon large language models (LLMs)\nand structured around a dual-level Mixture-of-Experts (MoE) design. Our\narchitecture comprises two specialized modules: a Location Semantics MoE that\noperates at the embedding level to encode rich functional semantics of\nlocations, and a Personalized MoE embedded within the Transformer backbone to\ndynamically adapt to individual user mobility patterns. In addition, we\nincorporate a history-aware routing mechanism that leverages long-term\ntrajectory data to enhance expert selection and ensure prediction stability.\nEmpirical evaluations across several real-world urban datasets show that\nNextLocMoE achieves superior performance in terms of predictive accuracy,\ncross-domain generalization, and interpretability"
                },
                "authors": [
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Ning Cao"
                    },
                    {
                        "name": "Yile Chen"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Gao Cong"
                    }
                ],
                "author_detail": {
                    "name": "Gao Cong"
                },
                "author": "Gao Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13682v3",
                "updated": "2025-05-30T13:35:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    35,
                    50,
                    4,
                    150,
                    0
                ],
                "published": "2024-12-18T10:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    10,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese\n  Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese\n  Travel Planning"
                },
                "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the \\emph{Language Agents} for real-world\ndevelopment. Among these, travel planning represents a prominent domain,\ncombining complex multi-objective planning challenges with practical deployment\ndemands. However, existing benchmarks often oversimplify real-world\nrequirements by focusing on synthetic queries and limited constraints. We\naddress the gap of evaluating language agents in multi-day, multi-POI travel\nplanning scenarios with diverse and open human needs. Specifically, we\nintroduce \\emph{ChinaTravel}, the first open-ended benchmark grounded in\nauthentic Chinese travel requirements collected from 1,154 human participants.\nWe design a compositionally generalizable domain-specific language (DSL) for\nscalable evaluation, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a 37.0\\% constraint satisfaction rate on\nhuman queries, a 10\\times improvement over purely neural models. These findings\nhighlight ChinaTravel as a pivotal milestone for advancing language agents in\ncomplex, real-world planning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the \\emph{Language Agents} for real-world\ndevelopment. Among these, travel planning represents a prominent domain,\ncombining complex multi-objective planning challenges with practical deployment\ndemands. However, existing benchmarks often oversimplify real-world\nrequirements by focusing on synthetic queries and limited constraints. We\naddress the gap of evaluating language agents in multi-day, multi-POI travel\nplanning scenarios with diverse and open human needs. Specifically, we\nintroduce \\emph{ChinaTravel}, the first open-ended benchmark grounded in\nauthentic Chinese travel requirements collected from 1,154 human participants.\nWe design a compositionally generalizable domain-specific language (DSL) for\nscalable evaluation, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a 37.0\\% constraint satisfaction rate on\nhuman queries, a 10\\times improvement over purely neural models. These findings\nhighlight ChinaTravel as a pivotal milestone for advancing language agents in\ncomplex, real-world planning scenarios."
                },
                "authors": [
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Baizhi Chen"
                    },
                    {
                        "name": "Si-Yu Han"
                    },
                    {
                        "name": "Wen-Da Wei"
                    },
                    {
                        "name": "Guohao Cai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-feng Li"
                },
                "author": "Yu-feng Li",
                "arxiv_comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v1",
                "updated": "2025-05-30T13:32:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24583v1",
                "updated": "2025-05-30T13:30:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    30,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:30:37Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    30,
                    37,
                    4,
                    150,
                    0
                ],
                "title": "Cognitive-Radio Functionality: A Novel Configuration for STAR-RIS\n  assisted RSMA Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive-Radio Functionality: A Novel Configuration for STAR-RIS\n  assisted RSMA Networks"
                },
                "summary": "Cognitive radio rate-splitting multiple access (CR-RSMA) has emerged as a\npromising multiple access framework that can efficiently manage interference\nand adapt dynamically to heterogeneous quality-of-service (QoS) requirements.\nTo effectively support such demanding access schemes, programmable wireless\nenvironments have attracted considerable attention, especially through\nsimultaneously transmitting and reflecting reconfigurable intelligent surfaces\n(STAR-RISs), which can enable full-space control of signal propagation in\nasymmetric user deployments. In this paper, we propose the cognitive radio (CR)\nfunctionality for STAR-RIS-assisted CR-RSMA systems, leveraging the unique\ncapability of the STAR-RIS to combine element and power splitting for adaptive\ncontrol of transmission and reflection in CR scenarios. Specifically, the\nproposed CR functionality partitions the STAR-RIS into two regions\nindependently controlling the transmission and reflection of signals,\nsimultaneously ensuring the required QoS for the primary user and enhancing the\nperformance of the secondary user. To accurately characterize the system\nperformance, we derive analytical expressions for the ergodic rate of the\nsecondary user and the outage rate of the primary user under Nakagami-m fading.\nFinally, simulation results show that the proposed approach effectively manages\ninterference, guarantees the QoS of the primary user, and significantly\nimproves the throughput of the secondary user, highlighting STAR-RIS as an\nefficient solution for CR-RSMA-based services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive radio rate-splitting multiple access (CR-RSMA) has emerged as a\npromising multiple access framework that can efficiently manage interference\nand adapt dynamically to heterogeneous quality-of-service (QoS) requirements.\nTo effectively support such demanding access schemes, programmable wireless\nenvironments have attracted considerable attention, especially through\nsimultaneously transmitting and reflecting reconfigurable intelligent surfaces\n(STAR-RISs), which can enable full-space control of signal propagation in\nasymmetric user deployments. In this paper, we propose the cognitive radio (CR)\nfunctionality for STAR-RIS-assisted CR-RSMA systems, leveraging the unique\ncapability of the STAR-RIS to combine element and power splitting for adaptive\ncontrol of transmission and reflection in CR scenarios. Specifically, the\nproposed CR functionality partitions the STAR-RIS into two regions\nindependently controlling the transmission and reflection of signals,\nsimultaneously ensuring the required QoS for the primary user and enhancing the\nperformance of the secondary user. To accurately characterize the system\nperformance, we derive analytical expressions for the ergodic rate of the\nsecondary user and the outage rate of the primary user under Nakagami-m fading.\nFinally, simulation results show that the proposed approach effectively manages\ninterference, guarantees the QoS of the primary user, and significantly\nimproves the throughput of the secondary user, highlighting STAR-RIS as an\nefficient solution for CR-RSMA-based services."
                },
                "authors": [
                    {
                        "name": "Saeed Ibrahim"
                    },
                    {
                        "name": "Yue Xiao"
                    },
                    {
                        "name": "Dimitrios Tyrovolas"
                    },
                    {
                        "name": "Sotiris A. Tegos"
                    },
                    {
                        "name": "Panagiotis D. Diamantoulakis"
                    },
                    {
                        "name": "Zheng Ma"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    },
                    {
                        "name": "Pinghzi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Pinghzi Fan"
                },
                "author": "Pinghzi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24575v1",
                "updated": "2025-05-30T13:26:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    26,
                    23,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:26:23Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    26,
                    23,
                    4,
                    150,
                    0
                ],
                "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization"
                },
                "summary": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains."
                },
                "authors": [
                    {
                        "name": "Hyuntak Kim"
                    },
                    {
                        "name": "Byung-Hak Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hak Kim"
                },
                "author": "Byung-Hak Kim",
                "arxiv_comment": "Accepted to the main track of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24574v1",
                "updated": "2025-05-30T13:26:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    26,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T13:26:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    26,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "Bias-field-free operation of nitrogen-vacancy ensembles in diamond for\n  accurate vector magnetometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias-field-free operation of nitrogen-vacancy ensembles in diamond for\n  accurate vector magnetometry"
                },
                "summary": "Accurate measurement of vector magnetic fields is critical for applications\nincluding navigation, geoscience, and space exploration. Nitrogen-vacancy (NV)\ncenter spin ensembles offer a promising solution for high-sensitivity vector\nmagnetometry, as their different orientations in the diamond lattice measure\ndifferent components of the magnetic field. However, the bias magnetic field\ntypically used to separate signals from each NV orientation introduces\ninaccuracy from drifts in permanent magnets or coils. Here, we present a novel\nbias-field-free approach that labels the NV orientations via the direction of\nthe microwave (MW) field in a variable-pulse-duration Ramsey sequence used to\nmanipulate the spin ensemble. Numerical simulations demonstrate the possibility\nto isolate each orientation's signal with sub-nT accuracy even without precise\nMW field calibration, at only a moderate cost to sensitivity. We also provide\nproof-of-principle experimental validation, observing relevant features that\nevolve as expected with applied magnetic field. Looking forward, by removing a\nkey source of drift, the proposed protocol lays the groundwork for future\ndeployment of NV magnetometers in high-accuracy or long-duration missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate measurement of vector magnetic fields is critical for applications\nincluding navigation, geoscience, and space exploration. Nitrogen-vacancy (NV)\ncenter spin ensembles offer a promising solution for high-sensitivity vector\nmagnetometry, as their different orientations in the diamond lattice measure\ndifferent components of the magnetic field. However, the bias magnetic field\ntypically used to separate signals from each NV orientation introduces\ninaccuracy from drifts in permanent magnets or coils. Here, we present a novel\nbias-field-free approach that labels the NV orientations via the direction of\nthe microwave (MW) field in a variable-pulse-duration Ramsey sequence used to\nmanipulate the spin ensemble. Numerical simulations demonstrate the possibility\nto isolate each orientation's signal with sub-nT accuracy even without precise\nMW field calibration, at only a moderate cost to sensitivity. We also provide\nproof-of-principle experimental validation, observing relevant features that\nevolve as expected with applied magnetic field. Looking forward, by removing a\nkey source of drift, the proposed protocol lays the groundwork for future\ndeployment of NV magnetometers in high-accuracy or long-duration missions."
                },
                "authors": [
                    {
                        "name": "Lilian Childress"
                    },
                    {
                        "name": "Vincent Halde"
                    },
                    {
                        "name": "Kayla Johnson"
                    },
                    {
                        "name": "Andrew Lowther"
                    },
                    {
                        "name": "David Roy-Guay"
                    },
                    {
                        "name": "Romain Ruhlmann"
                    },
                    {
                        "name": "Adrian Solyom"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Solyom"
                },
                "author": "Adrian Solyom",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22353v3",
                "updated": "2025-05-30T13:25:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    25,
                    15,
                    4,
                    150,
                    0
                ],
                "published": "2025-03-28T11:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    49,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firm or Fickle? Evaluating Large Language Models Consistency in\n  Sequential Interactions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11277v2",
                "updated": "2025-05-30T13:16:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    16,
                    10,
                    4,
                    150,
                    0
                ],
                "published": "2025-04-15T15:16:45Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    16,
                    45,
                    1,
                    105,
                    0
                ],
                "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning\n  Method for LLMs"
                },
                "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering~(QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering~(QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information."
                },
                "authors": [
                    {
                        "name": "Guocong Li"
                    },
                    {
                        "name": "Weize Liu"
                    },
                    {
                        "name": "Yihang Wu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Shuaihan Huang"
                    },
                    {
                        "name": "Hongxia Xu"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "arxiv_comment": "Accepted to ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21075v3",
                "updated": "2025-05-30T13:08:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    8,
                    27,
                    4,
                    150,
                    0
                ],
                "published": "2024-05-31T17:59:47Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    59,
                    47,
                    4,
                    152,
                    0
                ],
                "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of\n  Multi-modal LLMs in Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of\n  Multi-modal LLMs in Video Analysis"
                },
                "summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io"
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Yongdong Luo"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Shaohui Lin"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Project Page: https://video-mme.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.21075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]